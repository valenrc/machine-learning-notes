{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivadas parciales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro ejemplo en gradient_simple.ipynb es un ejemplo de modelo de regresión lineal en donde minimizamos una funcion de pérdida con un solo parámetro (w), donde fijamos b como una constante con valor b=0. \n",
    "Si agregamos el parámetro b a L, ahora nuestro problema se convierte en el de minimizar una función de dos variables (w,b).\n",
    "\n",
    "### $$L(w,b) = \\frac{1}{m} \\sum_{i=1}^{m} (wx_{i} - y_i)^2$$ \n",
    "$$\\text{Funcion de pérdida con b=0}$$\n",
    "\n",
    "### $$L(w,b) = \\frac{1}{m} \\sum_{i=1}^{m} ([wx_{i} + b] - y_i)^2$$ \n",
    "$$\\text{Función de pérdida con b como parametro}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definamos el gradiente para funciones de dos parámetros:\n",
    "\n",
    "Sea $f : \\R ^2 \\rightarrow \\R$ \n",
    "\n",
    "Entonces el gradiente \n",
    "\n",
    "### $$\\nabla f = [f_x, f_y]$$\n",
    "\n",
    "Es el vector de derivadas parciales $f_x$ y $f_y$ de f. \n",
    "Y apunta en la dirección de la **MAYOR** tasa de crecimiento de f en un punto dado (notar que nosotros queremos ir hacia la dirección de la **MENOR** tasa de crecimiento, por lo que debemos ir en la dirección opuesta al gradiente)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora calculemos el vector gradiente para L.\n",
    "### $$L_w = \\frac{\\partial L(w,b)}{\\partial w} = \\frac{2}{m} \\sum_{i=1}^{m} [x_i(wx_i + b - y_i)]$$ \n",
    "\n",
    "### $$L_b = \\frac{\\partial L(w,b)}{\\partial b} = \\frac{2}{m} \\sum_{i=1}^{m} [(wx_i + b) - y_i] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Igual que en el caso de una variable, podemos actualizar w y b en la dirección opuesta al gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definamos en python la funcion que calcula el gradiente para esta funcion de pérdida\n",
    "\n",
    "def predict(X,w,b):\n",
    "\treturn X*w + b\n",
    "\n",
    "def loss(X, Y, w, b):\n",
    "\terror = (predict(X, w, b) - Y)\n",
    "\tsquared_error = error ** 2\n",
    "\treturn np.average(squared_error)\n",
    "\n",
    "def gradient(X, Y, w, b):\n",
    "\tL_w = np.average((predict(X,w,b) - Y) * X) * 2\n",
    "\tL_b = np.average(predict(X,w,b) - Y) * 2\n",
    "\n",
    "\treturn (L_w, L_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y ahora la funcion train() que se va a encargar de entrenar el modelo ajustando los parametros w,b segun el gradiente\n",
    "\n",
    "def train(X, Y, iterations, learning_rate):\n",
    "\tw = 0\n",
    "\tb = 0\n",
    "\n",
    "\tfor i in range(iterations):\n",
    "\t\tl_w, l_b = gradient(X, Y, w, b)\n",
    "\n",
    "\t\tprint(f'Iteracion: {i} Gradiente: [{l_w},{l_b}] Loss: {loss(X, Y, w, b)}')\n",
    "\n",
    "\t\tw -= l_w * learning_rate\n",
    "\t\tb -= l_b * learning_rate\n",
    "\n",
    "\treturn (w,b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteracion: 0 Gradiente: [-806.8,-53.733333333333334] Loss: 812.8666666666667\n",
      "Iteracion: 1 Gradiente: [-452.3830755555556,-33.186933333333336] Loss: 302.57695615644445\n",
      "Iteracion: 2 Gradiente: [-253.579506048,-21.66018821925925] Loss: 141.98409032672075\n",
      "Iteracion: 3 Gradiente: [-142.06438943317406,-15.192853689604744] Loss: 91.42137662111031\n",
      "Iteracion: 4 Gradiente: [-79.51212699041373,-11.563503449918459] Loss: 75.47905765224317\n",
      "Iteracion: 5 Gradiente: [-44.42467813201069,-9.526069225928135] Loss: 70.42988348520574\n",
      "Iteracion: 6 Gradiente: [-24.743111894385887,-8.381591908132005] Loss: 68.80821027087441\n",
      "Iteracion: 7 Gradiente: [-13.70319246772993,-7.738003222991297] Loss: 68.26501573143898\n",
      "Iteracion: 8 Gradiente: [-7.510646028868871,-7.375379674029491] Loss: 68.06119337270614\n",
      "Iteracion: 9 Gradiente: [-4.037144374893697,-7.170359215283414] Loss: 67.96418751616304\n",
      "Iteracion: 10 Gradiente: [-2.0888408963197467,-7.053744172688877] Loss: 67.90082580727031\n",
      "Iteracion: 11 Gradiente: [-0.9960692677154038,-6.986719381636732] Loss: 67.84808494502518\n",
      "Iteracion: 12 Gradiente: [-0.38319246516170724,-6.947512188091336] Loss: 67.79872074202484\n",
      "Iteracion: 13 Gradiente: [-0.03950380030866683,-6.923909621264391] Loss: 67.75045381896952\n",
      "Iteracion: 14 Gradiente: [0.15318877311177678,-6.909061039080711] Loss: 67.70256694514084\n",
      "Iteracion: 15 Gradiente: [0.26118291232142166,-6.899123699254715] Loss: 67.65483441099774\n",
      "Iteracion: 16 Gradiente: [0.3216670702706622,-6.891942085635017] Loss: 67.60718516137254\n",
      "Iteracion: 17 Gradiente: [0.3555014264896788,-6.886307100577267] Loss: 67.55959680225291\n",
      "Iteracion: 18 Gradiente: [0.37438711547238956,-6.881540522513854] Loss: 67.51206225062072\n",
      "Iteracion: 19 Gradiente: [0.384887673645351,-6.877261915060793] Loss: 67.46457924101391\n",
      "Iteracion: 20 Gradiente: [0.3906847961729393,-6.873257878963025] Loss: 67.41714702382623\n",
      "Iteracion: 21 Gradiente: [0.3938436623014146,-6.869408711374811] Loss: 67.36976532644579\n",
      "Iteracion: 22 Gradiente: [0.39552269636648607,-6.865647266730366] Loss: 67.32243402638338\n",
      "Iteracion: 23 Gradiente: [0.396371695193614,-6.861935880504858] Loss: 67.27515304842406\n",
      "Iteracion: 24 Gradiente: [0.39675515034972336,-6.8582534250220855] Loss: 67.22792233226649\n",
      "Iteracion: 25 Gradiente: [0.3968775166572916,-6.854588048647561] Loss: 67.18074182234062\n",
      "Iteracion: 26 Gradiente: [0.39685347926706477,-6.850933102972257] Loss: 67.13361146460416\n",
      "Iteracion: 27 Gradiente: [0.3967473686817821,-6.847284858241079] Loss: 67.08653120553426\n",
      "Iteracion: 28 Gradiente: [0.39659526988875626,-6.843641221864533] Loss: 67.03950099181027\n",
      "Iteracion: 29 Gradiente: [0.39641742407260666,-6.840001019591322] Loss: 66.99252077021416\n",
      "Iteracion: 30 Gradiente: [0.3962251851280508,-6.8363635922953145] Loss: 66.94559048759866\n",
      "Iteracion: 31 Gradiente: [0.39602492178759074,-6.832728569800631] Loss: 66.8987100908777\n",
      "Iteracion: 32 Gradiente: [0.39582020644824356,-6.8290957440129825] Loss: 66.85187952702289\n",
      "Iteracion: 33 Gradiente: [0.39561304295489397,-6.825464997754977] Loss: 66.80509874306283\n",
      "Iteracion: 34 Gradiente: [0.39540455530088725,-6.821836264847659] Loss: 66.75836768608241\n",
      "Iteracion: 35 Gradiente: [0.3951953739440138,-6.81820950771892] Loss: 66.71168630322296\n",
      "Iteracion: 36 Gradiente: [0.39498585250162627,-6.8145847048434005] Loss: 66.66505454168188\n",
      "Iteracion: 37 Gradiente: [0.39477618930292807,-6.810961843697089] Loss: 66.61847234871281\n",
      "Iteracion: 38 Gradiente: [0.3945664955710143,-6.807340916805365] Loss: 66.57193967162537\n",
      "Iteracion: 39 Gradiente: [0.394356833668112,-6.803721919526224] Loss: 66.52545645778521\n",
      "Iteracion: 40 Gradiente: [0.3941472385496278,-6.800104848806765] Loss: 66.47902265461403\n",
      "Iteracion: 41 Gradiente: [0.39393772979678376,-6.796489702485739] Loss: 66.43263820958927\n",
      "Iteracion: 42 Gradiente: [0.39372831836728844,-6.79287647890229] Loss: 66.38630307024428\n",
      "Iteracion: 43 Gradiente: [0.3935190103820105,-6.789265176676453] Loss: 66.34001718416819\n",
      "Iteracion: 44 Gradiente: [0.3933098092479497,-6.7856557945861145] Loss: 66.29378049900582\n",
      "Iteracion: 45 Gradiente: [0.3931007168505881,-6.78204833149789] Loss: 66.24759296245762\n",
      "Iteracion: 46 Gradiente: [0.39289173422143764,-6.778442786328437] Loss: 66.20145452227966\n",
      "Iteracion: 47 Gradiente: [0.3926828619130082,-6.774839158022729] Loss: 66.15536512628357\n",
      "Iteracion: 48 Gradiente: [0.3924741002098718,-6.771237445541809] Loss: 66.10932472233641\n",
      "Iteracion: 49 Gradiente: [0.39226544924503914,-6.767637647856046] Loss: 66.06333325836064\n",
      "Iteracion: 50 Gradiente: [0.39205690906780005,-6.764039763941205] Loss: 66.01739068233415\n",
      "Iteracion: 51 Gradiente: [0.39184847967959086,-6.760443792776371] Loss: 65.97149694229012\n",
      "Iteracion: 52 Gradiente: [0.39164016105535165,-6.75684973334271] Loss: 65.92565198631686\n",
      "Iteracion: 53 Gradiente: [0.3914319531556027,-6.753257584622756] Loss: 65.87985576255798\n",
      "Iteracion: 54 Gradiente: [0.39122385593187187,-6.749667345600117] Loss: 65.83410821921217\n",
      "Iteracion: 55 Gradiente: [0.39101586933128707,-6.746079015259189] Loss: 65.78840930453316\n",
      "Iteracion: 56 Gradiente: [0.3908079932985648,-6.742492592585061] Loss: 65.74275896682978\n",
      "Iteracion: 57 Gradiente: [0.3906002277764813,-6.73890807656346] Loss: 65.69715715446564\n",
      "Iteracion: 58 Gradiente: [0.39039257270781086,-6.735325466180669] Loss: 65.65160381585943\n",
      "Iteracion: 59 Gradiente: [0.3901850280341572,-6.7317447604235685] Loss: 65.6060988994845\n",
      "Iteracion: 60 Gradiente: [0.3899775936971238,-6.728165958279589] Loss: 65.56064235386908\n",
      "Iteracion: 61 Gradiente: [0.38977026963843714,-6.724589058736685] Loss: 65.5152341275961\n",
      "Iteracion: 62 Gradiente: [0.3895630557992822,-6.721014060783387] Loss: 65.46987416930307\n",
      "Iteracion: 63 Gradiente: [0.38935595212130114,-6.717440963408739] Loss: 65.4245624276822\n",
      "Iteracion: 64 Gradiente: [0.3891489585460704,-6.713869765602326] Loss: 65.37929885148017\n",
      "Iteracion: 65 Gradiente: [0.38894207501485645,-6.710300466354291] Loss: 65.33408338949818\n",
      "Iteracion: 66 Gradiente: [0.38873530146935215,-6.7067330646552925] Loss: 65.28891599059185\n",
      "Iteracion: 67 Gradiente: [0.3885286378509657,-6.703167559496537] Loss: 65.24379660367106\n",
      "Iteracion: 68 Gradiente: [0.3883220841012876,-6.699603949869768] Loss: 65.19872517770017\n",
      "Iteracion: 69 Gradiente: [0.3881156401619345,-6.696042234767261] Loss: 65.15370166169771\n",
      "Iteracion: 70 Gradiente: [0.38790930597456946,-6.692482413181826] Loss: 65.10872600473641\n",
      "Iteracion: 71 Gradiente: [0.3877030814806152,-6.688924484106823] Loss: 65.06379815594308\n",
      "Iteracion: 72 Gradiente: [0.38749696662215316,-6.685368446536113] Loss: 65.01891806449868\n",
      "Iteracion: 73 Gradiente: [0.38729096134051794,-6.681814299464137] Loss: 64.97408567963814\n",
      "Iteracion: 74 Gradiente: [0.3870850655776678,-6.678262041885838] Loss: 64.92930095065041\n",
      "Iteracion: 75 Gradiente: [0.3868792792753439,-6.674711672796698] Loss: 64.88456382687828\n",
      "Iteracion: 76 Gradiente: [0.3866736023752736,-6.671163191192749] Loss: 64.83987425771845\n",
      "Iteracion: 77 Gradiente: [0.38646803481946534,-6.667616596070534] Loss: 64.79523219262136\n",
      "Iteracion: 78 Gradiente: [0.3862625765495295,-6.664071886427155] Loss: 64.75063758109115\n",
      "Iteracion: 79 Gradiente: [0.3860572275075602,-6.660529061260223] Loss: 64.70609037268575\n",
      "Iteracion: 80 Gradiente: [0.38585198763550227,-6.656988119567896] Loss: 64.66159051701665\n",
      "Iteracion: 81 Gradiente: [0.38564685687531153,-6.653449060348856] Loss: 64.61713796374887\n",
      "Iteracion: 82 Gradiente: [0.3854418351688201,-6.649911882602333] Loss: 64.57273266260094\n",
      "Iteracion: 83 Gradiente: [0.38523692245822566,-6.646376585328071] Loss: 64.52837456334493\n",
      "Iteracion: 84 Gradiente: [0.3850321186855586,-6.642843167526353] Loss: 64.48406361580616\n",
      "Iteracion: 85 Gradiente: [0.384827423792764,-6.639311628198001] Loss: 64.4397997698634\n",
      "Iteracion: 86 Gradiente: [0.38462283772206735,-6.635781966344355] Loss: 64.39558297544862\n",
      "Iteracion: 87 Gradiente: [0.3844183604156702,-6.632254180967291] Loss: 64.35141318254709\n",
      "Iteracion: 88 Gradiente: [0.3842139918156003,-6.628728271069218] Loss: 64.30729034119716\n",
      "Iteracion: 89 Gradiente: [0.38400973186419807,-6.625204235653074] Loss: 64.26321440149033\n",
      "Iteracion: 90 Gradiente: [0.38380558050363617,-6.621682073722327] Loss: 64.21918531357117\n",
      "Iteracion: 91 Gradiente: [0.3836015376762016,-6.618161784280975] Loss: 64.17520302763722\n",
      "Iteracion: 92 Gradiente: [0.38339760332426304,-6.61464336633354] Loss: 64.13126749393896\n",
      "Iteracion: 93 Gradiente: [0.38319377738997623,-6.611126818885093] Loss: 64.08737866277976\n",
      "Iteracion: 94 Gradiente: [0.3829900598159687,-6.607612140941196] Loss: 64.04353648451581\n",
      "Iteracion: 95 Gradiente: [0.382786450544278,-6.604099331507988] Loss: 63.99974090955607\n",
      "Iteracion: 96 Gradiente: [0.3825829495176848,-6.60058838959209] Loss: 63.95599188836225\n",
      "Iteracion: 97 Gradiente: [0.3823795566783493,-6.59707931420069] Loss: 63.91228937144861\n",
      "Iteracion: 98 Gradiente: [0.38217627196900705,-6.593572104341473] Loss: 63.86863330938217\n",
      "Iteracion: 99 Gradiente: [0.38197309533199664,-6.5900667590226725] Loss: 63.825023652782335\n",
      "Iteracion: 100 Gradiente: [0.3817700267099506,-6.586563277253039] Loss: 63.78146035232115\n",
      "Iteracion: 101 Gradiente: [0.38156706604541457,-6.583061658041854] Loss: 63.737943358722994\n",
      "Iteracion: 102 Gradiente: [0.38136421328099174,-6.57956190039892] Loss: 63.694472622764685\n",
      "Iteracion: 103 Gradiente: [0.3811614683593774,-6.576064003334571] Loss: 63.65104809527529\n",
      "Iteracion: 104 Gradiente: [0.38095883122315544,-6.572567965859673] Loss: 63.607669727136205\n",
      "Iteracion: 105 Gradiente: [0.3807563018150219,-6.569073786985606] Loss: 63.56433746928102\n",
      "Iteracion: 106 Gradiente: [0.380553880077737,-6.5655814657242795] Loss: 63.52105127269548\n",
      "Iteracion: 107 Gradiente: [0.38035156595409775,-6.562091001088133] Loss: 63.47781108841745\n",
      "Iteracion: 108 Gradiente: [0.3801493593868429,-6.558602392090127] Loss: 63.43461686753684\n",
      "Iteracion: 109 Gradiente: [0.3799472603187477,-6.555115637743749] Loss: 63.39146856119548\n",
      "Iteracion: 110 Gradiente: [0.379745268692705,-6.551630737063006] Loss: 63.3483661205872\n",
      "Iteracion: 111 Gradiente: [0.37954338445171437,-6.548147689062427] Loss: 63.30530949695774\n",
      "Iteracion: 112 Gradiente: [0.3793416075385939,-6.544666492757078] Loss: 63.26229864160463\n",
      "Iteracion: 113 Gradiente: [0.3791399378961292,-6.541187147162545] Loss: 63.21933350587708\n",
      "Iteracion: 114 Gradiente: [0.3789383754675877,-6.5377096512949215] Loss: 63.17641404117619\n",
      "Iteracion: 115 Gradiente: [0.3787369201958844,-6.534234004170838] Loss: 63.133540198954606\n",
      "Iteracion: 116 Gradiente: [0.3785355720237845,-6.530760204807461] Loss: 63.09071193071658\n",
      "Iteracion: 117 Gradiente: [0.37833433089452956,-6.527288252222454] Loss: 63.047929188017946\n",
      "Iteracion: 118 Gradiente: [0.3781331967514073,-6.523818145434003] Loss: 63.00519192246601\n",
      "Iteracion: 119 Gradiente: [0.37793216953738,-6.520349883460834] Loss: 62.96250008571958\n",
      "Iteracion: 120 Gradiente: [0.3777312491954698,-6.516883465322195] Loss: 62.91985362948874\n",
      "Iteracion: 121 Gradiente: [0.3775304356690446,-6.513418890037834] Loss: 62.877252505535004\n",
      "Iteracion: 122 Gradiente: [0.37732972890122674,-6.50995615662804] Loss: 62.83469666567109\n",
      "Iteracion: 123 Gradiente: [0.37712912883531874,-6.506495264113613] Loss: 62.792186061761\n",
      "Iteracion: 124 Gradiente: [0.3769286354145204,-6.503036211515882] Loss: 62.7497206457199\n",
      "Iteracion: 125 Gradiente: [0.3767282485822416,-6.499578997856684] Loss: 62.70730036951399\n",
      "Iteracion: 126 Gradiente: [0.3765279682816664,-6.496123622158389] Loss: 62.66492518516061\n",
      "Iteracion: 127 Gradiente: [0.3763277944562641,-6.492670083443872] Loss: 62.62259504472807\n",
      "Iteracion: 128 Gradiente: [0.3761277270495019,-6.489218380736543] Loss: 62.58030990033567\n",
      "Iteracion: 129 Gradiente: [0.3759277660045953,-6.485768513060324] Loss: 62.53806970415353\n",
      "Iteracion: 130 Gradiente: [0.3757279112651541,-6.482320479439655] Loss: 62.495874408402685\n",
      "Iteracion: 131 Gradiente: [0.3755281627747422,-6.478874278899488] Loss: 62.453723965354946\n",
      "Iteracion: 132 Gradiente: [0.3753285204766035,-6.4754299104653175] Loss: 62.411618327332796\n",
      "Iteracion: 133 Gradiente: [0.3751289843145126,-6.471987373163127] Loss: 62.36955744670947\n",
      "Iteracion: 134 Gradiente: [0.37492955423192076,-6.468546666019435] Loss: 62.32754127590881\n",
      "Iteracion: 135 Gradiente: [0.3747302301725256,-6.465107788061275] Loss: 62.28556976740522\n",
      "Iteracion: 136 Gradiente: [0.3745310120799521,-6.461670738316185] Loss: 62.24364287372362\n",
      "Iteracion: 137 Gradiente: [0.3743318998977898,-6.458235515812245] Loss: 62.20176054743942\n",
      "Iteracion: 138 Gradiente: [0.37413289356975193,-6.45480211957803] Loss: 62.15992274117839\n",
      "Iteracion: 139 Gradiente: [0.3739339930395985,-6.451370548642641] Loss: 62.11812940761671\n",
      "Iteracion: 140 Gradiente: [0.3737351982510546,-6.447940802035693] Loss: 62.07638049948085\n",
      "Iteracion: 141 Gradiente: [0.3735365091479726,-6.444512878787318] Loss: 62.03467596954756\n",
      "Iteracion: 142 Gradiente: [0.37333792567412777,-6.441086777928157] Loss: 61.99301577064366\n",
      "Iteracion: 143 Gradiente: [0.37313944777329106,-6.437662498489378] Loss: 61.951399855646294\n",
      "Iteracion: 144 Gradiente: [0.372941075389519,-6.434240039502652] Loss: 61.909828177482574\n",
      "Iteracion: 145 Gradiente: [0.3727428084664496,-6.430819400000184] Loss: 61.86830068912969\n",
      "Iteracion: 146 Gradiente: [0.3725446469481085,-6.427400579014669] Loss: 61.82681734361477\n",
      "Iteracion: 147 Gradiente: [0.3723465907787746,-6.423983575579319] Loss: 61.78537809401504\n",
      "Iteracion: 148 Gradiente: [0.3721486399019208,-6.4205683887278955] Loss: 61.74398289345732\n",
      "Iteracion: 149 Gradiente: [0.3719507942619752,-6.417155017494618] Loss: 61.702631695118534\n",
      "Iteracion: 150 Gradiente: [0.37175305380282564,-6.413743460914264] Loss: 61.66132445222521\n",
      "Iteracion: 151 Gradiente: [0.3715554184685189,-6.410333718022108] Loss: 61.62006111805362\n",
      "Iteracion: 152 Gradiente: [0.3713578882033142,-6.406925787853931] Loss: 61.57884164592977\n",
      "Iteracion: 153 Gradiente: [0.3711604629511304,-6.403519669446043] Loss: 61.537665989229225\n",
      "Iteracion: 154 Gradiente: [0.3709631426563372,-6.400115361835246] Loss: 61.49653410137709\n",
      "Iteracion: 155 Gradiente: [0.37076592726312135,-6.396712864058868] Loss: 61.45544593584804\n",
      "Iteracion: 156 Gradiente: [0.370568816715587,-6.393312175154748] Loss: 61.414401446166195\n",
      "Iteracion: 157 Gradiente: [0.3703718109581781,-6.38991329416123] Loss: 61.373400585904996\n",
      "Iteracion: 158 Gradiente: [0.37017490993492763,-6.386516220117182] Loss: 61.33244330868734\n",
      "Iteracion: 159 Gradiente: [0.369978113590336,-6.383120952061969] Loss: 61.29152956818535\n",
      "Iteracion: 160 Gradiente: [0.3697814218688,-6.379727489035466] Loss: 61.25065931812048\n",
      "Iteracion: 161 Gradiente: [0.36958483471450354,-6.376335830078077] Loss: 61.209832512263226\n",
      "Iteracion: 162 Gradiente: [0.3693883520720888,-6.372945974230686] Loss: 61.1690491044334\n",
      "Iteracion: 163 Gradiente: [0.3691919738858905,-6.369557920534716] Loss: 61.12830904849976\n",
      "Iteracion: 164 Gradiente: [0.3689957001002559,-6.366171668032094] Loss: 61.08761229838022\n",
      "Iteracion: 165 Gradiente: [0.3687995306598012,-6.362787215765238] Loss: 61.04695880804156\n",
      "Iteracion: 166 Gradiente: [0.3686034655091779,-6.359404562777087] Loss: 61.00634853149963\n",
      "Iteracion: 167 Gradiente: [0.3684075045926008,-6.356023708111106] Loss: 60.965781422818985\n",
      "Iteracion: 168 Gradiente: [0.3682116478550583,-6.352644650811228] Loss: 60.92525743611319\n",
      "Iteracion: 169 Gradiente: [0.3680158952408595,-6.349267389921934] Loss: 60.88477652554443\n",
      "Iteracion: 170 Gradiente: [0.367820246694843,-6.34589192448819] Loss: 60.84433864532377\n",
      "Iteracion: 171 Gradiente: [0.3676247021615287,-6.342518253555484] Loss: 60.80394374971078\n",
      "Iteracion: 172 Gradiente: [0.367429261585725,-6.339146376169797] Loss: 60.763591793013774\n",
      "Iteracion: 173 Gradiente: [0.3672339249121535,-6.335776291377629] Loss: 60.72328272958962\n",
      "Iteracion: 174 Gradiente: [0.36703869208547296,-6.332407998225981] Loss: 60.68301651384361\n",
      "Iteracion: 175 Gradiente: [0.36684356305061244,-6.32904149576236] Loss: 60.64279310022967\n",
      "Iteracion: 176 Gradiente: [0.3666485377522368,-6.32567678303479] Loss: 60.60261244324995\n",
      "Iteracion: 177 Gradiente: [0.3664536161354519,-6.322313859091774] Loss: 60.562474497455106\n",
      "Iteracion: 178 Gradiente: [0.3662587981449491,-6.318952722982353] Loss: 60.52237921744405\n",
      "Iteracion: 179 Gradiente: [0.3660640837255002,-6.315593373756064] Loss: 60.48232655786392\n",
      "Iteracion: 180 Gradiente: [0.365869472822384,-6.312235810462932] Loss: 60.44231647341017\n",
      "Iteracion: 181 Gradiente: [0.3656749653803838,-6.308880032153508] Loss: 60.4023489188263\n",
      "Iteracion: 182 Gradiente: [0.36548056134451484,-6.305526037878836] Loss: 60.36242384890402\n",
      "Iteracion: 183 Gradiente: [0.36528626065976416,-6.30217382669047] Loss: 60.322541218482954\n",
      "Iteracion: 184 Gradiente: [0.3650920632711916,-6.29882339764047] Loss: 60.28270098245087\n",
      "Iteracion: 185 Gradiente: [0.3648979691239925,-6.29547474978139] Loss: 60.2429030957434\n",
      "Iteracion: 186 Gradiente: [0.36470397816307476,-6.292127882166306] Loss: 60.20314751334412\n",
      "Iteracion: 187 Gradiente: [0.3645100903337787,-6.288782793848772] Loss: 60.163434190284455\n",
      "Iteracion: 188 Gradiente: [0.3643163055812835,-6.28543948388286] Loss: 60.12376308164365\n",
      "Iteracion: 189 Gradiente: [0.3641226238505761,-6.282097951323155] Loss: 60.08413414254862\n",
      "Iteracion: 190 Gradiente: [0.3639290450870187,-6.278758195224727] Loss: 60.0445473281741\n",
      "Iteracion: 191 Gradiente: [0.36373556923594824,-6.27542021464315] Loss: 60.00500259374232\n",
      "Iteracion: 192 Gradiente: [0.3635421962427429,-6.2720840086345] Loss: 59.9654998945233\n",
      "Iteracion: 193 Gradiente: [0.3633489260522585,-6.268749576255382] Loss: 59.9260391858344\n",
      "Iteracion: 194 Gradiente: [0.36315575861032506,-6.265416916562858] Loss: 59.88662042304064\n",
      "Iteracion: 195 Gradiente: [0.36296269386201485,-6.2620860286145295] Loss: 59.84724356155443\n",
      "Iteracion: 196 Gradiente: [0.3627697317529268,-6.25875691146847] Loss: 59.80790855683551\n",
      "Iteracion: 197 Gradiente: [0.36257687222840457,-6.255429564183273] Loss: 59.768615364391096\n",
      "Iteracion: 198 Gradiente: [0.36238411523390823,-6.2521039858180245] Loss: 59.7293639397756\n",
      "Iteracion: 199 Gradiente: [0.3621914607149108,-6.248780175432315] Loss: 59.69015423859069\n",
      "Iteracion: 200 Gradiente: [0.3619989086171207,-6.245458132086224] Loss: 59.65098621648528\n",
      "Iteracion: 201 Gradiente: [0.36180645888569624,-6.242137854840357] Loss: 59.61185982915534\n",
      "Iteracion: 202 Gradiente: [0.36161411146662015,-6.23881934275578] Loss: 59.57277503234404\n",
      "Iteracion: 203 Gradiente: [0.36142186630527484,-6.23550259489409] Loss: 59.53373178184151\n",
      "Iteracion: 204 Gradiente: [0.36122972334744075,-6.232187610317368] Loss: 59.494730033484934\n",
      "Iteracion: 205 Gradiente: [0.36103768253861734,-6.228874388088202] Loss: 59.455769743158406\n",
      "Iteracion: 206 Gradiente: [0.36084574382453666,-6.225562927269676] Loss: 59.41685086679289\n",
      "Iteracion: 207 Gradiente: [0.3606539071511506,-6.222253226925355] Loss: 59.377973360366276\n",
      "Iteracion: 208 Gradiente: [0.3604621724639036,-6.218945286119334] Loss: 59.33913717990318\n",
      "Iteracion: 209 Gradiente: [0.36027053970864625,-6.2156391039161845] Loss: 59.30034228147493\n",
      "Iteracion: 210 Gradiente: [0.3600790088314407,-6.212334679380968] Loss: 59.26158862119967\n",
      "Iteracion: 211 Gradiente: [0.3598875797777301,-6.20903201157927] Loss: 59.222876155242055\n",
      "Iteracion: 212 Gradiente: [0.35969625249377135,-6.205731099577141] Loss: 59.184204839813496\n",
      "Iteracion: 213 Gradiente: [0.35950502692502556,-6.2024319424411685] Loss: 59.14557463117175\n",
      "Iteracion: 214 Gradiente: [0.3593139030178714,-6.199134539238384] Loss: 59.10698548562123\n",
      "Iteracion: 215 Gradiente: [0.35912288071796894,-6.195838889036358] Loss: 59.06843735951273\n",
      "Iteracion: 216 Gradiente: [0.35893195997131877,-6.192544990903144] Loss: 59.02993020924345\n",
      "Iteracion: 217 Gradiente: [0.3587411407241286,-6.189252843907273] Loss: 58.991463991256964\n",
      "Iteracion: 218 Gradiente: [0.3585504229222034,-6.1859624471178085] Loss: 58.953038662043106\n",
      "Iteracion: 219 Gradiente: [0.3583598065117954,-6.182673799604266] Loss: 58.914654178137994\n",
      "Iteracion: 220 Gradiente: [0.35816929143881565,-6.179386900436693] Loss: 58.87631049612389\n",
      "Iteracion: 221 Gradiente: [0.3579788776496893,-6.176101748685595] Loss: 58.83800757262934\n",
      "Iteracion: 222 Gradiente: [0.35778856509020474,-6.172818343422018] Loss: 58.79974536432881\n",
      "Iteracion: 223 Gradiente: [0.35759835370670884,-6.169536683717464] Loss: 58.76152382794294\n",
      "Iteracion: 224 Gradiente: [0.35740824344543637,-6.166256768643935] Loss: 58.723342920238366\n",
      "Iteracion: 225 Gradiente: [0.35721823425269483,-6.162978597273931] Loss: 58.685202598027665\n",
      "Iteracion: 226 Gradiente: [0.357028326074726,-6.159702168680448] Loss: 58.64710281816931\n",
      "Iteracion: 227 Gradiente: [0.3568385188576556,-6.156427481936981] Loss: 58.609043537567636\n",
      "Iteracion: 228 Gradiente: [0.35664881254794145,-6.1531545361175] Loss: 58.57102471317281\n",
      "Iteracion: 229 Gradiente: [0.3564592070919337,-6.149883330296479] Loss: 58.53304630198071\n",
      "Iteracion: 230 Gradiente: [0.35626970243598016,-6.146613863548884] Loss: 58.495108261032996\n",
      "Iteracion: 231 Gradiente: [0.3560802985265833,-6.143346134950165] Loss: 58.457210547416935\n",
      "Iteracion: 232 Gradiente: [0.35589099531011875,-6.1400801435762675] Loss: 58.41935311826544\n",
      "Iteracion: 233 Gradiente: [0.3557017927330582,-6.136815888503637] Loss: 58.381535930756996\n",
      "Iteracion: 234 Gradiente: [0.35551269074179764,-6.133553368809202] Loss: 58.34375894211557\n",
      "Iteracion: 235 Gradiente: [0.35532368928300145,-6.130292583570377] Loss: 58.306022109610595\n",
      "Iteracion: 236 Gradiente: [0.3551347883031928,-6.127033531865073] Loss: 58.26832539055699\n",
      "Iteracion: 237 Gradiente: [0.3549459877489568,-6.123776212771689] Loss: 58.23066874231503\n",
      "Iteracion: 238 Gradiente: [0.3547572875668299,-6.120520625369124] Loss: 58.19305212229022\n",
      "Iteracion: 239 Gradiente: [0.35456868770365874,-6.117266768736742] Loss: 58.155475487933494\n",
      "Iteracion: 240 Gradiente: [0.35438018810585514,-6.114014641954429] Loss: 58.11793879674085\n",
      "Iteracion: 241 Gradiente: [0.354191788720289,-6.110764244102534] Loss: 58.08044200625365\n",
      "Iteracion: 242 Gradiente: [0.3540034894935568,-6.1075155742619085] Loss: 58.04298507405819\n",
      "Iteracion: 243 Gradiente: [0.35381529037252146,-6.104268631513887] Loss: 58.00556795778606\n",
      "Iteracion: 244 Gradiente: [0.35362719130376424,-6.1010234149402995] Loss: 57.9681906151137\n",
      "Iteracion: 245 Gradiente: [0.3534391922343943,-6.097779923623449] Loss: 57.93085300376265\n",
      "Iteracion: 246 Gradiente: [0.3532512931111313,-6.094538156646137] Loss: 57.89355508149937\n",
      "Iteracion: 247 Gradiente: [0.35306349388074476,-6.091298113091659] Loss: 57.856296806135134\n",
      "Iteracion: 248 Gradiente: [0.3528757944902414,-6.088059792043784] Loss: 57.81907813552625\n",
      "Iteracion: 249 Gradiente: [0.35268819488643716,-6.084823192586783] Loss: 57.78189902757358\n",
      "Iteracion: 250 Gradiente: [0.352500695016325,-6.0815883138054] Loss: 57.74475944022294\n",
      "Iteracion: 251 Gradiente: [0.35231329482690416,-6.078355154784869] Loss: 57.70765933146474\n",
      "Iteracion: 252 Gradiente: [0.3521259942652042,-6.075123714610915] Loss: 57.6705986593341\n",
      "Iteracion: 253 Gradiente: [0.3519387932782436,-6.071893992369742] Loss: 57.63357738191068\n",
      "Iteracion: 254 Gradiente: [0.3517516918130814,-6.0686659871480515] Loss: 57.59659545731877\n",
      "Iteracion: 255 Gradiente: [0.35156468981683797,-6.065439698033015] Loss: 57.559652843727164\n",
      "Iteracion: 256 Gradiente: [0.35137778723641255,-6.062215124112314] Loss: 57.52274949934908\n",
      "Iteracion: 257 Gradiente: [0.3511909840193146,-6.058992264474075] Loss: 57.48588538244215\n",
      "Iteracion: 258 Gradiente: [0.3510042801124636,-6.055771118206949] Loss: 57.44906045130846\n",
      "Iteracion: 259 Gradiente: [0.35081767546321935,-6.052551684400049] Loss: 57.41227466429436\n",
      "Iteracion: 260 Gradiente: [0.3506311700186624,-6.049333962142982] Loss: 57.37552797979044\n",
      "Iteracion: 261 Gradiente: [0.350444763726045,-6.046117950525841] Loss: 57.33882035623158\n",
      "Iteracion: 262 Gradiente: [0.35025845653280546,-6.042903648639183] Loss: 57.30215175209685\n",
      "Iteracion: 263 Gradiente: [0.35007224838616935,-6.039691055574074] Loss: 57.26552212590939\n",
      "Iteracion: 264 Gradiente: [0.3498861392335835,-6.036480170422042] Loss: 57.228931436236515\n",
      "Iteracion: 265 Gradiente: [0.3497001290223575,-6.033270992275116] Loss: 57.19237964168952\n",
      "Iteracion: 266 Gradiente: [0.34951421769977503,-6.0300635202257995] Loss: 57.1558667009237\n",
      "Iteracion: 267 Gradiente: [0.34932840521343184,-6.026857753367072] Loss: 57.119392572638304\n",
      "Iteracion: 268 Gradiente: [0.3491426915106767,-6.023653690792414] Loss: 57.082957215576535\n",
      "Iteracion: 269 Gradiente: [0.34895707653897906,-6.020451331595768] Loss: 57.04656058852533\n",
      "Iteracion: 270 Gradiente: [0.3487715602460189,-6.017250674871561] Loss: 57.01020265031559\n",
      "Iteracion: 271 Gradiente: [0.34858614257909676,-6.014051719714716] Loss: 56.973883359821876\n",
      "Iteracion: 272 Gradiente: [0.3484008234859033,-6.010854465220623] Loss: 56.93760267596243\n",
      "Iteracion: 273 Gradiente: [0.3482156029140692,-6.0076589104851585] Loss: 56.90136055769929\n",
      "Iteracion: 274 Gradiente: [0.3480304808111555,-6.00446505460468] Loss: 56.865156964038015\n",
      "Iteracion: 275 Gradiente: [0.3478454571247956,-6.001272896676022] Loss: 56.828991854027784\n",
      "Iteracion: 276 Gradiente: [0.3476605318028168,-5.998082435796495] Loss: 56.79286518676129\n",
      "Iteracion: 277 Gradiente: [0.34747570479271084,-5.99489367106391] Loss: 56.7567769213747\n",
      "Iteracion: 278 Gradiente: [0.34729097604237136,-5.991706601576531] Loss: 56.72072701704764\n",
      "Iteracion: 279 Gradiente: [0.3471063454993569,-5.988521226433125] Loss: 56.68471543300312\n",
      "Iteracion: 280 Gradiente: [0.34692181311181497,-5.985337544732908] Loss: 56.6487421285075\n",
      "Iteracion: 281 Gradiente: [0.3467373788273832,-5.9821555555756065] Loss: 56.61280706287041\n",
      "Iteracion: 282 Gradiente: [0.34655304259375347,-5.978975258061415] Loss: 56.576910195444775\n",
      "Iteracion: 283 Gradiente: [0.3463688043590423,-5.975796651290998] Loss: 56.54105148562673\n",
      "Iteracion: 284 Gradiente: [0.34618466407084014,-5.972619734365512] Loss: 56.5052308928555\n",
      "Iteracion: 285 Gradiente: [0.34600062167742274,-5.969444506386575] Loss: 56.46944837661351\n",
      "Iteracion: 286 Gradiente: [0.34581667712650677,-5.966270966456296] Loss: 56.43370389642618\n",
      "Iteracion: 287 Gradiente: [0.3456328303661863,-5.963099113677253] Loss: 56.39799741186202\n",
      "Iteracion: 288 Gradiente: [0.3454490813444764,-5.959928947152506] Loss: 56.36232888253252\n",
      "Iteracion: 289 Gradiente: [0.34526543000942406,-5.9567604659855915] Loss: 56.32669826809202\n",
      "Iteracion: 290 Gradiente: [0.34508187630896925,-5.953593669280523] Loss: 56.29110552823781\n",
      "Iteracion: 291 Gradiente: [0.3448984201912561,-5.950428556141787] Loss: 56.255550622710004\n",
      "Iteracion: 292 Gradiente: [0.3447150616045254,-5.947265125674349] Loss: 56.220033511291575\n",
      "Iteracion: 293 Gradiente: [0.34453180049675375,-5.94410337698365] Loss: 56.1845541538081\n",
      "Iteracion: 294 Gradiente: [0.3443486368163638,-5.940943309175596] Loss: 56.149112510127985\n",
      "Iteracion: 295 Gradiente: [0.3441655705112564,-5.937784921356596] Loss: 56.113708540162285\n",
      "Iteracion: 296 Gradiente: [0.34398260152994287,-5.934628212633498] Loss: 56.0783422038646\n",
      "Iteracion: 297 Gradiente: [0.3437997298205403,-5.931473182113654] Loss: 56.0430134612312\n",
      "Iteracion: 298 Gradiente: [0.34361695533123776,-5.9283198289048835] Loss: 56.00772227230072\n",
      "Iteracion: 299 Gradiente: [0.343434278010587,-5.9251681521154635] Loss: 55.97246859715446\n",
      "Iteracion: 300 Gradiente: [0.34325169780668197,-5.9220181508541705] Loss: 55.93725239591604\n",
      "Iteracion: 301 Gradiente: [0.34306921466818047,-5.918869824230226] Loss: 55.902073628751474\n",
      "Iteracion: 302 Gradiente: [0.3428868285431724,-5.9157231713533625] Loss: 55.86693225586912\n",
      "Iteracion: 303 Gradiente: [0.34270453938034584,-5.912578191333749] Loss: 55.83182823751973\n",
      "Iteracion: 304 Gradiente: [0.3425223471279234,-5.909434883282052] Loss: 55.796761533996154\n",
      "Iteracion: 305 Gradiente: [0.3423402517345603,-5.906293246309394] Loss: 55.761732105633534\n",
      "Iteracion: 306 Gradiente: [0.3421582531487147,-5.903153279527382] Loss: 55.72673991280923\n",
      "Iteracion: 307 Gradiente: [0.3419763513188646,-5.900014982048095] Loss: 55.69178491594256\n",
      "Iteracion: 308 Gradiente: [0.34179454619364597,-5.896878352984075] Loss: 55.65686707549505\n",
      "Iteracion: 309 Gradiente: [0.34161283772153134,-5.893743391448347] Loss: 55.62198635197024\n",
      "Iteracion: 310 Gradiente: [0.3414312258512749,-5.890610096554396] Loss: 55.587142705913635\n",
      "Iteracion: 311 Gradiente: [0.341249710531477,-5.887478467416187] Loss: 55.55233609791261\n",
      "Iteracion: 312 Gradiente: [0.34106829171083486,-5.884348503148149] Loss: 55.51756648859658\n",
      "Iteracion: 313 Gradiente: [0.3408869693378932,-5.881220202865196] Loss: 55.48283383863666\n",
      "Iteracion: 314 Gradiente: [0.34070574336151627,-5.878093565682693] Loss: 55.448138108745866\n",
      "Iteracion: 315 Gradiente: [0.34052461373047554,-5.874968590716489] Loss: 55.41347925967898\n",
      "Iteracion: 316 Gradiente: [0.34034358039352053,-5.871845277082894] Loss: 55.37885725223241\n",
      "Iteracion: 317 Gradiente: [0.34016264329938933,-5.868723623898696] Loss: 55.34427204724432\n",
      "Iteracion: 318 Gradiente: [0.33998180239703757,-5.865603630281147] Loss: 55.309723605594506\n",
      "Iteracion: 319 Gradiente: [0.33980105763522156,-5.8624852953479785] Loss: 55.27521188820428\n",
      "Iteracion: 320 Gradiente: [0.3396204089628791,-5.859368618217371] Loss: 55.240736856036584\n",
      "Iteracion: 321 Gradiente: [0.3394398563288779,-5.856253598007999] Loss: 55.20629847009575\n",
      "Iteracion: 322 Gradiente: [0.33925939968230473,-5.8531402338389755] Loss: 55.171896691427655\n",
      "Iteracion: 323 Gradiente: [0.3390790389719219,-5.850028524829917] Loss: 55.137531481119545\n",
      "Iteracion: 324 Gradiente: [0.33889877414681824,-5.8469184701008805] Loss: 55.103202800300004\n",
      "Iteracion: 325 Gradiente: [0.33871860515605806,-5.843810068772397] Loss: 55.068910610138964\n",
      "Iteracion: 326 Gradiente: [0.33853853194869404,-5.840703319965471] Loss: 55.03465487184765\n",
      "Iteracion: 327 Gradiente: [0.3383585544737097,-5.837598222801576] Loss: 55.0004355466785\n",
      "Iteracion: 328 Gradiente: [0.3381786726803867,-5.834494776402636] Loss: 54.966252595925134\n",
      "Iteracion: 329 Gradiente: [0.3379988865176557,-5.831392979891065] Loss: 54.932105980922294\n",
      "Iteracion: 330 Gradiente: [0.33781919593470205,-5.828292832389737] Loss: 54.897995663045876\n",
      "Iteracion: 331 Gradiente: [0.33763960088085293,-5.82519433302197] Loss: 54.863921603712804\n",
      "Iteracion: 332 Gradiente: [0.3374601013053014,-5.822097480911576] Loss: 54.82988376438103\n",
      "Iteracion: 333 Gradiente: [0.3372806971571476,-5.819002275182821] Loss: 54.79588210654943\n",
      "Iteracion: 334 Gradiente: [0.3371013883858417,-5.815908714960438] Loss: 54.76191659175784\n",
      "Iteracion: 335 Gradiente: [0.3369221749404749,-5.812816799369625] Loss: 54.727987181587004\n",
      "Iteracion: 336 Gradiente: [0.33674305677049976,-5.809726527536046] Loss: 54.694093837658436\n",
      "Iteracion: 337 Gradiente: [0.3365640338253487,-5.806637898585825] Loss: 54.66023652163457\n",
      "Iteracion: 338 Gradiente: [0.33638510605424726,-5.803550911645563] Loss: 54.62641519521841\n",
      "Iteracion: 339 Gradiente: [0.33620627340656406,-5.800465565842315] Loss: 54.5926298201538\n",
      "Iteracion: 340 Gradiente: [0.33602753583185435,-5.797381860303596] Loss: 54.558880358225245\n",
      "Iteracion: 341 Gradiente: [0.3358488932795396,-5.794299794157394] Loss: 54.52516677125782\n",
      "Iteracion: 342 Gradiente: [0.33567034569904164,-5.791219366532163] Loss: 54.49148902111723\n",
      "Iteracion: 343 Gradiente: [0.3354918930399123,-5.788140576556809] Loss: 54.45784706970964\n",
      "Iteracion: 344 Gradiente: [0.3353135352517936,-5.785063423360707] Loss: 54.42424087898181\n",
      "Iteracion: 345 Gradiente: [0.3351352722841213,-5.7819879060736925] Loss: 54.39067041092086\n",
      "Iteracion: 346 Gradiente: [0.334957104086431,-5.778914023826078] Loss: 54.35713562755434\n",
      "Iteracion: 347 Gradiente: [0.3347790306084806,-5.775841775748615] Loss: 54.32363649095023\n",
      "Iteracion: 348 Gradiente: [0.3346010517998927,-5.77277116097253] Loss: 54.29017296321677\n",
      "Iteracion: 349 Gradiente: [0.3344231676101747,-5.769702178629518] Loss: 54.256745006502435\n",
      "Iteracion: 350 Gradiente: [0.3342453779892537,-5.766634827851717] Loss: 54.22335258299602\n",
      "Iteracion: 351 Gradiente: [0.3340676828866783,-5.763569107771745] Loss: 54.18999565492651\n",
      "Iteracion: 352 Gradiente: [0.3338900822523712,-5.760505017522663] Loss: 54.156674184562966\n",
      "Iteracion: 353 Gradiente: [0.33371257603597543,-5.757442556238009] Loss: 54.123388134214615\n",
      "Iteracion: 354 Gradiente: [0.3335351641873648,-5.754381723051777] Loss: 54.09013746623074\n",
      "Iteracion: 355 Gradiente: [0.3333578466563073,-5.751322517098418] Loss: 54.05692214300061\n",
      "Iteracion: 356 Gradiente: [0.3331806233926964,-5.748264937512846] Loss: 54.023742126953536\n",
      "Iteracion: 357 Gradiente: [0.3330034943465122,-5.745208983430428] Loss: 53.99059738055875\n",
      "Iteracion: 358 Gradiente: [0.332826459467294,-5.742154653987018] Loss: 53.9574878663253\n",
      "Iteracion: 359 Gradiente: [0.332649518705414,-5.739101948318884] Loss: 53.924413546802185\n",
      "Iteracion: 360 Gradiente: [0.3324726720106786,-5.736050865562782] Loss: 53.89137438457819\n",
      "Iteracion: 361 Gradiente: [0.3322959193330182,-5.7330014048559255] Loss: 53.85837034228179\n",
      "Iteracion: 362 Gradiente: [0.3321192606225613,-5.729953565335986] Loss: 53.825401382581326\n",
      "Iteracion: 363 Gradiente: [0.3319426958293908,-5.726907346141081] Loss: 53.7924674681847\n",
      "Iteracion: 364 Gradiente: [0.3317662249033269,-5.723862746409812] Loss: 53.759568561839494\n",
      "Iteracion: 365 Gradiente: [0.3315898477946144,-5.720819765281212] Loss: 53.726704626332904\n",
      "Iteracion: 366 Gradiente: [0.33141356445349857,-5.717778401894781] Loss: 53.6938756244917\n",
      "Iteracion: 367 Gradiente: [0.33123737483001797,-5.71473865539048] Loss: 53.661081519182105\n",
      "Iteracion: 368 Gradiente: [0.3310612788742041,-5.711700524908728] Loss: 53.628322273309806\n",
      "Iteracion: 369 Gradiente: [0.3308852765365197,-5.708664009590392] Loss: 53.59559784982005\n",
      "Iteracion: 370 Gradiente: [0.33070936776710363,-5.705629108576803] Loss: 53.56290821169735\n",
      "Iteracion: 371 Gradiente: [0.3305335525161335,-5.7025958210097505] Loss: 53.53025332196556\n",
      "Iteracion: 372 Gradiente: [0.330357830734062,-5.699564146031471] Loss: 53.49763314368794\n",
      "Iteracion: 373 Gradiente: [0.33018220237089985,-5.696534082784675] Loss: 53.465047639966905\n",
      "Iteracion: 374 Gradiente: [0.33000666737731216,-5.693505630412497] Loss: 53.43249677394416\n",
      "Iteracion: 375 Gradiente: [0.32983122570344725,-5.690478788058566] Loss: 53.39998050880056\n",
      "Iteracion: 376 Gradiente: [0.32965587729981205,-5.687453554866933] Loss: 53.367498807756114\n",
      "Iteracion: 377 Gradiente: [0.3294806221166778,-5.684429929982129] Loss: 53.3350516340699\n",
      "Iteracion: 378 Gradiente: [0.3293054601047011,-5.681407912549118] Loss: 53.3026389510401\n",
      "Iteracion: 379 Gradiente: [0.3291303912140864,-5.67838750171334] Loss: 53.27026072200385\n",
      "Iteracion: 380 Gradiente: [0.32895541539548273,-5.675368696620672] Loss: 53.23791691033728\n",
      "Iteracion: 381 Gradiente: [0.32878053259948314,-5.67235149641745] Loss: 53.20560747945549\n",
      "Iteracion: 382 Gradiente: [0.32860574277654,-5.669335900250469] Loss: 53.173332392812455\n",
      "Iteracion: 383 Gradiente: [0.3284310458772121,-5.6663219072669735] Loss: 53.14109161390089\n",
      "Iteracion: 384 Gradiente: [0.32825644185203895,-5.6633095166146665] Loss: 53.1088851062525\n",
      "Iteracion: 385 Gradiente: [0.32808193065180025,-5.660298727441688] Loss: 53.076712833437604\n",
      "Iteracion: 386 Gradiente: [0.3279075122271216,-5.657289538896649] Loss: 53.0445747590653\n",
      "Iteracion: 387 Gradiente: [0.3277331865285947,-5.654281950128609] Loss: 53.012470846783415\n",
      "Iteracion: 388 Gradiente: [0.3275589535069495,-5.651275960287077] Loss: 52.98040106027834\n",
      "Iteracion: 389 Gradiente: [0.32738481311290807,-5.648271568522011] Loss: 52.9483653632751\n",
      "Iteracion: 390 Gradiente: [0.3272107652972908,-5.645268773983827] Loss: 52.916363719537294\n",
      "Iteracion: 391 Gradiente: [0.3270368100108191,-5.642267575823388] Loss: 52.884396092866986\n",
      "Iteracion: 392 Gradiente: [0.32686294720426806,-5.639267973192016] Loss: 52.852462447104806\n",
      "Iteracion: 393 Gradiente: [0.3266891768286044,-5.636269965241471] Loss: 52.82056274612975\n",
      "Iteracion: 394 Gradiente: [0.3265154988345024,-5.633273551123979] Loss: 52.78869695385918\n",
      "Iteracion: 395 Gradiente: [0.32634191317301886,-5.630278729992204] Loss: 52.756865034248975\n",
      "Iteracion: 396 Gradiente: [0.3261684197950357,-5.627285500999266] Loss: 52.72506695129313\n",
      "Iteracion: 397 Gradiente: [0.32599501865138003,-5.624293863298743] Loss: 52.69330266902403\n",
      "Iteracion: 398 Gradiente: [0.3258217096930937,-5.621303816044647] Loss: 52.661572151512225\n",
      "Iteracion: 399 Gradiente: [0.3256484928712015,-5.618315358391448] Loss: 52.62987536286655\n",
      "Iteracion: 400 Gradiente: [0.32547536813659356,-5.615328489494076] Loss: 52.5982122672339\n",
      "Iteracion: 401 Gradiente: [0.3253023354405935,-5.612343208507878] Loss: 52.566582828799355\n",
      "Iteracion: 402 Gradiente: [0.32512939473395996,-5.60935951458869] Loss: 52.53498701178599\n",
      "Iteracion: 403 Gradiente: [0.3249565459679611,-5.606377406892775] Loss: 52.503424780454935\n",
      "Iteracion: 404 Gradiente: [0.32478378909363337,-5.603396884576845] Loss: 52.47189609910537\n",
      "Iteracion: 405 Gradiente: [0.3246111240622668,-5.60041794679806] Loss: 52.44040093207437\n",
      "Iteracion: 406 Gradiente: [0.32443855082479395,-5.597440592714042] Loss: 52.40893924373689\n",
      "Iteracion: 407 Gradiente: [0.32426606933266744,-5.594464821482843] Loss: 52.37751099850587\n",
      "Iteracion: 408 Gradiente: [0.32409367953700824,-5.591490632262965] Loss: 52.34611616083189\n",
      "Iteracion: 409 Gradiente: [0.3239213813889054,-5.588518024213378] Loss: 52.3147546952035\n",
      "Iteracion: 410 Gradiente: [0.32374917483989735,-5.585546996493469] Loss: 52.28342656614688\n",
      "Iteracion: 411 Gradiente: [0.3235770598411577,-5.582577548263091] Loss: 52.252131738226005\n",
      "Iteracion: 412 Gradiente: [0.3234050363440855,-5.579609678682536] Loss: 52.22087017604246\n",
      "Iteracion: 413 Gradiente: [0.32323310429985336,-5.576643386912554] Loss: 52.18964184423545\n",
      "Iteracion: 414 Gradiente: [0.3230612636600007,-5.573678672114328] Loss: 52.158446707481815\n",
      "Iteracion: 415 Gradiente: [0.3228895143759075,-5.570715533449487] Loss: 52.12728473049591\n",
      "Iteracion: 416 Gradiente: [0.3227178563990909,-5.56775397008011] Loss: 52.09615587802962\n",
      "Iteracion: 417 Gradiente: [0.3225462896808293,-5.564793981168729] Loss: 52.06506011487227\n",
      "Iteracion: 418 Gradiente: [0.32237481417282393,-5.561835565878302] Loss: 52.03399740585062\n",
      "Iteracion: 419 Gradiente: [0.32220342982644656,-5.558878723372256] Loss: 52.00296771582885\n",
      "Iteracion: 420 Gradiente: [0.32203213659312385,-5.555923452814448] Loss: 51.97197100970843\n",
      "Iteracion: 421 Gradiente: [0.32186093442464264,-5.552969753369179] Loss: 51.94100725242822\n",
      "Iteracion: 422 Gradiente: [0.32168982327242335,-5.550017624201195] Loss: 51.910076408964265\n",
      "Iteracion: 423 Gradiente: [0.3215188030881649,-5.547067064475696] Loss: 51.879178444329874\n",
      "Iteracion: 424 Gradiente: [0.32134787382353786,-5.544118073358309] Loss: 51.8483133235756\n",
      "Iteracion: 425 Gradiente: [0.3211770354300583,-5.541170650015124] Loss: 51.81748101178901\n",
      "Iteracion: 426 Gradiente: [0.3210062878595638,-5.538224793612657] Loss: 51.786681474094976\n",
      "Iteracion: 427 Gradiente: [0.32083563106372115,-5.5352805033178765] Loss: 51.75591467565525\n",
      "Iteracion: 428 Gradiente: [0.32066506499425457,-5.532337778298188] Loss: 51.725180581668724\n",
      "Iteracion: 429 Gradiente: [0.3204945896029656,-5.5293966177214475] Loss: 51.694479157371255\n",
      "Iteracion: 430 Gradiente: [0.320324204841706,-5.526457020755945] Loss: 51.66381036803568\n",
      "Iteracion: 431 Gradiente: [0.3201539106621262,-5.523518986570422] Loss: 51.633174178971714\n",
      "Iteracion: 432 Gradiente: [0.3199837070161912,-5.520582514334055] Loss: 51.60257055552598\n",
      "Iteracion: 433 Gradiente: [0.3198135938556097,-5.517647603216465] Loss: 51.571999463081845\n",
      "Iteracion: 434 Gradiente: [0.3196435711326072,-5.514714252387707] Loss: 51.54146086705963\n",
      "Iteracion: 435 Gradiente: [0.3194736387987947,-5.51178246101829] Loss: 51.5109547329163\n",
      "Iteracion: 436 Gradiente: [0.3193037968062775,-5.508852228279153] Loss: 51.48048102614556\n",
      "Iteracion: 437 Gradiente: [0.31913404510691096,-5.505923553341692] Loss: 51.45003971227779\n",
      "Iteracion: 438 Gradiente: [0.31896438365273266,-5.502996435377719] Loss: 51.41963075688003\n",
      "Iteracion: 439 Gradiente: [0.31879481239585206,-5.500070873559499] Loss: 51.389254125555944\n",
      "Iteracion: 440 Gradiente: [0.3186253312882305,-5.497146867059744] Loss: 51.35890978394569\n",
      "Iteracion: 441 Gradiente: [0.3184559402820492,-5.494224415051591] Loss: 51.32859769772601\n",
      "Iteracion: 442 Gradiente: [0.31828663932926254,-5.491303516708632] Loss: 51.29831783261011\n",
      "Iteracion: 443 Gradiente: [0.3181174283820963,-5.4883841712048875] Loss: 51.2680701543476\n",
      "Iteracion: 444 Gradiente: [0.3179483073926065,-5.4854663777148245] Loss: 51.237854628724605\n",
      "Iteracion: 445 Gradiente: [0.3177792763130791,-5.482550135413339] Loss: 51.20767122156353\n",
      "Iteracion: 446 Gradiente: [0.3176103350955728,-5.479635443475781] Loss: 51.17751989872308\n",
      "Iteracion: 447 Gradiente: [0.31744148369253317,-5.4767223010779125] Loss: 51.14740062609839\n",
      "Iteracion: 448 Gradiente: [0.3172727220559141,-5.4738107073959705] Loss: 51.11731336962067\n",
      "Iteracion: 449 Gradiente: [0.31710405013829995,-5.470900661606595] Loss: 51.087258095257525\n",
      "Iteracion: 450 Gradiente: [0.3169354678917832,-5.467992162886889] Loss: 51.057234769012595\n",
      "Iteracion: 451 Gradiente: [0.31676697526879044,-5.465085210414372] Loss: 51.0272433569257\n",
      "Iteracion: 452 Gradiente: [0.3165985722216609,-5.462179803367021] Loss: 50.9972838250728\n",
      "Iteracion: 453 Gradiente: [0.3164302587026981,-5.459275940923238] Loss: 50.96735613956586\n",
      "Iteracion: 454 Gradiente: [0.31626203466447506,-5.456373622261861] Loss: 50.93746026655289\n",
      "Iteracion: 455 Gradiente: [0.31609390005925014,-5.45347284656217] Loss: 50.907596172217914\n",
      "Iteracion: 456 Gradiente: [0.31592585483954827,-5.45057361300388] Loss: 50.877763822780835\n",
      "Iteracion: 457 Gradiente: [0.31575789895788164,-5.447675920767139] Loss: 50.84796318449754\n",
      "Iteracion: 458 Gradiente: [0.31559003236669647,-5.444779769032539] Loss: 50.818194223659695\n",
      "Iteracion: 459 Gradiente: [0.31542225501854493,-5.441885156981093] Loss: 50.7884569065949\n",
      "Iteracion: 460 Gradiente: [0.31525456686595726,-5.438992083794267] Loss: 50.758751199666484\n",
      "Iteracion: 461 Gradiente: [0.3150869678615808,-5.4361005486539495] Loss: 50.72907706927352\n",
      "Iteracion: 462 Gradiente: [0.3149194579578636,-5.433210550742469] Loss: 50.69943448185083\n",
      "Iteracion: 463 Gradiente: [0.3147520371077076,-5.430322089242581] Loss: 50.66982340386894\n",
      "Iteracion: 464 Gradiente: [0.3145847052634795,-5.427435163337494] Loss: 50.64024380183392\n",
      "Iteracion: 465 Gradiente: [0.3144174623780825,-5.424549772210825] Loss: 50.61069564228756\n",
      "Iteracion: 466 Gradiente: [0.3142503084041053,-5.421665915046647] Loss: 50.58117889180714\n",
      "Iteracion: 467 Gradiente: [0.3140832432942567,-5.418783591029462] Loss: 50.551693517005496\n",
      "Iteracion: 468 Gradiente: [0.3139162670015134,-5.415902799344189] Loss: 50.522239484530964\n",
      "Iteracion: 469 Gradiente: [0.3137493794783043,-5.4130235391762085] Loss: 50.492816761067296\n",
      "Iteracion: 470 Gradiente: [0.31358258067773664,-5.410145809711306] Loss: 50.46342531333367\n",
      "Iteracion: 471 Gradiente: [0.3134158705525699,-5.407269610135715] Loss: 50.43406510808466\n",
      "Iteracion: 472 Gradiente: [0.31324924905548196,-5.404394939636114] Loss: 50.40473611211018\n",
      "Iteracion: 473 Gradiente: [0.3130827161396051,-5.401521797399577] Loss: 50.37543829223542\n",
      "Iteracion: 474 Gradiente: [0.3129162717577863,-5.398650182613644] Loss: 50.346171615320856\n",
      "Iteracion: 475 Gradiente: [0.3127499158627368,-5.395780094466286] Loss: 50.31693604826217\n",
      "Iteracion: 476 Gradiente: [0.31258364840763375,-5.392911532145875] Loss: 50.287731557990256\n",
      "Iteracion: 477 Gradiente: [0.31241746934541015,-5.390044494841247] Loss: 50.25855811147116\n",
      "Iteracion: 478 Gradiente: [0.3122513786292117,-5.387178981741649] Loss: 50.22941567570602\n",
      "Iteracion: 479 Gradiente: [0.31208537621192817,-5.3843149920367654] Loss: 50.200304217731045\n",
      "Iteracion: 480 Gradiente: [0.31191946204648746,-5.38145252491673] Loss: 50.17122370461754\n",
      "Iteracion: 481 Gradiente: [0.31175363608615736,-5.378591579572075] Loss: 50.14217410347175\n",
      "Iteracion: 482 Gradiente: [0.31158789828400774,-5.37573215519378] Loss: 50.1131553814349\n",
      "Iteracion: 483 Gradiente: [0.31142224859317574,-5.372874250973255] Loss: 50.0841675056832\n",
      "Iteracion: 484 Gradiente: [0.311256686966793,-5.370017866102334] Loss: 50.055210443427654\n",
      "Iteracion: 485 Gradiente: [0.3110912133581427,-5.367162999773283] Loss: 50.02628416191419\n",
      "Iteracion: 486 Gradiente: [0.31092582772015426,-5.3643096511788135] Loss: 49.99738862842353\n",
      "Iteracion: 487 Gradiente: [0.31076053000638976,-5.36145781951203] Loss: 49.96852381027121\n",
      "Iteracion: 488 Gradiente: [0.3105953201699478,-5.358607503966498] Loss: 49.93968967480743\n",
      "Iteracion: 489 Gradiente: [0.3104301981640461,-5.355758703736204] Loss: 49.91088618941715\n",
      "Iteracion: 490 Gradiente: [0.31026516394214665,-5.352911418015553] Loss: 49.882113321520016\n",
      "Iteracion: 491 Gradiente: [0.31010021745742644,-5.350065645999392] Loss: 49.853371038570266\n",
      "Iteracion: 492 Gradiente: [0.3099353586633349,-5.347221386882984] Loss: 49.82465930805676\n",
      "Iteracion: 493 Gradiente: [0.30977058751334446,-5.34437863986202] Loss: 49.79597809750291\n",
      "Iteracion: 494 Gradiente: [0.3096059039607075,-5.341537404132632] Loss: 49.76732737446664\n",
      "Iteracion: 495 Gradiente: [0.3094413079589193,-5.338697678891368] Loss: 49.73870710654037\n",
      "Iteracion: 496 Gradiente: [0.3092767994613119,-5.335859463335212] Loss: 49.710117261350966\n",
      "Iteracion: 497 Gradiente: [0.3091123784215296,-5.3330227566615624] Loss: 49.681557806559695\n",
      "Iteracion: 498 Gradiente: [0.3089480447930408,-5.33018755806825] Loss: 49.653028709862205\n",
      "Iteracion: 499 Gradiente: [0.3087837985293016,-5.327353866753539] Loss: 49.62452993898851\n",
      "Iteracion: 500 Gradiente: [0.30861963958397165,-5.324521681916107] Loss: 49.59606146170293\n",
      "Iteracion: 501 Gradiente: [0.3084555679104826,-5.321691002755074] Loss: 49.56762324580395\n",
      "Iteracion: 502 Gradiente: [0.30829158346264124,-5.318861828469963] Loss: 49.53921525912438\n",
      "Iteracion: 503 Gradiente: [0.30812768619397846,-5.316034158260741] Loss: 49.51083746953124\n",
      "Iteracion: 504 Gradiente: [0.3079638760580596,-5.313207991327804] Loss: 49.48248984492565\n",
      "Iteracion: 505 Gradiente: [0.3078001530086908,-5.310383326871954] Loss: 49.45417235324285\n",
      "Iteracion: 506 Gradiente: [0.3076365169994697,-5.307560164094432] Loss: 49.42588496245219\n",
      "Iteracion: 507 Gradiente: [0.30747296798421264,-5.304738502196896] Loss: 49.39762764055709\n",
      "Iteracion: 508 Gradiente: [0.30730950591669265,-5.301918340381434] Loss: 49.36940035559494\n",
      "Iteracion: 509 Gradiente: [0.30714613075048336,-5.299099677850563] Loss: 49.34120307563707\n",
      "Iteracion: 510 Gradiente: [0.3069828424396822,-5.2962825138072045] Loss: 49.31303576878887\n",
      "Iteracion: 511 Gradiente: [0.30681964093785935,-5.293466847454728] Loss: 49.28489840318953\n",
      "Iteracion: 512 Gradiente: [0.306656526198946,-5.290652677996913] Loss: 49.25679094701213\n",
      "Iteracion: 513 Gradiente: [0.3064934981769279,-5.287840004637957] Loss: 49.228713368463595\n",
      "Iteracion: 514 Gradiente: [0.3063305568255103,-5.285028826582499] Loss: 49.20066563578462\n",
      "Iteracion: 515 Gradiente: [0.3061677020987517,-5.282219143035579] Loss: 49.17264771724976\n",
      "Iteracion: 516 Gradiente: [0.3060049339505125,-5.279410953202681] Loss: 49.1446595811671\n",
      "Iteracion: 517 Gradiente: [0.3058422523349883,-5.2766042562896835] Loss: 49.11670119587862\n",
      "Iteracion: 518 Gradiente: [0.30567965720585355,-5.273799051502923] Loss: 49.088772529759794\n",
      "Iteracion: 519 Gradiente: [0.30551714851729295,-5.270995338049132] Loss: 49.060873551219814\n",
      "Iteracion: 520 Gradiente: [0.3053547262233825,-5.268193115135472] Loss: 49.0330042287014\n",
      "Iteracion: 521 Gradiente: [0.3051923902781525,-5.265392381969525] Loss: 49.00516453068082\n",
      "Iteracion: 522 Gradiente: [0.3050301406356638,-5.262593137759298] Loss: 48.97735442566787\n",
      "Iteracion: 523 Gradiente: [0.30486797725002573,-5.2597953817132215] Loss: 48.94957388220585\n",
      "Iteracion: 524 Gradiente: [0.30470590007552284,-5.256999113040126] Loss: 48.92182286887138\n",
      "Iteracion: 525 Gradiente: [0.3045439090660743,-5.254204330949296] Loss: 48.89410135427463\n",
      "Iteracion: 526 Gradiente: [0.3043820041761871,-5.2514110346504035] Loss: 48.86640930705904\n",
      "Iteracion: 527 Gradiente: [0.3042201853598224,-5.248619223353566] Loss: 48.83874669590144\n",
      "Iteracion: 528 Gradiente: [0.3040584525713025,-5.245828896269308] Loss: 48.81111348951186\n",
      "Iteracion: 529 Gradiente: [0.30389680576495265,-5.243040052608575] Loss: 48.78350965663375\n",
      "Iteracion: 530 Gradiente: [0.3037352448949851,-5.240252691582736] Loss: 48.75593516604359\n",
      "Iteracion: 531 Gradiente: [0.3035737699156783,-5.237466812403576] Loss: 48.7283899865512\n",
      "Iteracion: 532 Gradiente: [0.303412380781432,-5.234682414283304] Loss: 48.70087408699952\n",
      "Iteracion: 533 Gradiente: [0.30325107744669044,-5.23189949643453] Loss: 48.67338743626456\n",
      "Iteracion: 534 Gradiente: [0.30308985986564235,-5.229118058070314] Loss: 48.64593000325549\n",
      "Iteracion: 535 Gradiente: [0.3029287279928385,-5.2263380984041055] Loss: 48.61850175691447\n",
      "Iteracion: 536 Gradiente: [0.3027676817827441,-5.223559616649783] Loss: 48.59110266621668\n",
      "Iteracion: 537 Gradiente: [0.30260672118975523,-5.220782612021646] Loss: 48.56373270017032\n",
      "Iteracion: 538 Gradiente: [0.30244584616832737,-5.21800708373441] Loss: 48.53639182781645\n",
      "Iteracion: 539 Gradiente: [0.30228505667302485,-5.215233031003203] Loss: 48.509080018229135\n",
      "Iteracion: 540 Gradiente: [0.30212435265835513,-5.212460453043576] Loss: 48.481797240515256\n",
      "Iteracion: 541 Gradiente: [0.3019637340787862,-5.209689349071507] Loss: 48.4545434638146\n",
      "Iteracion: 542 Gradiente: [0.3018032008890451,-5.206919718303358] Loss: 48.427318657299615\n",
      "Iteracion: 543 Gradiente: [0.30164275304370797,-5.204151559955942] Loss: 48.40012279017568\n",
      "Iteracion: 544 Gradiente: [0.30148239049734304,-5.201384873246469] Loss: 48.372955831680805\n",
      "Iteracion: 545 Gradiente: [0.3013221132045741,-5.1986196573925785] Loss: 48.345817751085725\n",
      "Iteracion: 546 Gradiente: [0.30116192112014983,-5.195855911612309] Loss: 48.318708517693885\n",
      "Iteracion: 547 Gradiente: [0.30100181419875416,-5.193093635124133] Loss: 48.291628100841244\n",
      "Iteracion: 548 Gradiente: [0.3008417923952943,-5.190332827146915] Loss: 48.26457646989655\n",
      "Iteracion: 549 Gradiente: [0.3006818556641922,-5.187573486899966] Loss: 48.23755359426091\n",
      "Iteracion: 550 Gradiente: [0.3005220039603628,-5.184815613602992] Loss: 48.21055944336804\n",
      "Iteracion: 551 Gradiente: [0.3003622372385323,-5.182059206476118] Loss: 48.18359398668419\n",
      "Iteracion: 552 Gradiente: [0.30020255545364755,-5.179304264739877] Loss: 48.15665719370798\n",
      "Iteracion: 553 Gradiente: [0.3000429585604399,-5.176550787615228] Loss: 48.12974903397051\n",
      "Iteracion: 554 Gradiente: [0.29988344651400284,-5.173798774323528] Loss: 48.10286947703529\n",
      "Iteracion: 555 Gradiente: [0.29972401926901354,-5.171048224086567] Loss: 48.07601849249813\n",
      "Iteracion: 556 Gradiente: [0.2995646767804201,-5.168299136126545] Loss: 48.04919604998715\n",
      "Iteracion: 557 Gradiente: [0.2994054190031534,-5.165551509666062] Loss: 48.02240211916284\n",
      "Iteracion: 558 Gradiente: [0.29924624589223353,-5.162805343928143] Loss: 47.99563666971782\n",
      "Iteracion: 559 Gradiente: [0.2990871574026883,-5.16006063813622] Loss: 47.96889967137704\n",
      "Iteracion: 560 Gradiente: [0.2989281534894185,-5.157317391514151] Loss: 47.94219109389755\n",
      "Iteracion: 561 Gradiente: [0.298769234107556,-5.1545756032861805] Loss: 47.91551090706859\n",
      "Iteracion: 562 Gradiente: [0.2986103992119569,-5.151835272677004] Loss: 47.8888590807115\n",
      "Iteracion: 563 Gradiente: [0.29845164875792446,-5.14909639891169] Loss: 47.86223558467966\n",
      "Iteracion: 564 Gradiente: [0.29829298270057386,-5.146358981215729] Loss: 47.8356403888586\n",
      "Iteracion: 565 Gradiente: [0.2981344009948633,-5.143623018815051] Loss: 47.80907346316578\n",
      "Iteracion: 566 Gradiente: [0.29797590359617726,-5.140888510935954] Loss: 47.78253477755064\n",
      "Iteracion: 567 Gradiente: [0.29781749045954936,-5.138155456805188] Loss: 47.75602430199458\n",
      "Iteracion: 568 Gradiente: [0.29765916154017613,-5.135423855649885] Loss: 47.729542006510904\n",
      "Iteracion: 569 Gradiente: [0.29750091679327184,-5.132693706697606] Loss: 47.7030878611448\n",
      "Iteracion: 570 Gradiente: [0.29734275617421724,-5.129965009176305] Loss: 47.676661835973306\n",
      "Iteracion: 571 Gradiente: [0.29718467963810535,-5.127237762314372] Loss: 47.65026390110523\n",
      "Iteracion: 572 Gradiente: [0.2970266871404798,-5.124511965340573] Loss: 47.62389402668119\n",
      "Iteracion: 573 Gradiente: [0.2968687786364247,-5.121787617484117] Loss: 47.5975521828735\n",
      "Iteracion: 574 Gradiente: [0.2967109540813837,-5.119064717974607] Loss: 47.57123833988625\n",
      "Iteracion: 575 Gradiente: [0.29655321343069757,-5.116343266042052] Loss: 47.54495246795514\n",
      "Iteracion: 576 Gradiente: [0.2963955566397869,-5.1136232609168815] Loss: 47.51869453734755\n",
      "Iteracion: 577 Gradiente: [0.29623798366417825,-5.110904701829918] Loss: 47.49246451836239\n",
      "Iteracion: 578 Gradiente: [0.29608049445906154,-5.108187588012418] Loss: 47.46626238133029\n",
      "Iteracion: 579 Gradiente: [0.2959230889800821,-5.105471918696026] Loss: 47.44008809661325\n",
      "Iteracion: 580 Gradiente: [0.2957657671826714,-5.102757693112798] Loss: 47.41394163460488\n",
      "Iteracion: 581 Gradiente: [0.2956085290223553,-5.1000449104952015] Loss: 47.38782296573025\n",
      "Iteracion: 582 Gradiente: [0.2954513744547114,-5.097333570076112] Loss: 47.36173206044584\n",
      "Iteracion: 583 Gradiente: [0.2952943034352032,-5.0946236710888115] Loss: 47.33566888923948\n",
      "Iteracion: 584 Gradiente: [0.29513731591965,-5.09191521276699] Loss: 47.309633422630526\n",
      "Iteracion: 585 Gradiente: [0.2949804118632784,-5.089208194344754] Loss: 47.28362563116955\n",
      "Iteracion: 586 Gradiente: [0.2948235912220357,-5.086502615056598] Loss: 47.257645485438424\n",
      "Iteracion: 587 Gradiente: [0.2946668539512776,-5.083798474137449] Loss: 47.231692956050374\n",
      "Iteracion: 588 Gradiente: [0.2945102000070726,-5.081095770822602] Loss: 47.20576801364976\n",
      "Iteracion: 589 Gradiente: [0.29435362934480813,-5.078394504347805] Loss: 47.17987062891224\n",
      "Iteracion: 590 Gradiente: [0.2941971419203187,-5.075694673949177] Loss: 47.15400077254458\n",
      "Iteracion: 591 Gradiente: [0.2940407376893468,-5.072996278863261] Loss: 47.12815841528473\n",
      "Iteracion: 592 Gradiente: [0.2938844166077651,-5.070299318326993] Loss: 47.102343527901716\n",
      "Iteracion: 593 Gradiente: [0.2937281786310938,-5.067603791577741] Loss: 47.07655608119562\n",
      "Iteracion: 594 Gradiente: [0.29357202371546903,-5.064909697853237] Loss: 47.050796045997636\n",
      "Iteracion: 595 Gradiente: [0.29341595181653823,-5.062217036391655] Loss: 47.02506339316986\n",
      "Iteracion: 596 Gradiente: [0.29325996289025086,-5.059525806431554] Loss: 46.99935809360545\n",
      "Iteracion: 597 Gradiente: [0.29310405689241936,-5.056836007211911] Loss: 46.97368011822846\n",
      "Iteracion: 598 Gradiente: [0.2929482337790122,-5.054147637972094] Loss: 46.94802943799382\n",
      "Iteracion: 599 Gradiente: [0.2927924935059593,-5.0514606979518835] Loss: 46.92240602388744\n",
      "Iteracion: 600 Gradiente: [0.2926368360291939,-5.048775186391464] Loss: 46.89680984692595\n",
      "Iteracion: 601 Gradiente: [0.29248126130476254,-5.04609110253142] Loss: 46.871240878156875\n",
      "Iteracion: 602 Gradiente: [0.2923257692885597,-5.043408445612747] Loss: 46.84569908865845\n",
      "Iteracion: 603 Gradiente: [0.2921703599367374,-5.040727214876833] Loss: 46.82018444953975\n",
      "Iteracion: 604 Gradiente: [0.29201503320530076,-5.038047409565476] Loss: 46.79469693194042\n",
      "Iteracion: 605 Gradiente: [0.2918597890502715,-5.035369028920883] Loss: 46.76923650703092\n",
      "Iteracion: 606 Gradiente: [0.29170462742790426,-5.032692072185646] Loss: 46.7438031460123\n",
      "Iteracion: 607 Gradiente: [0.2915495482941412,-5.030016538602782] Loss: 46.71839682011615\n",
      "Iteracion: 608 Gradiente: [0.29139455160519245,-5.027342427415696] Loss: 46.693017500604796\n",
      "Iteracion: 609 Gradiente: [0.29123963731724084,-5.024669737868199] Loss: 46.66766515877096\n",
      "Iteracion: 610 Gradiente: [0.2910848053866241,-5.021998469204493] Loss: 46.64233976593797\n",
      "Iteracion: 611 Gradiente: [0.2909300557693122,-5.019328620669213] Loss: 46.617041293459614\n",
      "Iteracion: 612 Gradiente: [0.2907753884215474,-5.016660191507367] Loss: 46.5917697127201\n",
      "Iteracion: 613 Gradiente: [0.29062080329979806,-5.0139931809643645] Loss: 46.56652499513413\n",
      "Iteracion: 614 Gradiente: [0.2904663003602984,-5.011327588286027] Loss: 46.54130711214671\n",
      "Iteracion: 615 Gradiente: [0.29031187955921284,-5.008663412718582] Loss: 46.51611603523323\n",
      "Iteracion: 616 Gradiente: [0.2901575408529337,-5.006000653508648] Loss: 46.490951735899394\n",
      "Iteracion: 617 Gradiente: [0.29000328419791427,-5.003339309903237] Loss: 46.46581418568123\n",
      "Iteracion: 618 Gradiente: [0.28984910955043686,-5.000679381149779] Loss: 46.440703356144994\n",
      "Iteracion: 619 Gradiente: [0.289695016866956,-4.99802086649609] Loss: 46.41561921888714\n",
      "Iteracion: 620 Gradiente: [0.2895410061039357,-4.995363765190391] Loss: 46.39056174553437\n",
      "Iteracion: 621 Gradiente: [0.2893870772177053,-4.99270807648131] Loss: 46.36553090774351\n",
      "Iteracion: 622 Gradiente: [0.2892332301647765,-4.99005379961786] Loss: 46.34052667720153\n",
      "Iteracion: 623 Gradiente: [0.28907946490164277,-4.987400933849468] Loss: 46.31554902562549\n",
      "Iteracion: 624 Gradiente: [0.28892578138488645,-4.984749478425942] Loss: 46.290597924762515\n",
      "Iteracion: 625 Gradiente: [0.28877217957092327,-4.98209943259751] Loss: 46.265673346389725\n",
      "Iteracion: 626 Gradiente: [0.2886186594164741,-4.979450795614781] Loss: 46.24077526231434\n",
      "Iteracion: 627 Gradiente: [0.2884652208780435,-4.97680356672877] Loss: 46.21590364437341\n",
      "Iteracion: 628 Gradiente: [0.288311863912286,-4.974157745190889] Loss: 46.19105846443407\n",
      "Iteracion: 629 Gradiente: [0.2881585884757778,-4.971513330252951] Loss: 46.16623969439326\n",
      "Iteracion: 630 Gradiente: [0.2880053945251672,-4.968870321167164] Loss: 46.141447306177774\n",
      "Iteracion: 631 Gradiente: [0.28785228201717433,-4.9662287171861355] Loss: 46.116681271744355\n",
      "Iteracion: 632 Gradiente: [0.287699250908535,-4.963588517562864] Loss: 46.09194156307946\n",
      "Iteracion: 633 Gradiente: [0.28754630115592855,-4.960949721550751] Loss: 46.06722815219936\n",
      "Iteracion: 634 Gradiente: [0.2873934327160152,-4.9583123284036015] Loss: 46.04254101115006\n",
      "Iteracion: 635 Gradiente: [0.2872406455456925,-4.955676337375603] Loss: 46.01788011200733\n",
      "Iteracion: 636 Gradiente: [0.28708793960176493,-4.953041747721341] Loss: 45.99324542687652\n",
      "Iteracion: 637 Gradiente: [0.28693531484100776,-4.950408558695808] Loss: 45.968636927892746\n",
      "Iteracion: 638 Gradiente: [0.286782771220205,-4.947776769554388] Loss: 45.944054587220656\n",
      "Iteracion: 639 Gradiente: [0.28663030869621714,-4.945146379552864] Loss: 45.91949837705452\n",
      "Iteracion: 640 Gradiente: [0.28647792722621734,-4.942517387947388] Loss: 45.8949682696182\n",
      "Iteracion: 641 Gradiente: [0.28632562676660506,-4.939889793994561] Loss: 45.87046423716501\n",
      "Iteracion: 642 Gradiente: [0.28617340727473767,-4.937263596951326] Loss: 45.8459862519778\n",
      "Iteracion: 643 Gradiente: [0.28602126870737904,-4.934638796075052] Loss: 45.8215342863689\n",
      "Iteracion: 644 Gradiente: [0.2858692110216438,-4.932015390623488] Loss: 45.79710831268006\n",
      "Iteracion: 645 Gradiente: [0.285717234174322,-4.9293933798547895] Loss: 45.77270830328238\n",
      "Iteracion: 646 Gradiente: [0.28556533812260343,-4.926772763027499] Loss: 45.74833423057641\n",
      "Iteracion: 647 Gradiente: [0.2854135228235002,-4.92415353940055] Loss: 45.72398606699195\n",
      "Iteracion: 648 Gradiente: [0.2852617882341003,-4.921535708233277] Loss: 45.69966378498821\n",
      "Iteracion: 649 Gradiente: [0.2851101343114768,-4.9189192687854035] Loss: 45.67536735705358\n",
      "Iteracion: 650 Gradiente: [0.28495856101260153,-4.916304220317061] Loss: 45.65109675570575\n",
      "Iteracion: 651 Gradiente: [0.28480706829492725,-4.9136905620887426] Loss: 45.626851953491624\n",
      "Iteracion: 652 Gradiente: [0.2846556561152139,-4.911078293361376] Loss: 45.60263292298724\n",
      "Iteracion: 653 Gradiente: [0.2845043244310631,-4.908467413396237] Loss: 45.57843963679782\n",
      "Iteracion: 654 Gradiente: [0.28435307319946784,-4.905857921455026] Loss: 45.554272067557726\n",
      "Iteracion: 655 Gradiente: [0.28420190237746634,-4.903249816799843] Loss: 45.53013018793037\n",
      "Iteracion: 656 Gradiente: [0.28405081192273396,-4.900643098693136] Loss: 45.50601397060823\n",
      "Iteracion: 657 Gradiente: [0.2838998017921933,-4.898037766397794] Loss: 45.481923388312865\n",
      "Iteracion: 658 Gradiente: [0.28374887194333787,-4.895433819177068] Loss: 45.457858413794746\n",
      "Iteracion: 659 Gradiente: [0.2835980223333564,-4.892831256294615] Loss: 45.43381901983334\n",
      "Iteracion: 660 Gradiente: [0.2834472529197633,-4.890230077014469] Loss: 45.4098051792371\n",
      "Iteracion: 661 Gradiente: [0.2832965636596924,-4.887630280601078] Loss: 45.3858168648433\n",
      "Iteracion: 662 Gradiente: [0.28314595451085345,-4.8850318663192525] Loss: 45.36185404951818\n",
      "Iteracion: 663 Gradiente: [0.2829954254302294,-4.882434833434229] Loss: 45.33791670615669\n",
      "Iteracion: 664 Gradiente: [0.28284497637562206,-4.879839181211592] Loss: 45.31400480768276\n",
      "Iteracion: 665 Gradiente: [0.2826946073043539,-4.87724490891735] Loss: 45.29011832704897\n",
      "Iteracion: 666 Gradiente: [0.28254431817383835,-4.874652015817896] Loss: 45.26625723723672\n",
      "Iteracion: 667 Gradiente: [0.2823941089416664,-4.872060501179997] Loss: 45.24242151125608\n",
      "Iteracion: 668 Gradiente: [0.2822439795654264,-4.869470364270821] Loss: 45.21861112214584\n",
      "Iteracion: 669 Gradiente: [0.2820939300025299,-4.866881604357936] Loss: 45.194826042973475\n",
      "Iteracion: 670 Gradiente: [0.2819439602104898,-4.864294220709282] Loss: 45.17106624683502\n",
      "Iteracion: 671 Gradiente: [0.28179407014704483,-4.861708212593194] Loss: 45.14733170685516\n",
      "Iteracion: 672 Gradiente: [0.2816442597697735,-4.859123579278397] Loss: 45.12362239618714\n",
      "Iteracion: 673 Gradiente: [0.2814945290361929,-4.856540320034009] Loss: 45.0999382880127\n",
      "Iteracion: 674 Gradiente: [0.2813448779041894,-4.853958434129523] Loss: 45.076279355542205\n",
      "Iteracion: 675 Gradiente: [0.28119530633125506,-4.851377920834837] Loss: 45.05264557201438\n",
      "Iteracion: 676 Gradiente: [0.2810458142751545,-4.84879877942023] Loss: 45.02903691069639\n",
      "Iteracion: 677 Gradiente: [0.28089640169367935,-4.846221009156357] Loss: 45.005453344883925\n",
      "Iteracion: 678 Gradiente: [0.2807470685444535,-4.8436446093142855] Loss: 44.98189484790093\n",
      "Iteracion: 679 Gradiente: [0.28059781478540863,-4.841069579165446] Loss: 44.9583613930998\n",
      "Iteracion: 680 Gradiente: [0.28044864037412887,-4.838495917981683] Loss: 44.93485295386128\n",
      "Iteracion: 681 Gradiente: [0.2802995452686313,-4.835923625035195] Loss: 44.91136950359428\n",
      "Iteracion: 682 Gradiente: [0.28015052942666896,-4.833352699598595] Loss: 44.8879110157361\n",
      "Iteracion: 683 Gradiente: [0.2800015928060072,-4.8307831409448765] Loss: 44.8644774637522\n",
      "Iteracion: 684 Gradiente: [0.279852735364747,-4.828214948347405] Loss: 44.8410688211363\n",
      "Iteracion: 685 Gradiente: [0.2797039570606529,-4.825648121079945] Loss: 44.81768506141025\n",
      "Iteracion: 686 Gradiente: [0.2795552578516007,-4.823082658416656] Loss: 44.79432615812411\n",
      "Iteracion: 687 Gradiente: [0.2794066376956489,-4.820518559632062] Loss: 44.77099208485597\n",
      "Iteracion: 688 Gradiente: [0.2792580965507109,-4.8179558240010865] Loss: 44.74768281521202\n",
      "Iteracion: 689 Gradiente: [0.27910963437483194,-4.815394450799037] Loss: 44.724398322826616\n",
      "Iteracion: 690 Gradiente: [0.27896125112592546,-4.812834439301603] Loss: 44.701138581362\n",
      "Iteracion: 691 Gradiente: [0.2788129467622196,-4.810275788784855] Loss: 44.6779035645085\n",
      "Iteracion: 692 Gradiente: [0.27866472124166536,-4.807718498525261] Loss: 44.65469324598436\n",
      "Iteracion: 693 Gradiente: [0.2785165745222938,-4.805162567799663] Loss: 44.63150759953578\n",
      "Iteracion: 694 Gradiente: [0.27836850656227247,-4.802607995885295] Loss: 44.60834659893688\n",
      "Iteracion: 695 Gradiente: [0.27822051731967223,-4.800054782059772] Loss: 44.585210217989626\n",
      "Iteracion: 696 Gradiente: [0.2780726067527174,-4.797502925601086] Loss: 44.56209843052386\n",
      "Iteracion: 697 Gradiente: [0.27792477481963235,-4.794952425787619] Loss: 44.53901121039724\n",
      "Iteracion: 698 Gradiente: [0.27777702147849204,-4.792403281898142] Loss: 44.51594853149524\n",
      "Iteracion: 699 Gradiente: [0.27762934668757777,-4.789855493211803] Loss: 44.49291036773097\n",
      "Iteracion: 700 Gradiente: [0.2774817504051929,-4.787309059008127] Loss: 44.46989669304545\n",
      "Iteracion: 701 Gradiente: [0.2773342325893573,-4.784763978567046] Loss: 44.44690748140725\n",
      "Iteracion: 702 Gradiente: [0.2771867931985954,-4.782220251168843] Loss: 44.423942706812696\n",
      "Iteracion: 703 Gradiente: [0.2770394321911463,-4.779677876094204] Loss: 44.40100234328573\n",
      "Iteracion: 704 Gradiente: [0.27689214952536406,-4.77713685262419] Loss: 44.378086364877895\n",
      "Iteracion: 705 Gradiente: [0.2767449451595612,-4.774597180040249] Loss: 44.35519474566835\n",
      "Iteracion: 706 Gradiente: [0.2765978190521816,-4.772058857624207] Loss: 44.332327459763775\n",
      "Iteracion: 707 Gradiente: [0.27645077116139066,-4.769521884658284] Loss: 44.309484481298355\n",
      "Iteracion: 708 Gradiente: [0.27630380144581584,-4.766986260425057] Loss: 44.286665784433815\n",
      "Iteracion: 709 Gradiente: [0.2761569098639747,-4.764451984207496] Loss: 44.26387134335937\n",
      "Iteracion: 710 Gradiente: [0.27601009637409085,-4.76191905528897] Loss: 44.241101132291554\n",
      "Iteracion: 711 Gradiente: [0.27586336093468233,-4.759387472953205] Loss: 44.218355125474424\n",
      "Iteracion: 712 Gradiente: [0.2757167035045306,-4.756857236484308] Loss: 44.19563329717941\n",
      "Iteracion: 713 Gradiente: [0.275570124041829,-4.754328345166791] Loss: 44.17293562170521\n",
      "Iteracion: 714 Gradiente: [0.2754236225053925,-4.751800798285515] Loss: 44.15026207337791\n",
      "Iteracion: 715 Gradiente: [0.2752771988536021,-4.7492745951257485] Loss: 44.12761262655085\n",
      "Iteracion: 716 Gradiente: [0.2751308530451231,-4.746749734973121] Loss: 44.104987255604655\n",
      "Iteracion: 717 Gradiente: [0.2749845850385386,-4.744226217113652] Loss: 44.082385934947176\n",
      "Iteracion: 718 Gradiente: [0.27483839479255917,-4.741704040833733] Loss: 44.059808639013454\n",
      "Iteracion: 719 Gradiente: [0.2746922822658147,-4.739183205420143] Loss: 44.037255342265745\n",
      "Iteracion: 720 Gradiente: [0.27454624741688227,-4.73666371016004] Loss: 44.014726019193404\n",
      "Iteracion: 721 Gradiente: [0.2744002902046537,-4.734145554340947] Loss: 43.992220644312944\n",
      "Iteracion: 722 Gradiente: [0.2742544105877599,-4.731628737250782] Loss: 43.9697391921679\n",
      "Iteracion: 723 Gradiente: [0.2741086085248478,-4.729113258177837] Loss: 43.94728163732897\n",
      "Iteracion: 724 Gradiente: [0.2739628839748505,-4.726599116410781] Loss: 43.924847954393776\n",
      "Iteracion: 725 Gradiente: [0.27381723689654225,-4.724086311238654] Loss: 43.90243811798703\n",
      "Iteracion: 726 Gradiente: [0.2736716672487037,-4.721574841950891] Loss: 43.88005210276038\n",
      "Iteracion: 727 Gradiente: [0.27352617499006443,-4.719064707837288] Loss: 43.857689883392354\n",
      "Iteracion: 728 Gradiente: [0.27338076007967277,-4.716555908188027] Loss: 43.835351434588524\n",
      "Iteracion: 729 Gradiente: [0.2732354224762605,-4.714048442293668] Loss: 43.81303673108129\n",
      "Iteracion: 730 Gradiente: [0.2730901621386631,-4.71154230944515] Loss: 43.79074574762985\n",
      "Iteracion: 731 Gradiente: [0.27294497902612186,-4.70903750893377] Loss: 43.768478459020365\n",
      "Iteracion: 732 Gradiente: [0.2727998730972511,-4.706534040051233] Loss: 43.74623484006564\n",
      "Iteracion: 733 Gradiente: [0.2726548443112203,-4.704031902089592] Loss: 43.72401486560537\n",
      "Iteracion: 734 Gradiente: [0.2725098926268965,-4.701531094341296] Loss: 43.70181851050597\n",
      "Iteracion: 735 Gradiente: [0.2723650180033393,-4.6990316160991625] Loss: 43.67964574966054\n",
      "Iteracion: 736 Gradiente: [0.2722202203995581,-4.696533466656385] Loss: 43.657496557988914\n",
      "Iteracion: 737 Gradiente: [0.2720754997747001,-4.694036645306524] Loss: 43.635370910437565\n",
      "Iteracion: 738 Gradiente: [0.27193085608766765,-4.69154115134354] Loss: 43.613268781979535\n",
      "Iteracion: 739 Gradiente: [0.27178628929775855,-4.689046984061742] Loss: 43.59119014761462\n",
      "Iteracion: 740 Gradiente: [0.2716417993639399,-4.686554142755828] Loss: 43.56913498236903\n",
      "Iteracion: 741 Gradiente: [0.2714973862454488,-4.684062626720868] Loss: 43.54710326129561\n",
      "Iteracion: 742 Gradiente: [0.27135304990136716,-4.681572435252312] Loss: 43.52509495947369\n",
      "Iteracion: 743 Gradiente: [0.2712087902908832,-4.679083567645978] Loss: 43.50311005200911\n",
      "Iteracion: 744 Gradiente: [0.27106460737326094,-4.676596023198056] Loss: 43.481148514034174\n",
      "Iteracion: 745 Gradiente: [0.2709205011076861,-4.674109801205118] Loss: 43.45921032070762\n",
      "Iteracion: 746 Gradiente: [0.2707764714534645,-4.671624900964105] Loss: 43.437295447214545\n",
      "Iteracion: 747 Gradiente: [0.2706325183697852,-4.669141321772336] Loss: 43.415403868766504\n",
      "Iteracion: 748 Gradiente: [0.270488641815997,-4.666659062927495] Loss: 43.39353556060132\n",
      "Iteracion: 749 Gradiente: [0.2703448417515311,-4.66417812372764] Loss: 43.3716904979832\n",
      "Iteracion: 750 Gradiente: [0.27020111813549025,-4.661698503471227] Loss: 43.34986865620264\n",
      "Iteracion: 751 Gradiente: [0.27005747092736765,-4.65922020145705] Loss: 43.32807001057636\n",
      "Iteracion: 752 Gradiente: [0.2699139000864259,-4.656743216984296] Loss: 43.30629453644731\n",
      "Iteracion: 753 Gradiente: [0.2697704055722286,-4.6542675493525145] Loss: 43.284542209184714\n",
      "Iteracion: 754 Gradiente: [0.2696269873441122,-4.651793197861638] Loss: 43.26281300418399\n",
      "Iteracion: 755 Gradiente: [0.26948364536144237,-4.649320161811968] Loss: 43.24110689686656\n",
      "Iteracion: 756 Gradiente: [0.26934037958394724,-4.646848440504162] Loss: 43.21942386268017\n",
      "Iteracion: 757 Gradiente: [0.26919718997074255,-4.644378033239282] Loss: 43.19776387709853\n",
      "Iteracion: 758 Gradiente: [0.2690540764816845,-4.641908939318726] Loss: 43.17612691562146\n",
      "Iteracion: 759 Gradiente: [0.2689110390760708,-4.63944115804429] Loss: 43.154512953774834\n",
      "Iteracion: 760 Gradiente: [0.2687680777134633,-4.63697468871813] Loss: 43.13292196711054\n",
      "Iteracion: 761 Gradiente: [0.2686251923536183,-4.634509530642765] Loss: 43.11135393120644\n",
      "Iteracion: 762 Gradiente: [0.26848238295599053,-4.632045683121103] Loss: 43.08980882166637\n",
      "Iteracion: 763 Gradiente: [0.2683396494802148,-4.629583145456412] Loss: 43.06828661412009\n",
      "Iteracion: 764 Gradiente: [0.26819699188583523,-4.627121916952332] Loss: 43.04678728422324\n",
      "Iteracion: 765 Gradiente: [0.2680544101327598,-4.6246619969128675] Loss: 43.025310807657405\n",
      "Iteracion: 766 Gradiente: [0.2679119041804519,-4.622203384642406] Loss: 43.003857160129975\n",
      "Iteracion: 767 Gradiente: [0.26776947398870354,-4.619746079445691] Loss: 42.98242631737413\n",
      "Iteracion: 768 Gradiente: [0.2676271195171954,-4.617290080627848] Loss: 42.961018255148936\n",
      "Iteracion: 769 Gradiente: [0.2674848407256436,-4.614835387494365] Loss: 42.93963294923913\n",
      "Iteracion: 770 Gradiente: [0.2673426375739924,-4.61238199935109] Loss: 42.918270375455236\n",
      "Iteracion: 771 Gradiente: [0.26720051002189604,-4.609929915504261] Loss: 42.89693050963352\n",
      "Iteracion: 772 Gradiente: [0.2670584580290895,-4.607479135260474] Loss: 42.87561332763587\n",
      "Iteracion: 773 Gradiente: [0.2669164815555151,-4.605029657926689] Loss: 42.85431880534987\n",
      "Iteracion: 774 Gradiente: [0.2667745805609651,-4.602581482810241] Loss: 42.83304691868872\n",
      "Iteracion: 775 Gradiente: [0.2666327550053938,-4.600134609218829] Loss: 42.81179764359125\n",
      "Iteracion: 776 Gradiente: [0.2664910048485491,-4.597689036460528] Loss: 42.79057095602183\n",
      "Iteracion: 777 Gradiente: [0.2663493300504854,-4.595244763843772] Loss: 42.76936683197043\n",
      "Iteracion: 778 Gradiente: [0.26620773057103053,-4.592801790677366] Loss: 42.74818524745246\n",
      "Iteracion: 779 Gradiente: [0.2660662063703067,-4.590360116270479] Loss: 42.72702617850891\n",
      "Iteracion: 780 Gradiente: [0.2659247574081858,-4.587919739932653] Loss: 42.705889601206174\n",
      "Iteracion: 781 Gradiente: [0.2657833836446869,-4.58548066097379] Loss: 42.68477549163615\n",
      "Iteracion: 782 Gradiente: [0.26564208503980924,-4.583042878704176] Loss: 42.6636838259161\n",
      "Iteracion: 783 Gradiente: [0.2655008615535894,-4.580606392434439] Loss: 42.642614580188685\n",
      "Iteracion: 784 Gradiente: [0.26535971314614104,-4.578171201475592] Loss: 42.62156773062194\n",
      "Iteracion: 785 Gradiente: [0.26521863977740023,-4.57573730513901] Loss: 42.6005432534092\n",
      "Iteracion: 786 Gradiente: [0.26507764140768325,-4.573304702736425] Loss: 42.57954112476917\n",
      "Iteracion: 787 Gradiente: [0.2649367179970241,-4.570873393579946] Loss: 42.558561320945735\n",
      "Iteracion: 788 Gradiente: [0.2647958695055432,-4.568443376982045] Loss: 42.53760381820815\n",
      "Iteracion: 789 Gradiente: [0.2646550958935265,-4.566014652255551] Loss: 42.51666859285076\n",
      "Iteracion: 790 Gradiente: [0.2645143971210189,-4.563587218713676] Loss: 42.495755621193275\n",
      "Iteracion: 791 Gradiente: [0.2643737731482678,-4.56116107566998] Loss: 42.47486487958039\n",
      "Iteracion: 792 Gradiente: [0.26423322393557813,-4.5587362224383945] Loss: 42.45399634438209\n",
      "Iteracion: 793 Gradiente: [0.2640927494431097,-4.556312658333222] Loss: 42.43314999199345\n",
      "Iteracion: 794 Gradiente: [0.2639523496312838,-4.5538903826691115] Loss: 42.41232579883457\n",
      "Iteracion: 795 Gradiente: [0.2638120244602933,-4.551469394761097] Loss: 42.391523741350696\n",
      "Iteracion: 796 Gradiente: [0.2636717738904302,-4.549049693924568] Loss: 42.37074379601204\n",
      "Iteracion: 797 Gradiente: [0.2635315978820602,-4.546631279475277] Loss: 42.34998593931387\n",
      "Iteracion: 798 Gradiente: [0.2633914963955623,-4.544214150729338] Loss: 42.32925014777644\n",
      "Iteracion: 799 Gradiente: [0.2632514693913405,-4.541798307003235] Loss: 42.30853639794499\n",
      "Iteracion: 800 Gradiente: [0.2631115168297325,-4.539383747613812] Loss: 42.287844666389574\n",
      "Iteracion: 801 Gradiente: [0.2629716386712493,-4.536970471878272] Loss: 42.267174929705305\n",
      "Iteracion: 802 Gradiente: [0.2628318348762536,-4.534558479114188] Loss: 42.24652716451203\n",
      "Iteracion: 803 Gradiente: [0.26269210540526705,-4.532147768639493] Loss: 42.22590134745461\n",
      "Iteracion: 804 Gradiente: [0.2625524502188429,-4.529738339772479] Loss: 42.20529745520256\n",
      "Iteracion: 805 Gradiente: [0.26241286927726315,-4.527330191831813] Loss: 42.18471546445033\n",
      "Iteracion: 806 Gradiente: [0.2622733625413519,-4.524923324136504] Loss: 42.16415535191709\n",
      "Iteracion: 807 Gradiente: [0.2621339299713668,-4.522517736005944] Loss: 42.143617094346716\n",
      "Iteracion: 808 Gradiente: [0.261994571528084,-4.520113426759873] Loss: 42.123100668507874\n",
      "Iteracion: 809 Gradiente: [0.2618552871719454,-4.517710395718399] Loss: 42.10260605119389\n",
      "Iteracion: 810 Gradiente: [0.26171607686370374,-4.515308642201985] Loss: 42.08213321922278\n",
      "Iteracion: 811 Gradiente: [0.26157694056393743,-4.512908165531463] Loss: 42.06168214943719\n",
      "Iteracion: 812 Gradiente: [0.2614378782332866,-4.510508965028017] Loss: 42.04125281870434\n",
      "Iteracion: 813 Gradiente: [0.261298889832423,-4.508111040013205] Loss: 42.020845203916124\n",
      "Iteracion: 814 Gradiente: [0.2611599753221088,-4.505714389808935] Loss: 42.00045928198893\n",
      "Iteracion: 815 Gradiente: [0.2610211346629711,-4.503319013737477] Loss: 41.980095029863755\n",
      "Iteracion: 816 Gradiente: [0.260882367815749,-4.5009249111214675] Loss: 41.95975242450594\n",
      "Iteracion: 817 Gradiente: [0.26074367474131016,-4.49853208128389] Loss: 41.93943144290556\n",
      "Iteracion: 818 Gradiente: [0.2606050554004028,-4.496140523548102] Loss: 41.91913206207697\n",
      "Iteracion: 819 Gradiente: [0.2604665097537577,-4.4937502372378155] Loss: 41.898854259059\n",
      "Iteracion: 820 Gradiente: [0.26032803776217295,-4.491361221677101] Loss: 41.8785980109149\n",
      "Iteracion: 821 Gradiente: [0.26018963938650497,-4.488973476190396] Loss: 41.858363294732285\n",
      "Iteracion: 822 Gradiente: [0.26005131458781533,-4.486587000102471] Loss: 41.83815008762319\n",
      "Iteracion: 823 Gradiente: [0.2599130633267445,-4.484201792738491] Loss: 41.81795836672386\n",
      "Iteracion: 824 Gradiente: [0.2597748855643921,-4.4818178534239586] Loss: 41.79778810919496\n",
      "Iteracion: 825 Gradiente: [0.25963678126150047,-4.47943518148474] Loss: 41.77763929222135\n",
      "Iteracion: 826 Gradiente: [0.2594987503790082,-4.477053776247065] Loss: 41.757511893012186\n",
      "Iteracion: 827 Gradiente: [0.2593607928780865,-4.4746736370375055] Loss: 41.737405888800836\n",
      "Iteracion: 828 Gradiente: [0.259222908719614,-4.4722947631830055] Loss: 41.71732125684488\n",
      "Iteracion: 829 Gradiente: [0.2590850978646041,-4.469917154010868] Loss: 41.69725797442609\n",
      "Iteracion: 830 Gradiente: [0.25894736027395027,-4.467540808848751] Loss: 41.67721601885031\n",
      "Iteracion: 831 Gradiente: [0.2588096959089467,-4.465165727024659] Loss: 41.657195367447585\n",
      "Iteracion: 832 Gradiente: [0.2586721047305398,-4.462791907866967] Loss: 41.63719599757202\n",
      "Iteracion: 833 Gradiente: [0.2585345866996799,-4.46041935070441] Loss: 41.617217886601836\n",
      "Iteracion: 834 Gradiente: [0.2583971417777635,-4.458048054866058] Loss: 41.59726101193923\n",
      "Iteracion: 835 Gradiente: [0.25825976992574884,-4.455678019681363] Loss: 41.57732535101047\n",
      "Iteracion: 836 Gradiente: [0.2581224711048771,-4.453309244480119] Loss: 41.55741088126583\n",
      "Iteracion: 837 Gradiente: [0.2579852452762263,-4.450941728592481] Loss: 41.5375175801795\n",
      "Iteracion: 838 Gradiente: [0.2578480924010248,-4.44857547134896] Loss: 41.51764542524965\n",
      "Iteracion: 839 Gradiente: [0.25771101244052114,-4.446210472080419] Loss: 41.49779439399838\n",
      "Iteracion: 840 Gradiente: [0.25757400535594166,-4.443846730118083] Loss: 41.47796446397165\n",
      "Iteracion: 841 Gradiente: [0.2574370711085791,-4.441484244793528] Loss: 41.45815561273934\n",
      "Iteracion: 842 Gradiente: [0.25730020965956013,-4.4391230154386925] Loss: 41.43836781789511\n",
      "Iteracion: 843 Gradiente: [0.25716342097028005,-4.436763041385858] Loss: 41.41860105705644\n",
      "Iteracion: 844 Gradiente: [0.25702670500221836,-4.434404321967662] Loss: 41.39885530786471\n",
      "Iteracion: 845 Gradiente: [0.2568900617163886,-4.432046856517119] Loss: 41.379130547984914\n",
      "Iteracion: 846 Gradiente: [0.2567534910744799,-4.429690644367564] Loss: 41.35942675510589\n",
      "Iteracion: 847 Gradiente: [0.2566169930375374,-4.427335684852718] Loss: 41.33974390694017\n",
      "Iteracion: 848 Gradiente: [0.25648056756727844,-4.424981977306626] Loss: 41.320081981223915\n",
      "Iteracion: 849 Gradiente: [0.2563442146249007,-4.422629521063721] Loss: 41.300440955717065\n",
      "Iteracion: 850 Gradiente: [0.25620793417204457,-4.420278315458755] Loss: 41.280820808203124\n",
      "Iteracion: 851 Gradiente: [0.25607172616993074,-4.417928359826865] Loss: 41.261221516489236\n",
      "Iteracion: 852 Gradiente: [0.25593559058020005,-4.415579653503519] Loss: 41.241643058406105\n",
      "Iteracion: 853 Gradiente: [0.25579952736436623,-4.413232195824546] Loss: 41.22208541180803\n",
      "Iteracion: 854 Gradiente: [0.2556635364839719,-4.410885986126124] Loss: 41.202548554572836\n",
      "Iteracion: 855 Gradiente: [0.2555276179004899,-4.408541023744798] Loss: 41.183032464601915\n",
      "Iteracion: 856 Gradiente: [0.2553917715753343,-4.40619730801746] Loss: 41.16353711982007\n",
      "Iteracion: 857 Gradiente: [0.2552559974704709,-4.40385483828133] Loss: 41.14406249817565\n",
      "Iteracion: 858 Gradiente: [0.25512029554721305,-4.401513613874019] Loss: 41.12460857764033\n",
      "Iteracion: 859 Gradiente: [0.25498466576719303,-4.3991736341334695] Loss: 41.10517533620931\n",
      "Iteracion: 860 Gradiente: [0.2548491080922228,-4.3968348983979695] Loss: 41.085762751901136\n",
      "Iteracion: 861 Gradiente: [0.254713622483798,-4.3944974060061766] Loss: 41.066370802757724\n",
      "Iteracion: 862 Gradiente: [0.25457820890374283,-4.3921611562970835] Loss: 41.04699946684432\n",
      "Iteracion: 863 Gradiente: [0.25444286731361765,-4.389826148610054] Loss: 41.02764872224954\n",
      "Iteracion: 864 Gradiente: [0.25430759767526917,-4.387492382284781] Loss: 41.00831854708519\n",
      "Iteracion: 865 Gradiente: [0.25417239995044955,-4.3851598566613195] Loss: 40.989008919486466\n",
      "Iteracion: 866 Gradiente: [0.2540372741008556,-4.382828571080075] Loss: 40.969719817611704\n",
      "Iteracion: 867 Gradiente: [0.25390222008830177,-4.380498524881807] Loss: 40.95045121964249\n",
      "Iteracion: 868 Gradiente: [0.25376723787472005,-4.3781697174076095] Loss: 40.93120310378364\n",
      "Iteracion: 869 Gradiente: [0.25363232742174946,-4.375842147998954] Loss: 40.911975448263064\n",
      "Iteracion: 870 Gradiente: [0.25349748869128325,-4.373515815997641] Loss: 40.89276823133189\n",
      "Iteracion: 871 Gradiente: [0.25336272164527546,-4.371190720745823] Loss: 40.87358143126433\n",
      "Iteracion: 872 Gradiente: [0.253228026245507,-4.368866861586013] Loss: 40.85441502635764\n",
      "Iteracion: 873 Gradiente: [0.2530934024540251,-4.366544237861058] Loss: 40.83526899493229\n",
      "Iteracion: 874 Gradiente: [0.2529588502326324,-4.364222848914171] Loss: 40.81614331533164\n",
      "Iteracion: 875 Gradiente: [0.2528243695432863,-4.361902694088906] Loss: 40.79703796592212\n",
      "Iteracion: 876 Gradiente: [0.252689960348061,-4.359583772729156] Loss: 40.77795292509321\n",
      "Iteracion: 877 Gradiente: [0.25255562260888503,-4.357266084179185] Loss: 40.75888817125732\n",
      "Iteracion: 878 Gradiente: [0.25242135628773427,-4.354949627783586] Loss: 40.739843682849774\n",
      "Iteracion: 879 Gradiente: [0.2522871613466909,-4.352634402887309] Loss: 40.72081943832884\n",
      "Iteracion: 880 Gradiente: [0.2521530377479714,-4.350320408835646] Loss: 40.701815416175776\n",
      "Iteracion: 881 Gradiente: [0.25201898545324963,-4.348007644974259] Loss: 40.68283159489454\n",
      "Iteracion: 882 Gradiente: [0.25188500442493456,-4.345696110649126] Loss: 40.6638679530121\n",
      "Iteracion: 883 Gradiente: [0.2517510946250672,-4.343385805206589] Loss: 40.64492446907818\n",
      "Iteracion: 884 Gradiente: [0.2516172560156354,-4.341076727993349] Loss: 40.62600112166525\n",
      "Iteracion: 885 Gradiente: [0.2514834885590389,-4.338768878356424] Loss: 40.607097889368674\n",
      "Iteracion: 886 Gradiente: [0.25134979221736276,-4.3364622556432035] Loss: 40.58821475080649\n",
      "Iteracion: 887 Gradiente: [0.2512161669526234,-4.3341568592014275] Loss: 40.56935168461949\n",
      "Iteracion: 888 Gradiente: [0.2510826127272542,-4.331852688379157] Loss: 40.55050866947117\n",
      "Iteracion: 889 Gradiente: [0.25094912950333825,-4.329549742524827] Loss: 40.531685684047694\n",
      "Iteracion: 890 Gradiente: [0.25081571724340984,-4.327248020987191] Loss: 40.5128827070579\n",
      "Iteracion: 891 Gradiente: [0.25068237590934195,-4.324947523115382] Loss: 40.494099717233226\n",
      "Iteracion: 892 Gradiente: [0.25054910546364345,-4.322648248258857] Loss: 40.47533669332776\n",
      "Iteracion: 893 Gradiente: [0.25041590586860424,-4.3203501957674195] Loss: 40.45659361411813\n",
      "Iteracion: 894 Gradiente: [0.2502827770866612,-4.318053364991221] Loss: 40.43787045840358\n",
      "Iteracion: 895 Gradiente: [0.25014971907994715,-4.315757755280771] Loss: 40.41916720500585\n",
      "Iteracion: 896 Gradiente: [0.25001673181105727,-4.3134633659868955] Loss: 40.40048383276919\n",
      "Iteracion: 897 Gradiente: [0.24988381524215864,-4.311170196460807] Loss: 40.38182032056034\n",
      "Iteracion: 898 Gradiente: [0.2497509693358147,-4.3088782460540225] Loss: 40.36317664726854\n",
      "Iteracion: 899 Gradiente: [0.2496181940545578,-4.3065875141184184] Loss: 40.34455279180543\n",
      "Iteracion: 900 Gradiente: [0.24948548936057596,-4.304298000006231] Loss: 40.32594873310504\n",
      "Iteracion: 901 Gradiente: [0.24935285521650621,-4.3020097030700235] Loss: 40.3073644501239\n",
      "Iteracion: 902 Gradiente: [0.24922029158492595,-4.299722622662699] Loss: 40.28879992184079\n",
      "Iteracion: 903 Gradiente: [0.24908779842816575,-4.297436758137523] Loss: 40.270255127256924\n",
      "Iteracion: 904 Gradiente: [0.2489553757087331,-4.295152108848099] Loss: 40.25173004539576\n",
      "Iteracion: 905 Gradiente: [0.24882302338938136,-4.292868674148361] Loss: 40.23322465530311\n",
      "Iteracion: 906 Gradiente: [0.24869074143257752,-4.290586453392597] Loss: 40.21473893604706\n",
      "Iteracion: 907 Gradiente: [0.24855852980102877,-4.288305445935434] Loss: 40.1962728667179\n",
      "Iteracion: 908 Gradiente: [0.2484263884570339,-4.286025651131857] Loss: 40.177826426428176\n",
      "Iteracion: 909 Gradiente: [0.2482943173635635,-4.283747068337175] Loss: 40.15939959431267\n",
      "Iteracion: 910 Gradiente: [0.24816231648311107,-4.281469696907046] Loss: 40.14099234952827\n",
      "Iteracion: 911 Gradiente: [0.2480303857783781,-4.2791935361974724] Loss: 40.122604671254074\n",
      "Iteracion: 912 Gradiente: [0.24789852521213263,-4.276918585564796] Loss: 40.10423653869132\n",
      "Iteracion: 913 Gradiente: [0.24776673474696434,-4.274644844365706] Loss: 40.08588793106331\n",
      "Iteracion: 914 Gradiente: [0.24763501434563723,-4.27237231195723] Loss: 40.06755882761543\n",
      "Iteracion: 915 Gradiente: [0.24750336397091957,-4.270100987696738] Loss: 40.04924920761517\n",
      "Iteracion: 916 Gradiente: [0.2473717835855979,-4.267830870941938] Loss: 40.03095905035208\n",
      "Iteracion: 917 Gradiente: [0.2472402731523682,-4.2655619610508895] Loss: 40.01268833513762\n",
      "Iteracion: 918 Gradiente: [0.24710883263423136,-4.263294257381978] Loss: 39.99443704130532\n",
      "Iteracion: 919 Gradiente: [0.24697746199380505,-4.261027759293952] Loss: 39.976205148210674\n",
      "Iteracion: 920 Gradiente: [0.24684616119421296,-4.258762466145866] Loss: 39.95799263523117\n",
      "Iteracion: 921 Gradiente: [0.24671493019792598,-4.256498377297165] Loss: 39.93979948176604\n",
      "Iteracion: 922 Gradiente: [0.24658376896816492,-4.254235492107585] Loss: 39.92162566723661\n",
      "Iteracion: 923 Gradiente: [0.24645267746780655,-4.251973809937228] Loss: 39.903471171085975\n",
      "Iteracion: 924 Gradiente: [0.24632165565966962,-4.249713330146534] Loss: 39.88533597277912\n",
      "Iteracion: 925 Gradiente: [0.24619070350672936,-4.247454052096284] Loss: 39.86722005180285\n",
      "Iteracion: 926 Gradiente: [0.2460598209720035,-4.245195975147594] Loss: 39.849123387665784\n",
      "Iteracion: 927 Gradiente: [0.245929008018355,-4.242939098661925] Loss: 39.83104595989825\n",
      "Iteracion: 928 Gradiente: [0.2457982646089543,-4.240683422001066] Loss: 39.81298774805246\n",
      "Iteracion: 929 Gradiente: [0.24566759070681551,-4.238428944527156] Loss: 39.79494873170227\n",
      "Iteracion: 930 Gradiente: [0.24553698627489998,-4.236175665602673] Loss: 39.7769288904433\n",
      "Iteracion: 931 Gradiente: [0.24540645127631108,-4.233923584590429] Loss: 39.7589282038928\n",
      "Iteracion: 932 Gradiente: [0.2452759856741236,-4.231672700853579] Loss: 39.74094665168975\n",
      "Iteracion: 933 Gradiente: [0.2451455894314094,-4.229423013755618] Loss: 39.72298421349471\n",
      "Iteracion: 934 Gradiente: [0.24501526251133682,-4.227174522660371] Loss: 39.70504086898995\n",
      "Iteracion: 935 Gradiente: [0.24488500487713363,-4.224927226932002] Loss: 39.687116597879246\n",
      "Iteracion: 936 Gradiente: [0.24475481649184233,-4.222681125935023] Loss: 39.669211379887955\n",
      "Iteracion: 937 Gradiente: [0.24462469731870584,-4.220436219034281] Loss: 39.65132519476309\n",
      "Iteracion: 938 Gradiente: [0.2444946473208932,-4.2181925055949545] Loss: 39.63345802227306\n",
      "Iteracion: 939 Gradiente: [0.24436466646171104,-4.215949984982556] Loss: 39.61560984220785\n",
      "Iteracion: 940 Gradiente: [0.24423475470434397,-4.213708656562953] Loss: 39.59778063437894\n",
      "Iteracion: 941 Gradiente: [0.2441049120120584,-4.211468519702333] Loss: 39.579970378619244\n",
      "Iteracion: 942 Gradiente: [0.2439751383480034,-4.209229573767236] Loss: 39.562179054783094\n",
      "Iteracion: 943 Gradiente: [0.24384543367565997,-4.206991818124518] Loss: 39.544406642746246\n",
      "Iteracion: 944 Gradiente: [0.24371579795835555,-4.204755252141386] Loss: 39.526653122405904\n",
      "Iteracion: 945 Gradiente: [0.24358623115929987,-4.202519875185384] Loss: 39.508918473680545\n",
      "Iteracion: 946 Gradiente: [0.2434567332419661,-4.200285686624387] Loss: 39.49120267651006\n",
      "Iteracion: 947 Gradiente: [0.24332730416978285,-4.198052685826598] Loss: 39.473505710855676\n",
      "Iteracion: 948 Gradiente: [0.24319794390609342,-4.195820872160576] Loss: 39.455827556699866\n",
      "Iteracion: 949 Gradiente: [0.2430686524141862,-4.193590244995209] Loss: 39.43816819404642\n",
      "Iteracion: 950 Gradiente: [0.24293942965759713,-4.1913608036997125] Loss: 39.42052760292033\n",
      "Iteracion: 951 Gradiente: [0.24281027559974716,-4.189132547643644] Loss: 39.402905763367926\n",
      "Iteracion: 952 Gradiente: [0.2426811902042227,-4.186905476196886] Loss: 39.38530265545662\n",
      "Iteracion: 953 Gradiente: [0.2425521734344694,-4.184679588729669] Loss: 39.367718259275144\n",
      "Iteracion: 954 Gradiente: [0.24242322525409038,-4.182454884612546] Loss: 39.35015255493325\n",
      "Iteracion: 955 Gradiente: [0.24229434562647612,-4.1802313632164205] Loss: 39.33260552256195\n",
      "Iteracion: 956 Gradiente: [0.2421655345151424,-4.178009023912526] Loss: 39.31507714231337\n",
      "Iteracion: 957 Gradiente: [0.2420367918838044,-4.175787866072414] Loss: 39.297567394360605\n",
      "Iteracion: 958 Gradiente: [0.2419081176959298,-4.1735678890679955] Loss: 39.28007625889802\n",
      "Iteracion: 959 Gradiente: [0.24177951191530814,-4.171349092271486] Loss: 39.262603716140866\n",
      "Iteracion: 960 Gradiente: [0.24165097450535739,-4.169131475055464] Loss: 39.24514974632554\n",
      "Iteracion: 961 Gradiente: [0.24152250542985929,-4.166915036792824] Loss: 39.22771432970935\n",
      "Iteracion: 962 Gradiente: [0.2413941046524317,-4.164699776856801] Loss: 39.21029744657069\n",
      "Iteracion: 963 Gradiente: [0.24126577213693945,-4.162485694620945] Loss: 39.19289907720882\n",
      "Iteracion: 964 Gradiente: [0.24113750784691726,-4.16027278945917] Loss: 39.17551920194403\n",
      "Iteracion: 965 Gradiente: [0.24100931174611848,-4.158061060745706] Loss: 39.15815780111746\n",
      "Iteracion: 966 Gradiente: [0.2408811837982005,-4.155850507855117] Loss: 39.14081485509118\n",
      "Iteracion: 967 Gradiente: [0.24075312396715157,-4.153641130162293] Loss: 39.12349034424815\n",
      "Iteracion: 968 Gradiente: [0.24062513221650145,-4.151432927042473] Loss: 39.106184248992115\n",
      "Iteracion: 969 Gradiente: [0.24049720851037507,-4.149225897871202] Loss: 39.08889654974776\n",
      "Iteracion: 970 Gradiente: [0.24036935281225358,-4.147020042024392] Loss: 39.071627226960445\n",
      "Iteracion: 971 Gradiente: [0.24024156508619068,-4.1448153588782555] Loss: 39.05437626109645\n",
      "Iteracion: 972 Gradiente: [0.24011384529607557,-4.142611847809348] Loss: 39.03714363264272\n",
      "Iteracion: 973 Gradiente: [0.23998619340570926,-4.140409508194562] Loss: 39.01992932210701\n",
      "Iteracion: 974 Gradiente: [0.23985860937895537,-4.1382083394111175] Loss: 39.0027333100177\n",
      "Iteracion: 975 Gradiente: [0.23973109317975555,-4.136008340836564] Loss: 38.98555557692399\n",
      "Iteracion: 976 Gradiente: [0.23960364477215634,-4.133809511848778] Loss: 38.96839610339567\n",
      "Iteracion: 977 Gradiente: [0.2394762641200868,-4.131611851825974] Loss: 38.95125487002321\n",
      "Iteracion: 978 Gradiente: [0.23934895118746244,-4.129415360146692] Loss: 38.934131857417725\n",
      "Iteracion: 979 Gradiente: [0.23922170593825645,-4.1272200361898115] Loss: 38.91702704621093\n",
      "Iteracion: 980 Gradiente: [0.23909452833654493,-4.1250258793345305] Loss: 38.8999404170551\n",
      "Iteracion: 981 Gradiente: [0.23896741834626642,-4.122832888960388] Loss: 38.88287195062311\n",
      "Iteracion: 982 Gradiente: [0.23884037593159774,-4.12064106444724] Loss: 38.86582162760838\n",
      "Iteracion: 983 Gradiente: [0.2387134010565383,-4.118450405175283] Loss: 38.84878942872483\n",
      "Iteracion: 984 Gradiente: [0.2385864936852414,-4.116260910525034] Loss: 38.83177533470691\n",
      "Iteracion: 985 Gradiente: [0.2384596537819545,-4.11407257987734] Loss: 38.81477932630953\n",
      "Iteracion: 986 Gradiente: [0.23833288131052158,-4.111885412613393] Loss: 38.79780138430804\n",
      "Iteracion: 987 Gradiente: [0.23820617623529472,-4.109699408114698] Loss: 38.78084148949824\n",
      "Iteracion: 988 Gradiente: [0.2380795385202461,-4.1075145657631005] Loss: 38.7638996226964\n",
      "Iteracion: 989 Gradiente: [0.23795296812981376,-4.105330884940751] Loss: 38.746975764739084\n",
      "Iteracion: 990 Gradiente: [0.23782646502808003,-4.1031483650301555] Loss: 38.73006989648331\n",
      "Iteracion: 991 Gradiente: [0.2377000291792508,-4.100967005414139] Loss: 38.71318199880634\n",
      "Iteracion: 992 Gradiente: [0.237573660547541,-4.098786805475854] Loss: 38.69631205260588\n",
      "Iteracion: 993 Gradiente: [0.23744735909731793,-4.096607764598773] Loss: 38.67946003879992\n",
      "Iteracion: 994 Gradiente: [0.23732112479280143,-4.09442988216671] Loss: 38.66262593832664\n",
      "Iteracion: 995 Gradiente: [0.2371949575983687,-4.0922531575637935] Loss: 38.64580973214459\n",
      "Iteracion: 996 Gradiente: [0.2370688574782508,-4.0900775901744915] Loss: 38.62901140123249\n",
      "Iteracion: 997 Gradiente: [0.23694282439692885,-4.087903179383587] Loss: 38.612230926589355\n",
      "Iteracion: 998 Gradiente: [0.23681685831855243,-4.08572992457621] Loss: 38.59546828923429\n",
      "Iteracion: 999 Gradiente: [0.23669095920764532,-4.083557825137792] Loss: 38.57872347020668\n",
      "Iteracion: 1000 Gradiente: [0.23656512702854424,-4.08138688045411] Loss: 38.561996450566035\n",
      "Iteracion: 1001 Gradiente: [0.23643936174560973,-4.079217089911263] Loss: 38.54528721139192\n",
      "Iteracion: 1002 Gradiente: [0.23631366332353895,-4.077048452895659] Loss: 38.52859573378414\n",
      "Iteracion: 1003 Gradiente: [0.23618803172651823,-4.074880968794064] Loss: 38.511921998862476\n",
      "Iteracion: 1004 Gradiente: [0.23606246691912058,-4.072714636993547] Loss: 38.49526598776687\n",
      "Iteracion: 1005 Gradiente: [0.23593696886585083,-4.070549456881508] Loss: 38.47862768165727\n",
      "Iteracion: 1006 Gradiente: [0.23581153753112313,-4.068385427845683] Loss: 38.46200706171363\n",
      "Iteracion: 1007 Gradiente: [0.2356861728795901,-4.066222549274113] Loss: 38.44540410913592\n",
      "Iteracion: 1008 Gradiente: [0.23556087487573715,-4.064060820555183] Loss: 38.428818805144154\n",
      "Iteracion: 1009 Gradiente: [0.23543564348423895,-4.061900241077586] Loss: 38.41225113097822\n",
      "Iteracion: 1010 Gradiente: [0.2353104786695236,-4.059740810230365] Loss: 38.395701067897996\n",
      "Iteracion: 1011 Gradiente: [0.2351853803962643,-4.057582527402864] Loss: 38.37916859718327\n",
      "Iteracion: 1012 Gradiente: [0.23506034862903225,-4.055425391984766] Loss: 38.36265370013374\n",
      "Iteracion: 1013 Gradiente: [0.23493538333262975,-4.053269403366062] Loss: 38.34615635806897\n",
      "Iteracion: 1014 Gradiente: [0.23481048447156283,-4.05111456093709] Loss: 38.32967655232843\n",
      "Iteracion: 1015 Gradiente: [0.2346856520105483,-4.048960864088495] Loss: 38.31321426427129\n",
      "Iteracion: 1016 Gradiente: [0.2345608859143077,-4.046808312211251] Loss: 38.296769475276655\n",
      "Iteracion: 1017 Gradiente: [0.2344361861475754,-4.044656904696656] Loss: 38.28034216674343\n",
      "Iteracion: 1018 Gradiente: [0.23431155267508194,-4.042506640936334] Loss: 38.26393232009021\n",
      "Iteracion: 1019 Gradiente: [0.23418698546151476,-4.040357520322229] Loss: 38.24753991675541\n",
      "Iteracion: 1020 Gradiente: [0.2340624844718029,-4.038209542246605] Loss: 38.23116493819719\n",
      "Iteracion: 1021 Gradiente: [0.23393804967047913,-4.036062706102067] Loss: 38.214807365893314\n",
      "Iteracion: 1022 Gradiente: [0.23381368102253147,-4.033917011281519] Loss: 38.19846718134134\n",
      "Iteracion: 1023 Gradiente: [0.23368937849289892,-4.0317724571781905] Loss: 38.182144366058466\n",
      "Iteracion: 1024 Gradiente: [0.2335651420462651,-4.029629043185652] Loss: 38.165838901581495\n",
      "Iteracion: 1025 Gradiente: [0.23344097164754346,-4.027486768697787] Loss: 38.14955076946693\n",
      "Iteracion: 1026 Gradiente: [0.23331686726162695,-4.0253456331087945] Loss: 38.1332799512908\n",
      "Iteracion: 1027 Gradiente: [0.23319282885337103,-4.023205635813203] Loss: 38.117026428648764\n",
      "Iteracion: 1028 Gradiente: [0.233068856387658,-4.021066776205867] Loss: 38.10079018315606\n",
      "Iteracion: 1029 Gradiente: [0.2329449498296197,-4.018929053681942] Loss: 38.08457119644742\n",
      "Iteracion: 1030 Gradiente: [0.23282110914411547,-4.01679246763693] Loss: 38.06836945017715\n",
      "Iteracion: 1031 Gradiente: [0.23269733429619066,-4.014657017466636] Loss: 38.05218492601899\n",
      "Iteracion: 1032 Gradiente: [0.2325736252506734,-4.012522702567206] Loss: 38.036017605666224\n",
      "Iteracion: 1033 Gradiente: [0.23244998197265687,-4.01038952233509] Loss: 38.019867470831585\n",
      "Iteracion: 1034 Gradiente: [0.23232640442731498,-4.008257476167056] Loss: 38.00373450324722\n",
      "Iteracion: 1035 Gradiente: [0.23220289257949578,-4.006126563460213] Loss: 37.98761868466468\n",
      "Iteracion: 1036 Gradiente: [0.23207944639441488,-4.003996783611969] Loss: 37.971519996855\n",
      "Iteracion: 1037 Gradiente: [0.23195606583711925,-4.001868136020068] Loss: 37.95543842160851\n",
      "Iteracion: 1038 Gradiente: [0.23183275087262653,-3.999740620082569] Loss: 37.93937394073488\n",
      "Iteracion: 1039 Gradiente: [0.23170950146625888,-3.99761423519784] Loss: 37.92332653606319\n",
      "Iteracion: 1040 Gradiente: [0.23158631758289233,-3.995488980764593] Loss: 37.90729618944177\n",
      "Iteracion: 1041 Gradiente: [0.2314631991880314,-3.9933648561818282] Loss: 37.89128288273831\n",
      "Iteracion: 1042 Gradiente: [0.23134014624668234,-3.9912418608488927] Loss: 37.875286597839754\n",
      "Iteracion: 1043 Gradiente: [0.2312171587239727,-3.9891199941654425] Loss: 37.85930731665224\n",
      "Iteracion: 1044 Gradiente: [0.23109423658526584,-3.9869992555314506] Loss: 37.84334502110119\n",
      "Iteracion: 1045 Gradiente: [0.23097137979565285,-3.984879644347216] Loss: 37.82739969313126\n",
      "Iteracion: 1046 Gradiente: [0.2308485883205473,-3.982761160013344] Loss: 37.81147131470625\n",
      "Iteracion: 1047 Gradiente: [0.23072586212515014,-3.980643801930771] Loss: 37.79555986780916\n",
      "Iteracion: 1048 Gradiente: [0.23060320117471397,-3.97852756950075] Loss: 37.77966533444212\n",
      "Iteracion: 1049 Gradiente: [0.23048060543467178,-3.97641246212484] Loss: 37.76378769662645\n",
      "Iteracion: 1050 Gradiente: [0.2303580748702577,-3.9742984792049367] Loss: 37.747926936402486\n",
      "Iteracion: 1051 Gradiente: [0.2302356094469829,-3.972185620143236] Loss: 37.73208303582975\n",
      "Iteracion: 1052 Gradiente: [0.2301132091299318,-3.9700738843422743] Loss: 37.71625597698676\n",
      "Iteracion: 1053 Gradiente: [0.2299908738846407,-3.967963271204884] Loss: 37.70044574197116\n",
      "Iteracion: 1054 Gradiente: [0.22986860367660805,-3.9658537801342155] Loss: 37.68465231289951\n",
      "Iteracion: 1055 Gradiente: [0.22974639847117734,-3.963745410533751] Loss: 37.668875671907486\n",
      "Iteracion: 1056 Gradiente: [0.22962425823362528,-3.9616381618072922] Loss: 37.65311580114971\n",
      "Iteracion: 1057 Gradiente: [0.2295021829297814,-3.959532033358925] Loss: 37.637372682799814\n",
      "Iteracion: 1058 Gradiente: [0.22938017252475998,-3.957427024593097] Loss: 37.62164629905028\n",
      "Iteracion: 1059 Gradiente: [0.2292582269842697,-3.955323134914539] Loss: 37.605936632112645\n",
      "Iteracion: 1060 Gradiente: [0.22913634627382085,-3.953220363728309] Loss: 37.590243664217255\n",
      "Iteracion: 1061 Gradiente: [0.22901453035884364,-3.9511187104397902] Loss: 37.5745673776134\n",
      "Iteracion: 1062 Gradiente: [0.22889277920487341,-3.9490181744546726] Loss: 37.558907754569184\n",
      "Iteracion: 1063 Gradiente: [0.22877109277764685,-3.946918755178954] Loss: 37.543264777371654\n",
      "Iteracion: 1064 Gradiente: [0.2286494710426325,-3.944820452018966] Loss: 37.52763842832659\n",
      "Iteracion: 1065 Gradiente: [0.2285279139655169,-3.9427232643813426] Loss: 37.51202868975864\n",
      "Iteracion: 1066 Gradiente: [0.22840642151184057,-3.9406271916730415] Loss: 37.496435544011206\n",
      "Iteracion: 1067 Gradiente: [0.22828499364728477,-3.9385322333013306] Loss: 37.48085897344649\n",
      "Iteracion: 1068 Gradiente: [0.22816363033760326,-3.9364383886737886] Loss: 37.465298960445395\n",
      "Iteracion: 1069 Gradiente: [0.22804233154826742,-3.934345657198327] Loss: 37.44975548740761\n",
      "Iteracion: 1070 Gradiente: [0.22792109724513523,-3.9322540382831517] Loss: 37.434228536751476\n",
      "Iteracion: 1071 Gradiente: [0.22779992739387514,-3.930163531336793] Loss: 37.418718090914105\n",
      "Iteracion: 1072 Gradiente: [0.22767882196017364,-3.9280741357680977] Loss: 37.40322413235118\n",
      "Iteracion: 1073 Gradiente: [0.2275577809097977,-3.9259858509862218] Loss: 37.38774664353708\n",
      "Iteracion: 1074 Gradiente: [0.22743680420869813,-3.9238986764006296] Loss: 37.372285606964816\n",
      "Iteracion: 1075 Gradiente: [0.22731589182235723,-3.92181261142112] Loss: 37.356841005146\n",
      "Iteracion: 1076 Gradiente: [0.22719504371685276,-3.919727655457781] Loss: 37.341412820610884\n",
      "Iteracion: 1077 Gradiente: [0.2270742598579351,-3.917643807921027] Loss: 37.32600103590819\n",
      "Iteracion: 1078 Gradiente: [0.22695354021137792,-3.9155610682215882] Loss: 37.31060563360526\n",
      "Iteracion: 1079 Gradiente: [0.22683288474323338,-3.9134794357704967] Loss: 37.29522659628797\n",
      "Iteracion: 1080 Gradiente: [0.22671229341907473,-3.9113989099791184] Loss: 37.27986390656066\n",
      "Iteracion: 1081 Gradiente: [0.2265917662050967,-3.909319490259106] Loss: 37.264517547046204\n",
      "Iteracion: 1082 Gradiente: [0.22647130306699997,-3.9072411760224504] Loss: 37.249187500385915\n",
      "Iteracion: 1083 Gradiente: [0.22635090397074198,-3.9051639666814384] Loss: 37.23387374923959\n",
      "Iteracion: 1084 Gradiente: [0.2262305688824033,-3.9030878616486686] Loss: 37.21857627628547\n",
      "Iteracion: 1085 Gradiente: [0.22611029776784097,-3.901012860337061] Loss: 37.203295064220136\n",
      "Iteracion: 1086 Gradiente: [0.22599009059315023,-3.8989389621598405] Loss: 37.18803009575864\n",
      "Iteracion: 1087 Gradiente: [0.2258699473242861,-3.8968661665305473] Loss: 37.17278135363435\n",
      "Iteracion: 1088 Gradiente: [0.22574986792725252,-3.8947944728630364] Loss: 37.15754882059904\n",
      "Iteracion: 1089 Gradiente: [0.22562985236816738,-3.8927238805714675] Loss: 37.14233247942278\n",
      "Iteracion: 1090 Gradiente: [0.22550990061301945,-3.890654389070318] Loss: 37.127132312893984\n",
      "Iteracion: 1091 Gradiente: [0.2253900126278509,-3.8885859977743746] Loss: 37.11194830381931\n",
      "Iteracion: 1092 Gradiente: [0.22527018837891197,-3.8865187060987285] Loss: 37.096780435023774\n",
      "Iteracion: 1093 Gradiente: [0.22515042783207662,-3.8844525134587986] Loss: 37.081628689350545\n",
      "Iteracion: 1094 Gradiente: [0.22503073095375944,-3.8823874192702923] Loss: 37.06649304966113\n",
      "Iteracion: 1095 Gradiente: [0.22491109770990836,-3.8803234229492474] Loss: 37.051373498835204\n",
      "Iteracion: 1096 Gradiente: [0.22479152806673,-3.8782605239120005] Loss: 37.036270019770654\n",
      "Iteracion: 1097 Gradiente: [0.22467202199051664,-3.8761987215751996] Loss: 37.02118259538348\n",
      "Iteracion: 1098 Gradiente: [0.22455257944739676,-3.8741380153558085] Loss: 37.00611120860799\n",
      "Iteracion: 1099 Gradiente: [0.22443320040359444,-3.8720784046710945] Loss: 36.9910558423965\n",
      "Iteracion: 1100 Gradiente: [0.224313884825357,-3.870019888938641] Loss: 36.97601647971948\n",
      "Iteracion: 1101 Gradiente: [0.22419463267886783,-3.867962467576341] Loss: 36.96099310356553\n",
      "Iteracion: 1102 Gradiente: [0.22407544393049458,-3.8659061400023873] Loss: 36.94598569694129\n",
      "Iteracion: 1103 Gradiente: [0.2239563185466551,-3.8638509056352857] Loss: 36.930994242871535\n",
      "Iteracion: 1104 Gradiente: [0.22383725649333144,-3.8617967638938664] Loss: 36.916018724398995\n",
      "Iteracion: 1105 Gradiente: [0.22371825773715984,-3.859743714197244] Loss: 36.9010591245845\n",
      "Iteracion: 1106 Gradiente: [0.22359932224442833,-3.8576917559648534] Loss: 36.886115426506834\n",
      "Iteracion: 1107 Gradiente: [0.22348044998134733,-3.8556408886164517] Loss: 36.8711876132628\n",
      "Iteracion: 1108 Gradiente: [0.2233616409144247,-3.8535911115720807] Loss: 36.85627566796713\n",
      "Iteracion: 1109 Gradiente: [0.22324289501014316,-3.8515424242521004] Loss: 36.84137957375258\n",
      "Iteracion: 1110 Gradiente: [0.22312421223475762,-3.8494948260771866] Loss: 36.82649931376977\n",
      "Iteracion: 1111 Gradiente: [0.2230055925547371,-3.8474483164683155] Loss: 36.81163487118723\n",
      "Iteracion: 1112 Gradiente: [0.2228870359366051,-3.845402894846767] Loss: 36.796786229191405\n",
      "Iteracion: 1113 Gradiente: [0.22276854234688603,-3.8433585606341345] Loss: 36.78195337098661\n",
      "Iteracion: 1114 Gradiente: [0.22265011175198737,-3.8413153132523186] Loss: 36.767136279795004\n",
      "Iteracion: 1115 Gradiente: [0.22253174411840604,-3.8392731521235297] Loss: 36.75233493885661\n",
      "Iteracion: 1116 Gradiente: [0.2224134394126627,-3.837232076670281] Loss: 36.73754933142923\n",
      "Iteracion: 1117 Gradiente: [0.2222951976012941,-3.835192086315396] Loss: 36.72277944078847\n",
      "Iteracion: 1118 Gradiente: [0.22217701865101322,-3.8331531804819945] Loss: 36.708025250227706\n",
      "Iteracion: 1119 Gradiente: [0.2220589025281607,-3.831115358593526] Loss: 36.69328674305812\n",
      "Iteracion: 1120 Gradiente: [0.22194084919956555,-3.829078620073717] Loss: 36.6785639026086\n",
      "Iteracion: 1121 Gradiente: [0.22182285863178966,-3.8270429643466204] Loss: 36.66385671222572\n",
      "Iteracion: 1122 Gradiente: [0.22170493079133186,-3.825008390836598] Loss: 36.64916515527382\n",
      "Iteracion: 1123 Gradiente: [0.221587065644907,-3.8229748989683054] Loss: 36.634489215134884\n",
      "Iteracion: 1124 Gradiente: [0.22146926315912008,-3.8209424881667116] Loss: 36.61982887520857\n",
      "Iteracion: 1125 Gradiente: [0.22135152330100566,-3.8189111578570705] Loss: 36.6051841189122\n",
      "Iteracion: 1126 Gradiente: [0.22123384603691393,-3.81688090746498] Loss: 36.59055492968067\n",
      "Iteracion: 1127 Gradiente: [0.2211162313335379,-3.8148517364163212] Loss: 36.57594129096658\n",
      "Iteracion: 1128 Gradiente: [0.2209986791578956,-3.8128236441372696] Loss: 36.561343186239995\n",
      "Iteracion: 1129 Gradiente: [0.22088118947654653,-3.8107966300543277] Loss: 36.54676059898865\n",
      "Iteracion: 1130 Gradiente: [0.22076376225632355,-3.8087706935942927] Loss: 36.532193512717825\n",
      "Iteracion: 1131 Gradiente: [0.22064639746404188,-3.8067458341842624] Loss: 36.51764191095023\n",
      "Iteracion: 1132 Gradiente: [0.2205290950664344,-3.8047220512516495] Loss: 36.503105777226246\n",
      "Iteracion: 1133 Gradiente: [0.2204118550303491,-3.802699344224166] Loss: 36.48858509510363\n",
      "Iteracion: 1134 Gradiente: [0.22029467732273153,-3.8006777125298226] Loss: 36.47407984815767\n",
      "Iteracion: 1135 Gradiente: [0.220177561910387,-3.798657155596937] Loss: 36.45959001998109\n",
      "Iteracion: 1136 Gradiente: [0.22006050876020591,-3.7966376728541387] Loss: 36.44511559418408\n",
      "Iteracion: 1137 Gradiente: [0.21994351783911176,-3.794619263730353] Loss: 36.430656554394226\n",
      "Iteracion: 1138 Gradiente: [0.21982658911386183,-3.7926019276548173] Loss: 36.416212884256524\n",
      "Iteracion: 1139 Gradiente: [0.21970972255156293,-3.7905856640570574] Loss: 36.401784567433396\n",
      "Iteracion: 1140 Gradiente: [0.21959291811916207,-3.788570472366914] Loss: 36.38737158760456\n",
      "Iteracion: 1141 Gradiente: [0.219476175783484,-3.7865563520145336] Loss: 36.3729739284671\n",
      "Iteracion: 1142 Gradiente: [0.21935949551165473,-3.7845433024303525] Loss: 36.358591573735524\n",
      "Iteracion: 1143 Gradiente: [0.21924287727069042,-3.782531323045119] Loss: 36.34422450714149\n",
      "Iteracion: 1144 Gradiente: [0.21912632102751364,-3.7805204132898855] Loss: 36.32987271243407\n",
      "Iteracion: 1145 Gradiente: [0.21900982674916372,-3.778510572596006] Loss: 36.31553617337955\n",
      "Iteracion: 1146 Gradiente: [0.21889339440286715,-3.7765018003951245] Loss: 36.30121487376155\n",
      "Iteracion: 1147 Gradiente: [0.218777023955505,-3.7744940961192075] Loss: 36.28690879738079\n",
      "Iteracion: 1148 Gradiente: [0.21866071537429477,-3.772487459200506] Loss: 36.27261792805538\n",
      "Iteracion: 1149 Gradiente: [0.21854446862620472,-3.770481889071588] Loss: 36.258342249620476\n",
      "Iteracion: 1150 Gradiente: [0.21842828367859443,-3.768477385165306] Loss: 36.244081745928526\n",
      "Iteracion: 1151 Gradiente: [0.2183121604983569,-3.766473946914834] Loss: 36.22983640084909\n",
      "Iteracion: 1152 Gradiente: [0.2181960990527325,-3.7644715737536307] Loss: 36.21560619826884\n",
      "Iteracion: 1153 Gradiente: [0.21808009930897446,-3.762470265115462] Loss: 36.201391122091685\n",
      "Iteracion: 1154 Gradiente: [0.21796416123434714,-3.7604700204343904] Loss: 36.18719115623857\n",
      "Iteracion: 1155 Gradiente: [0.2178482847958302,-3.7584708391447923] Loss: 36.173006284647514\n",
      "Iteracion: 1156 Gradiente: [0.2177324699608306,-3.756472720681331] Loss: 36.15883649127363\n",
      "Iteracion: 1157 Gradiente: [0.21761671669658256,-3.7544756644789743] Loss: 36.14468176008912\n",
      "Iteracion: 1158 Gradiente: [0.21750102497032914,-3.752479669972994] Loss: 36.1305420750832\n",
      "Iteracion: 1159 Gradiente: [0.21738539474928514,-3.750484736598964] Loss: 36.11641742026208\n",
      "Iteracion: 1160 Gradiente: [0.21726982600078676,-3.748490863792752] Loss: 36.10230777964902\n",
      "Iteracion: 1161 Gradiente: [0.21715431869225507,-3.7464980509905197] Loss: 36.0882131372842\n",
      "Iteracion: 1162 Gradiente: [0.2170388727910203,-3.7445062976287384] Loss: 36.07413347722484\n",
      "Iteracion: 1163 Gradiente: [0.21692348826423496,-3.7425156031441893] Loss: 36.06006878354503\n",
      "Iteracion: 1164 Gradiente: [0.216808165079406,-3.740525966973932] Loss: 36.04601904033586\n",
      "Iteracion: 1165 Gradiente: [0.21669290320406798,-3.7385373885553244] Loss: 36.03198423170527\n",
      "Iteracion: 1166 Gradiente: [0.2165777026053263,-3.7365498673260524] Loss: 36.01796434177813\n",
      "Iteracion: 1167 Gradiente: [0.2164625632508089,-3.7345634027240693] Loss: 36.0039593546962\n",
      "Iteracion: 1168 Gradiente: [0.21634748510792765,-3.732577994187642] Loss: 35.98996925461801\n",
      "Iteracion: 1169 Gradiente: [0.21623246814408884,-3.7305936411553358] Loss: 35.97599402571905\n",
      "Iteracion: 1170 Gradiente: [0.21611751232680448,-3.72861034306601] Loss: 35.96203365219154\n",
      "Iteracion: 1171 Gradiente: [0.216002617623649,-3.7266280993588228] Loss: 35.94808811824459\n",
      "Iteracion: 1172 Gradiente: [0.21588778400191017,-3.7246469094732393] Loss: 35.93415740810396\n",
      "Iteracion: 1173 Gradiente: [0.2157730114293578,-3.7226667728490073] Loss: 35.9202415060123\n",
      "Iteracion: 1174 Gradiente: [0.21565829987340213,-3.720687688926185] Loss: 35.906340396228984\n",
      "Iteracion: 1175 Gradiente: [0.21554364930161043,-3.7187096571451255] Loss: 35.89245406303007\n",
      "Iteracion: 1176 Gradiente: [0.21542905968153267,-3.716732676946478] Loss: 35.878582490708375\n",
      "Iteracion: 1177 Gradiente: [0.21531453098080036,-3.7147567477711863] Loss: 35.86472566357341\n",
      "Iteracion: 1178 Gradiente: [0.21520006316713586,-3.7127818690604926] Loss: 35.85088356595136\n",
      "Iteracion: 1179 Gradiente: [0.21508565620801742,-3.710808040255942] Loss: 35.837056182185044\n",
      "Iteracion: 1180 Gradiente: [0.21497131007118925,-3.708835260799367] Loss: 35.82324349663396\n",
      "Iteracion: 1181 Gradiente: [0.21485702472432092,-3.7068635301329036] Loss: 35.80944549367419\n",
      "Iteracion: 1182 Gradiente: [0.21474280013503655,-3.704892847698984] Loss: 35.7956621576985\n",
      "Iteracion: 1183 Gradiente: [0.2146286362709039,-3.7029232129403438] Loss: 35.781893473116156\n",
      "Iteracion: 1184 Gradiente: [0.2145145330998858,-3.7009546252999956] Loss: 35.768139424353045\n",
      "Iteracion: 1185 Gradiente: [0.21440049058970204,-3.6989870842212556] Loss: 35.75439999585162\n",
      "Iteracion: 1186 Gradiente: [0.21428650870789454,-3.697020589147754] Loss: 35.74067517207084\n",
      "Iteracion: 1187 Gradiente: [0.21417258742246142,-3.6950551395233875] Loss: 35.7269649374862\n",
      "Iteracion: 1188 Gradiente: [0.21405872670096288,-3.693090734792378] Loss: 35.7132692765897\n",
      "Iteracion: 1189 Gradiente: [0.2139449265113484,-3.6911273743992195] Loss: 35.69958817388983\n",
      "Iteracion: 1190 Gradiente: [0.2138311868214051,-3.689165057788708] Loss: 35.685921613911496\n",
      "Iteracion: 1191 Gradiente: [0.21371750759902947,-3.6872037844059387] Loss: 35.67226958119613\n",
      "Iteracion: 1192 Gradiente: [0.21360388881191195,-3.685243553696306] Loss: 35.65863206030154\n",
      "Iteracion: 1193 Gradiente: [0.2134903304281406,-3.6832843651054805] Loss: 35.64500903580199\n",
      "Iteracion: 1194 Gradiente: [0.21337683241549285,-3.681326218079448] Loss: 35.63140049228809\n",
      "Iteracion: 1195 Gradiente: [0.21326339474179695,-3.679369112064482] Loss: 35.61780641436688\n",
      "Iteracion: 1196 Gradiente: [0.21315001737513176,-3.6774130465071435] Loss: 35.60422678666177\n",
      "Iteracion: 1197 Gradiente: [0.21303670028320595,-3.6754580208543035] Loss: 35.59066159381243\n",
      "Iteracion: 1198 Gradiente: [0.2129234434342069,-3.6735040345531056] Loss: 35.57711082047491\n",
      "Iteracion: 1199 Gradiente: [0.21281024679610236,-3.6715510870509993] Loss: 35.56357445132167\n",
      "Iteracion: 1200 Gradiente: [0.21269711033670358,-3.6695991777957335] Loss: 35.55005247104126\n",
      "Iteracion: 1201 Gradiente: [0.21258403402418366,-3.6676483062353387] Loss: 35.536544864338694\n",
      "Iteracion: 1202 Gradiente: [0.212471017826532,-3.6656984718181453] Loss: 35.523051615935124\n",
      "Iteracion: 1203 Gradiente: [0.2123580617116512,-3.6637496739927835] Loss: 35.509572710567944\n",
      "Iteracion: 1204 Gradiente: [0.21224516564776436,-3.661801912208162] Loss: 35.496108132990884\n",
      "Iteracion: 1205 Gradiente: [0.21213232960294306,-3.659855185913486] Loss: 35.4826578679738\n",
      "Iteracion: 1206 Gradiente: [0.2120195535451361,-3.657909494558269] Loss: 35.46922190030269\n",
      "Iteracion: 1207 Gradiente: [0.21190683744262817,-3.655964837592294] Loss: 35.45580021477982\n",
      "Iteracion: 1208 Gradiente: [0.21179418126336838,-3.6540212144656588] Loss: 35.44239279622356\n",
      "Iteracion: 1209 Gradiente: [0.21168158497568731,-3.6520786246287313] Loss: 35.42899962946844\n",
      "Iteracion: 1210 Gradiente: [0.21156904854754544,-3.6501370675321936] Loss: 35.415620699365085\n",
      "Iteracion: 1211 Gradiente: [0.21145657194726628,-3.648196542627001] Loss: 35.40225599078023\n",
      "Iteracion: 1212 Gradiente: [0.21134415514303556,-3.6462570493644124] Loss: 35.38890548859672\n",
      "Iteracion: 1213 Gradiente: [0.21123179810295292,-3.644318587195975] Loss: 35.375569177713444\n",
      "Iteracion: 1214 Gradiente: [0.2111195007954611,-3.642381155573522] Loss: 35.36224704304536\n",
      "Iteracion: 1215 Gradiente: [0.21100726318857047,-3.640444753949192] Loss: 35.34893906952343\n",
      "Iteracion: 1216 Gradiente: [0.2108950852505681,-3.6385093817754073] Loss: 35.33564524209467\n",
      "Iteracion: 1217 Gradiente: [0.21078296694991536,-3.636575038504872] Loss: 35.3223655457221\n",
      "Iteracion: 1218 Gradiente: [0.2106709082547084,-3.6346417235905943] Loss: 35.30909996538467\n",
      "Iteracion: 1219 Gradiente: [0.2105589091333608,-3.632709436485869] Loss: 35.29584848607735\n",
      "Iteracion: 1220 Gradiente: [0.21044696955421288,-3.630778176644277] Loss: 35.28261109281107\n",
      "Iteracion: 1221 Gradiente: [0.21033508948558505,-3.628847943519697] Loss: 35.269387770612646\n",
      "Iteracion: 1222 Gradiente: [0.21022326889585372,-3.6269187365662914] Loss: 35.25617850452482\n",
      "Iteracion: 1223 Gradiente: [0.2101115077534312,-3.624990555238518] Loss: 35.24298327960627\n",
      "Iteracion: 1224 Gradiente: [0.2099998060265174,-3.6230633989911314] Loss: 35.229802080931535\n",
      "Iteracion: 1225 Gradiente: [0.20988816368374638,-3.6211372672791553] Loss: 35.216634893590985\n",
      "Iteracion: 1226 Gradiente: [0.2097765806934447,-3.61921215955792] Loss: 35.20348170269087\n",
      "Iteracion: 1227 Gradiente: [0.20966505702410956,-3.6172880752830396] Loss: 35.190342493353306\n",
      "Iteracion: 1228 Gradiente: [0.20955359264423404,-3.6153650139104143] Loss: 35.17721725071616\n",
      "Iteracion: 1229 Gradiente: [0.2094421875221201,-3.6134429748962513] Loss: 35.16410595993314\n",
      "Iteracion: 1230 Gradiente: [0.20933084162653184,-3.611521957697016] Loss: 35.1510086061737\n",
      "Iteracion: 1231 Gradiente: [0.20921955492578684,-3.6096019617694926] Loss: 35.13792517462311\n",
      "Iteracion: 1232 Gradiente: [0.20910832738850235,-3.6076829865707363] Loss: 35.12485565048236\n",
      "Iteracion: 1233 Gradiente: [0.2089971589830744,-3.605765031558104] Loss: 35.11180001896814\n",
      "Iteracion: 1234 Gradiente: [0.20888604967816066,-3.6038480961892287] Loss: 35.098758265312895\n",
      "Iteracion: 1235 Gradiente: [0.2087749994425432,-3.6019321799220254] Loss: 35.08573037476476\n",
      "Iteracion: 1236 Gradiente: [0.20866400824442716,-3.6000172822147296] Loss: 35.07271633258753\n",
      "Iteracion: 1237 Gradiente: [0.20855307605279946,-3.598103402525825] Loss: 35.059716124060714\n",
      "Iteracion: 1238 Gradiente: [0.20844220283612885,-3.5961905403141077] Loss: 35.0467297344794\n",
      "Iteracion: 1239 Gradiente: [0.20833138856297756,-3.5942786950386627] Loss: 35.033757149154354\n",
      "Iteracion: 1240 Gradiente: [0.20822063320214543,-3.5923678661588467] Loss: 35.02079835341198\n",
      "Iteracion: 1241 Gradiente: [0.20810993672225397,-3.590458053134316] Loss: 35.00785333259419\n",
      "Iteracion: 1242 Gradiente: [0.2079992990920175,-3.5885492554250105] Loss: 34.994922072058564\n",
      "Iteracion: 1243 Gradiente: [0.20788872028014088,-3.586641472491156] Loss: 34.98200455717819\n",
      "Iteracion: 1244 Gradiente: [0.2077782002553069,-3.5847347037932726] Loss: 34.969100773341765\n",
      "Iteracion: 1245 Gradiente: [0.20766773898632304,-3.582828948792156] Loss: 34.956210705953445\n",
      "Iteracion: 1246 Gradiente: [0.20755733644196064,-3.5809242069488922] Loss: 34.94333434043295\n",
      "Iteracion: 1247 Gradiente: [0.2074469925909805,-3.579020477724858] Loss: 34.93047166221551\n",
      "Iteracion: 1248 Gradiente: [0.20733670740219357,-3.5771177605817135] Loss: 34.91762265675177\n",
      "Iteracion: 1249 Gradiente: [0.20722648084432269,-3.57521605498141] Loss: 34.9047873095079\n",
      "Iteracion: 1250 Gradiente: [0.20711631288637378,-3.5733153603861703] Loss: 34.891965605965524\n",
      "Iteracion: 1251 Gradiente: [0.2070062034971097,-3.5714156762585167] Loss: 34.879157531621644\n",
      "Iteracion: 1252 Gradiente: [0.20689615264536035,-3.569517002061259] Loss: 34.866363071988744\n",
      "Iteracion: 1253 Gradiente: [0.20678616030000352,-3.5676193372574834] Loss: 34.85358221259464\n",
      "Iteracion: 1254 Gradiente: [0.2066762264299039,-3.565722681310569] Loss: 34.84081493898259\n",
      "Iteracion: 1255 Gradiente: [0.20656635100402382,-3.563827033684173] Loss: 34.828061236711186\n",
      "Iteracion: 1256 Gradiente: [0.20645653399135994,-3.5619323938422394] Loss: 34.81532109135441\n",
      "Iteracion: 1257 Gradiente: [0.20634677536072085,-3.560038761249004] Loss: 34.80259448850151\n",
      "Iteracion: 1258 Gradiente: [0.2062370750811181,-3.558146135368981] Loss: 34.789881413757115\n",
      "Iteracion: 1259 Gradiente: [0.206127433121686,-3.556254515666962] Loss: 34.777181852741116\n",
      "Iteracion: 1260 Gradiente: [0.2060178494511329,-3.5543639016080473] Loss: 34.76449579108871\n",
      "Iteracion: 1261 Gradiente: [0.20590832403871057,-3.5524742926575943] Loss: 34.75182321445039\n",
      "Iteracion: 1262 Gradiente: [0.2057988568533536,-3.5505856882812585] Loss: 34.739164108491835\n",
      "Iteracion: 1263 Gradiente: [0.20568944786413965,-3.54869808794498] Loss: 34.72651845889403\n",
      "Iteracion: 1264 Gradiente: [0.20558009704003932,-3.5468114911149837] Loss: 34.71388625135311\n",
      "Iteracion: 1265 Gradiente: [0.2054708043503415,-3.544925897257762] Loss: 34.701267471580486\n",
      "Iteracion: 1266 Gradiente: [0.205361569763762,-3.543041305840125] Loss: 34.68866210530272\n",
      "Iteracion: 1267 Gradiente: [0.20525239324983577,-3.5411577163291224] Loss: 34.67607013826153\n",
      "Iteracion: 1268 Gradiente: [0.20514327477733046,-3.539275128192129] Loss: 34.66349155621385\n",
      "Iteracion: 1269 Gradiente: [0.20503421431560204,-3.5373935408967725] Loss: 34.6509263449317\n",
      "Iteracion: 1270 Gradiente: [0.20492521183382167,-3.535512953910974] Loss: 34.63837449020223\n",
      "Iteracion: 1271 Gradiente: [0.20481626730103647,-3.5336333667029454] Loss: 34.62583597782772\n",
      "Iteracion: 1272 Gradiente: [0.2047073806865986,-3.5317547787411656] Loss: 34.61331079362552\n",
      "Iteracion: 1273 Gradiente: [0.2045985519596499,-3.5298771894944063] Loss: 34.600798923428066\n",
      "Iteracion: 1274 Gradiente: [0.20448978108931543,-3.5280005984317264] Loss: 34.588300353082865\n",
      "Iteracion: 1275 Gradiente: [0.20438106804484496,-3.5261250050224615] Loss: 34.57581506845244\n",
      "Iteracion: 1276 Gradiente: [0.20427241279569042,-3.524250408736218] Loss: 34.56334305541439\n",
      "Iteracion: 1277 Gradiente: [0.20416381531094957,-3.5223768090429046] Loss: 34.55088429986126\n",
      "Iteracion: 1278 Gradiente: [0.20405527555994543,-3.520504205412698] Loss: 34.53843878770064\n",
      "Iteracion: 1279 Gradiente: [0.20394679351203326,-3.518632597316057] Loss: 34.52600650485508\n",
      "Iteracion: 1280 Gradiente: [0.20383836913647901,-3.5167619842237303] Loss: 34.5135874372621\n",
      "Iteracion: 1281 Gradiente: [0.2037300024026782,-3.51489236560674] Loss: 34.501181570874174\n",
      "Iteracion: 1282 Gradiente: [0.20362169327998386,-3.513023740936396] Loss: 34.4887888916587\n",
      "Iteracion: 1283 Gradiente: [0.20351344173771507,-3.5111561096842823] Loss: 34.476409385598\n",
      "Iteracion: 1284 Gradiente: [0.20340524774527277,-3.50928947132227] Loss: 34.464043038689255\n",
      "Iteracion: 1285 Gradiente: [0.20329711127208253,-3.5074238253225074] Loss: 34.4516898369446\n",
      "Iteracion: 1286 Gradiente: [0.20318903228766488,-3.505559171157417] Loss: 34.43934976639102\n",
      "Iteracion: 1287 Gradiente: [0.20308101076124388,-3.503695508299723] Loss: 34.42702281307029\n",
      "Iteracion: 1288 Gradiente: [0.20297304666242275,-3.5018328362224063] Loss: 34.4147089630391\n",
      "Iteracion: 1289 Gradiente: [0.2028651399606268,-3.499971154398741] Loss: 34.40240820236893\n",
      "Iteracion: 1290 Gradiente: [0.20275729062530867,-3.4981104623022787] Loss: 34.39012051714607\n",
      "Iteracion: 1291 Gradiente: [0.2026494986259602,-3.4962507594068506] Loss: 34.37784589347158\n",
      "Iteracion: 1292 Gradiente: [0.20254176393218373,-3.494392045186562] Loss: 34.36558431746133\n",
      "Iteracion: 1293 Gradiente: [0.2024340865134813,-3.492534319115806] Loss: 34.35333577524592\n",
      "Iteracion: 1294 Gradiente: [0.20232646633947388,-3.490677580669248] Loss: 34.341100252970705\n",
      "Iteracion: 1295 Gradiente: [0.2022189033795693,-3.4888218293218456] Loss: 34.32887773679576\n",
      "Iteracion: 1296 Gradiente: [0.20211139760355767,-3.4869670645488138] Loss: 34.3166682128959\n",
      "Iteracion: 1297 Gradiente: [0.20200394898076085,-3.4851132858256744] Loss: 34.30447166746057\n",
      "Iteracion: 1298 Gradiente: [0.20189655748104846,-3.483260492628203] Loss: 34.292288086693965\n",
      "Iteracion: 1299 Gradiente: [0.20178922307391953,-3.4814086844324654] Loss: 34.28011745681494\n",
      "Iteracion: 1300 Gradiente: [0.20168194572913406,-3.479557860714803] Loss: 34.26795976405695\n",
      "Iteracion: 1301 Gradiente: [0.20157472541617263,-3.477708020951845] Loss: 34.25581499466809\n",
      "Iteracion: 1302 Gradiente: [0.2014675621047978,-3.475859164620487] Loss: 34.24368313491114\n",
      "Iteracion: 1303 Gradiente: [0.20136045576478617,-3.4740112911979] Loss: 34.231564171063376\n",
      "Iteracion: 1304 Gradiente: [0.20125340636585326,-3.4721644001615433] Loss: 34.21945808941677\n",
      "Iteracion: 1305 Gradiente: [0.2011464138776226,-3.4703184909891527] Loss: 34.207364876277815\n",
      "Iteracion: 1306 Gradiente: [0.20103947826987442,-3.468473563158738] Loss: 34.19528451796752\n",
      "Iteracion: 1307 Gradiente: [0.20093259951232223,-3.466629616148592] Loss: 34.18321700082152\n",
      "Iteracion: 1308 Gradiente: [0.20082577757486794,-3.464786649437271] Loss: 34.171162311189924\n",
      "Iteracion: 1309 Gradiente: [0.20071901242709866,-3.4629446625036304] Loss: 34.15912043543733\n",
      "Iteracion: 1310 Gradiente: [0.2006123040390984,-3.461103654826776] Loss: 34.14709135994287\n",
      "Iteracion: 1311 Gradiente: [0.20050565238060472,-3.459263625886109] Loss: 34.135075071100154\n",
      "Iteracion: 1312 Gradiente: [0.2003990574212916,-3.4574245751613133] Loss: 34.123071555317225\n",
      "Iteracion: 1313 Gradiente: [0.20029251913114193,-3.4555865021323293] Loss: 34.111080799016584\n",
      "Iteracion: 1314 Gradiente: [0.20018603748001831,-3.4537494062793894] Loss: 34.09910278863521\n",
      "Iteracion: 1315 Gradiente: [0.20007961243783684,-3.451913287082991] Loss: 34.08713751062443\n",
      "Iteracion: 1316 Gradiente: [0.1999732439745171,-3.4500781440239137] Loss: 34.07518495144999\n",
      "Iteracion: 1317 Gradiente: [0.19986693205984427,-3.4482439765832225] Loss: 34.063245097592095\n",
      "Iteracion: 1318 Gradiente: [0.19976067666394404,-3.4464107842422353] Loss: 34.05131793554522\n",
      "Iteracion: 1319 Gradiente: [0.19965447775665268,-3.4445785664825697] Loss: 34.03940345181824\n",
      "Iteracion: 1320 Gradiente: [0.1995483353078157,-3.442747322786109] Loss: 34.02750163293438\n",
      "Iteracion: 1321 Gradiente: [0.19944224928769122,-3.440917052635002] Loss: 34.01561246543118\n",
      "Iteracion: 1322 Gradiente: [0.1993362196661754,-3.439087755511686] Loss: 34.00373593586048\n",
      "Iteracion: 1323 Gradiente: [0.1992302464131645,-3.4372594308988753] Loss: 33.99187203078845\n",
      "Iteracion: 1324 Gradiente: [0.19912432949895778,-3.4354320782795393] Loss: 33.98002073679552\n",
      "Iteracion: 1325 Gradiente: [0.19901846889327146,-3.433605697136956] Loss: 33.968182040476336\n",
      "Iteracion: 1326 Gradiente: [0.19891266456635914,-3.4317802869546457] Loss: 33.95635592843987\n",
      "Iteracion: 1327 Gradiente: [0.19880691648832022,-3.4299558472164153] Loss: 33.94454238730929\n",
      "Iteracion: 1328 Gradiente: [0.1987012246292373,-3.428132377406351] Loss: 33.932741403722005\n",
      "Iteracion: 1329 Gradiente: [0.19859558895915488,-3.4263098770088116] Loss: 33.9209529643296\n",
      "Iteracion: 1330 Gradiente: [0.1984900094481406,-3.424488345508426] Loss: 33.90917705579785\n",
      "Iteracion: 1331 Gradiente: [0.19838448606655276,-3.4226677823900937] Loss: 33.89741366480671\n",
      "Iteracion: 1332 Gradiente: [0.19827901878441193,-3.420848187138999] Loss: 33.88566277805036\n",
      "Iteracion: 1333 Gradiente: [0.19817360757189798,-3.4190295592405904] Loss: 33.873924382237014\n",
      "Iteracion: 1334 Gradiente: [0.19806825239919162,-3.4172118981805966] Loss: 33.862198464089055\n",
      "Iteracion: 1335 Gradiente: [0.19796295323655286,-3.4153952034450157] Loss: 33.850485010343036\n",
      "Iteracion: 1336 Gradiente: [0.19785771005421537,-3.4135794745201165] Loss: 33.838784007749524\n",
      "Iteracion: 1337 Gradiente: [0.19775252282231032,-3.411764710892451] Loss: 33.82709544307321\n",
      "Iteracion: 1338 Gradiente: [0.19764739151119243,-3.4099509120488305] Loss: 33.815419303092874\n",
      "Iteracion: 1339 Gradiente: [0.1975423160910876,-3.4081380774763526] Loss: 33.803755574601304\n",
      "Iteracion: 1340 Gradiente: [0.19743729653234182,-3.4063262066623765] Loss: 33.79210424440539\n",
      "Iteracion: 1341 Gradiente: [0.19733233280525062,-3.4045152990945367] Loss: 33.78046529932595\n",
      "Iteracion: 1342 Gradiente: [0.19722742488005524,-3.402705354260748] Loss: 33.76883872619791\n",
      "Iteracion: 1343 Gradiente: [0.19712257272715614,-3.400896371649188] Loss: 33.75722451187015\n",
      "Iteracion: 1344 Gradiente: [0.19701777631687634,-3.39908835074831] Loss: 33.7456226432055\n",
      "Iteracion: 1345 Gradiente: [0.196913035619563,-3.397281291046841] Loss: 33.73403310708079\n",
      "Iteracion: 1346 Gradiente: [0.19680835060564827,-3.395475192033776] Loss: 33.722455890386826\n",
      "Iteracion: 1347 Gradiente: [0.19670372124541374,-3.3936700531983868] Loss: 33.710890980028275\n",
      "Iteracion: 1348 Gradiente: [0.19659914750946494,-3.391865874030206] Loss: 33.69933836292377\n",
      "Iteracion: 1349 Gradiente: [0.196494629368091,-3.3900626540190526] Loss: 33.687798026005865\n",
      "Iteracion: 1350 Gradiente: [0.19639016679184257,-3.3882603926550012] Loss: 33.67626995622097\n",
      "Iteracion: 1351 Gradiente: [0.196285759750945,-3.38645908942842] Loss: 33.664754140529354\n",
      "Iteracion: 1352 Gradiente: [0.19618140821622773,-3.384658743829916] Loss: 33.653250565905225\n",
      "Iteracion: 1353 Gradiente: [0.19607711215775533,-3.382859355350404] Loss: 33.64175921933655\n",
      "Iteracion: 1354 Gradiente: [0.19597287154641543,-3.381060923481032] Loss: 33.63028008782521\n",
      "Iteracion: 1355 Gradiente: [0.19586868635255453,-3.379263447713245] Loss: 33.61881315838681\n",
      "Iteracion: 1356 Gradiente: [0.19576455654673358,-3.3774669275387508] Loss: 33.60735841805082\n",
      "Iteracion: 1357 Gradiente: [0.1956604820995352,-3.375671362449524] Loss: 33.59591585386049\n",
      "Iteracion: 1358 Gradiente: [0.19555646298146717,-3.3738767519378143] Loss: 33.58448545287285\n",
      "Iteracion: 1359 Gradiente: [0.19545249916321267,-3.3720830954961363] Loss: 33.57306720215866\n",
      "Iteracion: 1360 Gradiente: [0.19534859061533444,-3.3702903926172763] Loss: 33.56166108880243\n",
      "Iteracion: 1361 Gradiente: [0.19524473730835107,-3.3684986427942984] Loss: 33.550267099902406\n",
      "Iteracion: 1362 Gradiente: [0.19514093921295791,-3.366707845520522] Loss: 33.538885222570556\n",
      "Iteracion: 1363 Gradiente: [0.1950371962999481,-3.3649180002895407] Loss: 33.52751544393255\n",
      "Iteracion: 1364 Gradiente: [0.19493350853969105,-3.3631291065952307] Loss: 33.516157751127714\n",
      "Iteracion: 1365 Gradiente: [0.19482987590315967,-3.3613411639317095] Loss: 33.50481213130907\n",
      "Iteracion: 1366 Gradiente: [0.1947262983609074,-3.359554171793391] Loss: 33.493478571643315\n",
      "Iteracion: 1367 Gradiente: [0.19462277588366064,-3.357768129674945] Loss: 33.48215705931073\n",
      "Iteracion: 1368 Gradiente: [0.19451930844208148,-3.355983037071314] Loss: 33.47084758150526\n",
      "Iteracion: 1369 Gradiente: [0.19441589600696788,-3.3541988934777036] Loss: 33.45955012543446\n",
      "Iteracion: 1370 Gradiente: [0.19431253854911598,-3.352415698389591] Loss: 33.448264678319504\n",
      "Iteracion: 1371 Gradiente: [0.19420923603923487,-3.350633451302722] Loss: 33.4369912273951\n",
      "Iteracion: 1372 Gradiente: [0.19410598844817267,-3.3488521517131082] Loss: 33.42572975990956\n",
      "Iteracion: 1373 Gradiente: [0.19400279574661378,-3.347071799117037] Loss: 33.41448026312474\n",
      "Iteracion: 1374 Gradiente: [0.19389965790556427,-3.345292393011049] Loss: 33.40324272431606\n",
      "Iteracion: 1375 Gradiente: [0.193796574895597,-3.343513932891973] Loss: 33.392017130772416\n",
      "Iteracion: 1376 Gradiente: [0.1936935466879895,-3.341736418256872] Loss: 33.38080346979627\n",
      "Iteracion: 1377 Gradiente: [0.19359057325317033,-3.33995984860312] Loss: 33.36960172870353\n",
      "Iteracion: 1378 Gradiente: [0.19348765456225034,-3.3381842234283243] Loss: 33.358411894823654\n",
      "Iteracion: 1379 Gradiente: [0.19338479058597075,-3.33640954223038] Loss: 33.34723395549947\n",
      "Iteracion: 1380 Gradiente: [0.19328198129541743,-3.3346358045074282] Loss: 33.33606789808737\n",
      "Iteracion: 1381 Gradiente: [0.19317922666144607,-3.332863009757895] Loss: 33.3249137099571\n",
      "Iteracion: 1382 Gradiente: [0.19307652665492395,-3.3310911574804694] Loss: 33.313771378491865\n",
      "Iteracion: 1383 Gradiente: [0.19297388124695444,-3.3293202471740964] Loss: 33.30264089108832\n",
      "Iteracion: 1384 Gradiente: [0.1928712904083658,-3.327550278338004] Loss: 33.29152223515646\n",
      "Iteracion: 1385 Gradiente: [0.19276875411017802,-3.3257812504716773] Loss: 33.28041539811967\n",
      "Iteracion: 1386 Gradiente: [0.19266627232357128,-3.324013163074854] Loss: 33.269320367414714\n",
      "Iteracion: 1387 Gradiente: [0.19256384501936735,-3.3222460156475657] Loss: 33.258237130491736\n",
      "Iteracion: 1388 Gradiente: [0.1924614721686699,-3.320479807690093] Loss: 33.24716567481416\n",
      "Iteracion: 1389 Gradiente: [0.19235915374244145,-3.3187145387029875] Loss: 33.236105987858814\n",
      "Iteracion: 1390 Gradiente: [0.19225688971186042,-3.3169502081870585] Loss: 33.22505805711576\n",
      "Iteracion: 1391 Gradiente: [0.19215468004803427,-3.315186815643385] Loss: 33.214021870088416\n",
      "Iteracion: 1392 Gradiente: [0.19205252472195866,-3.3134243605733165] Loss: 33.20299741429344\n",
      "Iteracion: 1393 Gradiente: [0.19195042370484242,-3.311662842478457] Loss: 33.191984677260834\n",
      "Iteracion: 1394 Gradiente: [0.1918483769677356,-3.3099022608606883] Loss: 33.18098364653374\n",
      "Iteracion: 1395 Gradiente: [0.19174638448174144,-3.3081426152221534] Loss: 33.169994309668624\n",
      "Iteracion: 1396 Gradiente: [0.19164444621814405,-3.306383905065247] Loss: 33.15901665423517\n",
      "Iteracion: 1397 Gradiente: [0.19154256214801535,-3.3046261298926463] Loss: 33.14805066781624\n",
      "Iteracion: 1398 Gradiente: [0.19144073224266833,-3.3028692892072757] Loss: 33.137096338007964\n",
      "Iteracion: 1399 Gradiente: [0.19133895647316498,-3.301113382512345] Loss: 33.12615365241958\n",
      "Iteracion: 1400 Gradiente: [0.19123723481084293,-3.299358409311305] Loss: 33.115222598673526\n",
      "Iteracion: 1401 Gradiente: [0.1911355672268788,-3.297604369107887] Loss: 33.104303164405415\n",
      "Iteracion: 1402 Gradiente: [0.19103395369250695,-3.2958512614060846] Loss: 33.09339533726399\n",
      "Iteracion: 1403 Gradiente: [0.19093239417889077,-3.2940990857101533] Loss: 33.082499104911115\n",
      "Iteracion: 1404 Gradiente: [0.19083088865752557,-3.2923478415245984] Loss: 33.071614455021766\n",
      "Iteracion: 1405 Gradiente: [0.19072943709955723,-3.2905975283542093] Loss: 33.06074137528404\n",
      "Iteracion: 1406 Gradiente: [0.19062803947645648,-3.2888481457040215] Loss: 33.04987985339913\n",
      "Iteracion: 1407 Gradiente: [0.19052669575938808,-3.287099693079351] Loss: 33.03902987708124\n",
      "Iteracion: 1408 Gradiente: [0.19042540591973514,-3.285352169985764] Loss: 33.02819143405773\n",
      "Iteracion: 1409 Gradiente: [0.19032416992892537,-3.2836055759290925] Loss: 33.01736451206893\n",
      "Iteracion: 1410 Gradiente: [0.19022298775815236,-3.281859910415437] Loss: 33.0065490988682\n",
      "Iteracion: 1411 Gradiente: [0.19012185937901524,-3.280115172951148] Loss: 32.995745182221974\n",
      "Iteracion: 1412 Gradiente: [0.1900207847629294,-3.2783713630428433] Loss: 32.98495274990966\n",
      "Iteracion: 1413 Gradiente: [0.1899197638810345,-3.2766284801974224] Loss: 32.97417178972364\n",
      "Iteracion: 1414 Gradiente: [0.18981879670507643,-3.2748865239220115] Loss: 32.96340228946929\n",
      "Iteracion: 1415 Gradiente: [0.18971788320625366,-3.27314549372403] Loss: 32.95264423696496\n",
      "Iteracion: 1416 Gradiente: [0.18961702335625147,-3.2714053891111385] Loss: 32.94189762004191\n",
      "Iteracion: 1417 Gradiente: [0.18951621712637395,-3.269666209591274] Loss: 32.93116242654437\n",
      "Iteracion: 1418 Gradiente: [0.18941546448818844,-3.2679279546726274] Loss: 32.92043864432948\n",
      "Iteracion: 1419 Gradiente: [0.1893147654132046,-3.2661906238636487] Loss: 32.90972626126731\n",
      "Iteracion: 1420 Gradiente: [0.18921411987285192,-3.2644542166730592] Loss: 32.899025265240766\n",
      "Iteracion: 1421 Gradiente: [0.18911352783886645,-3.2627187326098275] Loss: 32.88833564414572\n",
      "Iteracion: 1422 Gradiente: [0.1890129892826503,-3.2609841711831953] Loss: 32.87765738589081\n",
      "Iteracion: 1423 Gradiente: [0.18891250417583383,-3.259250531902659] Loss: 32.866990478397604\n",
      "Iteracion: 1424 Gradiente: [0.18881207249000814,-3.2575178142779757] Loss: 32.85633490960049\n",
      "Iteracion: 1425 Gradiente: [0.1887116941968041,-3.2557860178191627] Loss: 32.845690667446654\n",
      "Iteracion: 1426 Gradiente: [0.18861136926771374,-3.254055142036508] Loss: 32.83505773989613\n",
      "Iteracion: 1427 Gradiente: [0.18851109767442437,-3.2523251864405505] Loss: 32.824436114921724\n",
      "Iteracion: 1428 Gradiente: [0.18841087938857679,-3.25059615054209] Loss: 32.81382578050907\n",
      "Iteracion: 1429 Gradiente: [0.18831071438178715,-3.2488680338521876] Loss: 32.803226724656476\n",
      "Iteracion: 1430 Gradiente: [0.188210602625936,-3.247140835882155] Loss: 32.79263893537516\n",
      "Iteracion: 1431 Gradiente: [0.18811054409251965,-3.2454145561435794] Loss: 32.78206240068891\n",
      "Iteracion: 1432 Gradiente: [0.1880105387532107,-3.2436891941483057] Loss: 32.771497108634364\n",
      "Iteracion: 1433 Gradiente: [0.18791058657984744,-3.2419647494084254] Loss: 32.76094304726087\n",
      "Iteracion: 1434 Gradiente: [0.18781068754419247,-3.2402412214362983] Loss: 32.750400204630424\n",
      "Iteracion: 1435 Gradiente: [0.18771084161796997,-3.238518609744543] Loss: 32.73986856881774\n",
      "Iteracion: 1436 Gradiente: [0.187611048772816,-3.236796913846042] Loss: 32.72934812791024\n",
      "Iteracion: 1437 Gradiente: [0.18751130898057417,-3.235076133253928] Loss: 32.71883887000792\n",
      "Iteracion: 1438 Gradiente: [0.18741162221304108,-3.233356267481599] Loss: 32.70834078322353\n",
      "Iteracion: 1439 Gradiente: [0.18731198844212713,-3.2316373160427005] Loss: 32.6978538556824\n",
      "Iteracion: 1440 Gradiente: [0.18721240763962044,-3.229919278451149] Loss: 32.68737807552248\n",
      "Iteracion: 1441 Gradiente: [0.1871128797772864,-3.2282021542211177] Loss: 32.67691343089435\n",
      "Iteracion: 1442 Gradiente: [0.18701340482702544,-3.226485942867032] Loss: 32.66645990996116\n",
      "Iteracion: 1443 Gradiente: [0.18691398276068866,-3.2247706439035806] Loss: 32.65601750089868\n",
      "Iteracion: 1444 Gradiente: [0.18681461355024662,-3.223056256845707] Loss: 32.64558619189522\n",
      "Iteracion: 1445 Gradiente: [0.1867152971673813,-3.2213427812086235] Loss: 32.63516597115164\n",
      "Iteracion: 1446 Gradiente: [0.18661603358430437,-3.2196302165077753] Loss: 32.62475682688141\n",
      "Iteracion: 1447 Gradiente: [0.18651682277262296,-3.2179185622588977] Loss: 32.6143587473104\n",
      "Iteracion: 1448 Gradiente: [0.18641766470459278,-3.2162078179779527] Loss: 32.60397172067711\n",
      "Iteracion: 1449 Gradiente: [0.18631855935194844,-3.21449798318118] Loss: 32.59359573523249\n",
      "Iteracion: 1450 Gradiente: [0.18621950668672677,-3.2127890573850713] Loss: 32.58323077923998\n",
      "Iteracion: 1451 Gradiente: [0.18612050668108773,-3.211081040106362] Loss: 32.57287684097554\n",
      "Iteracion: 1452 Gradiente: [0.1860215593068214,-3.2093739308620695] Loss: 32.56253390872754\n",
      "Iteracion: 1453 Gradiente: [0.1859226645359769,-3.207667729169451] Loss: 32.55220197079681\n",
      "Iteracion: 1454 Gradiente: [0.18582382234064596,-3.2059624345460245] Loss: 32.54188101549663\n",
      "Iteracion: 1455 Gradiente: [0.1857250326928759,-3.2042580465095636] Loss: 32.53157103115271\n",
      "Iteracion: 1456 Gradiente: [0.18562629556476035,-3.2025545645780946] Loss: 32.521272006103146\n",
      "Iteracion: 1457 Gradiente: [0.1855276109282073,-3.2008519882699153] Loss: 32.51098392869842\n",
      "Iteracion: 1458 Gradiente: [0.1854289787555416,-3.1991503171035562] Loss: 32.50070678730147\n",
      "Iteracion: 1459 Gradiente: [0.18533039901876028,-3.1974495505978213] Loss: 32.490440570287504\n",
      "Iteracion: 1460 Gradiente: [0.1852318716899987,-3.1957496882717664] Loss: 32.48018526604416\n",
      "Iteracion: 1461 Gradiente: [0.18513339674135237,-3.194050729644701] Loss: 32.46994086297136\n",
      "Iteracion: 1462 Gradiente: [0.18503497414498232,-3.192352674236194] Loss: 32.45970734948144\n",
      "Iteracion: 1463 Gradiente: [0.18493660387317448,-3.1906555215660584] Loss: 32.449484713999006\n",
      "Iteracion: 1464 Gradiente: [0.18483828589796947,-3.188959271154378] Loss: 32.439272944960905\n",
      "Iteracion: 1465 Gradiente: [0.1847400201915614,-3.187263922521486] Loss: 32.4290720308164\n",
      "Iteracion: 1466 Gradiente: [0.1846418067262988,-3.185569475187962] Loss: 32.418881960026944\n",
      "Iteracion: 1467 Gradiente: [0.18454364547422367,-3.1838759286746563] Loss: 32.40870272106626\n",
      "Iteracion: 1468 Gradiente: [0.18444553640787262,-3.18218328250265] Loss: 32.39853430242039\n",
      "Iteracion: 1469 Gradiente: [0.1843474794991721,-3.1804915361933115] Loss: 32.388376692587556\n",
      "Iteracion: 1470 Gradiente: [0.1842494747205843,-3.1788006892682366] Loss: 32.37822988007818\n",
      "Iteracion: 1471 Gradiente: [0.18415152204428012,-3.177110741249291] Loss: 32.36809385341499\n",
      "Iteracion: 1472 Gradiente: [0.1840536214426976,-3.175421691658579] Loss: 32.35796860113282\n",
      "Iteracion: 1473 Gradiente: [0.1839557728880384,-3.1737335400184787] Loss: 32.347854111778766\n",
      "Iteracion: 1474 Gradiente: [0.18385797635266102,-3.172046285851608] Loss: 32.33775037391203\n",
      "Iteracion: 1475 Gradiente: [0.183760231809012,-3.1703599286808366] Loss: 32.32765737610404\n",
      "Iteracion: 1476 Gradiente: [0.1836625392292338,-3.168674468029307] Loss: 32.3175751069383\n",
      "Iteracion: 1477 Gradiente: [0.1835648985859341,-3.1669899034203888] Loss: 32.30750355501057\n",
      "Iteracion: 1478 Gradiente: [0.18346730985135384,-3.165306234377726] Loss: 32.29744270892857\n",
      "Iteracion: 1479 Gradiente: [0.18336977299801258,-3.1636234604252036] Loss: 32.28739255731226\n",
      "Iteracion: 1480 Gradiente: [0.18327228799815315,-3.1619415810869707] Loss: 32.27735308879361\n",
      "Iteracion: 1481 Gradiente: [0.18317485482437804,-3.1602605958874155] Loss: 32.267324292016745\n",
      "Iteracion: 1482 Gradiente: [0.18307747344900824,-3.1585805043511934] Loss: 32.25730615563785\n",
      "Iteracion: 1483 Gradiente: [0.18298014384457606,-3.1569013060032014] Loss: 32.24729866832511\n",
      "Iteracion: 1484 Gradiente: [0.1828828659835736,-3.1552230003685935] Loss: 32.23730181875879\n",
      "Iteracion: 1485 Gradiente: [0.18278563983854593,-3.1535455869727724] Loss: 32.22731559563123\n",
      "Iteracion: 1486 Gradiente: [0.18268846538177855,-3.151869065341405] Loss: 32.21733998764673\n",
      "Iteracion: 1487 Gradiente: [0.1825913425860686,-3.1501934350003915] Loss: 32.20737498352162\n",
      "Iteracion: 1488 Gradiente: [0.18249427142373614,-3.148518695475906] Loss: 32.19742057198422\n",
      "Iteracion: 1489 Gradiente: [0.18239725186744427,-3.146844846294355] Loss: 32.18747674177483\n",
      "Iteracion: 1490 Gradiente: [0.18230028388965897,-3.1451718869824106] Loss: 32.177543481645735\n",
      "Iteracion: 1491 Gradiente: [0.182203367463084,-3.143499817066983] Loss: 32.16762078036114\n",
      "Iteracion: 1492 Gradiente: [0.18210650256033603,-3.141828636075242] Loss: 32.15770862669726\n",
      "Iteracion: 1493 Gradiente: [0.1820096891539104,-3.1401583435346176] Loss: 32.147807009442175\n",
      "Iteracion: 1494 Gradiente: [0.1819129272164067,-3.138488938972779] Loss: 32.13791591739588\n",
      "Iteracion: 1495 Gradiente: [0.1818162167204548,-3.1368204219176503] Loss: 32.128035339370356\n",
      "Iteracion: 1496 Gradiente: [0.1817195576388002,-3.135152791897402] Loss: 32.11816526418938\n",
      "Iteracion: 1497 Gradiente: [0.1816229499441903,-3.1334860484404525] Loss: 32.10830568068865\n",
      "Iteracion: 1498 Gradiente: [0.1815263936090697,-3.1318201910754926] Loss: 32.09845657771577\n",
      "Iteracion: 1499 Gradiente: [0.18142988860625356,-3.1301552193314413] Loss: 32.088617944130114\n",
      "Iteracion: 1500 Gradiente: [0.18133343490851198,-3.128491132737473] Loss: 32.07878976880299\n",
      "Iteracion: 1501 Gradiente: [0.18123703248860845,-3.1268279308230116] Loss: 32.06897204061749\n",
      "Iteracion: 1502 Gradiente: [0.18114068131913352,-3.125165613117743] Loss: 32.05916474846847\n",
      "Iteracion: 1503 Gradiente: [0.1810443813729525,-3.123504179151587] Loss: 32.049367881262704\n",
      "Iteracion: 1504 Gradiente: [0.18094813262261672,-3.1218436284547333] Loss: 32.03958142791866\n",
      "Iteracion: 1505 Gradiente: [0.18085193504110747,-3.120183960557602] Loss: 32.02980537736666\n",
      "Iteracion: 1506 Gradiente: [0.18075578860123756,-3.1185251749908587] Loss: 32.02003971854871\n",
      "Iteracion: 1507 Gradiente: [0.18065969327576947,-3.1168672712854426] Loss: 32.010284440418665\n",
      "Iteracion: 1508 Gradiente: [0.18056364903749986,-3.115210248972525] Loss: 32.00053953194205\n",
      "Iteracion: 1509 Gradiente: [0.1804676558593087,-3.1135541075835325] Loss: 31.990804982096154\n",
      "Iteracion: 1510 Gradiente: [0.18037171371411015,-3.111898846650131] Loss: 31.98108077986996\n",
      "Iteracion: 1511 Gradiente: [0.18027582257459188,-3.1102444657042563] Loss: 31.97136691426416\n",
      "Iteracion: 1512 Gradiente: [0.18017998241375796,-3.1085909642780734] Loss: 31.961663374291177\n",
      "Iteracion: 1513 Gradiente: [0.18008419320460783,-3.1069383419039953] Loss: 31.95197014897507\n",
      "Iteracion: 1514 Gradiente: [0.17998845491974672,-3.1052865981147084] Loss: 31.94228722735155\n",
      "Iteracion: 1515 Gradiente: [0.17989276753243644,-3.1036357324431134] Loss: 31.932614598468053\n",
      "Iteracion: 1516 Gradiente: [0.1797971310154016,-3.1019857444223855] Loss: 31.92295225138359\n",
      "Iteracion: 1517 Gradiente: [0.17970154534179225,-3.1003366335859295] Loss: 31.913300175168846\n",
      "Iteracion: 1518 Gradiente: [0.17960601048443056,-3.098688399467415] Loss: 31.903658358906082\n",
      "Iteracion: 1519 Gradiente: [0.17951052641629606,-3.0970410416007508] Loss: 31.894026791689196\n",
      "Iteracion: 1520 Gradiente: [0.1794150931103758,-3.0953945595200976] Loss: 31.884405462623683\n",
      "Iteracion: 1521 Gradiente: [0.17931971053973542,-3.093748952759857] Loss: 31.874794360826577\n",
      "Iteracion: 1522 Gradiente: [0.17922437867745866,-3.0921042208546767] Loss: 31.865193475426526\n",
      "Iteracion: 1523 Gradiente: [0.17912909749647288,-3.090460363339466] Loss: 31.85560279556372\n",
      "Iteracion: 1524 Gradiente: [0.17903386697003518,-3.0888173797493605] Loss: 31.8460223103899\n",
      "Iteracion: 1525 Gradiente: [0.1789386870709336,-3.087175269619768] Loss: 31.836452009068292\n",
      "Iteracion: 1526 Gradiente: [0.17884355777234,-3.0855340324863287] Loss: 31.82689188077371\n",
      "Iteracion: 1527 Gradiente: [0.17874847904750718,-3.0838936678849205] Loss: 31.817341914692438\n",
      "Iteracion: 1528 Gradiente: [0.17865345086944917,-3.0822541753516854] Loss: 31.807802100022244\n",
      "Iteracion: 1529 Gradiente: [0.17855847321117815,-3.08061555442301] Loss: 31.7982724259724\n",
      "Iteracion: 1530 Gradiente: [0.17846354604601705,-3.078977804635514] Loss: 31.788752881763646\n",
      "Iteracion: 1531 Gradiente: [0.17836866934707796,-3.077340925526074] Loss: 31.77924345662819\n",
      "Iteracion: 1532 Gradiente: [0.1782738430874242,-3.0757049166318153] Loss: 31.76974413980964\n",
      "Iteracion: 1533 Gradiente: [0.17817906724046065,-3.074069777490096] Loss: 31.76025492056311\n",
      "Iteracion: 1534 Gradiente: [0.17808434177912222,-3.0724355076385392] Loss: 31.750775788155046\n",
      "Iteracion: 1535 Gradiente: [0.17798966667679206,-3.0708021066149986] Loss: 31.741306731863403\n",
      "Iteracion: 1536 Gradiente: [0.1778950419066054,-3.0691695739575806] Loss: 31.731847740977425\n",
      "Iteracion: 1537 Gradiente: [0.17780046744185352,-3.067537909204634] Loss: 31.72239880479785\n",
      "Iteracion: 1538 Gradiente: [0.17770594325588623,-3.065907111894747] Loss: 31.712959912636737\n",
      "Iteracion: 1539 Gradiente: [0.17761146932167454,-3.06427718156678] Loss: 31.703531053817485\n",
      "Iteracion: 1540 Gradiente: [0.17751704561292078,-3.0626481177597924] Loss: 31.694112217674878\n",
      "Iteracion: 1541 Gradiente: [0.1774226721026082,-3.0610199200131336] Loss: 31.684703393555015\n",
      "Iteracion: 1542 Gradiente: [0.17732834876423453,-3.0593925878663706] Loss: 31.67530457081534\n",
      "Iteracion: 1543 Gradiente: [0.17723407557092277,-3.0577661208593336] Loss: 31.66591573882457\n",
      "Iteracion: 1544 Gradiente: [0.17713985249615974,-3.05614051853208] Loss: 31.65653688696278\n",
      "Iteracion: 1545 Gradiente: [0.17704567951333747,-3.0545157804249174] Loss: 31.647168004621303\n",
      "Iteracion: 1546 Gradiente: [0.17695155659567283,-3.052891906078408] Loss: 31.63780908120272\n",
      "Iteracion: 1547 Gradiente: [0.1768574837167364,-3.0512688950333415] Loss: 31.62846010612096\n",
      "Iteracion: 1548 Gradiente: [0.17676346084985065,-3.0496467468307626] Loss: 31.619121068801128\n",
      "Iteracion: 1549 Gradiente: [0.17666948796832807,-3.0480254610119646] Loss: 31.609791958679573\n",
      "Iteracion: 1550 Gradiente: [0.1765755650456981,-3.0464050371184705] Loss: 31.60047276520391\n",
      "Iteracion: 1551 Gradiente: [0.17648169205534145,-3.04478547469206] Loss: 31.591163477832968\n",
      "Iteracion: 1552 Gradiente: [0.17638786897076197,-3.0431667732747445] Loss: 31.58186408603674\n",
      "Iteracion: 1553 Gradiente: [0.17629409576552083,-3.041548932408786] Loss: 31.572574579296486\n",
      "Iteracion: 1554 Gradiente: [0.17620037241289074,-3.0399319516366936] Loss: 31.563294947104545\n",
      "Iteracion: 1555 Gradiente: [0.176106698886539,-3.0383158305012103] Loss: 31.55402517896453\n",
      "Iteracion: 1556 Gradiente: [0.1760130751598036,-3.036700568545334] Loss: 31.544765264391124\n",
      "Iteracion: 1557 Gradiente: [0.17591950120636426,-3.0350861653122907] Loss: 31.53551519291023\n",
      "Iteracion: 1558 Gradiente: [0.17582597699968402,-3.033472620345562] Loss: 31.52627495405881\n",
      "Iteracion: 1559 Gradiente: [0.17573250251340938,-3.031859933188861] Loss: 31.517044537385022\n",
      "Iteracion: 1560 Gradiente: [0.1756390777209769,-3.0302481033861577] Loss: 31.507823932448098\n",
      "Iteracion: 1561 Gradiente: [0.1755457025960671,-3.028637130481649] Loss: 31.49861312881836\n",
      "Iteracion: 1562 Gradiente: [0.17545237711222228,-3.027027014019787] Loss: 31.489412116077215\n",
      "Iteracion: 1563 Gradiente: [0.17535910124311727,-3.0254177535452547] Loss: 31.480220883817186\n",
      "Iteracion: 1564 Gradiente: [0.17526587496219767,-3.0238093486029936] Loss: 31.471039421641795\n",
      "Iteracion: 1565 Gradiente: [0.17517269824330128,-3.022201798738166] Loss: 31.461867719165674\n",
      "Iteracion: 1566 Gradiente: [0.1750795710600651,-3.0205951034961873] Loss: 31.45270576601445\n",
      "Iteracion: 1567 Gradiente: [0.17498649338615213,-3.018989262422713] Loss: 31.443553551824838\n",
      "Iteracion: 1568 Gradiente: [0.17489346519501747,-3.017384275063653] Loss: 31.43441106624448\n",
      "Iteracion: 1569 Gradiente: [0.17480048646065807,-3.01578014096513] Loss: 31.425278298932096\n",
      "Iteracion: 1570 Gradiente: [0.17470755715663888,-3.014176859673535] Loss: 31.41615523955738\n",
      "Iteracion: 1571 Gradiente: [0.17461467725662488,-3.0125744307354885] Loss: 31.407041877800975\n",
      "Iteracion: 1572 Gradiente: [0.17452184673447846,-3.0109728536978486] Loss: 31.39793820335455\n",
      "Iteracion: 1573 Gradiente: [0.17442906556389867,-3.009372128107722] Loss: 31.388844205920673\n",
      "Iteracion: 1574 Gradiente: [0.17433633371847843,-3.0077722535124614] Loss: 31.379759875212894\n",
      "Iteracion: 1575 Gradiente: [0.17424365117223933,-3.0061732294596393] Loss: 31.370685200955702\n",
      "Iteracion: 1576 Gradiente: [0.1741510178989311,-3.004575055497083] Loss: 31.36162017288448\n",
      "Iteracion: 1577 Gradiente: [0.1740584338722594,-3.0029777311728623] Loss: 31.35256478074553\n",
      "Iteracion: 1578 Gradiente: [0.17396589906609847,-3.0013812560352817] Loss: 31.343519014296085\n",
      "Iteracion: 1579 Gradiente: [0.17387341345440557,-2.999785629632883] Loss: 31.334482863304242\n",
      "Iteracion: 1580 Gradiente: [0.17378097701079157,-2.9981908515144626] Loss: 31.325456317548944\n",
      "Iteracion: 1581 Gradiente: [0.17368858970918666,-2.996596921229042] Loss: 31.31643936682004\n",
      "Iteracion: 1582 Gradiente: [0.17359625152356795,-2.995003838325884] Loss: 31.30743200091822\n",
      "Iteracion: 1583 Gradiente: [0.17350396242781585,-2.993411602354494] Loss: 31.29843420965504\n",
      "Iteracion: 1584 Gradiente: [0.17341172239578384,-2.9918202128646194] Loss: 31.28944598285285\n",
      "Iteracion: 1585 Gradiente: [0.17331953140129694,-2.9902296694062493] Loss: 31.280467310344807\n",
      "Iteracion: 1586 Gradiente: [0.17322738941840612,-2.988639971529603] Loss: 31.271498181974934\n",
      "Iteracion: 1587 Gradiente: [0.17313529642096956,-2.9870511187851436] Loss: 31.26253858759799\n",
      "Iteracion: 1588 Gradiente: [0.17304325238303164,-2.9854631107235727] Loss: 31.25358851707955\n",
      "Iteracion: 1589 Gradiente: [0.17295125727860258,-2.9838759468958256] Loss: 31.24464796029596\n",
      "Iteracion: 1590 Gradiente: [0.1728593110815268,-2.98228962685309] Loss: 31.23571690713431\n",
      "Iteracion: 1591 Gradiente: [0.17276741376592022,-2.9807041501467806] Loss: 31.226795347492462\n",
      "Iteracion: 1592 Gradiente: [0.17267556530564906,-2.979119516328556] Loss: 31.21788327127901\n",
      "Iteracion: 1593 Gradiente: [0.17258376567481074,-2.9775357249503136] Loss: 31.208980668413265\n",
      "Iteracion: 1594 Gradiente: [0.1724920148476244,-2.975952775564173] Loss: 31.200087528825268\n",
      "Iteracion: 1595 Gradiente: [0.17240031279794957,-2.9743706677225163] Loss: 31.19120384245574\n",
      "Iteracion: 1596 Gradiente: [0.1723086594999226,-2.972789400977951] Loss: 31.18232959925611\n",
      "Iteracion: 1597 Gradiente: [0.17221705492746414,-2.97120897488333] Loss: 31.17346478918851\n",
      "Iteracion: 1598 Gradiente: [0.1721254990549511,-2.9696293889917227] Loss: 31.16460940222571\n",
      "Iteracion: 1599 Gradiente: [0.17203399185628332,-2.9680506428564666] Loss: 31.155763428351136\n",
      "Iteracion: 1600 Gradiente: [0.17194253330566484,-2.9664727360311134] Loss: 31.146926857558892\n",
      "Iteracion: 1601 Gradiente: [0.1718511233771674,-2.9648956680694645] Loss: 31.138099679853713\n",
      "Iteracion: 1602 Gradiente: [0.17175976204504054,-2.963319438525549] Loss: 31.129281885250908\n",
      "Iteracion: 1603 Gradiente: [0.17166844928353128,-2.961744046953633] Loss: 31.12047346377648\n",
      "Iteracion: 1604 Gradiente: [0.17157718506647843,-2.960169492908245] Loss: 31.111674405466946\n",
      "Iteracion: 1605 Gradiente: [0.17148596936840382,-2.9585957759441146] Loss: 31.10288470036952\n",
      "Iteracion: 1606 Gradiente: [0.17139480216333994,-2.957022895616227] Loss: 31.094104338541875\n",
      "Iteracion: 1607 Gradiente: [0.17130368342558172,-2.9554508514798017] Loss: 31.085333310052373\n",
      "Iteracion: 1608 Gradiente: [0.17121261312937955,-2.9538796430902883] Loss: 31.076571604979826\n",
      "Iteracion: 1609 Gradiente: [0.17112159124887114,-2.9523092700033886] Loss: 31.06781921341366\n",
      "Iteracion: 1610 Gradiente: [0.17103061775844425,-2.950739731775019] Loss: 31.05907612545381\n",
      "Iteracion: 1611 Gradiente: [0.17093969263232367,-2.94917102796135] Loss: 31.05034233121075\n",
      "Iteracion: 1612 Gradiente: [0.1708488158447319,-2.9476031581187803] Loss: 31.041617820805453\n",
      "Iteracion: 1613 Gradiente: [0.17075798737009176,-2.9460361218039437] Loss: 31.032902584369392\n",
      "Iteracion: 1614 Gradiente: [0.17066720718270384,-2.944469918573708] Loss: 31.024196612044534\n",
      "Iteracion: 1615 Gradiente: [0.17057647525679015,-2.942904547985186] Loss: 31.01549989398332\n",
      "Iteracion: 1616 Gradiente: [0.17048579156669585,-2.9413400095957227] Loss: 31.00681242034869\n",
      "Iteracion: 1617 Gradiente: [0.17039515608678926,-2.9397763029628914] Loss: 30.998134181313976\n",
      "Iteracion: 1618 Gradiente: [0.1703045687916775,-2.9382134276444942] Loss: 30.989465167063038\n",
      "Iteracion: 1619 Gradiente: [0.17021402965537882,-2.9366513831985968] Loss: 30.980805367790083\n",
      "Iteracion: 1620 Gradiente: [0.17012353865260177,-2.935090169183465] Loss: 30.97215477369983\n",
      "Iteracion: 1621 Gradiente: [0.17003309575748393,-2.9335297851576354] Loss: 30.963513375007338\n",
      "Iteracion: 1622 Gradiente: [0.1699427009446964,-2.9319702306798416] Loss: 30.95488116193808\n",
      "Iteracion: 1623 Gradiente: [0.16985235418849431,-2.9304115053090816] Loss: 30.946258124727965\n",
      "Iteracion: 1624 Gradiente: [0.16976205546348336,-2.928853608604571] Loss: 30.937644253623223\n",
      "Iteracion: 1625 Gradiente: [0.16967180474401242,-2.927296540125767] Loss: 30.929039538880506\n",
      "Iteracion: 1626 Gradiente: [0.16958160200452804,-2.9257402994323654] Loss: 30.920443970766776\n",
      "Iteracion: 1627 Gradiente: [0.16949144721971834,-2.9241848860842774] Loss: 30.911857539559364\n",
      "Iteracion: 1628 Gradiente: [0.1694013403638408,-2.922630299641674] Loss: 30.903280235545942\n",
      "Iteracion: 1629 Gradiente: [0.16931128141151627,-2.9210765396649427] Loss: 30.894712049024488\n",
      "Iteracion: 1630 Gradiente: [0.1692212703372737,-2.91952360571471] Loss: 30.886152970303304\n",
      "Iteracion: 1631 Gradiente: [0.16913130711586177,-2.917971497351821] Loss: 30.877602989701\n",
      "Iteracion: 1632 Gradiente: [0.16904139172145752,-2.9164202141373883] Loss: 30.86906209754644\n",
      "Iteracion: 1633 Gradiente: [0.16895152412898823,-2.9148697556327243] Loss: 30.860530284178832\n",
      "Iteracion: 1634 Gradiente: [0.16886170431278155,-2.913320121399396] Loss: 30.852007539947596\n",
      "Iteracion: 1635 Gradiente: [0.16877193224766576,-2.9117713109991863] Loss: 30.84349385521244\n",
      "Iteracion: 1636 Gradiente: [0.16868220790807792,-2.9102233239941273] Loss: 30.83498922034333\n",
      "Iteracion: 1637 Gradiente: [0.16859253126865117,-2.9086761599464794] Loss: 30.826493625720428\n",
      "Iteracion: 1638 Gradiente: [0.1685029023041769,-2.9071298184187233] Loss: 30.818007061734146\n",
      "Iteracion: 1639 Gradiente: [0.168413320989103,-2.9055842989735936] Loss: 30.809529518785105\n",
      "Iteracion: 1640 Gradiente: [0.16832378729825356,-2.904039601174039] Loss: 30.801060987284135\n",
      "Iteracion: 1641 Gradiente: [0.1682343012062707,-2.902495724583246] Loss: 30.792601457652278\n",
      "Iteracion: 1642 Gradiente: [0.1681448626879141,-2.900952668764636] Loss: 30.784150920320727\n",
      "Iteracion: 1643 Gradiente: [0.16805547171767235,-2.89941043328187] Loss: 30.775709365730847\n",
      "Iteracion: 1644 Gradiente: [0.16796612827057136,-2.8978690176988158] Loss: 30.767276784334193\n",
      "Iteracion: 1645 Gradiente: [0.1678768323210524,-2.896328421579606] Loss: 30.758853166592438\n",
      "Iteracion: 1646 Gradiente: [0.16778758384407647,-2.8947886444885786] Loss: 30.75043850297741\n",
      "Iteracion: 1647 Gradiente: [0.16769838281423072,-2.893249685990321] Loss: 30.74203278397107\n",
      "Iteracion: 1648 Gradiente: [0.1676092292064548,-2.891711545649636] Loss: 30.73363600006546\n",
      "Iteracion: 1649 Gradiente: [0.16752012299554242,-2.890174223031565] Loss: 30.72524814176281\n",
      "Iteracion: 1650 Gradiente: [0.1674310641561263,-2.8886377177013904] Loss: 30.716869199575335\n",
      "Iteracion: 1651 Gradiente: [0.1673420526631247,-2.887102029224612] Loss: 30.708499164025444\n",
      "Iteracion: 1652 Gradiente: [0.16725308849145032,-2.88556715716696] Loss: 30.70013802564554\n",
      "Iteracion: 1653 Gradiente: [0.1671641716157694,-2.884033101094415] Loss: 30.691785774978133\n",
      "Iteracion: 1654 Gradiente: [0.16707530201119264,-2.8824998605731524] Loss: 30.68344240257577\n",
      "Iteracion: 1655 Gradiente: [0.16698647965227262,-2.880967435169623] Loss: 30.67510789900104\n",
      "Iteracion: 1656 Gradiente: [0.16689770451407593,-2.8794358244504763] Loss: 30.66678225482658\n",
      "Iteracion: 1657 Gradiente: [0.16680897657138113,-2.8779050279826017] Loss: 30.658465460635004\n",
      "Iteracion: 1658 Gradiente: [0.16672029579933442,-2.8763750453331114] Loss: 30.650157507019014\n",
      "Iteracion: 1659 Gradiente: [0.1666316621726594,-2.8748458760693603] Loss: 30.641858384581226\n",
      "Iteracion: 1660 Gradiente: [0.1665430756662848,-2.8733175197589294] Loss: 30.63356808393431\n",
      "Iteracion: 1661 Gradiente: [0.16645453625527012,-2.871789975969626] Loss: 30.625286595700892\n",
      "Iteracion: 1662 Gradiente: [0.16636604391457155,-2.8702632442694846] Loss: 30.61701391051355\n",
      "Iteracion: 1663 Gradiente: [0.1662775986190449,-2.8687373242267813] Loss: 30.608750019014828\n",
      "Iteracion: 1664 Gradiente: [0.1661892003437212,-2.8672122154100124] Loss: 30.60049491185723\n",
      "Iteracion: 1665 Gradiente: [0.16610084906366088,-2.8656879173879015] Loss: 30.592248579703192\n",
      "Iteracion: 1666 Gradiente: [0.16601254475394142,-2.8641644297294033] Loss: 30.58401101322507\n",
      "Iteracion: 1667 Gradiente: [0.16592428738938406,-2.862641752003713] Loss: 30.57578220310513\n",
      "Iteracion: 1668 Gradiente: [0.16583607694516142,-2.86111988378024] Loss: 30.56756214003555\n",
      "Iteracion: 1669 Gradiente: [0.16574791339637093,-2.8595988246286255] Loss: 30.559350814718403\n",
      "Iteracion: 1670 Gradiente: [0.16565979671809478,-2.8580785741187382] Loss: 30.55114821786566\n",
      "Iteracion: 1671 Gradiente: [0.16557172688526217,-2.856559131820693] Loss: 30.542954340199127\n",
      "Iteracion: 1672 Gradiente: [0.1654837038730283,-2.8550404973048145] Loss: 30.534769172450517\n",
      "Iteracion: 1673 Gradiente: [0.1653957276565772,-2.8535226701416563] Loss: 30.526592705361345\n",
      "Iteracion: 1674 Gradiente: [0.16530779821100766,-2.852005649902004] Loss: 30.51842492968301\n",
      "Iteracion: 1675 Gradiente: [0.1652199155113114,-2.850489436156883] Loss: 30.510265836176703\n",
      "Iteracion: 1676 Gradiente: [0.16513207953282272,-2.8489740284775253] Loss: 30.502115415613503\n",
      "Iteracion: 1677 Gradiente: [0.16504429025072795,-2.8474594264354] Loss: 30.493973658774227\n",
      "Iteracion: 1678 Gradiente: [0.16495654764000658,-2.8459456296022165] Loss: 30.485840556449524\n",
      "Iteracion: 1679 Gradiente: [0.164868851675989,-2.844432637549892] Loss: 30.47771609943983\n",
      "Iteracion: 1680 Gradiente: [0.16478120233386023,-2.842920449850583] Loss: 30.46960027855537\n",
      "Iteracion: 1681 Gradiente: [0.164693599588828,-2.8414090660766704] Loss: 30.46149308461611\n",
      "Iteracion: 1682 Gradiente: [0.16460604341601007,-2.8398984858007688] Loss: 30.453394508451808\n",
      "Iteracion: 1683 Gradiente: [0.16451853379079703,-2.838388708595703] Loss: 30.44530454090192\n",
      "Iteracion: 1684 Gradiente: [0.16443107068836676,-2.836879734034546] Loss: 30.437223172815706\n",
      "Iteracion: 1685 Gradiente: [0.1643436540840109,-2.8353715616905815] Loss: 30.42915039505209\n",
      "Iteracion: 1686 Gradiente: [0.16425628395306166,-2.833864191137327] Loss: 30.421086198479774\n",
      "Iteracion: 1687 Gradiente: [0.1641689602707146,-2.832357621948527] Loss: 30.413030573977128\n",
      "Iteracion: 1688 Gradiente: [0.16408168301230527,-2.8308518536981544] Loss: 30.404983512432196\n",
      "Iteracion: 1689 Gradiente: [0.1639944521530405,-2.8293468859604074] Loss: 30.396945004742754\n",
      "Iteracion: 1690 Gradiente: [0.16390726766857852,-2.8278427183096957] Loss: 30.38891504181624\n",
      "Iteracion: 1691 Gradiente: [0.16382012953393144,-2.8263393503206826] Loss: 30.380893614569747\n",
      "Iteracion: 1692 Gradiente: [0.16373303772465608,-2.8248367815682363] Loss: 30.372880713930023\n",
      "Iteracion: 1693 Gradiente: [0.16364599221608575,-2.823335011627457] Loss: 30.364876330833486\n",
      "Iteracion: 1694 Gradiente: [0.163558992983567,-2.8218340400736754] Loss: 30.356880456226147\n",
      "Iteracion: 1695 Gradiente: [0.16347204000245388,-2.8203338664824478] Loss: 30.348893081063647\n",
      "Iteracion: 1696 Gradiente: [0.1633851332482692,-2.8188344904295453] Loss: 30.34091419631131\n",
      "Iteracion: 1697 Gradiente: [0.1632982726962851,-2.817335911490979] Loss: 30.33294379294397\n",
      "Iteracion: 1698 Gradiente: [0.16321145832220432,-2.815838129242968] Loss: 30.324981861946107\n",
      "Iteracion: 1699 Gradiente: [0.16312469010128627,-2.814341143261975] Loss: 30.317028394311762\n",
      "Iteracion: 1700 Gradiente: [0.163037968008931,-2.812844953124684] Loss: 30.309083381044566\n",
      "Iteracion: 1701 Gradiente: [0.16295129202075742,-2.8113495584079913] Loss: 30.30114681315771\n",
      "Iteracion: 1702 Gradiente: [0.1628646621122035,-2.809854958689032] Loss: 30.293218681673952\n",
      "Iteracion: 1703 Gradiente: [0.16277807825872134,-2.808361153545161] Loss: 30.285298977625544\n",
      "Iteracion: 1704 Gradiente: [0.1626915404357926,-2.8068681425539603] Loss: 30.277387692054287\n",
      "Iteracion: 1705 Gradiente: [0.16260504861915875,-2.8053759252932244] Loss: 30.269484816011566\n",
      "Iteracion: 1706 Gradiente: [0.1625186027841186,-2.803884501340993] Loss: 30.261590340558207\n",
      "Iteracion: 1707 Gradiente: [0.1624322029064525,-2.8023938702755076] Loss: 30.25370425676455\n",
      "Iteracion: 1708 Gradiente: [0.16234584896165388,-2.8009040316752487] Loss: 30.24582655571048\n",
      "Iteracion: 1709 Gradiente: [0.16225954092510525,-2.799414985118929] Loss: 30.237957228485268\n",
      "Iteracion: 1710 Gradiente: [0.16217327877257048,-2.797926730185463] Loss: 30.23009626618774\n",
      "Iteracion: 1711 Gradiente: [0.16208706247968935,-2.7964392664539997] Loss: 30.22224365992618\n",
      "Iteracion: 1712 Gradiente: [0.16200089202212098,-2.794952593503907] Loss: 30.214399400818248\n",
      "Iteracion: 1713 Gradiente: [0.1619147673752754,-2.7934667109147977] Loss: 30.206563479991136\n",
      "Iteracion: 1714 Gradiente: [0.16182868851501883,-2.7919816182664747] Loss: 30.198735888581396\n",
      "Iteracion: 1715 Gradiente: [0.16174265541698105,-2.790497315138985] Loss: 30.190916617735066\n",
      "Iteracion: 1716 Gradiente: [0.16165666805669662,-2.7890138011126044] Loss: 30.183105658607552\n",
      "Iteracion: 1717 Gradiente: [0.1615707264098854,-2.7875310757678196] Loss: 30.17530300236369\n",
      "Iteracion: 1718 Gradiente: [0.16148483045235196,-2.7860491386853354] Loss: 30.167508640177644\n",
      "Iteracion: 1719 Gradiente: [0.1613989801598019,-2.7845679894460886] Loss: 30.159722563233046\n",
      "Iteracion: 1720 Gradiente: [0.1613131755078257,-2.7830876276312466] Loss: 30.15194476272287\n",
      "Iteracion: 1721 Gradiente: [0.16122741647226302,-2.781608052822181] Loss: 30.144175229849406\n",
      "Iteracion: 1722 Gradiente: [0.16114170302881708,-2.780129264600501] Loss: 30.13641395582433\n",
      "Iteracion: 1723 Gradiente: [0.16105603515336592,-2.778651262548025] Loss: 30.128660931868705\n",
      "Iteracion: 1724 Gradiente: [0.16097041282153554,-2.7771740462468126] Loss: 30.12091614921287\n",
      "Iteracion: 1725 Gradiente: [0.16088483600902823,-2.775697615279134] Loss: 30.11317959909646\n",
      "Iteracion: 1726 Gradiente: [0.16079930469183618,-2.7742219692274728] Loss: 30.10545127276849\n",
      "Iteracion: 1727 Gradiente: [0.16071381884581182,-2.7727471076745434] Loss: 30.097731161487268\n",
      "Iteracion: 1728 Gradiente: [0.16062837844661573,-2.7712730302032895] Loss: 30.090019256520314\n",
      "Iteracion: 1729 Gradiente: [0.16054298347025575,-2.7697997363968607] Loss: 30.082315549144557\n",
      "Iteracion: 1730 Gradiente: [0.16045763389234177,-2.7683272258386493] Loss: 30.074620030646084\n",
      "Iteracion: 1731 Gradiente: [0.1603723296889915,-2.766855498112243] Loss: 30.066932692320304\n",
      "Iteracion: 1732 Gradiente: [0.160287070835921,-2.7653845528014735] Loss: 30.059253525471878\n",
      "Iteracion: 1733 Gradiente: [0.1602018573091063,-2.7639143894903806] Loss: 30.051582521414698\n",
      "Iteracion: 1734 Gradiente: [0.1601166890844065,-2.762445007763229] Loss: 30.043919671471887\n",
      "Iteracion: 1735 Gradiente: [0.16003156613778866,-2.7609764072045055] Loss: 30.036264966975786\n",
      "Iteracion: 1736 Gradiente: [0.15994648844500953,-2.7595085873989236] Loss: 30.028618399267987\n",
      "Iteracion: 1737 Gradiente: [0.15986145598225884,-2.7580415479313984] Loss: 30.020979959699208\n",
      "Iteracion: 1738 Gradiente: [0.1597764687253568,-2.7565752883870873] Loss: 30.01334963962946\n",
      "Iteracion: 1739 Gradiente: [0.15969152665028902,-2.755109808351354] Loss: 30.005727430427854\n",
      "Iteracion: 1740 Gradiente: [0.15960662973299927,-2.753645107409794] Loss: 29.998113323472715\n",
      "Iteracion: 1741 Gradiente: [0.15952177794962855,-2.752181185148206] Loss: 29.99050731015154\n",
      "Iteracion: 1742 Gradiente: [0.15943697127596296,-2.750718041152633] Loss: 29.982909381860946\n",
      "Iteracion: 1743 Gradiente: [0.15935220968811545,-2.749255675009319] Loss: 29.97531953000676\n",
      "Iteracion: 1744 Gradiente: [0.15926749316217817,-2.7477940863047317] Loss: 29.967737746003852\n",
      "Iteracion: 1745 Gradiente: [0.15918282167421202,-2.7463332746255604] Loss: 29.960164021276306\n",
      "Iteracion: 1746 Gradiente: [0.15909819520004617,-2.7448732395587245] Loss: 29.952598347257243\n",
      "Iteracion: 1747 Gradiente: [0.15901361371601297,-2.7434139806913387] Loss: 29.945040715388973\n",
      "Iteracion: 1748 Gradiente: [0.1589290771981368,-2.7419554976107587] Loss: 29.93749111712284\n",
      "Iteracion: 1749 Gradiente: [0.15884458562235343,-2.740497789904558] Loss: 29.9299495439193\n",
      "Iteracion: 1750 Gradiente: [0.15876013896484456,-2.73904085716052] Loss: 29.922415987247867\n",
      "Iteracion: 1751 Gradiente: [0.15867573720199554,-2.737584698966636] Loss: 29.914890438587165\n",
      "Iteracion: 1752 Gradiente: [0.1585913803094821,-2.736129314911156] Loss: 29.907372889424828\n",
      "Iteracion: 1753 Gradiente: [0.15850706826381183,-2.7346747045825084] Loss: 29.89986333125756\n",
      "Iteracion: 1754 Gradiente: [0.15842280104105697,-2.733220867569356] Loss: 29.892361755591125\n",
      "Iteracion: 1755 Gradiente: [0.15833857861724615,-2.7317678034605914] Loss: 29.884868153940264\n",
      "Iteracion: 1756 Gradiente: [0.15825440096867235,-2.7303155118453066] Loss: 29.877382517828785\n",
      "Iteracion: 1757 Gradiente: [0.15817026807155893,-2.7288639923128204] Loss: 29.869904838789488\n",
      "Iteracion: 1758 Gradiente: [0.15808617990207996,-2.7274132444526713] Loss: 29.86243510836417\n",
      "Iteracion: 1759 Gradiente: [0.15800213643640576,-2.7259632678546173] Loss: 29.8549733181036\n",
      "Iteracion: 1760 Gradiente: [0.15791813765079182,-2.7245140621086334] Loss: 29.847519459567614\n",
      "Iteracion: 1761 Gradiente: [0.15783418352162262,-2.723065626804898] Loss: 29.840073524324904\n",
      "Iteracion: 1762 Gradiente: [0.15775027402497793,-2.721617961533836] Loss: 29.83263550395319\n",
      "Iteracion: 1763 Gradiente: [0.15766640913721705,-2.720171065886065] Loss: 29.825205390039148\n",
      "Iteracion: 1764 Gradiente: [0.15758258883460458,-2.7187249394524304] Loss: 29.81778317417835\n",
      "Iteracion: 1765 Gradiente: [0.1574988130933491,-2.7172795818240063] Loss: 29.81036884797537\n",
      "Iteracion: 1766 Gradiente: [0.15741508188986017,-2.715834992592058] Loss: 29.80296240304363\n",
      "Iteracion: 1767 Gradiente: [0.15733139520049852,-2.7143911713480833] Loss: 29.795563831005545\n",
      "Iteracion: 1768 Gradiente: [0.1572477530015135,-2.7129481176838035] Loss: 29.78817312349235\n",
      "Iteracion: 1769 Gradiente: [0.15716415526948377,-2.7115058311911344] Loss: 29.78079027214429\n",
      "Iteracion: 1770 Gradiente: [0.15708060198042423,-2.710064311462245] Loss: 29.773415268610368\n",
      "Iteracion: 1771 Gradiente: [0.15699709311080595,-2.7086235580894926] Loss: 29.766048104548542\n",
      "Iteracion: 1772 Gradiente: [0.15691362863709818,-2.7071835706654555] Loss: 29.758688771625618\n",
      "Iteracion: 1773 Gradiente: [0.15683020853574542,-2.7057443487829285] Loss: 29.751337261517296\n",
      "Iteracion: 1774 Gradiente: [0.15674683278297058,-2.704305892034937] Loss: 29.74399356590803\n",
      "Iteracion: 1775 Gradiente: [0.15666350135532808,-2.7028682000147044] Loss: 29.736657676491216\n",
      "Iteracion: 1776 Gradiente: [0.15658021422930612,-2.7014312723156744] Loss: 29.72932958496904\n",
      "Iteracion: 1777 Gradiente: [0.15649697138121182,-2.6999951085315197] Loss: 29.72200928305247\n",
      "Iteracion: 1778 Gradiente: [0.15641377278761817,-2.698559708256113] Loss: 29.71469676246135\n",
      "Iteracion: 1779 Gradiente: [0.1563306184248527,-2.697125071083557] Loss: 29.707392014924288\n",
      "Iteracion: 1780 Gradiente: [0.15624750826959447,-2.6956911966081525] Loss: 29.700095032178687\n",
      "Iteracion: 1781 Gradiente: [0.1561644422982037,-2.6942580844244333] Loss: 29.69280580597075\n",
      "Iteracion: 1782 Gradiente: [0.1560814204872178,-2.692825734127141] Loss: 29.68552432805544\n",
      "Iteracion: 1783 Gradiente: [0.1559984428132547,-2.6913941453112273] Loss: 29.678250590196473\n",
      "Iteracion: 1784 Gradiente: [0.15591550925268033,-2.6899633175718782] Loss: 29.67098458416636\n",
      "Iteracion: 1785 Gradiente: [0.15583261978215907,-2.68853325050447] Loss: 29.663726301746298\n",
      "Iteracion: 1786 Gradiente: [0.1557497743782875,-2.6871039437046083] Loss: 29.65647573472629\n",
      "Iteracion: 1787 Gradiente: [0.15566697301757604,-2.6856753967681137] Loss: 29.649232874905003\n",
      "Iteracion: 1788 Gradiente: [0.15558421567650244,-2.6842476092910252] Loss: 29.641997714089836\n",
      "Iteracion: 1789 Gradiente: [0.1555015023318693,-2.682820580869579] Loss: 29.634770244096952\n",
      "Iteracion: 1790 Gradiente: [0.15541883296020273,-2.681394311100244] Loss: 29.627550456751145\n",
      "Iteracion: 1791 Gradiente: [0.15533620753800884,-2.679968799579704] Loss: 29.620338343885916\n",
      "Iteracion: 1792 Gradiente: [0.155253626042087,-2.6785440459048377] Loss: 29.613133897343477\n",
      "Iteracion: 1793 Gradiente: [0.15517108844896466,-2.677120049672763] Loss: 29.60593710897466\n",
      "Iteracion: 1794 Gradiente: [0.15508859473547432,-2.675696810480787] Loss: 29.598747970639018\n",
      "Iteracion: 1795 Gradiente: [0.15500614487810083,-2.6742743279264554] Loss: 29.591566474204715\n",
      "Iteracion: 1796 Gradiente: [0.1549237388536298,-2.6728526016075116] Loss: 29.584392611548587\n",
      "Iteracion: 1797 Gradiente: [0.1548413766386626,-2.6714316311219233] Loss: 29.577226374556066\n",
      "Iteracion: 1798 Gradiente: [0.15475905821000424,-2.670011416067859] Loss: 29.570067755121237\n",
      "Iteracion: 1799 Gradiente: [0.15467678354439254,-2.668591956043708] Loss: 29.562916745146822\n",
      "Iteracion: 1800 Gradiente: [0.15459455261849797,-2.6671732506480774] Loss: 29.55577333654409\n",
      "Iteracion: 1801 Gradiente: [0.15451236540905267,-2.665755299479782] Loss: 29.54863752123299\n",
      "Iteracion: 1802 Gradiente: [0.15443022189294217,-2.6643381021378487] Loss: 29.54150929114198\n",
      "Iteracion: 1803 Gradiente: [0.1543481220466968,-2.66292165822153] Loss: 29.53438863820813\n",
      "Iteracion: 1804 Gradiente: [0.15426606584729258,-2.6615059673302737] Loss: 29.527275554377095\n",
      "Iteracion: 1805 Gradiente: [0.1541840532715412,-2.660091029063746] Loss: 29.520170031603104\n",
      "Iteracion: 1806 Gradiente: [0.15410208429618658,-2.6586768430218295] Loss: 29.513072061848863\n",
      "Iteracion: 1807 Gradiente: [0.15402015889807358,-2.6572634088046234] Loss: 29.505981637085704\n",
      "Iteracion: 1808 Gradiente: [0.15393827705405366,-2.655850726012429] Loss: 29.49889874929347\n",
      "Iteracion: 1809 Gradiente: [0.15385643874080682,-2.6544387942457757] Loss: 29.491823390460507\n",
      "Iteracion: 1810 Gradiente: [0.1537746439353536,-2.6530276131053863] Loss: 29.484755552583657\n",
      "Iteracion: 1811 Gradiente: [0.15369289261461366,-2.651617182192203] Loss: 29.477695227668374\n",
      "Iteracion: 1812 Gradiente: [0.15361118475525756,-2.6502075011073933] Loss: 29.47064240772848\n",
      "Iteracion: 1813 Gradiente: [0.1535295203343812,-2.6487985694523126] Loss: 29.463597084786382\n",
      "Iteracion: 1814 Gradiente: [0.153447899328881,-2.647390386828543] Loss: 29.456559250872907\n",
      "Iteracion: 1815 Gradiente: [0.15336632171559708,-2.6459829528378824] Loss: 29.4495288980274\n",
      "Iteracion: 1816 Gradiente: [0.15328478747137902,-2.644576267082337] Loss: 29.442506018297617\n",
      "Iteracion: 1817 Gradiente: [0.15320329657328385,-2.6431703291641164] Loss: 29.435490603739808\n",
      "Iteracion: 1818 Gradiente: [0.1531218489983535,-2.6417651386856402] Loss: 29.42848264641866\n",
      "Iteracion: 1819 Gradiente: [0.15304044472343795,-2.64036069524956] Loss: 29.42148213840731\n",
      "Iteracion: 1820 Gradiente: [0.15295908372543038,-2.638956998458722] Loss: 29.41448907178723\n",
      "Iteracion: 1821 Gradiente: [0.15287776598145703,-2.637554047916183] Loss: 29.40750343864845\n",
      "Iteracion: 1822 Gradiente: [0.15279649146854316,-2.6361518432252096] Loss: 29.400525231089293\n",
      "Iteracion: 1823 Gradiente: [0.1527152601636421,-2.6347503839892954] Loss: 29.393554441216562\n",
      "Iteracion: 1824 Gradiente: [0.15263407204377633,-2.633349669812128] Loss: 29.38659106114538\n",
      "Iteracion: 1825 Gradiente: [0.15255292708600715,-2.6319497002976115] Loss: 29.379635082999297\n",
      "Iteracion: 1826 Gradiente: [0.15247182526736652,-2.6305504750498625] Loss: 29.37268649891023\n",
      "Iteracion: 1827 Gradiente: [0.1523907665649271,-2.6291519936732035] Loss: 29.36574530101846\n",
      "Iteracion: 1828 Gradiente: [0.15230975095579993,-2.6277542557721705] Loss: 29.35881148147259\n",
      "Iteracion: 1829 Gradiente: [0.15222877841719176,-2.6263572609515013] Loss: 29.35188503242962\n",
      "Iteracion: 1830 Gradiente: [0.1521478489258584,-2.6249610088161726] Loss: 29.344965946054852\n",
      "Iteracion: 1831 Gradiente: [0.15206696245922538,-2.6235654989713297] Loss: 29.338054214521943\n",
      "Iteracion: 1832 Gradiente: [0.15198611899434128,-2.6221707310223534] Loss: 29.331149830012816\n",
      "Iteracion: 1833 Gradiente: [0.15190531850837108,-2.6207767045748294] Loss: 29.324252784717768\n",
      "Iteracion: 1834 Gradiente: [0.15182456097834063,-2.6193834192345595] Loss: 29.317363070835366\n",
      "Iteracion: 1835 Gradiente: [0.1517438463814609,-2.6179908746075418] Loss: 29.31048068057247\n",
      "Iteracion: 1836 Gradiente: [0.15166317469504473,-2.6165990702999875] Loss: 29.303605606144224\n",
      "Iteracion: 1837 Gradiente: [0.1515825458961719,-2.6152080059183245] Loss: 29.296737839774064\n",
      "Iteracion: 1838 Gradiente: [0.15150195996192697,-2.6138176810691904] Loss: 29.289877373693685\n",
      "Iteracion: 1839 Gradiente: [0.1514214168697303,-2.612428095359417] Loss: 29.283024200143004\n",
      "Iteracion: 1840 Gradiente: [0.15134091659666163,-2.6110392483960636] Loss: 29.27617831137023\n",
      "Iteracion: 1841 Gradiente: [0.15126045911998603,-2.609651139786387] Loss: 29.26933969963181\n",
      "Iteracion: 1842 Gradiente: [0.15118004441697508,-2.608263769137856] Loss: 29.262508357192416\n",
      "Iteracion: 1843 Gradiente: [0.15109967246493264,-2.606877136058143] Loss: 29.255684276324914\n",
      "Iteracion: 1844 Gradiente: [0.15101934324107827,-2.605491240155137] Loss: 29.24886744931042\n",
      "Iteracion: 1845 Gradiente: [0.15093905672274274,-2.6041060810369334] Loss: 29.242057868438245\n",
      "Iteracion: 1846 Gradiente: [0.1508588128871608,-2.602721658311834] Loss: 29.235255526005876\n",
      "Iteracion: 1847 Gradiente: [0.15077861171163248,-2.601337971588353] Loss: 29.228460414319056\n",
      "Iteracion: 1848 Gradiente: [0.15069845317353744,-2.599955020475203] Loss: 29.221672525691595\n",
      "Iteracion: 1849 Gradiente: [0.1506183372501487,-2.5985728045813166] Loss: 29.214891852445557\n",
      "Iteracion: 1850 Gradiente: [0.1505382639188961,-2.597191323515823] Loss: 29.208118386911153\n",
      "Iteracion: 1851 Gradiente: [0.15045823315702572,-2.595810576888072] Loss: 29.201352121426744\n",
      "Iteracion: 1852 Gradiente: [0.1503782449420143,-2.5944305643076073] Loss: 29.194593048338795\n",
      "Iteracion: 1853 Gradiente: [0.15029829925122773,-2.5930512853841874] Loss: 29.18784116000197\n",
      "Iteracion: 1854 Gradiente: [0.150218396061976,-2.5916727397277817] Loss: 29.181096448779027\n",
      "Iteracion: 1855 Gradiente: [0.15013853535169328,-2.590294926948563] Loss: 29.174358907040848\n",
      "Iteracion: 1856 Gradiente: [0.15005871709781787,-2.5889178466569094] Loss: 29.167628527166418\n",
      "Iteracion: 1857 Gradiente: [0.14997894127779052,-2.587541498463408] Loss: 29.160905301542837\n",
      "Iteracion: 1858 Gradiente: [0.1498992078690776,-2.586165881978848] Loss: 29.15418922256526\n",
      "Iteracion: 1859 Gradiente: [0.1498195168490753,-2.5847909968142373] Loss: 29.14748028263701\n",
      "Iteracion: 1860 Gradiente: [0.14973986819519544,-2.583416842580787] Loss: 29.14077847416937\n",
      "Iteracion: 1861 Gradiente: [0.1496602618850787,-2.5820434188899006] Loss: 29.13408378958178\n",
      "Iteracion: 1862 Gradiente: [0.14958069789606251,-2.5806707253532073] Loss: 29.1273962213017\n",
      "Iteracion: 1863 Gradiente: [0.14950117620566344,-2.5792987615825376] Loss: 29.12071576176466\n",
      "Iteracion: 1864 Gradiente: [0.14942169679152167,-2.577927527189914] Loss: 29.114042403414185\n",
      "Iteracion: 1865 Gradiente: [0.14934225963104003,-2.576557021787585] Loss: 29.107376138701873\n",
      "Iteracion: 1866 Gradiente: [0.14926286470178288,-2.5751872449879962] Loss: 29.100716960087365\n",
      "Iteracion: 1867 Gradiente: [0.14918351198134966,-2.573818196403797] Loss: 29.09406486003828\n",
      "Iteracion: 1868 Gradiente: [0.14910420144715886,-2.572449875647852] Loss: 29.087419831030218\n",
      "Iteracion: 1869 Gradiente: [0.14902493307694678,-2.571082282333219] Loss: 29.080781865546864\n",
      "Iteracion: 1870 Gradiente: [0.14894570684830863,-2.5697154160731657] Loss: 29.074150956079816\n",
      "Iteracion: 1871 Gradiente: [0.14886652273872916,-2.568349276481173] Loss: 29.06752709512867\n",
      "Iteracion: 1872 Gradiente: [0.1487873807257993,-2.5669838631709245] Loss: 29.060910275201024\n",
      "Iteracion: 1873 Gradiente: [0.14870828078715828,-2.565619175756304] Loss: 29.054300488812395\n",
      "Iteracion: 1874 Gradiente: [0.14862922290051397,-2.5642552138514003] Loss: 29.047697728486312\n",
      "Iteracion: 1875 Gradiente: [0.14855020704352645,-2.5628919770705085] Loss: 29.041101986754192\n",
      "Iteracion: 1876 Gradiente: [0.14847123319371983,-2.5615294650281366] Loss: 29.034513256155417\n",
      "Iteracion: 1877 Gradiente: [0.14839230132894746,-2.560167677338983] Loss: 29.027931529237332\n",
      "Iteracion: 1878 Gradiente: [0.14831341142657378,-2.558806613617975] Loss: 29.021356798555143\n",
      "Iteracion: 1879 Gradiente: [0.1482345634646388,-2.5574462734802137] Loss: 29.01478905667202\n",
      "Iteracion: 1880 Gradiente: [0.1481557574206652,-2.556086656541024] Loss: 29.008228296158997\n",
      "Iteracion: 1881 Gradiente: [0.14807699327235752,-2.5547277624159364] Loss: 29.00167450959504\n",
      "Iteracion: 1882 Gradiente: [0.14799827099752652,-2.5533695907206733] Loss: 28.995127689567006\n",
      "Iteracion: 1883 Gradiente: [0.14791959057394025,-2.552012141071169] Loss: 28.988587828669573\n",
      "Iteracion: 1884 Gradiente: [0.14784095197922983,-2.5506554130835677] Loss: 28.98205491950536\n",
      "Iteracion: 1885 Gradiente: [0.14776235519124015,-2.549299406374208] Loss: 28.975528954684833\n",
      "Iteracion: 1886 Gradiente: [0.147683800187599,-2.5479441205596425] Loss: 28.969009926826274\n",
      "Iteracion: 1887 Gradiente: [0.1476052869463274,-2.54658955525661] Loss: 28.962497828555858\n",
      "Iteracion: 1888 Gradiente: [0.1475268154451238,-2.5452357100820704] Loss: 28.95599265250758\n",
      "Iteracion: 1889 Gradiente: [0.14744838566174395,-2.5438825846531823] Loss: 28.94949439132326\n",
      "Iteracion: 1890 Gradiente: [0.14736999757407243,-2.5425301785873065] Loss: 28.943003037652545\n",
      "Iteracion: 1891 Gradiente: [0.14729165115986972,-2.5411784915020084] Loss: 28.93651858415289\n",
      "Iteracion: 1892 Gradiente: [0.14721334639702519,-2.539827523015054] Loss: 28.930041023489558\n",
      "Iteracion: 1893 Gradiente: [0.14713508326337507,-2.5384772727444176] Loss: 28.923570348335627\n",
      "Iteracion: 1894 Gradiente: [0.1470568617368163,-2.53712774030827] Loss: 28.917106551371944\n",
      "Iteracion: 1895 Gradiente: [0.1469786817952818,-2.5357789253249847] Loss: 28.910649625287125\n",
      "Iteracion: 1896 Gradiente: [0.14690054341650124,-2.534430827413152] Loss: 28.904199562777567\n",
      "Iteracion: 1897 Gradiente: [0.14682244657863303,-2.533083446191541] Loss: 28.897756356547465\n",
      "Iteracion: 1898 Gradiente: [0.14674439125935426,-2.5317367812791494] Loss: 28.8913199993087\n",
      "Iteracion: 1899 Gradiente: [0.14666637743663719,-2.530390832295161] Loss: 28.884890483780968\n",
      "Iteracion: 1900 Gradiente: [0.14658840508856155,-2.5290455988589637] Loss: 28.87846780269168\n",
      "Iteracion: 1901 Gradiente: [0.1465104741928883,-2.527701080590157] Loss: 28.872051948775958\n",
      "Iteracion: 1902 Gradiente: [0.14643258472767448,-2.526357277108531] Loss: 28.86564291477665\n",
      "Iteracion: 1903 Gradiente: [0.14635473667101168,-2.525014188034078] Loss: 28.859240693444367\n",
      "Iteracion: 1904 Gradiente: [0.14627693000065894,-2.523671812987007] Loss: 28.852845277537345\n",
      "Iteracion: 1905 Gradiente: [0.14619916469473537,-2.5223301515877146] Loss: 28.846456659821587\n",
      "Iteracion: 1906 Gradiente: [0.14612144073118477,-2.520989203456807] Loss: 28.84007483307077\n",
      "Iteracion: 1907 Gradiente: [0.14604375808811862,-2.5196489682150838] Loss: 28.833699790066216\n",
      "Iteracion: 1908 Gradiente: [0.14596611674360152,-2.5183094454835486] Loss: 28.82733152359697\n",
      "Iteracion: 1909 Gradiente: [0.1458885166755427,-2.5169706348834193] Loss: 28.82097002645971\n",
      "Iteracion: 1910 Gradiente: [0.14581095786193288,-2.5156325360361063] Loss: 28.814615291458757\n",
      "Iteracion: 1911 Gradiente: [0.14573344028114502,-2.5142951485632015] Loss: 28.80826731140614\n",
      "Iteracion: 1912 Gradiente: [0.1456559639110007,-2.5129584720865332] Loss: 28.80192607912148\n",
      "Iteracion: 1913 Gradiente: [0.14557852872971183,-2.511622506228107] Loss: 28.79559158743202\n",
      "Iteracion: 1914 Gradiente: [0.1455011347153871,-2.510287250610136] Loss: 28.7892638291727\n",
      "Iteracion: 1915 Gradiente: [0.145423781846083,-2.5089527048550377] Loss: 28.782942797185974\n",
      "Iteracion: 1916 Gradiente: [0.14534647009987414,-2.5076188685854297] Loss: 28.776628484321986\n",
      "Iteracion: 1917 Gradiente: [0.1452691994550354,-2.5062857414241204] Loss: 28.770320883438465\n",
      "Iteracion: 1918 Gradiente: [0.14519196988956698,-2.5049533229941336] Loss: 28.764019987400676\n",
      "Iteracion: 1919 Gradiente: [0.1451147813817542,-2.503621612918681] Loss: 28.757725789081544\n",
      "Iteracion: 1920 Gradiente: [0.1450376339096806,-2.5022906108211838] Loss: 28.751438281361526\n",
      "Iteracion: 1921 Gradiente: [0.14496052745163912,-2.500960316325251] Loss: 28.745157457128652\n",
      "Iteracion: 1922 Gradiente: [0.14488346198563845,-2.4996307290547124] Loss: 28.738883309278528\n",
      "Iteracion: 1923 Gradiente: [0.14480643749011884,-2.4983018486335724] Loss: 28.73261583071429\n",
      "Iteracion: 1924 Gradiente: [0.14472945394317377,-2.496973674686053] Loss: 28.726355014346627\n",
      "Iteracion: 1925 Gradiente: [0.1446525113230564,-2.495646206836574] Loss: 28.720100853093793\n",
      "Iteracion: 1926 Gradiente: [0.14457560960797386,-2.49431944470975] Loss: 28.71385333988151\n",
      "Iteracion: 1927 Gradiente: [0.14449874877614613,-2.4929933879304] Loss: 28.707612467643056\n",
      "Iteracion: 1928 Gradiente: [0.1444219288059737,-2.4916680361235324] Loss: 28.70137822931924\n",
      "Iteracion: 1929 Gradiente: [0.14434514967556045,-2.4903433889143733] Loss: 28.695150617858346\n",
      "Iteracion: 1930 Gradiente: [0.14426841136327226,-2.4890194459283306] Loss: 28.688929626216122\n",
      "Iteracion: 1931 Gradiente: [0.14419171384752,-2.4876962067910124] Loss: 28.682715247355873\n",
      "Iteracion: 1932 Gradiente: [0.14411505710649947,-2.4863736711282374] Loss: 28.67650747424836\n",
      "Iteracion: 1933 Gradiente: [0.14403844111866704,-2.485051838566009] Loss: 28.670306299871804\n",
      "Iteracion: 1934 Gradiente: [0.14396186586212753,-2.4837307087305507] Loss: 28.664111717211863\n",
      "Iteracion: 1935 Gradiente: [0.14388533131542733,-2.482410281248261] Loss: 28.657923719261735\n",
      "Iteracion: 1936 Gradiente: [0.14380883745677028,-2.481090555745753] Loss: 28.65174229902197\n",
      "Iteracion: 1937 Gradiente: [0.14373238426454643,-2.4797715318498357] Loss: 28.645567449500607\n",
      "Iteracion: 1938 Gradiente: [0.1436559717172173,-2.4784532091875064] Loss: 28.639399163713154\n",
      "Iteracion: 1939 Gradiente: [0.14357959979323265,-2.4771355873859653] Loss: 28.633237434682428\n",
      "Iteracion: 1940 Gradiente: [0.14350326847080244,-2.4758186660726236] Loss: 28.6270822554388\n",
      "Iteracion: 1941 Gradiente: [0.143426977728375,-2.4745024448750783] Loss: 28.620933619019954\n",
      "Iteracion: 1942 Gradiente: [0.14335072754464306,-2.473186923421112] Loss: 28.614791518471016\n",
      "Iteracion: 1943 Gradiente: [0.1432745178978261,-2.4718721013387306] Loss: 28.608655946844504\n",
      "Iteracion: 1944 Gradiente: [0.1431983487663525,-2.47055797825613] Loss: 28.60252689720027\n",
      "Iteracion: 1945 Gradiente: [0.14312222012871084,-2.469244553801697] Loss: 28.59640436260564\n",
      "Iteracion: 1946 Gradiente: [0.1430461319633101,-2.467931827604023] Loss: 28.59028833613523\n",
      "Iteracion: 1947 Gradiente: [0.14297008424880317,-2.4666197992918866] Loss: 28.58417881087104\n",
      "Iteracion: 1948 Gradiente: [0.14289407696356685,-2.465308468494275] Loss: 28.578075779902438\n",
      "Iteracion: 1949 Gradiente: [0.1428181100861271,-2.4639978348403644] Loss: 28.571979236326104\n",
      "Iteracion: 1950 Gradiente: [0.14274218359506488,-2.462687897959532] Loss: 28.565889173246088\n",
      "Iteracion: 1951 Gradiente: [0.1426662974688327,-2.461378657481354] Loss: 28.559805583773777\n",
      "Iteracion: 1952 Gradiente: [0.142590451685966,-2.4600701130356044] Loss: 28.553728461027834\n",
      "Iteracion: 1953 Gradiente: [0.14251464622507606,-2.458762264252244] Loss: 28.547657798134296\n",
      "Iteracion: 1954 Gradiente: [0.1424388810647206,-2.4574551107614413] Loss: 28.541593588226455\n",
      "Iteracion: 1955 Gradiente: [0.14236315618341563,-2.4561486521935576] Loss: 28.535535824444935\n",
      "Iteracion: 1956 Gradiente: [0.14228747155979798,-2.45484288817915] Loss: 28.529484499937645\n",
      "Iteracion: 1957 Gradiente: [0.142211827172423,-2.453537818348976] Loss: 28.52343960785975\n",
      "Iteracion: 1958 Gradiente: [0.1421362230000175,-2.452233442333974] Loss: 28.51740114137373\n",
      "Iteracion: 1959 Gradiente: [0.14206065902095494,-2.4509297597653097] Loss: 28.511369093649343\n",
      "Iteracion: 1960 Gradiente: [0.14198513521408812,-2.449626770274311] Loss: 28.505343457863532\n",
      "Iteracion: 1961 Gradiente: [0.1419096515580724,-2.448324473492515] Loss: 28.49932422720057\n",
      "Iteracion: 1962 Gradiente: [0.14183420803140098,-2.4470228690516675] Loss: 28.493311394851947\n",
      "Iteracion: 1963 Gradiente: [0.14175880461288556,-2.445721956583691] Loss: 28.4873049540164\n",
      "Iteracion: 1964 Gradiente: [0.14168344128102842,-2.444421735720719] Loss: 28.481304897899854\n",
      "Iteracion: 1965 Gradiente: [0.14160811801464254,-2.4431222060950675] Loss: 28.475311219715522\n",
      "Iteracion: 1966 Gradiente: [0.14153283479260353,-2.441823367339242] Loss: 28.46932391268377\n",
      "Iteracion: 1967 Gradiente: [0.14145759159329147,-2.4405252190859765] Loss: 28.463342970032226\n",
      "Iteracion: 1968 Gradiente: [0.14138238839560177,-2.4392277609681656] Loss: 28.45736838499568\n",
      "Iteracion: 1969 Gradiente: [0.1413072251781936,-2.437930992618919] Loss: 28.451400150816095\n",
      "Iteracion: 1970 Gradiente: [0.141232101919924,-2.4366349136715284] Loss: 28.44543826074268\n",
      "Iteracion: 1971 Gradiente: [0.1411570185994838,-2.4353395237594877] Loss: 28.439482708031793\n",
      "Iteracion: 1972 Gradiente: [0.14108197519559507,-2.434044822516489] Loss: 28.433533485946896\n",
      "Iteracion: 1973 Gradiente: [0.14100697168703866,-2.432750809576414] Loss: 28.42759058775873\n",
      "Iteracion: 1974 Gradiente: [0.14093200805277822,-2.4314574845733294] Loss: 28.421654006745094\n",
      "Iteracion: 1975 Gradiente: [0.14085708427140892,-2.4301648471415205] Loss: 28.415723736190976\n",
      "Iteracion: 1976 Gradiente: [0.14078220032180203,-2.428872896915448] Loss: 28.409799769388478\n",
      "Iteracion: 1977 Gradiente: [0.1407073561828658,-2.427581633529769] Loss: 28.40388209963688\n",
      "Iteracion: 1978 Gradiente: [0.1406325518333342,-2.42629105661934] Loss: 28.39797072024249\n",
      "Iteracion: 1979 Gradiente: [0.14055778725208748,-2.4250011658192134] Loss: 28.39206562451887\n",
      "Iteracion: 1980 Gradiente: [0.14048306241795244,-2.42371196076463] Loss: 28.38616680578654\n",
      "Iteracion: 1981 Gradiente: [0.1404083773098989,-2.4224234410910213] Loss: 28.380274257373216\n",
      "Iteracion: 1982 Gradiente: [0.14033373190667456,-2.4211356064340266] Loss: 28.374387972613704\n",
      "Iteracion: 1983 Gradiente: [0.140259126187388,-2.419848456429457] Loss: 28.368507944849853\n",
      "Iteracion: 1984 Gradiente: [0.14018456013066044,-2.4185619907133438] Loss: 28.362634167430606\n",
      "Iteracion: 1985 Gradiente: [0.14011003371548678,-2.4172762089218987] Loss: 28.356766633712002\n",
      "Iteracion: 1986 Gradiente: [0.14003554692099082,-2.4159911106915115] Loss: 28.3509053370571\n",
      "Iteracion: 1987 Gradiente: [0.13996109972586718,-2.4147066956587935] Loss: 28.34505027083604\n",
      "Iteracion: 1988 Gradiente: [0.13988669210915675,-2.4134229634605333] Loss: 28.339201428425987\n",
      "Iteracion: 1989 Gradiente: [0.13981232404987243,-2.4121399137337107] Loss: 28.33335880321119\n",
      "Iteracion: 1990 Gradiente: [0.139737995526833,-2.41085754611551] Loss: 28.32752238858287\n",
      "Iteracion: 1991 Gradiente: [0.13966370651925786,-2.409575860243291] Loss: 28.32169217793936\n",
      "Iteracion: 1992 Gradiente: [0.13958945700592834,-2.408294855754624] Loss: 28.315868164685902\n",
      "Iteracion: 1993 Gradiente: [0.13951524696591094,-2.4070145322872656] Loss: 28.310050342234824\n",
      "Iteracion: 1994 Gradiente: [0.13944107637826875,-2.4057348894791586] Loss: 28.30423870400541\n",
      "Iteracion: 1995 Gradiente: [0.13936694522195409,-2.4044559269684505] Loss: 28.298433243424004\n",
      "Iteracion: 1996 Gradiente: [0.1392928534760614,-2.403177644393467] Loss: 28.292633953923897\n",
      "Iteracion: 1997 Gradiente: [0.1392188011195311,-2.401900041392743] Loss: 28.28684082894531\n",
      "Iteracion: 1998 Gradiente: [0.1391447881316528,-2.4006231176049817] Loss: 28.281053861935547\n",
      "Iteracion: 1999 Gradiente: [0.1390708144912158,-2.3993468726691067] Loss: 28.27527304634876\n",
      "Iteracion: 2000 Gradiente: [0.13899688017746997,-2.3980713062242134] Loss: 28.269498375646165\n",
      "Iteracion: 2001 Gradiente: [0.13892298516951768,-2.396796417909593] Loss: 28.26372984329586\n",
      "Iteracion: 2002 Gradiente: [0.1388491294463165,-2.395522207364739] Loss: 28.25796744277291\n",
      "Iteracion: 2003 Gradiente: [0.13877531298720905,-2.3942486742293125] Loss: 28.25221116755931\n",
      "Iteracion: 2004 Gradiente: [0.13870153577121158,-2.3929758181431944] Loss: 28.246461011144\n",
      "Iteracion: 2005 Gradiente: [0.1386277977773214,-2.3917036387464474] Loss: 28.240716967022806\n",
      "Iteracion: 2006 Gradiente: [0.1385540989848683,-2.3904321356793132] Loss: 28.23497902869849\n",
      "Iteracion: 2007 Gradiente: [0.13848043937297472,-2.389161308582238] Loss: 28.229247189680756\n",
      "Iteracion: 2008 Gradiente: [0.13840681892077622,-2.3878911570958565] Loss: 28.223521443486128\n",
      "Iteracion: 2009 Gradiente: [0.13833323760751598,-2.386621680860988] Loss: 28.217801783638084\n",
      "Iteracion: 2010 Gradiente: [0.13825969541226849,-2.3853528795186567] Loss: 28.212088203666976\n",
      "Iteracion: 2011 Gradiente: [0.1381861923143352,-2.3840847527100637] Loss: 28.206380697109978\n",
      "Iteracion: 2012 Gradiente: [0.13811272829285315,-2.3828173000766104] Loss: 28.20067925751124\n",
      "Iteracion: 2013 Gradiente: [0.13803930332719289,-2.3815505212598747] Loss: 28.19498387842168\n",
      "Iteracion: 2014 Gradiente: [0.13796591739643324,-2.3802844159016443] Loss: 28.18929455339912\n",
      "Iteracion: 2015 Gradiente: [0.13789257047993525,-2.3790189836438844] Loss: 28.1836112760082\n",
      "Iteracion: 2016 Gradiente: [0.13781926255687058,-2.3777542241287564] Loss: 28.177934039820418\n",
      "Iteracion: 2017 Gradiente: [0.13774599360657666,-2.3764901369986053] Loss: 28.172262838414117\n",
      "Iteracion: 2018 Gradiente: [0.13767276360834632,-2.375226721895973] Loss: 28.166597665374436\n",
      "Iteracion: 2019 Gradiente: [0.13759957254132,-2.3739639784635953] Loss: 28.16093851429338\n",
      "Iteracion: 2020 Gradiente: [0.13752642038506432,-2.3727019063443766] Loss: 28.155285378769715\n",
      "Iteracion: 2021 Gradiente: [0.13745330711852072,-2.371440505181448] Loss: 28.149638252408987\n",
      "Iteracion: 2022 Gradiente: [0.1373802327213904,-2.3701797746180877] Loss: 28.143997128823667\n",
      "Iteracion: 2023 Gradiente: [0.13730719717285447,-2.368919714297794] Loss: 28.13836200163289\n",
      "Iteracion: 2024 Gradiente: [0.1372342004521935,-2.3676603238642446] Loss: 28.132732864462604\n",
      "Iteracion: 2025 Gradiente: [0.13716124253889792,-2.3664016029613033] Loss: 28.127109710945554\n",
      "Iteracion: 2026 Gradiente: [0.13708832341217533,-2.3651435512330363] Loss: 28.121492534721245\n",
      "Iteracion: 2027 Gradiente: [0.13701544305158014,-2.3638861683236794] Loss: 28.11588132943597\n",
      "Iteracion: 2028 Gradiente: [0.1369426014363569,-2.3626294538776738] Loss: 28.110276088742694\n",
      "Iteracion: 2029 Gradiente: [0.13686979854605472,-2.3613734075396393] Loss: 28.104676806301235\n",
      "Iteracion: 2030 Gradiente: [0.13679703436003385,-2.36011802895439] Loss: 28.099083475778052\n",
      "Iteracion: 2031 Gradiente: [0.13672430885760178,-2.358863317766935] Loss: 28.093496090846394\n",
      "Iteracion: 2032 Gradiente: [0.1366516220182471,-2.357609273622462] Loss: 28.087914645186267\n",
      "Iteracion: 2033 Gradiente: [0.13657897382145498,-2.356355896166349] Loss: 28.082339132484307\n",
      "Iteracion: 2034 Gradiente: [0.13650636424677268,-2.3551031850441575] Loss: 28.076769546433905\n",
      "Iteracion: 2035 Gradiente: [0.1364337932735116,-2.3538511399016526] Loss: 28.071205880735157\n",
      "Iteracion: 2036 Gradiente: [0.1363612608812365,-2.352599760384777] Loss: 28.06564812909489\n",
      "Iteracion: 2037 Gradiente: [0.13628876704929666,-2.3513490461396684] Loss: 28.060096285226543\n",
      "Iteracion: 2038 Gradiente: [0.13621631175745866,-2.350098996812634] Loss: 28.05455034285031\n",
      "Iteracion: 2039 Gradiente: [0.13614389498493817,-2.348849612050201] Loss: 28.049010295693027\n",
      "Iteracion: 2040 Gradiente: [0.13607151671145487,-2.3476008914990527] Loss: 28.04347613748816\n",
      "Iteracion: 2041 Gradiente: [0.1359991769165049,-2.3463528348060776] Loss: 28.037947861975933\n",
      "Iteracion: 2042 Gradiente: [0.1359268755795938,-2.345105441618351] Loss: 28.032425462903152\n",
      "Iteracion: 2043 Gradiente: [0.1358546126802679,-2.343858711583132] Loss: 28.02690893402326\n",
      "Iteracion: 2044 Gradiente: [0.13578238819823932,-2.3426126443478608] Loss: 28.02139826909639\n",
      "Iteracion: 2045 Gradiente: [0.13571020211281185,-2.341367239560188] Loss: 28.015893461889295\n",
      "Iteracion: 2046 Gradiente: [0.13563805440377952,-2.340122496867925] Loss: 28.010394506175324\n",
      "Iteracion: 2047 Gradiente: [0.13556594505069577,-2.3388784159190834] Loss: 28.004901395734464\n",
      "Iteracion: 2048 Gradiente: [0.1354938740331439,-2.3376349963618623] Loss: 27.999414124353326\n",
      "Iteracion: 2049 Gradiente: [0.13542184133071752,-2.3363922378446462] Loss: 27.993932685825087\n",
      "Iteracion: 2050 Gradiente: [0.1353498469231179,-2.3351501400160006] Loss: 27.988457073949597\n",
      "Iteracion: 2051 Gradiente: [0.13527789078997046,-2.333908702524688] Loss: 27.98298728253321\n",
      "Iteracion: 2052 Gradiente: [0.13520597291088166,-2.3326679250196523] Loss: 27.977523305388903\n",
      "Iteracion: 2053 Gradiente: [0.13513409326557166,-2.331427807150023] Loss: 27.97206513633626\n",
      "Iteracion: 2054 Gradiente: [0.1350622518336621,-2.33018834856512] Loss: 27.96661276920137\n",
      "Iteracion: 2055 Gradiente: [0.13499044859491013,-2.328949548914441] Loss: 27.96116619781691\n",
      "Iteracion: 2056 Gradiente: [0.13491868352894015,-2.327711407847683] Loss: 27.95572541602215\n",
      "Iteracion: 2057 Gradiente: [0.13484695661546528,-2.3264739250147217] Loss: 27.95029041766287\n",
      "Iteracion: 2058 Gradiente: [0.13477526783432173,-2.3252371000656136] Loss: 27.944861196591383\n",
      "Iteracion: 2059 Gradiente: [0.13470361716501505,-2.324000932650619] Loss: 27.939437746666577\n",
      "Iteracion: 2060 Gradiente: [0.1346320045874156,-2.3227654224201646] Loss: 27.93402006175385\n",
      "Iteracion: 2061 Gradiente: [0.13456043008126148,-2.321530569024873] Loss: 27.92860813572508\n",
      "Iteracion: 2062 Gradiente: [0.13448889362631936,-2.320296372115549] Loss: 27.92320196245873\n",
      "Iteracion: 2063 Gradiente: [0.13441739520236903,-2.3190628313431842] Loss: 27.917801535839725\n",
      "Iteracion: 2064 Gradiente: [0.1343459347891572,-2.3178299463589593] Loss: 27.91240684975951\n",
      "Iteracion: 2065 Gradiente: [0.13427451236649118,-2.316597716814235] Loss: 27.907017898116013\n",
      "Iteracion: 2066 Gradiente: [0.1342031279142584,-2.3153661423605545] Loss: 27.90163467481363\n",
      "Iteracion: 2067 Gradiente: [0.13413178141206003,-2.314135222649666] Loss: 27.89625717376328\n",
      "Iteracion: 2068 Gradiente: [0.1340604728398849,-2.3129049573334743] Loss: 27.890885388882314\n",
      "Iteracion: 2069 Gradiente: [0.1339892021775512,-2.3116753460640864] Loss: 27.885519314094545\n",
      "Iteracion: 2070 Gradiente: [0.13391796940499603,-2.310446388493787] Loss: 27.88015894333033\n",
      "Iteracion: 2071 Gradiente: [0.1338467745018936,-2.3092180842750585] Loss: 27.874804270526358\n",
      "Iteracion: 2072 Gradiente: [0.13377561744815647,-2.307990433060558] Loss: 27.869455289625833\n",
      "Iteracion: 2073 Gradiente: [0.13370449822365912,-2.306763434503125] Loss: 27.86411199457839\n",
      "Iteracion: 2074 Gradiente: [0.13363341680842353,-2.305537088255783] Loss: 27.85877437934008\n",
      "Iteracion: 2075 Gradiente: [0.1335623731822267,-2.304311393971751] Loss: 27.853442437873397\n",
      "Iteracion: 2076 Gradiente: [0.13349136732496306,-2.3030863513044237] Loss: 27.848116164147207\n",
      "Iteracion: 2077 Gradiente: [0.133420399216629,-2.3018619599073795] Loss: 27.842795552136867\n",
      "Iteracion: 2078 Gradiente: [0.1333494688370384,-2.300638219434389] Loss: 27.83748059582408\n",
      "Iteracion: 2079 Gradiente: [0.133278576166191,-2.2994151295393968] Loss: 27.832171289196925\n",
      "Iteracion: 2080 Gradiente: [0.1332077211841991,-2.2981926898765295] Loss: 27.826867626249975\n",
      "Iteracion: 2081 Gradiente: [0.13313690387083074,-2.2969709001001113] Loss: 27.82156960098407\n",
      "Iteracion: 2082 Gradiente: [0.13306612420615144,-2.2957497598646395] Loss: 27.816277207406465\n",
      "Iteracion: 2083 Gradiente: [0.13299538217017073,-2.2945292688247956] Loss: 27.81099043953084\n",
      "Iteracion: 2084 Gradiente: [0.13292467774266944,-2.2933094266354606] Loss: 27.805709291377138\n",
      "Iteracion: 2085 Gradiente: [0.13285401090393142,-2.29209023295167] Loss: 27.800433756971756\n",
      "Iteracion: 2086 Gradiente: [0.13278338163383124,-2.2908716874286643] Loss: 27.795163830347406\n",
      "Iteracion: 2087 Gradiente: [0.13271278991234017,-2.289653789721867] Loss: 27.78989950554308\n",
      "Iteracion: 2088 Gradiente: [0.13264223571961556,-2.2884365394868715] Loss: 27.78464077660421\n",
      "Iteracion: 2089 Gradiente: [0.13257171903575937,-2.287219936379458] Loss: 27.7793876375825\n",
      "Iteracion: 2090 Gradiente: [0.13250123984067413,-2.2860039800556033] Loss: 27.774140082535983\n",
      "Iteracion: 2091 Gradiente: [0.1324307981144661,-2.2847886701714573] Loss: 27.76889810552904\n",
      "Iteracion: 2092 Gradiente: [0.132360393837223,-2.2835740063833487] Loss: 27.76366170063226\n",
      "Iteracion: 2093 Gradiente: [0.13229002698906336,-2.282359988347793] Loss: 27.75843086192268\n",
      "Iteracion: 2094 Gradiente: [0.13221969755012944,-2.2811466157214872] Loss: 27.753205583483545\n",
      "Iteracion: 2095 Gradiente: [0.1321494055004995,-2.2799338881613114] Loss: 27.747985859404398\n",
      "Iteracion: 2096 Gradiente: [0.13207915082021676,-2.2787218053243365] Loss: 27.742771683781047\n",
      "Iteracion: 2097 Gradiente: [0.13200893348956175,-2.2775103668677974] Loss: 27.737563050715657\n",
      "Iteracion: 2098 Gradiente: [0.13193875348853604,-2.2762995724491284] Loss: 27.73235995431655\n",
      "Iteracion: 2099 Gradiente: [0.13186861079726622,-2.2750894217259447] Loss: 27.727162388698396\n",
      "Iteracion: 2100 Gradiente: [0.1317985053962057,-2.2738799143560176] Loss: 27.721970347982122\n",
      "Iteracion: 2101 Gradiente: [0.13172843726514807,-2.272671049997345] Loss: 27.716783826294805\n",
      "Iteracion: 2102 Gradiente: [0.13165840638457288,-2.271462828308064] Loss: 27.711602817769865\n",
      "Iteracion: 2103 Gradiente: [0.13158841273445887,-2.270255248946526] Loss: 27.70642731654696\n",
      "Iteracion: 2104 Gradiente: [0.13151845629516998,-2.2690483115712388] Loss: 27.701257316771912\n",
      "Iteracion: 2105 Gradiente: [0.1314485370469394,-2.267842015840903] Loss: 27.696092812596785\n",
      "Iteracion: 2106 Gradiente: [0.13137865496984394,-2.266636361414409] Loss: 27.690933798179916\n",
      "Iteracion: 2107 Gradiente: [0.1313088100441386,-2.2654313479508215] Loss: 27.6857802676858\n",
      "Iteracion: 2108 Gradiente: [0.13123900225029245,-2.2642269751093687] Loss: 27.680632215285108\n",
      "Iteracion: 2109 Gradiente: [0.1311692315683113,-2.2630232425494925] Loss: 27.675489635154754\n",
      "Iteracion: 2110 Gradiente: [0.13109949797854484,-2.261820149930793] Loss: 27.670352521477856\n",
      "Iteracion: 2111 Gradiente: [0.13102980146134988,-2.260617696913056] Loss: 27.66522086844367\n",
      "Iteracion: 2112 Gradiente: [0.13096014199702494,-2.259415883156249] Loss: 27.66009467024766\n",
      "Iteracion: 2113 Gradiente: [0.1308905195657521,-2.258214708320528] Loss: 27.654973921091436\n",
      "Iteracion: 2114 Gradiente: [0.13082093414783647,-2.2570141720662233] Loss: 27.649858615182822\n",
      "Iteracion: 2115 Gradiente: [0.13075138572381917,-2.255814274053832] Loss: 27.644748746735686\n",
      "Iteracion: 2116 Gradiente: [0.13068187427379457,-2.2546150139440586] Loss: 27.63964430997019\n",
      "Iteracion: 2117 Gradiente: [0.13061239977813605,-2.253416391397776] Loss: 27.63454529911255\n",
      "Iteracion: 2118 Gradiente: [0.13054296221724446,-2.25221840607603] Loss: 27.629451708395138\n",
      "Iteracion: 2119 Gradiente: [0.1304735615716287,-2.2510210576400453] Loss: 27.62436353205647\n",
      "Iteracion: 2120 Gradiente: [0.13040419782135093,-2.249824345751251] Loss: 27.619280764341163\n",
      "Iteracion: 2121 Gradiente: [0.13033487094714408,-2.2486282700712197] Loss: 27.61420339949996\n",
      "Iteracion: 2122 Gradiente: [0.13026558092906934,-2.2474328302617415] Loss: 27.60913143178973\n",
      "Iteracion: 2123 Gradiente: [0.1301963277478535,-2.246238025984753] Loss: 27.604064855473464\n",
      "Iteracion: 2124 Gradiente: [0.13012711138366057,-2.245043856902397] Loss: 27.599003664820195\n",
      "Iteracion: 2125 Gradiente: [0.13005793181706007,-2.243850322676977] Loss: 27.593947854105064\n",
      "Iteracion: 2126 Gradiente: [0.12998878902838745,-2.242657422970989] Loss: 27.588897417609346\n",
      "Iteracion: 2127 Gradiente: [0.12991968299809997,-2.2414651574471036] Loss: 27.583852349620322\n",
      "Iteracion: 2128 Gradiente: [0.1298506137068344,-2.2402735257681594] Loss: 27.578812644431423\n",
      "Iteracion: 2129 Gradiente: [0.129781581134813,-2.239082527597198] Loss: 27.573778296342088\n",
      "Iteracion: 2130 Gradiente: [0.12971258526270144,-2.237892162597418] Loss: 27.568749299657817\n",
      "Iteracion: 2131 Gradiente: [0.1296436260708731,-2.2367024304322123] Loss: 27.563725648690216\n",
      "Iteracion: 2132 Gradiente: [0.12957470353986006,-2.2355133307651442] Loss: 27.558707337756875\n",
      "Iteracion: 2133 Gradiente: [0.12950581765019062,-2.2343248632599577] Loss: 27.553694361181467\n",
      "Iteracion: 2134 Gradiente: [0.12943696838234472,-2.2331370275805784] Loss: 27.548686713293666\n",
      "Iteracion: 2135 Gradiente: [0.12936815571689994,-2.2319498233911053] Loss: 27.543684388429188\n",
      "Iteracion: 2136 Gradiente: [0.12929937963447316,-2.2307632503558152] Loss: 27.538687380929787\n",
      "Iteracion: 2137 Gradiente: [0.12923064011543584,-2.2295773081391768] Loss: 27.533695685143222\n",
      "Iteracion: 2138 Gradiente: [0.1291619371404555,-2.228391996405823] Loss: 27.52870929542326\n",
      "Iteracion: 2139 Gradiente: [0.1290932706900416,-2.227207314820571] Loss: 27.52372820612963\n",
      "Iteracion: 2140 Gradiente: [0.12902464074483647,-2.2260232630484125] Loss: 27.51875241162811\n",
      "Iteracion: 2141 Gradiente: [0.12895604728546647,-2.2248398407545174] Loss: 27.51378190629045\n",
      "Iteracion: 2142 Gradiente: [0.12888749029251112,-2.2236570476042368] Loss: 27.508816684494416\n",
      "Iteracion: 2143 Gradiente: [0.1288189697465204,-2.2224748832631036] Loss: 27.503856740623647\n",
      "Iteracion: 2144 Gradiente: [0.1287504856280601,-2.2212933473968253] Loss: 27.498902069067867\n",
      "Iteracion: 2145 Gradiente: [0.1286820379179436,-2.2201124396712753] Loss: 27.493952664222697\n",
      "Iteracion: 2146 Gradiente: [0.12861362659671868,-2.218932159752521] Loss: 27.489008520489758\n",
      "Iteracion: 2147 Gradiente: [0.12854525164502775,-2.217752507306801] Loss: 27.48406963227657\n",
      "Iteracion: 2148 Gradiente: [0.12847691304357386,-2.2165734820005287] Loss: 27.479135993996646\n",
      "Iteracion: 2149 Gradiente: [0.12840861077299337,-2.215395083500303] Loss: 27.474207600069423\n",
      "Iteracion: 2150 Gradiente: [0.12834034481410583,-2.214217311472884] Loss: 27.469284444920252\n",
      "Iteracion: 2151 Gradiente: [0.12827211514749876,-2.213040165585225] Loss: 27.46436652298043\n",
      "Iteracion: 2152 Gradiente: [0.12820392175378667,-2.2118636455044576] Loss: 27.459453828687167\n",
      "Iteracion: 2153 Gradiente: [0.12813576461378537,-2.2106877508978773] Loss: 27.454546356483583\n",
      "Iteracion: 2154 Gradiente: [0.12806764370820362,-2.209512481432964] Loss: 27.449644100818706\n",
      "Iteracion: 2155 Gradiente: [0.1279995590178648,-2.208337836777369] Loss: 27.444747056147488\n",
      "Iteracion: 2156 Gradiente: [0.1279315105233697,-2.207163816598933] Loss: 27.439855216930706\n",
      "Iteracion: 2157 Gradiente: [0.1278634982055318,-2.2059904205656578] Loss: 27.434968577635104\n",
      "Iteracion: 2158 Gradiente: [0.1277955220450598,-2.2048176483457373] Loss: 27.430087132733288\n",
      "Iteracion: 2159 Gradiente: [0.12772758202289652,-2.203645499607522] Loss: 27.425210876703705\n",
      "Iteracion: 2160 Gradiente: [0.12765967811978907,-2.202473974019549] Loss: 27.420339804030686\n",
      "Iteracion: 2161 Gradiente: [0.12759181031630598,-2.20130307125055] Loss: 27.41547390920447\n",
      "Iteracion: 2162 Gradiente: [0.12752397859357056,-2.200132790969396] Loss: 27.41061318672109\n",
      "Iteracion: 2163 Gradiente: [0.12745618293225647,-2.1989631328451593] Loss: 27.405757631082448\n",
      "Iteracion: 2164 Gradiente: [0.12738842331313027,-2.197794096547091] Loss: 27.400907236796325\n",
      "Iteracion: 2165 Gradiente: [0.12732069971719967,-2.196625681744593] Loss: 27.396061998376297\n",
      "Iteracion: 2166 Gradiente: [0.12725301212513027,-2.1954578881072733] Loss: 27.391221910341766\n",
      "Iteracion: 2167 Gradiente: [0.12718536051780802,-2.194290715304901] Loss: 27.38638696721803\n",
      "Iteracion: 2168 Gradiente: [0.12711774487631544,-2.193124163007406] Loss: 27.38155716353613\n",
      "Iteracion: 2169 Gradiente: [0.12705016518130113,-2.191958230884924] Loss: 27.376732493832947\n",
      "Iteracion: 2170 Gradiente: [0.12698262141367234,-2.1907929186077495] Loss: 27.37191295265116\n",
      "Iteracion: 2171 Gradiente: [0.1269151135545016,-2.1896282258463446] Loss: 27.367098534539306\n",
      "Iteracion: 2172 Gradiente: [0.1268476415844532,-2.188464152271369] Loss: 27.362289234051605\n",
      "Iteracion: 2173 Gradiente: [0.1267802054845755,-2.187300697553636] Loss: 27.35748504574819\n",
      "Iteracion: 2174 Gradiente: [0.1267128052358686,-2.186137861364138] Loss: 27.352685964194873\n",
      "Iteracion: 2175 Gradiente: [0.12664544081925821,-2.1849756433740493] Loss: 27.347891983963315\n",
      "Iteracion: 2176 Gradiente: [0.12657811221554502,-2.1838140432547237] Loss: 27.34310309963092\n",
      "Iteracion: 2177 Gradiente: [0.12651081940586362,-2.1826530606776724] Loss: 27.338319305780843\n",
      "Iteracion: 2178 Gradiente: [0.126443562370928,-2.1814926953146037] Loss: 27.333540597002\n",
      "Iteracion: 2179 Gradiente: [0.12637634109206364,-2.1803329468373716] Loss: 27.3287669678891\n",
      "Iteracion: 2180 Gradiente: [0.12630915555003525,-2.1791738149180295] Loss: 27.323998413042524\n",
      "Iteracion: 2181 Gradiente: [0.12624200572588126,-2.178015299228797] Loss: 27.31923492706848\n",
      "Iteracion: 2182 Gradiente: [0.12617489160066858,-2.1768573994420635] Loss: 27.31447650457884\n",
      "Iteracion: 2183 Gradiente: [0.12610781315542852,-2.1757001152303945] Loss: 27.3097231401912\n",
      "Iteracion: 2184 Gradiente: [0.12604077037118297,-2.1745434462665343] Loss: 27.30497482852895\n",
      "Iteracion: 2185 Gradiente: [0.12597376322882117,-2.1733873922234057] Loss: 27.300231564221143\n",
      "Iteracion: 2186 Gradiente: [0.12590679170951802,-2.1722319527740916] Loss: 27.295493341902546\n",
      "Iteracion: 2187 Gradiente: [0.1258398557944176,-2.171077127591849] Loss: 27.29076015621362\n",
      "Iteracion: 2188 Gradiente: [0.12577295546446407,-2.169922916350122] Loss: 27.28603200180054\n",
      "Iteracion: 2189 Gradiente: [0.12570609070073288,-2.1687693187225223] Loss: 27.281308873315155\n",
      "Iteracion: 2190 Gradiente: [0.12563926148439652,-2.1676163343828296] Loss: 27.276590765415037\n",
      "Iteracion: 2191 Gradiente: [0.12557246779653894,-2.1664639630050004] Loss: 27.2718776727634\n",
      "Iteracion: 2192 Gradiente: [0.1255057096182739,-2.165312204263168] Loss: 27.26716959002913\n",
      "Iteracion: 2193 Gradiente: [0.1254389869306692,-2.1641610578316364] Loss: 27.262466511886796\n",
      "Iteracion: 2194 Gradiente: [0.12537229971485953,-2.1630105233848838] Loss: 27.257768433016633\n",
      "Iteracion: 2195 Gradiente: [0.1253056479521097,-2.161860600597555] Loss: 27.253075348104506\n",
      "Iteracion: 2196 Gradiente: [0.1252390316233554,-2.1607112891444813] Loss: 27.248387251841965\n",
      "Iteracion: 2197 Gradiente: [0.1251724507099676,-2.1595625887006507] Loss: 27.24370413892615\n",
      "Iteracion: 2198 Gradiente: [0.12510590519309944,-2.1584144989412324] Loss: 27.239026004059912\n",
      "Iteracion: 2199 Gradiente: [0.12503939505383907,-2.1572670195415715] Loss: 27.23435284195164\n",
      "Iteracion: 2200 Gradiente: [0.12497292027331876,-2.1561201501771867] Loss: 27.229684647315434\n",
      "Iteracion: 2201 Gradiente: [0.1249064808328806,-2.1549738905237557] Loss: 27.225021414870966\n",
      "Iteracion: 2202 Gradiente: [0.12484007671370231,-2.1538282402571403] Loss: 27.220363139343533\n",
      "Iteracion: 2203 Gradiente: [0.12477370789690477,-2.1526831990533766] Loss: 27.21570981546403\n",
      "Iteracion: 2204 Gradiente: [0.12470737436396935,-2.151538766588655] Loss: 27.211061437969\n",
      "Iteracion: 2205 Gradiente: [0.12464107609580376,-2.150394942539369] Loss: 27.206418001600458\n",
      "Iteracion: 2206 Gradiente: [0.12457481307397321,-2.1492517265820488] Loss: 27.20177950110617\n",
      "Iteracion: 2207 Gradiente: [0.12450858527954362,-2.148109118393426] Loss: 27.197145931239376\n",
      "Iteracion: 2208 Gradiente: [0.12444239269385662,-2.146967117650388] Loss: 27.192517286758914\n",
      "Iteracion: 2209 Gradiente: [0.12437623529813967,-2.1458257240299994] Loss: 27.18789356242922\n",
      "Iteracion: 2210 Gradiente: [0.12431011307382249,-2.1446849372094894] Loss: 27.183274753020275\n",
      "Iteracion: 2211 Gradiente: [0.12424402600198997,-2.1435447568662758] Loss: 27.17866085330762\n",
      "Iteracion: 2212 Gradiente: [0.12417797406404436,-2.1424051826779325] Loss: 27.17405185807235\n",
      "Iteracion: 2213 Gradiente: [0.12411195724151534,-2.1412662143221968] Loss: 27.16944776210111\n",
      "Iteracion: 2214 Gradiente: [0.12404597551544565,-2.1401278514770046] Loss: 27.16484856018609\n",
      "Iteracion: 2215 Gradiente: [0.12398002886734313,-2.138990093820439] Loss: 27.16025424712503\n",
      "Iteracion: 2216 Gradiente: [0.12391411727849212,-2.137852941030768] Loss: 27.155664817721142\n",
      "Iteracion: 2217 Gradiente: [0.12384824073017171,-2.1367163927864303] Loss: 27.151080266783215\n",
      "Iteracion: 2218 Gradiente: [0.12378239920397884,-2.135580448766017] Loss: 27.146500589125576\n",
      "Iteracion: 2219 Gradiente: [0.12371659268105759,-2.13444510864832] Loss: 27.141925779567988\n",
      "Iteracion: 2220 Gradiente: [0.12365082114292344,-2.1333103721122755] Loss: 27.137355832935793\n",
      "Iteracion: 2221 Gradiente: [0.12358508457097438,-2.1321762388370047] Loss: 27.13279074405979\n",
      "Iteracion: 2222 Gradiente: [0.12351938294662214,-2.131042708501793] Loss: 27.1282305077763\n",
      "Iteracion: 2223 Gradiente: [0.12345371625123676,-2.1299097807861034] Loss: 27.123675118927103\n",
      "Iteracion: 2224 Gradiente: [0.12338808446626928,-2.1287774553695633] Loss: 27.1191245723595\n",
      "Iteracion: 2225 Gradiente: [0.12332248757320485,-2.127645731931968] Loss: 27.114578862926212\n",
      "Iteracion: 2226 Gradiente: [0.12325692555348317,-2.126514610153291] Loss: 27.11003798548552\n",
      "Iteracion: 2227 Gradiente: [0.12319139838857182,-2.1253840897136675] Loss: 27.105501934901074\n",
      "Iteracion: 2228 Gradiente: [0.12312590605981769,-2.124254170293416] Loss: 27.100970706042066\n",
      "Iteracion: 2229 Gradiente: [0.1230604485487466,-2.1231248515730146] Loss: 27.096444293783065\n",
      "Iteracion: 2230 Gradiente: [0.12299502583704083,-2.1219961332330994] Loss: 27.091922693004147\n",
      "Iteracion: 2231 Gradiente: [0.12292963790601684,-2.1208680149545045] Loss: 27.087405898590827\n",
      "Iteracion: 2232 Gradiente: [0.12286428473722376,-2.119740496418213] Loss: 27.082893905434016\n",
      "Iteracion: 2233 Gradiente: [0.12279896631213015,-2.118613577305387] Loss: 27.07838670843008\n",
      "Iteracion: 2234 Gradiente: [0.12273368261231592,-2.1174872572973524] Loss: 27.07388430248083\n",
      "Iteracion: 2235 Gradiente: [0.12266843361933202,-2.1163615360756043] Loss: 27.069386682493462\n",
      "Iteracion: 2236 Gradiente: [0.12260321931478207,-2.1152364133218082] Loss: 27.064893843380602\n",
      "Iteracion: 2237 Gradiente: [0.1225380396801564,-2.114111888717803] Loss: 27.060405780060286\n",
      "Iteracion: 2238 Gradiente: [0.12247289469697194,-2.112987961945597] Loss: 27.05592248745595\n",
      "Iteracion: 2239 Gradiente: [0.1224077843469317,-2.111864632687359] Loss: 27.0514439604964\n",
      "Iteracion: 2240 Gradiente: [0.12234270861142373,-2.1107419006254435] Loss: 27.046970194115904\n",
      "Iteracion: 2241 Gradiente: [0.12227766747226572,-2.109619765442347] Loss: 27.042501183254032\n",
      "Iteracion: 2242 Gradiente: [0.12221266091087037,-2.1084982268207635] Loss: 27.038036922855746\n",
      "Iteracion: 2243 Gradiente: [0.12214768890912543,-2.1073772844435283] Loss: 27.03357740787147\n",
      "Iteracion: 2244 Gradiente: [0.12208275144847439,-2.1062569379936678] Loss: 27.029122633256886\n",
      "Iteracion: 2245 Gradiente: [0.12201784851040429,-2.105137187154377] Loss: 27.02467259397309\n",
      "Iteracion: 2246 Gradiente: [0.12195298007681146,-2.104018031609] Loss: 27.02022728498652\n",
      "Iteracion: 2247 Gradiente: [0.1218881461292573,-2.1028994710410625] Loss: 27.01578670126899\n",
      "Iteracion: 2248 Gradiente: [0.12182334664943871,-2.101781505134257] Loss: 27.01135083779763\n",
      "Iteracion: 2249 Gradiente: [0.1217585816191009,-2.100664133572439] Loss: 27.00691968955489\n",
      "Iteracion: 2250 Gradiente: [0.1216938510197347,-2.099547356039646] Loss: 27.002493251528634\n",
      "Iteracion: 2251 Gradiente: [0.12162915483313555,-2.0984311722200686] Loss: 26.998071518711967\n",
      "Iteracion: 2252 Gradiente: [0.12156449304101216,-2.0973155817980706] Loss: 26.993654486103342\n",
      "Iteracion: 2253 Gradiente: [0.12149986562516991,-2.096200584458179] Loss: 26.989242148706563\n",
      "Iteracion: 2254 Gradiente: [0.1214352725672048,-2.0950861798851004] Loss: 26.98483450153073\n",
      "Iteracion: 2255 Gradiente: [0.12137071384890703,-2.093972367763698] Loss: 26.980431539590203\n",
      "Iteracion: 2256 Gradiente: [0.12130618945188681,-2.092859147779014] Loss: 26.976033257904692\n",
      "Iteracion: 2257 Gradiente: [0.12124169935817927,-2.091746519616235] Loss: 26.971639651499185\n",
      "Iteracion: 2258 Gradiente: [0.12117724354934675,-2.0906344829607426] Loss: 26.96725071540397\n",
      "Iteracion: 2259 Gradiente: [0.12111282200716574,-2.0895230374980702] Loss: 26.96286644465459\n",
      "Iteracion: 2260 Gradiente: [0.1210484347134468,-2.0884121829139217] Loss: 26.95848683429188\n",
      "Iteracion: 2261 Gradiente: [0.12098408165004125,-2.087301918894165] Loss: 26.95411187936197\n",
      "Iteracion: 2262 Gradiente: [0.12091976279861522,-2.0861922451248462] Loss: 26.9497415749162\n",
      "Iteracion: 2263 Gradiente: [0.12085547814109437,-2.0850831612921614] Loss: 26.94537591601124\n",
      "Iteracion: 2264 Gradiente: [0.1207912276592604,-2.0839746670824852] Loss: 26.941014897708946\n",
      "Iteracion: 2265 Gradiente: [0.12072701133496082,-2.082866762182358] Loss: 26.936658515076488\n",
      "Iteracion: 2266 Gradiente: [0.12066282915005597,-2.0817594462784808] Loss: 26.932306763186237\n",
      "Iteracion: 2267 Gradiente: [0.12059868108635025,-2.080652719057726] Loss: 26.92795963711578\n",
      "Iteracion: 2268 Gradiente: [0.12053456712572483,-2.079546580207134] Loss: 26.923617131948024\n",
      "Iteracion: 2269 Gradiente: [0.12047048725003104,-2.0784410294139084] Loss: 26.919279242770994\n",
      "Iteracion: 2270 Gradiente: [0.12040644144126228,-2.0773360663654126] Loss: 26.91494596467802\n",
      "Iteracion: 2271 Gradiente: [0.12034242968118083,-2.076231690749192] Loss: 26.910617292767643\n",
      "Iteracion: 2272 Gradiente: [0.1202784519516598,-2.075127902252951] Loss: 26.90629322214352\n",
      "Iteracion: 2273 Gradiente: [0.12021450823473004,-2.0740247005645513] Loss: 26.901973747914646\n",
      "Iteracion: 2274 Gradiente: [0.12015059851224616,-2.0729220853720323] Loss: 26.897658865195115\n",
      "Iteracion: 2275 Gradiente: [0.12008672276600739,-2.0718200563635993] Loss: 26.893348569104255\n",
      "Iteracion: 2276 Gradiente: [0.12002288097814358,-2.0707186132276116] Loss: 26.88904285476659\n",
      "Iteracion: 2277 Gradiente: [0.11995907313052072,-2.069617755652605] Loss: 26.884741717311787\n",
      "Iteracion: 2278 Gradiente: [0.11989529920519905,-2.0685174833272693] Loss: 26.880445151874753\n",
      "Iteracion: 2279 Gradiente: [0.11983155918397917,-2.0674177959404787] Loss: 26.876153153595517\n",
      "Iteracion: 2280 Gradiente: [0.11976785304889433,-2.066318693181259] Loss: 26.871865717619276\n",
      "Iteracion: 2281 Gradiente: [0.11970418078198482,-2.0652201747388] Loss: 26.867582839096396\n",
      "Iteracion: 2282 Gradiente: [0.11964054236517731,-2.0641222403024657] Loss: 26.863304513182413\n",
      "Iteracion: 2283 Gradiente: [0.11957693778055045,-2.0630248895617758] Loss: 26.859030735038004\n",
      "Iteracion: 2284 Gradiente: [0.11951336700997454,-2.0619281222064285] Loss: 26.854761499828992\n",
      "Iteracion: 2285 Gradiente: [0.11944983003566895,-2.0608319379262663] Loss: 26.850496802726298\n",
      "Iteracion: 2286 Gradiente: [0.11938632683948261,-2.05973633641132] Loss: 26.846236638906046\n",
      "Iteracion: 2287 Gradiente: [0.11932285740365339,-2.058641317351763] Loss: 26.841981003549446\n",
      "Iteracion: 2288 Gradiente: [0.11925942171003356,-2.057546880437952] Loss: 26.837729891842816\n",
      "Iteracion: 2289 Gradiente: [0.11919601974081881,-2.0564530253603968] Loss: 26.833483298977647\n",
      "Iteracion: 2290 Gradiente: [0.11913265147799829,-2.0553597518097786] Loss: 26.82924122015048\n",
      "Iteracion: 2291 Gradiente: [0.11906931690380086,-2.054267059476932] Loss: 26.825003650563005\n",
      "Iteracion: 2292 Gradiente: [0.11900601600005985,-2.0531749480528783] Loss: 26.82077058542199\n",
      "Iteracion: 2293 Gradiente: [0.11894274874908982,-2.052083417228775] Loss: 26.81654201993931\n",
      "Iteracion: 2294 Gradiente: [0.11887951513295245,-2.050992466695961] Loss: 26.81231794933188\n",
      "Iteracion: 2295 Gradiente: [0.11881631513374676,-2.049902096145936] Loss: 26.80809836882181\n",
      "Iteracion: 2296 Gradiente: [0.11875314873352967,-2.0488123052703684] Loss: 26.803883273636167\n",
      "Iteracion: 2297 Gradiente: [0.11869001591465983,-2.0477230937610735] Loss: 26.799672659007214\n",
      "Iteracion: 2298 Gradiente: [0.11862691665900134,-2.0466344613100556] Loss: 26.795466520172134\n",
      "Iteracion: 2299 Gradiente: [0.11856385094880911,-2.0455464076094683] Loss: 26.79126485237329\n",
      "Iteracion: 2300 Gradiente: [0.11850081876637736,-2.0444589323516187] Loss: 26.787067650858077\n",
      "Iteracion: 2301 Gradiente: [0.11843782009379804,-2.0433720352289964] Loss: 26.782874910878924\n",
      "Iteracion: 2302 Gradiente: [0.11837485491326068,-2.0422857159342453] Loss: 26.778686627693308\n",
      "Iteracion: 2303 Gradiente: [0.11831192320688615,-2.0411999741601794] Loss: 26.774502796563738\n",
      "Iteracion: 2304 Gradiente: [0.11824902495699992,-2.0401148095997637] Loss: 26.770323412757804\n",
      "Iteracion: 2305 Gradiente: [0.11818616014567358,-2.0390302219461423] Loss: 26.766148471548078\n",
      "Iteracion: 2306 Gradiente: [0.11812332875525158,-2.0379462108926076] Loss: 26.76197796821217\n",
      "Iteracion: 2307 Gradiente: [0.11806053076788837,-2.0368627761326215] Loss: 26.757811898032703\n",
      "Iteracion: 2308 Gradiente: [0.11799776616588721,-2.0357799173598092] Loss: 26.753650256297366\n",
      "Iteracion: 2309 Gradiente: [0.11793503493151812,-2.034697634267956] Loss: 26.749493038298777\n",
      "Iteracion: 2310 Gradiente: [0.11787233704693752,-2.03361592655102] Loss: 26.745340239334627\n",
      "Iteracion: 2311 Gradiente: [0.11780967249444814,-2.032534793903108] Loss: 26.741191854707555\n",
      "Iteracion: 2312 Gradiente: [0.11774704125644178,-2.031454236018493] Loss: 26.73704787972522\n",
      "Iteracion: 2313 Gradiente: [0.1176844433150914,-2.0303742525916197] Loss: 26.732908309700285\n",
      "Iteracion: 2314 Gradiente: [0.1176218786527367,-2.0292948433170843] Loss: 26.72877313995034\n",
      "Iteracion: 2315 Gradiente: [0.11755934725169367,-2.028216007889653] Loss: 26.72464236579801\n",
      "Iteracion: 2316 Gradiente: [0.11749684909412063,-2.027137746004256] Loss: 26.720515982570852\n",
      "Iteracion: 2317 Gradiente: [0.11743438416273667,-2.0260600573559584] Loss: 26.716393985601414\n",
      "Iteracion: 2318 Gradiente: [0.1173719524394907,-2.024982941640036] Loss: 26.712276370227205\n",
      "Iteracion: 2319 Gradiente: [0.11730955390679962,-2.0239063985518917] Loss: 26.708163131790663\n",
      "Iteracion: 2320 Gradiente: [0.11724718854714998,-2.022830427787095] Loss: 26.704054265639208\n",
      "Iteracion: 2321 Gradiente: [0.1171848563428938,-2.0217550290413797] Loss: 26.6999497671252\n",
      "Iteracion: 2322 Gradiente: [0.1171225572762746,-2.0206802020106513] Loss: 26.695849631605906\n",
      "Iteracion: 2323 Gradiente: [0.11706029132980215,-2.0196059463909615] Loss: 26.691753854443593\n",
      "Iteracion: 2324 Gradiente: [0.11699805848577588,-2.0185322618785353] Loss: 26.68766243100541\n",
      "Iteracion: 2325 Gradiente: [0.11693585872658142,-2.0174591481697552] Loss: 26.683575356663415\n",
      "Iteracion: 2326 Gradiente: [0.1168736920348626,-2.0163866049611503] Loss: 26.67949262679464\n",
      "Iteracion: 2327 Gradiente: [0.11681155839272227,-2.0153146319494484] Loss: 26.675414236780977\n",
      "Iteracion: 2328 Gradiente: [0.11674945778275875,-2.0142432288314995] Loss: 26.671340182009292\n",
      "Iteracion: 2329 Gradiente: [0.11668739018748416,-2.01317239530433] Loss: 26.667270457871272\n",
      "Iteracion: 2330 Gradiente: [0.11662535558918895,-2.012102131065135] Loss: 26.66320505976357\n",
      "Iteracion: 2331 Gradiente: [0.11656335397033217,-2.011032435811264] Loss: 26.6591439830877\n",
      "Iteracion: 2332 Gradiente: [0.11650138531350744,-2.009963309240222] Loss: 26.65508722325006\n",
      "Iteracion: 2333 Gradiente: [0.11643944960102127,-2.0088947510496857] Loss: 26.65103477566197\n",
      "Iteracion: 2334 Gradiente: [0.11637754681560418,-2.0078267609374754] Loss: 26.646986635739577\n",
      "Iteracion: 2335 Gradiente: [0.11631567693950634,-2.006759338601595] Loss: 26.642942798903935\n",
      "Iteracion: 2336 Gradiente: [0.11625383995541796,-2.005692483740189] Loss: 26.638903260580946\n",
      "Iteracion: 2337 Gradiente: [0.11619203584561717,-2.004626196051584] Loss: 26.6348680162014\n",
      "Iteracion: 2338 Gradiente: [0.1161302645928572,-2.003560475234237] Loss: 26.630837061200904\n",
      "Iteracion: 2339 Gradiente: [0.11606852617958055,-2.0024953209867906] Loss: 26.626810391019944\n",
      "Iteracion: 2340 Gradiente: [0.11600682058840638,-2.0014307330080316] Loss: 26.622788001103878\n",
      "Iteracion: 2341 Gradiente: [0.11594514780183356,-2.0003667109969197] Loss: 26.618769886902832\n",
      "Iteracion: 2342 Gradiente: [0.11588350780233346,-1.9993032546525726] Loss: 26.614756043871843\n",
      "Iteracion: 2343 Gradiente: [0.11582190057253001,-1.9982403636742616] Loss: 26.61074646747071\n",
      "Iteracion: 2344 Gradiente: [0.11576032609510728,-1.9971780377614148] Loss: 26.606741153164133\n",
      "Iteracion: 2345 Gradiente: [0.11569878435251534,-1.9961162766136349] Loss: 26.6027400964216\n",
      "Iteracion: 2346 Gradiente: [0.11563727532738804,-1.995055079930672] Loss: 26.598743292717362\n",
      "Iteracion: 2347 Gradiente: [0.11557579900231806,-1.9939944474124407] Loss: 26.594750737530596\n",
      "Iteracion: 2348 Gradiente: [0.11551435536003302,-1.9929343787590075] Loss: 26.59076242634518\n",
      "Iteracion: 2349 Gradiente: [0.11545294438305831,-1.9918748736706087] Loss: 26.58677835464983\n",
      "Iteracion: 2350 Gradiente: [0.11539156605407186,-1.990815931847636] Loss: 26.582798517938055\n",
      "Iteracion: 2351 Gradiente: [0.11533022035561373,-1.9897575529906442] Loss: 26.578822911708173\n",
      "Iteracion: 2352 Gradiente: [0.11526890727044095,-1.988699736800337] Loss: 26.574851531463274\n",
      "Iteracion: 2353 Gradiente: [0.11520762678112723,-1.9876424829775898] Loss: 26.5708843727112\n",
      "Iteracion: 2354 Gradiente: [0.1151463788704215,-1.9865857912234273] Loss: 26.56692143096462\n",
      "Iteracion: 2355 Gradiente: [0.11508516352106853,-1.9855296612390296] Loss: 26.562962701740933\n",
      "Iteracion: 2356 Gradiente: [0.11502398071565476,-1.9844740927257507] Loss: 26.559008180562326\n",
      "Iteracion: 2357 Gradiente: [0.11496283043684154,-1.9834190853850977] Loss: 26.55505786295573\n",
      "Iteracion: 2358 Gradiente: [0.11490171266739727,-1.9823646389187302] Loss: 26.55111174445283\n",
      "Iteracion: 2359 Gradiente: [0.11484062739010407,-1.981310753028466] Loss: 26.547169820590103\n",
      "Iteracion: 2360 Gradiente: [0.1147795745876266,-1.9802574274162883] Loss: 26.54323208690868\n",
      "Iteracion: 2361 Gradiente: [0.11471855424255419,-1.9792046617843464] Loss: 26.539298538954526\n",
      "Iteracion: 2362 Gradiente: [0.11465756633793375,-1.97815245583492] Loss: 26.535369172278294\n",
      "Iteracion: 2363 Gradiente: [0.11459661085625328,-1.9771008092704774] Loss: 26.531443982435363\n",
      "Iteracion: 2364 Gradiente: [0.1145356877803882,-1.97604972179363] Loss: 26.527522964985863\n",
      "Iteracion: 2365 Gradiente: [0.11447479709320968,-1.9749991931071413] Loss: 26.52360611549464\n",
      "Iteracion: 2366 Gradiente: [0.11441393877726824,-1.9739492229139544] Loss: 26.519693429531184\n",
      "Iteracion: 2367 Gradiente: [0.11435311281551937,-1.9728998109171494] Loss: 26.515784902669832\n",
      "Iteracion: 2368 Gradiente: [0.11429231919064335,-1.971850956819977] Loss: 26.511880530489506\n",
      "Iteracion: 2369 Gradiente: [0.11423155788565063,-1.9708026603258295] Loss: 26.50798030857385\n",
      "Iteracion: 2370 Gradiente: [0.1141708288830851,-1.9697549211382843] Loss: 26.504084232511257\n",
      "Iteracion: 2371 Gradiente: [0.11411013216608788,-1.9687077389610421] Loss: 26.500192297894777\n",
      "Iteracion: 2372 Gradiente: [0.11404946771724174,-1.9676611134979933] Loss: 26.496304500322104\n",
      "Iteracion: 2373 Gradiente: [0.11398883551945146,-1.9666150444531676] Loss: 26.492420835395667\n",
      "Iteracion: 2374 Gradiente: [0.11392823555565694,-1.9655695315307513] Loss: 26.48854129872256\n",
      "Iteracion: 2375 Gradiente: [0.11386766780867484,-1.9645245744350959] Loss: 26.48466588591452\n",
      "Iteracion: 2376 Gradiente: [0.11380713226121392,-1.9634801728707165] Loss: 26.48079459258797\n",
      "Iteracion: 2377 Gradiente: [0.11374662889640158,-1.9624363265422606] Loss: 26.476927414364006\n",
      "Iteracion: 2378 Gradiente: [0.11368615769713983,-1.961393035154548] Loss: 26.473064346868338\n",
      "Iteracion: 2379 Gradiente: [0.113625718646144,-1.9603502984125667] Loss: 26.46920538573138\n",
      "Iteracion: 2380 Gradiente: [0.11356531172639232,-1.959308116021442] Loss: 26.46535052658812\n",
      "Iteracion: 2381 Gradiente: [0.11350493692080098,-1.958266487686468] Loss: 26.461499765078255\n",
      "Iteracion: 2382 Gradiente: [0.11344459421233637,-1.957225413113087] Loss: 26.457653096846073\n",
      "Iteracion: 2383 Gradiente: [0.113384283583891,-1.9561848920069065] Loss: 26.453810517540514\n",
      "Iteracion: 2384 Gradiente: [0.11332400501844925,-1.9551449240736833] Loss: 26.449972022815135\n",
      "Iteracion: 2385 Gradiente: [0.11326375849892448,-1.9541055090193353] Loss: 26.446137608328115\n",
      "Iteracion: 2386 Gradiente: [0.11320354400828876,-1.9530666465499351] Loss: 26.442307269742237\n",
      "Iteracion: 2387 Gradiente: [0.11314336152953691,-1.9520283363717117] Loss: 26.438481002724927\n",
      "Iteracion: 2388 Gradiente: [0.11308321104566706,-1.9509905781910475] Loss: 26.434658802948185\n",
      "Iteracion: 2389 Gradiente: [0.11302309253957646,-1.9499533717144895] Loss: 26.430840666088578\n",
      "Iteracion: 2390 Gradiente: [0.112963005994393,-1.9489167166487278] Loss: 26.427026587827378\n",
      "Iteracion: 2391 Gradiente: [0.11290295139296518,-1.9478806127006256] Loss: 26.42321656385036\n",
      "Iteracion: 2392 Gradiente: [0.11284292871853173,-1.9468450595771762] Loss: 26.41941058984789\n",
      "Iteracion: 2393 Gradiente: [0.11278293795392311,-1.9458100569855588] Loss: 26.41560866151493\n",
      "Iteracion: 2394 Gradiente: [0.11272297908226439,-1.9447756046330862] Loss: 26.411810774551043\n",
      "Iteracion: 2395 Gradiente: [0.11266305208655704,-1.943741702227239] Loss: 26.40801692466031\n",
      "Iteracion: 2396 Gradiente: [0.11260315694996782,-1.9427083494756399] Loss: 26.404227107551428\n",
      "Iteracion: 2397 Gradiente: [0.11254329365533332,-1.941675546086089] Loss: 26.400441318937634\n",
      "Iteracion: 2398 Gradiente: [0.1124834621859397,-1.94064329176652] Loss: 26.396659554536722\n",
      "Iteracion: 2399 Gradiente: [0.1124236625247813,-1.9396115862250316] Loss: 26.392881810071035\n",
      "Iteracion: 2400 Gradiente: [0.11236389465501873,-1.9385804291698734] Loss: 26.389108081267462\n",
      "Iteracion: 2401 Gradiente: [0.11230415855967711,-1.9375498203094565] Loss: 26.385338363857446\n",
      "Iteracion: 2402 Gradiente: [0.11224445422176738,-1.9365197593523507] Loss: 26.38157265357697\n",
      "Iteracion: 2403 Gradiente: [0.11218478162454014,-1.935490246007267] Loss: 26.37781094616655\n",
      "Iteracion: 2404 Gradiente: [0.11212514075113707,-1.9344612799830754] Loss: 26.374053237371175\n",
      "Iteracion: 2405 Gradiente: [0.11206553158467045,-1.9334328609888045] Loss: 26.370299522940424\n",
      "Iteracion: 2406 Gradiente: [0.11200595410824359,-1.93240498873364] Loss: 26.366549798628398\n",
      "Iteracion: 2407 Gradiente: [0.11194640830504644,-1.9313776629269153] Loss: 26.362804060193646\n",
      "Iteracion: 2408 Gradiente: [0.1118868941582889,-1.9303508832781195] Loss: 26.35906230339931\n",
      "Iteracion: 2409 Gradiente: [0.1118274116509544,-1.9293246494969094] Loss: 26.355324524012946\n",
      "Iteracion: 2410 Gradiente: [0.11176796076643948,-1.928298961293072] Loss: 26.351590717806687\n",
      "Iteracion: 2411 Gradiente: [0.1117085414877773,-1.92727381837657] Loss: 26.347860880557104\n",
      "Iteracion: 2412 Gradiente: [0.11164915379830707,-1.9262492204575057] Loss: 26.34413500804529\n",
      "Iteracion: 2413 Gradiente: [0.11158979768109703,-1.925225167246148] Loss: 26.340413096056817\n",
      "Iteracion: 2414 Gradiente: [0.11153047311943899,-1.9242016584529087] Loss: 26.33669514038172\n",
      "Iteracion: 2415 Gradiente: [0.11147118009653904,-1.9231786937883608] Loss: 26.33298113681455\n",
      "Iteracion: 2416 Gradiente: [0.11141191859557438,-1.9221562729632315] Loss: 26.329271081154253\n",
      "Iteracion: 2417 Gradiente: [0.11135268859981882,-1.921134395688395] Loss: 26.325564969204326\n",
      "Iteracion: 2418 Gradiente: [0.11129349009264189,-1.9201130616748818] Loss: 26.321862796772674\n",
      "Iteracion: 2419 Gradiente: [0.11123432305726103,-1.9190922706338753] Loss: 26.318164559671693\n",
      "Iteracion: 2420 Gradiente: [0.1111751874768667,-1.91807202227672] Loss: 26.31447025371817\n",
      "Iteracion: 2421 Gradiente: [0.11111608333459581,-1.917052316314918] Loss: 26.310779874733417\n",
      "Iteracion: 2422 Gradiente: [0.11105701061401968,-1.9160331524600978] Loss: 26.307093418543143\n",
      "Iteracion: 2423 Gradiente: [0.11099796929829799,-1.9150145304240673] Loss: 26.303410880977466\n",
      "Iteracion: 2424 Gradiente: [0.11093895937073674,-1.9139964499187783] Loss: 26.29973225787102\n",
      "Iteracion: 2425 Gradiente: [0.11087998081469881,-1.912978910656335] Loss: 26.29605754506279\n",
      "Iteracion: 2426 Gradiente: [0.1108210336134429,-1.911961912348996] Loss: 26.292386738396225\n",
      "Iteracion: 2427 Gradiente: [0.11076211775041903,-1.9109454547091695] Loss: 26.28871983371915\n",
      "Iteracion: 2428 Gradiente: [0.11070323320876886,-1.9099295374494314] Loss: 26.28505682688388\n",
      "Iteracion: 2429 Gradiente: [0.11064437997192016,-1.9089141602824915] Loss: 26.28139771374705\n",
      "Iteracion: 2430 Gradiente: [0.11058555802339404,-1.9078993229212131] Loss: 26.277742490169782\n",
      "Iteracion: 2431 Gradiente: [0.11052676734632125,-1.9068850250786327] Loss: 26.27409115201751\n",
      "Iteracion: 2432 Gradiente: [0.1104680079242911,-1.905871266467912] Loss: 26.270443695160136\n",
      "Iteracion: 2433 Gradiente: [0.11040927974050163,-1.9048580468023906] Loss: 26.266800115471938\n",
      "Iteracion: 2434 Gradiente: [0.11035058277831951,-1.903845365795548] Loss: 26.26316040883155\n",
      "Iteracion: 2435 Gradiente: [0.11029191702131887,-1.90283322316101] Loss: 26.259524571122004\n",
      "Iteracion: 2436 Gradiente: [0.11023328245287019,-1.9018216186125605] Loss: 26.255892598230712\n",
      "Iteracion: 2437 Gradiente: [0.1101746790563586,-1.90081055186414] Loss: 26.252264486049448\n",
      "Iteracion: 2438 Gradiente: [0.1101161068152237,-1.8998000226298373] Loss: 26.24864023047436\n",
      "Iteracion: 2439 Gradiente: [0.1100575657127242,-1.8987900306239036] Loss: 26.24501982740598\n",
      "Iteracion: 2440 Gradiente: [0.1099990557326663,-1.8977805755607098] Loss: 26.241403272749142\n",
      "Iteracion: 2441 Gradiente: [0.10994057685825614,-1.8967716571548157] Loss: 26.23779056241309\n",
      "Iteracion: 2442 Gradiente: [0.10988212907298163,-1.895763275120917] Loss: 26.234181692311395\n",
      "Iteracion: 2443 Gradiente: [0.10982371236037806,-1.8947554291738575] Loss: 26.23057665836195\n",
      "Iteracion: 2444 Gradiente: [0.10976532670377707,-1.8937481190286447] Loss: 26.226975456487008\n",
      "Iteracion: 2445 Gradiente: [0.10970697208701238,-1.8927413444004126] Loss: 26.223378082613184\n",
      "Iteracion: 2446 Gradiente: [0.10964864849321003,-1.8917351050044828] Loss: 26.219784532671365\n",
      "Iteracion: 2447 Gradiente: [0.10959035590601142,-1.8907294005563022] Loss: 26.216194802596796\n",
      "Iteracion: 2448 Gradiente: [0.10953209430893669,-1.8897242307714777] Loss: 26.212608888329058\n",
      "Iteracion: 2449 Gradiente: [0.1094738636854989,-1.8887195953657643] Loss: 26.209026785812\n",
      "Iteracion: 2450 Gradiente: [0.10941566401932998,-1.887715494055065] Loss: 26.20544849099383\n",
      "Iteracion: 2451 Gradiente: [0.10935749529386479,-1.8867119265554446] Loss: 26.201873999827043\n",
      "Iteracion: 2452 Gradiente: [0.10929935749264388,-1.8857088925831136] Loss: 26.19830330826844\n",
      "Iteracion: 2453 Gradiente: [0.10924125059929538,-1.8847063918544273] Loss: 26.194736412279116\n",
      "Iteracion: 2454 Gradiente: [0.10918317459737258,-1.8837044240859] Loss: 26.19117330782448\n",
      "Iteracion: 2455 Gradiente: [0.10912512947040748,-1.8827029889941949] Loss: 26.18761399087416\n",
      "Iteracion: 2456 Gradiente: [0.10906711520191313,-1.8817020862961287] Loss: 26.18405845740216\n",
      "Iteracion: 2457 Gradiente: [0.1090091317757043,-1.880701715708653] Loss: 26.18050670338672\n",
      "Iteracion: 2458 Gradiente: [0.10895117917530399,-1.879701876948885] Loss: 26.176958724810373\n",
      "Iteracion: 2459 Gradiente: [0.10889325738426028,-1.878702569734093] Loss: 26.17341451765987\n",
      "Iteracion: 2460 Gradiente: [0.1088353663861208,-1.877703793781696] Loss: 26.169874077926277\n",
      "Iteracion: 2461 Gradiente: [0.1087775061647515,-1.8767055488092452] Loss: 26.166337401604935\n",
      "Iteracion: 2462 Gradiente: [0.10871967670356403,-1.8757078345344667] Loss: 26.162804484695382\n",
      "Iteracion: 2463 Gradiente: [0.10866187798623486,-1.874710650675224] Loss: 26.159275323201467\n",
      "Iteracion: 2464 Gradiente: [0.10860410999659535,-1.8737139969495236] Loss: 26.15574991313125\n",
      "Iteracion: 2465 Gradiente: [0.1085463727181145,-1.8727178730755394] Loss: 26.152228250497046\n",
      "Iteracion: 2466 Gradiente: [0.10848866613452704,-1.8717222787715826] Loss: 26.148710331315428\n",
      "Iteracion: 2467 Gradiente: [0.10843099022969321,-1.870727213756111] Loss: 26.145196151607163\n",
      "Iteracion: 2468 Gradiente: [0.10837334498696218,-1.8697326777477523] Loss: 26.141685707397293\n",
      "Iteracion: 2469 Gradiente: [0.10831573039030881,-1.868738670465259] Loss: 26.13817899471504\n",
      "Iteracion: 2470 Gradiente: [0.1082581464233139,-1.8677451916275483] Loss: 26.134676009593885\n",
      "Iteracion: 2471 Gradiente: [0.10820059306965059,-1.8667522409536863] Loss: 26.13117674807149\n",
      "Iteracion: 2472 Gradiente: [0.10814307031322747,-1.8657598181628752] Loss: 26.127681206189795\n",
      "Iteracion: 2473 Gradiente: [0.10808557813756654,-1.8647679229744876] Loss: 26.12418937999484\n",
      "Iteracion: 2474 Gradiente: [0.10802811652666643,-1.8637765551080183] Loss: 26.12070126553697\n",
      "Iteracion: 2475 Gradiente: [0.10797068546402737,-1.862785714283143] Loss: 26.11721685887067\n",
      "Iteracion: 2476 Gradiente: [0.1079132849334788,-1.8617954002196655] Loss: 26.113736156054646\n",
      "Iteracion: 2477 Gradiente: [0.10785591491881187,-1.8608056126375416] Loss: 26.110259153151787\n",
      "Iteracion: 2478 Gradiente: [0.10779857540383044,-1.859816351256876] Loss: 26.106785846229158\n",
      "Iteracion: 2479 Gradiente: [0.10774126637223229,-1.85882761579793] Loss: 26.103316231357983\n",
      "Iteracion: 2480 Gradiente: [0.10768398780800084,-1.8578394059810937] Loss: 26.099850304613735\n",
      "Iteracion: 2481 Gradiente: [0.10762673969471734,-1.8568517215269362] Loss: 26.09638806207599\n",
      "Iteracion: 2482 Gradiente: [0.10756952201634486,-1.8558645621561471] Loss: 26.092929499828525\n",
      "Iteracion: 2483 Gradiente: [0.10751233475664321,-1.8548779275895806] Loss: 26.08947461395927\n",
      "Iteracion: 2484 Gradiente: [0.10745517789935898,-1.8538918175482384] Loss: 26.08602340056028\n",
      "Iteracion: 2485 Gradiente: [0.10739805142844716,-1.852906231753262] Loss: 26.08257585572783\n",
      "Iteracion: 2486 Gradiente: [0.10734095532784711,-1.8519211699259377] Loss: 26.079131975562312\n",
      "Iteracion: 2487 Gradiente: [0.10728388958121968,-1.8509366317877218] Loss: 26.07569175616826\n",
      "Iteracion: 2488 Gradiente: [0.10722685417243269,-1.8499526170602034] Loss: 26.07225519365435\n",
      "Iteracion: 2489 Gradiente: [0.1071698490854492,-1.8489691254651168] Loss: 26.068822284133397\n",
      "Iteracion: 2490 Gradiente: [0.10711287430407973,-1.8479861567243536] Loss: 26.065393023722322\n",
      "Iteracion: 2491 Gradiente: [0.10705592981228354,-1.8470037105599417] Loss: 26.06196740854223\n",
      "Iteracion: 2492 Gradiente: [0.10699901559403457,-1.8460217866940616] Loss: 26.058545434718322\n",
      "Iteracion: 2493 Gradiente: [0.10694213163300788,-1.8450403848490546] Loss: 26.055127098379888\n",
      "Iteracion: 2494 Gradiente: [0.10688527791323092,-1.8440595047473944] Loss: 26.051712395660385\n",
      "Iteracion: 2495 Gradiente: [0.10682845441871128,-1.8430791461116987] Loss: 26.04830132269733\n",
      "Iteracion: 2496 Gradiente: [0.1067716611332287,-1.8420993086647512] Loss: 26.04489387563238\n",
      "Iteracion: 2497 Gradiente: [0.10671489804079735,-1.8411199921294648] Loss: 26.041490050611266\n",
      "Iteracion: 2498 Gradiente: [0.10665816512542771,-1.840141196228907] Loss: 26.03808984378386\n",
      "Iteracion: 2499 Gradiente: [0.10660146237108516,-1.8391629206862887] Loss: 26.03469325130408\n",
      "Iteracion: 2500 Gradiente: [0.10654478976151628,-1.838185165224985] Loss: 26.031300269329975\n",
      "Iteracion: 2501 Gradiente: [0.10648814728091291,-1.837207929568493] Loss: 26.027910894023606\n",
      "Iteracion: 2502 Gradiente: [0.10643153491316232,-1.8362312134404737] Loss: 26.024525121551214\n",
      "Iteracion: 2503 Gradiente: [0.10637495264236918,-1.835255016564724] Loss: 26.02114294808303\n",
      "Iteracion: 2504 Gradiente: [0.10631840045229761,-1.8342793386652056] Loss: 26.017764369793397\n",
      "Iteracion: 2505 Gradiente: [0.10626187832716028,-1.8333041794660043] Loss: 26.014389382860728\n",
      "Iteracion: 2506 Gradiente: [0.10620538625097045,-1.8323295386913618] Loss: 26.011017983467465\n",
      "Iteracion: 2507 Gradiente: [0.10614892420771772,-1.8313554160656706] Loss: 26.007650167800172\n",
      "Iteracion: 2508 Gradiente: [0.10609249218140586,-1.83038181131347] Loss: 26.004285932049367\n",
      "Iteracion: 2509 Gradiente: [0.10603609015609265,-1.8294087241594388] Loss: 26.00092527240971\n",
      "Iteracion: 2510 Gradiente: [0.10597971811579991,-1.8284361543284087] Loss: 25.997568185079874\n",
      "Iteracion: 2511 Gradiente: [0.10592337604462519,-1.827464101545353] Loss: 25.994214666262543\n",
      "Iteracion: 2512 Gradiente: [0.10586706392668219,-1.8264925655353912] Loss: 25.99086471216449\n",
      "Iteracion: 2513 Gradiente: [0.10581078174601923,-1.825521546023791] Loss: 25.98751831899649\n",
      "Iteracion: 2514 Gradiente: [0.10575452948657944,-1.8245510427359752] Loss: 25.98417548297334\n",
      "Iteracion: 2515 Gradiente: [0.10569830713247796,-1.8235810553975005] Loss: 25.980836200313867\n",
      "Iteracion: 2516 Gradiente: [0.10564211466801415,-1.8226115837340633] Loss: 25.977500467240926\n",
      "Iteracion: 2517 Gradiente: [0.10558595207726237,-1.8216426274715165] Loss: 25.974168279981413\n",
      "Iteracion: 2518 Gradiente: [0.10552981934411605,-1.8206741863358686] Loss: 25.97083963476614\n",
      "Iteracion: 2519 Gradiente: [0.10547371645299299,-1.8197062600532468] Loss: 25.967514527830044\n",
      "Iteracion: 2520 Gradiente: [0.10541764338780742,-1.8187388483499527] Loss: 25.964192955412\n",
      "Iteracion: 2521 Gradiente: [0.10536160013280191,-1.8177719509524128] Loss: 25.960874913754868\n",
      "Iteracion: 2522 Gradiente: [0.10530558667216307,-1.816805567587205] Loss: 25.957560399105546\n",
      "Iteracion: 2523 Gradiente: [0.10524960298992028,-1.8158396979810605] Loss: 25.95424940771487\n",
      "Iteracion: 2524 Gradiente: [0.10519364907042833,-1.814874341860841] Loss: 25.950941935837726\n",
      "Iteracion: 2525 Gradiente: [0.10513772489772985,-1.8139094989535678] Loss: 25.94763797973293\n",
      "Iteracion: 2526 Gradiente: [0.10508183045593569,-1.8129451689864053] Loss: 25.94433753566329\n",
      "Iteracion: 2527 Gradiente: [0.10502596572938927,-1.8119813516866514] Loss: 25.94104059989559\n",
      "Iteracion: 2528 Gradiente: [0.10497013070225118,-1.811018046781757] Loss: 25.937747168700582\n",
      "Iteracion: 2529 Gradiente: [0.10491432535872368,-1.8100552539993187] Loss: 25.934457238352977\n",
      "Iteracion: 2530 Gradiente: [0.10485854968312272,-1.8090929730670708] Loss: 25.931170805131444\n",
      "Iteracion: 2531 Gradiente: [0.10480280365943126,-1.8081312037129122] Loss: 25.927887865318606\n",
      "Iteracion: 2532 Gradiente: [0.10474708727209976,-1.8071699456648598] Loss: 25.924608415201064\n",
      "Iteracion: 2533 Gradiente: [0.10469140050533762,-1.8062091986510906] Loss: 25.92133245106934\n",
      "Iteracion: 2534 Gradiente: [0.10463574334332434,-1.8052489623999257] Loss: 25.91805996921789\n",
      "Iteracion: 2535 Gradiente: [0.1045801157704337,-1.804289236639822] Loss: 25.914790965945144\n",
      "Iteracion: 2536 Gradiente: [0.10452451777081724,-1.8033300210993948] Loss: 25.911525437553436\n",
      "Iteracion: 2537 Gradiente: [0.10446894932873883,-1.8023713155073944] Loss: 25.908263380349037\n",
      "Iteracion: 2538 Gradiente: [0.10441341042869444,-1.8014131195927068] Loss: 25.905004790642163\n",
      "Iteracion: 2539 Gradiente: [0.10435790105473615,-1.8004554330843836] Loss: 25.901749664746923\n",
      "Iteracion: 2540 Gradiente: [0.10430242119136987,-1.7994982557115997] Loss: 25.898497998981373\n",
      "Iteracion: 2541 Gradiente: [0.10424697082277513,-1.7985415872036872] Loss: 25.89524978966745\n",
      "Iteracion: 2542 Gradiente: [0.10419154993326743,-1.7975854272901206] Loss: 25.892005033131056\n",
      "Iteracion: 2543 Gradiente: [0.10413615850712432,-1.7966297757005179] Loss: 25.888763725701917\n",
      "Iteracion: 2544 Gradiente: [0.10408079652883467,-1.7956746321646295] Loss: 25.885525863713713\n",
      "Iteracion: 2545 Gradiente: [0.10402546398262966,-1.7947199964123652] Loss: 25.882291443504027\n",
      "Iteracion: 2546 Gradiente: [0.10397016085298579,-1.793765868173765] Loss: 25.879060461414326\n",
      "Iteracion: 2547 Gradiente: [0.10391488712414845,-1.792812247179025] Loss: 25.875832913789967\n",
      "Iteracion: 2548 Gradiente: [0.10385964278043976,-1.791859133158481] Loss: 25.87260879698017\n",
      "Iteracion: 2549 Gradiente: [0.10380442780640399,-1.790906525842602] Loss: 25.869388107338054\n",
      "Iteracion: 2550 Gradiente: [0.10374924218632108,-1.789954424962013] Loss: 25.866170841220598\n",
      "Iteracion: 2551 Gradiente: [0.10369408590469267,-1.7890028302474714] Loss: 25.862956994988703\n",
      "Iteracion: 2552 Gradiente: [0.10363895894571064,-1.7880517414298964] Loss: 25.85974656500707\n",
      "Iteracion: 2553 Gradiente: [0.10358386129398649,-1.787101158240327] Loss: 25.85653954764435\n",
      "Iteracion: 2554 Gradiente: [0.10352879293380207,-1.786151080409961] Loss: 25.8533359392729\n",
      "Iteracion: 2555 Gradiente: [0.10347375384970738,-1.7852015076701306] Loss: 25.850135736269163\n",
      "Iteracion: 2556 Gradiente: [0.10341874402604152,-1.7842524397523163] Loss: 25.846938935013213\n",
      "Iteracion: 2557 Gradiente: [0.10336376344733035,-1.783303876388137] Loss: 25.843745531889105\n",
      "Iteracion: 2558 Gradiente: [0.10330881209794805,-1.7823558173093594] Loss: 25.840555523284657\n",
      "Iteracion: 2559 Gradiente: [0.10325388996244461,-1.7814082622478848] Loss: 25.837368905591617\n",
      "Iteracion: 2560 Gradiente: [0.10319899702517242,-1.7804612109357698] Loss: 25.83418567520549\n",
      "Iteracion: 2561 Gradiente: [0.10314413327065684,-1.7795146631052028] Loss: 25.831005828525637\n",
      "Iteracion: 2562 Gradiente: [0.10308929868341703,-1.778568618488515] Loss: 25.82782936195527\n",
      "Iteracion: 2563 Gradiente: [0.1030344932479674,-1.7776230768181833] Loss: 25.824656271901397\n",
      "Iteracion: 2564 Gradiente: [0.10297971694867745,-1.7766780378268312] Loss: 25.821486554774857\n",
      "Iteracion: 2565 Gradiente: [0.10292496977016961,-1.775733501247214] Loss: 25.81832020699027\n",
      "Iteracion: 2566 Gradiente: [0.10287025169698628,-1.7747894668122308] Loss: 25.81515722496613\n",
      "Iteracion: 2567 Gradiente: [0.10281556271365654,-1.7738459342549275] Loss: 25.811997605124695\n",
      "Iteracion: 2568 Gradiente: [0.102760902804647,-1.7729029033084962] Loss: 25.80884134389204\n",
      "Iteracion: 2569 Gradiente: [0.10270627195448488,-1.7719603737062635] Loss: 25.805688437698027\n",
      "Iteracion: 2570 Gradiente: [0.10265167014771351,-1.7710183451817008] Loss: 25.80253888297632\n",
      "Iteracion: 2571 Gradiente: [0.10259709736904199,-1.7700768174684114] Loss: 25.79939267616439\n",
      "Iteracion: 2572 Gradiente: [0.10254255360280193,-1.7691357903001612] Loss: 25.79624981370346\n",
      "Iteracion: 2573 Gradiente: [0.10248803883382986,-1.7681952634108318] Loss: 25.79311029203858\n",
      "Iteracion: 2574 Gradiente: [0.10243355304657815,-1.7672552365344663] Loss: 25.78997410761853\n",
      "Iteracion: 2575 Gradiente: [0.1023790962255731,-1.7663157094052453] Loss: 25.786841256895908\n",
      "Iteracion: 2576 Gradiente: [0.10232466835554609,-1.7653766817574814] Loss: 25.783711736327053\n",
      "Iteracion: 2577 Gradiente: [0.10227026942095468,-1.7644381533256435] Loss: 25.780585542372084\n",
      "Iteracion: 2578 Gradiente: [0.10221589940661839,-1.7635001238443224] Loss: 25.777462671494888\n",
      "Iteracion: 2579 Gradiente: [0.10216155829701279,-1.7625625930482676] Loss: 25.774343120163092\n",
      "Iteracion: 2580 Gradiente: [0.10210724607683185,-1.7616255606723594] Loss: 25.771226884848108\n",
      "Iteracion: 2581 Gradiente: [0.10205296273067006,-1.7606890264516268] Loss: 25.768113962025062\n",
      "Iteracion: 2582 Gradiente: [0.10199870824316169,-1.759752990121235] Loss: 25.765004348172862\n",
      "Iteracion: 2583 Gradiente: [0.1019444825990334,-1.7588174514164854] Loss: 25.76189803977412\n",
      "Iteracion: 2584 Gradiente: [0.1018902857829796,-1.757882410072823] Loss: 25.758795033315224\n",
      "Iteracion: 2585 Gradiente: [0.10183611777953179,-1.7569478658258448] Loss: 25.755695325286293\n",
      "Iteracion: 2586 Gradiente: [0.1017819785734711,-1.7560138184112764] Loss: 25.752598912181156\n",
      "Iteracion: 2587 Gradiente: [0.10172786814935458,-1.7550802675649875] Loss: 25.749505790497356\n",
      "Iteracion: 2588 Gradiente: [0.10167378649218412,-1.7541472130229745] Loss: 25.746415956736225\n",
      "Iteracion: 2589 Gradiente: [0.1016197335863879,-1.7532146545214013] Loss: 25.743329407402747\n",
      "Iteracion: 2590 Gradiente: [0.1015657094168598,-1.7522825917965472] Loss: 25.740246139005638\n",
      "Iteracion: 2591 Gradiente: [0.10151171396826252,-1.7513510245848456] Loss: 25.73716614805734\n",
      "Iteracion: 2592 Gradiente: [0.1014577472252133,-1.7504199526228728] Loss: 25.734089431073997\n",
      "Iteracion: 2593 Gradiente: [0.10140380917264338,-1.7494893756473289] Loss: 25.731015984575468\n",
      "Iteracion: 2594 Gradiente: [0.10134989979508949,-1.7485592933950747] Loss: 25.727945805085266\n",
      "Iteracion: 2595 Gradiente: [0.1012960190774,-1.7476297056030945] Loss: 25.724878889130633\n",
      "Iteracion: 2596 Gradiente: [0.10124216700443943,-1.7467006120085142] Loss: 25.721815233242527\n",
      "Iteracion: 2597 Gradiente: [0.10118834356086097,-1.7457720123486091] Loss: 25.71875483395556\n",
      "Iteracion: 2598 Gradiente: [0.10113454873149123,-1.7448439063607852] Loss: 25.715697687808028\n",
      "Iteracion: 2599 Gradiente: [0.10108078250104455,-1.7439162937825956] Loss: 25.712643791341893\n",
      "Iteracion: 2600 Gradiente: [0.10102704485446215,-1.7429891743517207] Loss: 25.70959314110285\n",
      "Iteracion: 2601 Gradiente: [0.1009733357763821,-1.742062547805998] Loss: 25.706545733640215\n",
      "Iteracion: 2602 Gradiente: [0.10091965525167552,-1.7411364138833905] Loss: 25.70350156550696\n",
      "Iteracion: 2603 Gradiente: [0.10086600326523486,-1.740210772322001] Loss: 25.700460633259784\n",
      "Iteracion: 2604 Gradiente: [0.10081237980183459,-1.7392856228600768] Loss: 25.697422933458974\n",
      "Iteracion: 2605 Gradiente: [0.10075878484629706,-1.7383609652360064] Loss: 25.69438846266853\n",
      "Iteracion: 2606 Gradiente: [0.10070521838353177,-1.7374367991883075] Loss: 25.691357217456094\n",
      "Iteracion: 2607 Gradiente: [0.10065168039841504,-1.7365131244556418] Loss: 25.688329194392928\n",
      "Iteracion: 2608 Gradiente: [0.1005981708756129,-1.735589940776824] Loss: 25.685304390053954\n",
      "Iteracion: 2609 Gradiente: [0.10054468980004146,-1.7346672478907905] Loss: 25.682282801017738\n",
      "Iteracion: 2610 Gradiente: [0.10049123715680443,-1.7337450455366081] Loss: 25.679264423866492\n",
      "Iteracion: 2611 Gradiente: [0.1004378129305953,-1.7328233334535066] Loss: 25.676249255186033\n",
      "Iteracion: 2612 Gradiente: [0.10038441710633492,-1.7319021113808404] Loss: 25.673237291565837\n",
      "Iteracion: 2613 Gradiente: [0.10033104966890534,-1.7309813790581066] Loss: 25.670228529599\n",
      "Iteracion: 2614 Gradiente: [0.1002777106031876,-1.73006113622494] Loss: 25.667222965882196\n",
      "Iteracion: 2615 Gradiente: [0.10022439989430107,-1.7291413826211024] Loss: 25.664220597015795\n",
      "Iteracion: 2616 Gradiente: [0.10017111752696668,-1.7282221179865151] Loss: 25.661221419603724\n",
      "Iteracion: 2617 Gradiente: [0.1001178634860878,-1.7273033420612294] Loss: 25.658225430253538\n",
      "Iteracion: 2618 Gradiente: [0.10006463775688038,-1.7263850545854174] Loss: 25.655232625576396\n",
      "Iteracion: 2619 Gradiente: [0.10001144032388633,-1.7254672552994246] Loss: 25.65224300218704\n",
      "Iteracion: 2620 Gradiente: [0.09995827117253384,-1.7245499439436898] Loss: 25.64925655670385\n",
      "Iteracion: 2621 Gradiente: [0.09990513028731934,-1.7236331202588413] Loss: 25.64627328574878\n",
      "Iteracion: 2622 Gradiente: [0.09985201765351803,-1.7227167839855997] Loss: 25.64329318594736\n",
      "Iteracion: 2623 Gradiente: [0.09979893325594133,-1.7218009348648529] Loss: 25.640316253928713\n",
      "Iteracion: 2624 Gradiente: [0.09974587707974791,-1.720885572637605] Loss: 25.637342486325565\n",
      "Iteracion: 2625 Gradiente: [0.09969284910982547,-1.7199706970450153] Loss: 25.634371879774225\n",
      "Iteracion: 2626 Gradiente: [0.09963984933112992,-1.7190563078283765] Loss: 25.631404430914543\n",
      "Iteracion: 2627 Gradiente: [0.09958687772882654,-1.7181424047291076] Loss: 25.628440136389983\n",
      "Iteracion: 2628 Gradiente: [0.09953393428776754,-1.7172289874887836] Loss: 25.62547899284751\n",
      "Iteracion: 2629 Gradiente: [0.09948101899318829,-1.7163160558490937] Loss: 25.622520996937737\n",
      "Iteracion: 2630 Gradiente: [0.09942813182992485,-1.7154036095518905] Loss: 25.61956614531478\n",
      "Iteracion: 2631 Gradiente: [0.09937527278319844,-1.7144916483391412] Loss: 25.616614434636347\n",
      "Iteracion: 2632 Gradiente: [0.09932244183791994,-1.713580171952968] Loss: 25.613665861563643\n",
      "Iteracion: 2633 Gradiente: [0.09926963897905239,-1.7126691801356257] Loss: 25.6107204227615\n",
      "Iteracion: 2634 Gradiente: [0.0992168641919927,-1.7117586726294869] Loss: 25.60777811489823\n",
      "Iteracion: 2635 Gradiente: [0.09916411746148886,-1.7108486491770944] Loss: 25.60483893464572\n",
      "Iteracion: 2636 Gradiente: [0.09911139877280799,-1.709939109521099] Loss: 25.60190287867937\n",
      "Iteracion: 2637 Gradiente: [0.09905870811108551,-1.7090300534042984] Loss: 25.598969943678174\n",
      "Iteracion: 2638 Gradiente: [0.09900604546127502,-1.7081214805696352] Loss: 25.596040126324567\n",
      "Iteracion: 2639 Gradiente: [0.09895341080850244,-1.707213390760182] Loss: 25.593113423304565\n",
      "Iteracion: 2640 Gradiente: [0.09890080413795203,-1.7063057837191438] Loss: 25.59018983130772\n",
      "Iteracion: 2641 Gradiente: [0.09884822543475594,-1.7053986591898662] Loss: 25.587269347027057\n",
      "Iteracion: 2642 Gradiente: [0.09879567468402399,-1.704492016915831] Loss: 25.584351967159158\n",
      "Iteracion: 2643 Gradiente: [0.09874315187082346,-1.7035858566406603] Loss: 25.58143768840407\n",
      "Iteracion: 2644 Gradiente: [0.09869065698039492,-1.7026801781081065] Loss: 25.578526507465423\n",
      "Iteracion: 2645 Gradiente: [0.09863818999779994,-1.7017749810620622] Loss: 25.575618421050255\n",
      "Iteracion: 2646 Gradiente: [0.0985857509082725,-1.7008702652465544] Loss: 25.57271342586919\n",
      "Iteracion: 2647 Gradiente: [0.09853333969699586,-1.6999660304057413] Loss: 25.569811518636268\n",
      "Iteracion: 2648 Gradiente: [0.09848095634923529,-1.699062276283918] Loss: 25.566912696069117\n",
      "Iteracion: 2649 Gradiente: [0.09842860084995901,-1.6981590026255335] Loss: 25.564016954888768\n",
      "Iteracion: 2650 Gradiente: [0.09837627318457388,-1.6972562091751457] Loss: 25.561124291819787\n",
      "Iteracion: 2651 Gradiente: [0.09832397333809363,-1.6963538956774717] Loss: 25.5582347035902\n",
      "Iteracion: 2652 Gradiente: [0.09827170129589907,-1.695452061877345] Loss: 25.555348186931543\n",
      "Iteracion: 2653 Gradiente: [0.09821945704301717,-1.6945507075197546] Loss: 25.55246473857875\n",
      "Iteracion: 2654 Gradiente: [0.09816724056484721,-1.6936498323498035] Loss: 25.549584355270316\n",
      "Iteracion: 2655 Gradiente: [0.09811505184652608,-1.6927494361127473] Loss: 25.546707033748156\n",
      "Iteracion: 2656 Gradiente: [0.0980628908734559,-1.6918495185539604] Loss: 25.543832770757653\n",
      "Iteracion: 2657 Gradiente: [0.0980107576305007,-1.6909500794189853] Loss: 25.54096156304765\n",
      "Iteracion: 2658 Gradiente: [0.09795865210339419,-1.6900511184534504] Loss: 25.53809340737047\n",
      "Iteracion: 2659 Gradiente: [0.09790657427704398,-1.6891526354031663] Loss: 25.535228300481833\n",
      "Iteracion: 2660 Gradiente: [0.09785452413696825,-1.6882546300140437] Loss: 25.532366239140973\n",
      "Iteracion: 2661 Gradiente: [0.09780250166833279,-1.6873571020321514] Loss: 25.52950722011051\n",
      "Iteracion: 2662 Gradiente: [0.09775050685643312,-1.6864600512036845] Loss: 25.52665124015657\n",
      "Iteracion: 2663 Gradiente: [0.09769853968653545,-1.685563477274974] Loss: 25.523798296048636\n",
      "Iteracion: 2664 Gradiente: [0.09764660014403906,-1.684667379992481] Loss: 25.520948384559677\n",
      "Iteracion: 2665 Gradiente: [0.09759468821413529,-1.683771759102813] Loss: 25.518101502466113\n",
      "Iteracion: 2666 Gradiente: [0.09754280388213109,-1.6828766143527045] Loss: 25.51525764654774\n",
      "Iteracion: 2667 Gradiente: [0.09749094713364077,-1.681981945489009] Loss: 25.51241681358779\n",
      "Iteracion: 2668 Gradiente: [0.09743911795370934,-1.6810877522587477] Loss: 25.50957900037294\n",
      "Iteracion: 2669 Gradiente: [0.09738731632767876,-1.6801940344090598] Loss: 25.506744203693255\n",
      "Iteracion: 2670 Gradiente: [0.09733554224102078,-1.6793007916872116] Loss: 25.503912420342235\n",
      "Iteracion: 2671 Gradiente: [0.0972837956790471,-1.6784080238406118] Loss: 25.50108364711674\n",
      "Iteracion: 2672 Gradiente: [0.09723207662724083,-1.6775157306167974] Loss: 25.49825788081711\n",
      "Iteracion: 2673 Gradiente: [0.0971803850707829,-1.6766239117634545] Loss: 25.495435118247045\n",
      "Iteracion: 2674 Gradiente: [0.09712872099518013,-1.6757325670283862] Loss: 25.492615356213612\n",
      "Iteracion: 2675 Gradiente: [0.09707708438571956,-1.6748416961595411] Loss: 25.489798591527336\n",
      "Iteracion: 2676 Gradiente: [0.09702547522790753,-1.6739512989049947] Loss: 25.486984821002103\n",
      "Iteracion: 2677 Gradiente: [0.09697389350705,-1.673061375012961] Loss: 25.484174041455145\n",
      "Iteracion: 2678 Gradiente: [0.09692233920870592,-1.6721719242317794] Loss: 25.481366249707147\n",
      "Iteracion: 2679 Gradiente: [0.09687081231821869,-1.6712829463099337] Loss: 25.478561442582137\n",
      "Iteracion: 2680 Gradiente: [0.09681931282098333,-1.6703944409960392] Loss: 25.475759616907506\n",
      "Iteracion: 2681 Gradiente: [0.09676784070242993,-1.6695064080388458] Loss: 25.472960769514064\n",
      "Iteracion: 2682 Gradiente: [0.0967163959480113,-1.6686188471872312] Loss: 25.470164897235957\n",
      "Iteracion: 2683 Gradiente: [0.09666497854324708,-1.667731758190205] Loss: 25.467371996910686\n",
      "Iteracion: 2684 Gradiente: [0.09661358847350716,-1.666845140796923] Loss: 25.46458206537914\n",
      "Iteracion: 2685 Gradiente: [0.09656222572440688,-1.665958994756655] Loss: 25.461795099485546\n",
      "Iteracion: 2686 Gradiente: [0.09651089028119392,-1.6650733198188297] Loss: 25.459011096077486\n",
      "Iteracion: 2687 Gradiente: [0.09645958212947979,-1.6641881157329863] Loss: 25.45623005200592\n",
      "Iteracion: 2688 Gradiente: [0.09640830125482439,-1.6633033822488028] Loss: 25.45345196412514\n",
      "Iteracion: 2689 Gradiente: [0.09635704764265493,-1.6624191191160966] Loss: 25.450676829292753\n",
      "Iteracion: 2690 Gradiente: [0.0963058212784702,-1.661535326084813] Loss: 25.447904644369746\n",
      "Iteracion: 2691 Gradiente: [0.09625462214787982,-1.6606520029050291] Loss: 25.445135406220444\n",
      "Iteracion: 2692 Gradiente: [0.09620345023618644,-1.6597691493269673] Loss: 25.44236911171244\n",
      "Iteracion: 2693 Gradiente: [0.09615230552916311,-1.6588867651009616] Loss: 25.439605757716752\n",
      "Iteracion: 2694 Gradiente: [0.09610118801211721,-1.6580048499775002] Loss: 25.436845341107656\n",
      "Iteracion: 2695 Gradiente: [0.09605009767082037,-1.657123403707183] Loss: 25.434087858762798\n",
      "Iteracion: 2696 Gradiente: [0.09599903449063495,-1.6562424260407633] Loss: 25.431333307563087\n",
      "Iteracion: 2697 Gradiente: [0.09594799845723163,-1.6553619167291118] Loss: 25.428581684392793\n",
      "Iteracion: 2698 Gradiente: [0.09589698955612297,-1.6544818755232393] Loss: 25.425832986139472\n",
      "Iteracion: 2699 Gradiente: [0.09584600777296069,-1.6536023021742807] Loss: 25.42308720969402\n",
      "Iteracion: 2700 Gradiente: [0.09579505309328624,-1.6527231964335123] Loss: 25.420344351950586\n",
      "Iteracion: 2701 Gradiente: [0.09574412550262441,-1.6518445580523418] Loss: 25.417604409806685\n",
      "Iteracion: 2702 Gradiente: [0.09569322498664264,-1.650966386782306] Loss: 25.41486738016308\n",
      "Iteracion: 2703 Gradiente: [0.09564235153097418,-1.65008868237507] Loss: 25.412133259923852\n",
      "Iteracion: 2704 Gradiente: [0.09559150512112022,-1.6492114445824415] Loss: 25.409402045996362\n",
      "Iteracion: 2705 Gradiente: [0.09554068574292861,-1.6483346731563404] Loss: 25.406673735291236\n",
      "Iteracion: 2706 Gradiente: [0.09548989338175508,-1.6474583678488517] Loss: 25.403948324722467\n",
      "Iteracion: 2707 Gradiente: [0.09543912802338544,-1.6465825284121598] Loss: 25.40122581120723\n",
      "Iteracion: 2708 Gradiente: [0.09538838965341465,-1.6457071545985964] Loss: 25.398506191666023\n",
      "Iteracion: 2709 Gradiente: [0.09533767825747456,-1.6448322461606248] Loss: 25.395789463022616\n",
      "Iteracion: 2710 Gradiente: [0.09528699382140786,-1.6439578028508268] Loss: 25.393075622204062\n",
      "Iteracion: 2711 Gradiente: [0.09523633633076921,-1.6430838244219308] Loss: 25.390364666140645\n",
      "Iteracion: 2712 Gradiente: [0.09518570577112276,-1.6422103106267996] Loss: 25.38765659176594\n",
      "Iteracion: 2713 Gradiente: [0.09513510212819512,-1.6413372612184163] Loss: 25.384951396016756\n",
      "Iteracion: 2714 Gradiente: [0.09508452538781521,-1.6404646759498904] Loss: 25.382249075833215\n",
      "Iteracion: 2715 Gradiente: [0.09503397553553299,-1.6395925545744805] Loss: 25.379549628158617\n",
      "Iteracion: 2716 Gradiente: [0.09498345255711911,-1.6387208968455622] Loss: 25.37685304993955\n",
      "Iteracion: 2717 Gradiente: [0.09493295643814624,-1.6378497025166543] Loss: 25.374159338125857\n",
      "Iteracion: 2718 Gradiente: [0.09488248716461953,-1.6369789713413851] Loss: 25.371468489670608\n",
      "Iteracion: 2719 Gradiente: [0.09483204472199418,-1.6361087030735413] Loss: 25.368780501530104\n",
      "Iteracion: 2720 Gradiente: [0.09478162909611428,-1.6352388974670222] Loss: 25.366095370663892\n",
      "Iteracion: 2721 Gradiente: [0.09473124027282485,-1.6343695542758563] Loss: 25.363413094034755\n",
      "Iteracion: 2722 Gradiente: [0.09468087823771706,-1.6335006732542174] Loss: 25.360733668608688\n",
      "Iteracion: 2723 Gradiente: [0.09463054297673353,-1.6326322541563945] Loss: 25.358057091354958\n",
      "Iteracion: 2724 Gradiente: [0.09458023447536874,-1.6317642967368287] Loss: 25.355383359245984\n",
      "Iteracion: 2725 Gradiente: [0.09452995271961366,-1.6308968007500637] Loss: 25.352712469257444\n",
      "Iteracion: 2726 Gradiente: [0.09447969769526973,-1.6300297659507896] Loss: 25.35004441836823\n",
      "Iteracion: 2727 Gradiente: [0.09442946938790536,-1.629163192093835] Loss: 25.347379203560433\n",
      "Iteracion: 2728 Gradiente: [0.09437926778348119,-1.6282970789341396] Loss: 25.344716821819365\n",
      "Iteracion: 2729 Gradiente: [0.09432909286777734,-1.6274314262267855] Loss: 25.342057270133544\n",
      "Iteracion: 2730 Gradiente: [0.09427894462653891,-1.6265662337269844] Loss: 25.33940054549464\n",
      "Iteracion: 2731 Gradiente: [0.09422882304574264,-1.6257015011900686] Loss: 25.336746644897644\n",
      "Iteracion: 2732 Gradiente: [0.09417872811108766,-1.6248372283715133] Loss: 25.334095565340572\n",
      "Iteracion: 2733 Gradiente: [0.09412865980841048,-1.6239734150269172] Loss: 25.331447303824763\n",
      "Iteracion: 2734 Gradiente: [0.09407861812364615,-1.6231100609120077] Loss: 25.328801857354694\n",
      "Iteracion: 2735 Gradiente: [0.09402860304250708,-1.6222471657826494] Loss: 25.32615922293804\n",
      "Iteracion: 2736 Gradiente: [0.09397861455099038,-1.6213847293948251] Loss: 25.323519397585628\n",
      "Iteracion: 2737 Gradiente: [0.09392865263475537,-1.620522751504663] Loss: 25.32088237831148\n",
      "Iteracion: 2738 Gradiente: [0.09387871727998345,-1.6196612318683965] Loss: 25.31824816213283\n",
      "Iteracion: 2739 Gradiente: [0.09382880847227663,-1.618800170242418] Loss: 25.31561674607\n",
      "Iteracion: 2740 Gradiente: [0.09377892619763249,-1.6179395663832297] Loss: 25.31298812714655\n",
      "Iteracion: 2741 Gradiente: [0.09372907044189986,-1.6170794200474707] Loss: 25.31036230238919\n",
      "Iteracion: 2742 Gradiente: [0.09367924119107916,-1.6162197309919037] Loss: 25.30773926882777\n",
      "Iteracion: 2743 Gradiente: [0.09362943843099032,-1.6153604989734278] Loss: 25.305119023495315\n",
      "Iteracion: 2744 Gradiente: [0.09357966214755796,-1.614501723749067] Loss: 25.302501563427974\n",
      "Iteracion: 2745 Gradiente: [0.09352991232675407,-1.613643405075974] Loss: 25.29988688566509\n",
      "Iteracion: 2746 Gradiente: [0.09348018895450329,-1.6127855427114333] Loss: 25.297274987249136\n",
      "Iteracion: 2747 Gradiente: [0.09343049201672594,-1.6119281364128564] Loss: 25.294665865225717\n",
      "Iteracion: 2748 Gradiente: [0.09338082149928748,-1.611071185937791] Loss: 25.292059516643572\n",
      "Iteracion: 2749 Gradiente: [0.09333117738820013,-1.6102146910439017] Loss: 25.289455938554607\n",
      "Iteracion: 2750 Gradiente: [0.09328155966956521,-1.6093586514889824] Loss: 25.286855128013865\n",
      "Iteracion: 2751 Gradiente: [0.09323196832925855,-1.6085030670309648] Loss: 25.284257082079467\n",
      "Iteracion: 2752 Gradiente: [0.09318240335311997,-1.6076479374279131] Loss: 25.281661797812724\n",
      "Iteracion: 2753 Gradiente: [0.09313286472724466,-1.6067932624380066] Loss: 25.279069272278015\n",
      "Iteracion: 2754 Gradiente: [0.09308335243766616,-1.6059390418195558] Loss: 25.276479502542884\n",
      "Iteracion: 2755 Gradiente: [0.09303386647033847,-1.605085275331006] Loss: 25.273892485677965\n",
      "Iteracion: 2756 Gradiente: [0.09298440681126342,-1.6042319627309276] Loss: 25.271308218757017\n",
      "Iteracion: 2757 Gradiente: [0.09293497344653096,-1.603379103778017] Loss: 25.268726698856945\n",
      "Iteracion: 2758 Gradiente: [0.09288556636212254,-1.6025266982311008] Loss: 25.26614792305766\n",
      "Iteracion: 2759 Gradiente: [0.09283618554385148,-1.6016747458491485] Loss: 25.263571888442275\n",
      "Iteracion: 2760 Gradiente: [0.09278683097807487,-1.6008232463912264] Loss: 25.260998592096975\n",
      "Iteracion: 2761 Gradiente: [0.09273750265058472,-1.5999721996165581] Loss: 25.25842803111102\n",
      "Iteracion: 2762 Gradiente: [0.0926882005477637,-1.599121605284466] Loss: 25.255860202576805\n",
      "Iteracion: 2763 Gradiente: [0.09263892465518495,-1.5982714631544446] Loss: 25.253295103589757\n",
      "Iteracion: 2764 Gradiente: [0.0925896749593079,-1.5974217729860678] Loss: 25.250732731248466\n",
      "Iteracion: 2765 Gradiente: [0.0925404514461448,-1.5965725345390627] Loss: 25.24817308265452\n",
      "Iteracion: 2766 Gradiente: [0.09249125410157338,-1.5957237475732897] Loss: 25.24561615491268\n",
      "Iteracion: 2767 Gradiente: [0.09244208291198674,-1.5948754118487118] Loss: 25.243061945130712\n",
      "Iteracion: 2768 Gradiente: [0.09239293786314656,-1.594027527125455] Loss: 25.240510450419478\n",
      "Iteracion: 2769 Gradiente: [0.09234381894147958,-1.5931800931637325] Loss: 25.23796166789293\n",
      "Iteracion: 2770 Gradiente: [0.09229472613273894,-1.592333109723928] Loss: 25.23541559466806\n",
      "Iteracion: 2771 Gradiente: [0.09224565942340585,-1.5914865765665098] Loss: 25.23287222786496\n",
      "Iteracion: 2772 Gradiente: [0.09219661879932156,-1.5906404934521063] Loss: 25.230331564606733\n",
      "Iteracion: 2773 Gradiente: [0.09214760424695735,-1.5897948601414476] Loss: 25.22779360201959\n",
      "Iteracion: 2774 Gradiente: [0.09209861575201331,-1.5889496763954232] Loss: 25.22525833723275\n",
      "Iteracion: 2775 Gradiente: [0.09204965330098579,-1.5881049419750144] Loss: 25.22272576737855\n",
      "Iteracion: 2776 Gradiente: [0.09200071687980464,-1.5872606566413583] Loss: 25.220195889592286\n",
      "Iteracion: 2777 Gradiente: [0.09195180647480565,-1.5864168201556954] Loss: 25.217668701012407\n",
      "Iteracion: 2778 Gradiente: [0.09190292207202806,-1.5855734322794137] Loss: 25.21514419878027\n",
      "Iteracion: 2779 Gradiente: [0.0918540636576831,-1.584730492774015] Loss: 25.212622380040386\n",
      "Iteracion: 2780 Gradiente: [0.0918052312180592,-1.5838880014011267] Loss: 25.210103241940267\n",
      "Iteracion: 2781 Gradiente: [0.09175642473921926,-1.5830459579225147] Loss: 25.20758678163043\n",
      "Iteracion: 2782 Gradiente: [0.0917076442073674,-1.5822043621000637] Loss: 25.205072996264448\n",
      "Iteracion: 2783 Gradiente: [0.09165888960882285,-1.5813632136957803] Loss: 25.20256188299892\n",
      "Iteracion: 2784 Gradiente: [0.09161016092966369,-1.5805225124718105] Loss: 25.200053438993457\n",
      "Iteracion: 2785 Gradiente: [0.09156145815610255,-1.5796822581904197] Loss: 25.19754766141067\n",
      "Iteracion: 2786 Gradiente: [0.09151278127451784,-1.578842450613991] Loss: 25.195044547416238\n",
      "Iteracion: 2787 Gradiente: [0.09146413027100474,-1.5780030895050514] Loss: 25.192544094178803\n",
      "Iteracion: 2788 Gradiente: [0.09141550513191893,-1.577164174626237] Loss: 25.190046298870058\n",
      "Iteracion: 2789 Gradiente: [0.09136690584343322,-1.5763257057403237] Loss: 25.187551158664668\n",
      "Iteracion: 2790 Gradiente: [0.09131833239173753,-1.575487682610212] Loss: 25.185058670740307\n",
      "Iteracion: 2791 Gradiente: [0.0912697847632387,-1.5746501049989161] Loss: 25.182568832277678\n",
      "Iteracion: 2792 Gradiente: [0.09122126294406978,-1.573812972669591] Loss: 25.180081640460415\n",
      "Iteracion: 2793 Gradiente: [0.09117276692073707,-1.572976285385501] Loss: 25.177597092475246\n",
      "Iteracion: 2794 Gradiente: [0.09112429667932816,-1.5721400429100545] Loss: 25.175115185511824\n",
      "Iteracion: 2795 Gradiente: [0.09107585220621531,-1.571304245006775] Loss: 25.17263591676276\n",
      "Iteracion: 2796 Gradiente: [0.09102743348761919,-1.5704688914393186] Loss: 25.170159283423718\n",
      "Iteracion: 2797 Gradiente: [0.09097904050990735,-1.5696339819714593] Loss: 25.167685282693313\n",
      "Iteracion: 2798 Gradiente: [0.09093067325938761,-1.5687995163671014] Loss: 25.165213911773137\n",
      "Iteracion: 2799 Gradiente: [0.09088233172228305,-1.5679654943902768] Loss: 25.162745167867733\n",
      "Iteracion: 2800 Gradiente: [0.09083401588516722,-1.567131915805128] Loss: 25.160279048184687\n",
      "Iteracion: 2801 Gradiente: [0.09078572573425608,-1.566298780375939] Loss: 25.15781554993445\n",
      "Iteracion: 2802 Gradiente: [0.09073746125580774,-1.5654660878671218] Loss: 25.155354670330546\n",
      "Iteracion: 2803 Gradiente: [0.09068922243625935,-1.5646338380432006] Loss: 25.15289640658939\n",
      "Iteracion: 2804 Gradiente: [0.09064100926191543,-1.5638020306688316] Loss: 25.150440755930344\n",
      "Iteracion: 2805 Gradiente: [0.09059282171924536,-1.562970665508793] Loss: 25.147987715575816\n",
      "Iteracion: 2806 Gradiente: [0.09054465979449541,-1.562139742327994] Loss: 25.14553728275108\n",
      "Iteracion: 2807 Gradiente: [0.09049652347400704,-1.5613092608914685] Loss: 25.143089454684365\n",
      "Iteracion: 2808 Gradiente: [0.09044841274436616,-1.560479220964361] Loss: 25.1406442286069\n",
      "Iteracion: 2809 Gradiente: [0.09040032759183182,-1.5596496223119574] Loss: 25.138201601752815\n",
      "Iteracion: 2810 Gradiente: [0.09035226800285016,-1.5588204646996615] Loss: 25.135761571359183\n",
      "Iteracion: 2811 Gradiente: [0.09030423396386406,-1.557991747893001] Loss: 25.133324134666044\n",
      "Iteracion: 2812 Gradiente: [0.0902562254611747,-1.5571634716576364] Loss: 25.130889288916315\n",
      "Iteracion: 2813 Gradiente: [0.09020824248136515,-1.5563356357593376] Loss: 25.12845703135591\n",
      "Iteracion: 2814 Gradiente: [0.09016028501071625,-1.5555082399640152] Loss: 25.126027359233614\n",
      "Iteracion: 2815 Gradiente: [0.09011235303579876,-1.5546812840376916] Loss: 25.1236002698012\n",
      "Iteracion: 2816 Gradiente: [0.09006444654287311,-1.5538547677465266] Loss: 25.121175760313267\n",
      "Iteracion: 2817 Gradiente: [0.09001656551863514,-1.553028690856787] Loss: 25.118753828027444\n",
      "Iteracion: 2818 Gradiente: [0.08996870994937467,-1.5522030531348794] Loss: 25.116334470204194\n",
      "Iteracion: 2819 Gradiente: [0.08992087982161887,-1.551377854347326] Loss: 25.113917684106887\n",
      "Iteracion: 2820 Gradiente: [0.08987307512183046,-1.550553094260778] Loss: 25.111503467001892\n",
      "Iteracion: 2821 Gradiente: [0.08982529583648548,-1.5497287726420077] Loss: 25.109091816158394\n",
      "Iteracion: 2822 Gradiente: [0.08977754195202674,-1.548904889257917] Loss: 25.106682728848522\n",
      "Iteracion: 2823 Gradiente: [0.08972981345500936,-1.548081443875518] Loss: 25.104276202347275\n",
      "Iteracion: 2824 Gradiente: [0.08968211033197378,-1.54725843626196] Loss: 25.10187223393258\n",
      "Iteracion: 2825 Gradiente: [0.08963443256933298,-1.5464358661845135] Loss: 25.099470820885255\n",
      "Iteracion: 2826 Gradiente: [0.08958678015360183,-1.5456137334105704] Loss: 25.097071960488975\n",
      "Iteracion: 2827 Gradiente: [0.08953915307149979,-1.5447920377076376] Loss: 25.09467565003035\n",
      "Iteracion: 2828 Gradiente: [0.08949155130930678,-1.5439707788433688] Loss: 25.092281886798844\n",
      "Iteracion: 2829 Gradiente: [0.08944397485378867,-1.5431499565855138] Loss: 25.0898906680868\n",
      "Iteracion: 2830 Gradiente: [0.08939642369127038,-1.5423295707019717] Loss: 25.087501991189455\n",
      "Iteracion: 2831 Gradiente: [0.08934889780848797,-1.5415096209607428] Loss: 25.085115853404922\n",
      "Iteracion: 2832 Gradiente: [0.08930139719182459,-1.540690107129971] Loss: 25.08273225203416\n",
      "Iteracion: 2833 Gradiente: [0.0892539218279173,-1.5398710289779072] Loss: 25.080351184381016\n",
      "Iteracion: 2834 Gradiente: [0.08920647170342401,-1.5390523862729275] Loss: 25.077972647752215\n",
      "Iteracion: 2835 Gradiente: [0.08915904680482829,-1.53823417878354] Loss: 25.07559663945732\n",
      "Iteracion: 2836 Gradiente: [0.08911164711890175,-1.5374164062783617] Loss: 25.073223156808787\n",
      "Iteracion: 2837 Gradiente: [0.08906427263209479,-1.5365990685261484] Loss: 25.070852197121894\n",
      "Iteracion: 2838 Gradiente: [0.08901692333093744,-1.5357821652957757] Loss: 25.06848375771476\n",
      "Iteracion: 2839 Gradiente: [0.08896959920212737,-1.534965696356236] Loss: 25.06611783590843\n",
      "Iteracion: 2840 Gradiente: [0.08892230023234049,-1.5341496614766426] Loss: 25.06375442902672\n",
      "Iteracion: 2841 Gradiente: [0.08887502640807175,-1.5333340604262409] Loss: 25.061393534396302\n",
      "Iteracion: 2842 Gradiente: [0.08882777771603495,-1.5325188929743925] Loss: 25.059035149346762\n",
      "Iteracion: 2843 Gradiente: [0.0887805541428681,-1.5317041588905838] Loss: 25.056679271210417\n",
      "Iteracion: 2844 Gradiente: [0.08873335567510025,-1.5308898579444246] Loss: 25.054325897322475\n",
      "Iteracion: 2845 Gradiente: [0.08868618229956837,-1.5300759899056409] Loss: 25.051975025021008\n",
      "Iteracion: 2846 Gradiente: [0.08863903400293983,-1.5292625545440823] Loss: 25.049626651646847\n",
      "Iteracion: 2847 Gradiente: [0.08859191077175031,-1.5284495516297332] Loss: 25.04728077454371\n",
      "Iteracion: 2848 Gradiente: [0.08854481259259331,-1.5276369809326937] Loss: 25.04493739105808\n",
      "Iteracion: 2849 Gradiente: [0.08849773945237777,-1.5268248422231734] Loss: 25.042596498539346\n",
      "Iteracion: 2850 Gradiente: [0.08845069133761664,-1.5260131352715234] Loss: 25.040258094339617\n",
      "Iteracion: 2851 Gradiente: [0.08840366823510427,-1.5252018598482033] Loss: 25.037922175813886\n",
      "Iteracion: 2852 Gradiente: [0.08835667013157054,-1.5243910157237963] Loss: 25.03558874031993\n",
      "Iteracion: 2853 Gradiente: [0.08830969701376527,-1.5235806026690095] Loss: 25.033257785218343\n",
      "Iteracion: 2854 Gradiente: [0.08826274886813508,-1.5227706204546876] Loss: 25.03092930787249\n",
      "Iteracion: 2855 Gradiente: [0.08821582568165336,-1.5219610688517695] Loss: 25.028603305648637\n",
      "Iteracion: 2856 Gradiente: [0.08816892744091925,-1.5211519476313362] Loss: 25.026279775915715\n",
      "Iteracion: 2857 Gradiente: [0.08812205413277638,-1.5203432565645765] Loss: 25.023958716045552\n",
      "Iteracion: 2858 Gradiente: [0.08807520574391958,-1.5195349954228101] Loss: 25.021640123412716\n",
      "Iteracion: 2859 Gradiente: [0.08802838226104844,-1.5187271639774802] Loss: 25.0193239953946\n",
      "Iteracion: 2860 Gradiente: [0.08798158367107854,-1.517919762000136] Loss: 25.017010329371374\n",
      "Iteracion: 2861 Gradiente: [0.08793480996053328,-1.5171127892624754] Loss: 25.014699122725986\n",
      "Iteracion: 2862 Gradiente: [0.08788806111645991,-1.5163062455362837] Loss: 25.012390372844163\n",
      "Iteracion: 2863 Gradiente: [0.08784133712553152,-1.515500130593492] Loss: 25.01008407711443\n",
      "Iteracion: 2864 Gradiente: [0.08779463797446282,-1.51469444420615] Loss: 25.007780232928077\n",
      "Iteracion: 2865 Gradiente: [0.08774796365002922,-1.5138891861464259] Loss: 25.005478837679128\n",
      "Iteracion: 2866 Gradiente: [0.08770131413913589,-1.513084356186601] Loss: 25.00317988876446\n",
      "Iteracion: 2867 Gradiente: [0.08765468942858282,-1.5122799540990859] Loss: 25.00088338358365\n",
      "Iteracion: 2868 Gradiente: [0.08760808950513592,-1.5114759796564126] Loss: 24.998589319539075\n",
      "Iteracion: 2869 Gradiente: [0.08756151435561416,-1.5106724326312317] Loss: 24.996297694035835\n",
      "Iteracion: 2870 Gradiente: [0.0875149639669606,-1.5098693127963096] Loss: 24.99400850448185\n",
      "Iteracion: 2871 Gradiente: [0.0874684383258265,-1.5090666199245475] Loss: 24.991721748287706\n",
      "Iteracion: 2872 Gradiente: [0.08742193741921274,-1.5082643537889524] Loss: 24.989437422866846\n",
      "Iteracion: 2873 Gradiente: [0.08737546123392406,-1.507462514162658] Loss: 24.98715552563537\n",
      "Iteracion: 2874 Gradiente: [0.08732900975676235,-1.5066611008189255] Loss: 24.984876054012172\n",
      "Iteracion: 2875 Gradiente: [0.08728258297466406,-1.505860113531122] Loss: 24.982599005418916\n",
      "Iteracion: 2876 Gradiente: [0.08723618087432973,-1.5050595520727563] Loss: 24.98032437727994\n",
      "Iteracion: 2877 Gradiente: [0.08718980344293073,-1.5042594162174252] Loss: 24.978052167022344\n",
      "Iteracion: 2878 Gradiente: [0.08714345066712023,-1.5034597057388797] Loss: 24.97578237207599\n",
      "Iteracion: 2879 Gradiente: [0.08709712253396162,-1.5026604204109653] Loss: 24.973514989873465\n",
      "Iteracion: 2880 Gradiente: [0.08705081903016113,-1.5018615600076715] Loss: 24.971250017850036\n",
      "Iteracion: 2881 Gradiente: [0.08700454014270538,-1.5010631243030896] Loss: 24.968987453443766\n",
      "Iteracion: 2882 Gradiente: [0.08695828585866915,-1.5002651130714284] Loss: 24.966727294095406\n",
      "Iteracion: 2883 Gradiente: [0.08691205616470464,-1.4994675260870396] Loss: 24.964469537248416\n",
      "Iteracion: 2884 Gradiente: [0.08686585104790558,-1.4986703631243712] Loss: 24.962214180349\n",
      "Iteracion: 2885 Gradiente: [0.08681967049511456,-1.4978736239580053] Loss: 24.959961220846065\n",
      "Iteracion: 2886 Gradiente: [0.08677351449338365,-1.497077308362632] Loss: 24.957710656191203\n",
      "Iteracion: 2887 Gradiente: [0.08672738302965209,-1.4962814161130704] Loss: 24.955462483838797\n",
      "Iteracion: 2888 Gradiente: [0.08668127609073509,-1.4954859469842618] Loss: 24.953216701245818\n",
      "Iteracion: 2889 Gradiente: [0.08663519366374241,-1.4946909007512563] Loss: 24.950973305872058\n",
      "Iteracion: 2890 Gradiente: [0.08658913573551862,-1.4938962771892355] Loss: 24.948732295179898\n",
      "Iteracion: 2891 Gradiente: [0.08654310229311098,-1.4931020760734908] Loss: 24.946493666634517\n",
      "Iteracion: 2892 Gradiente: [0.08649709332348342,-1.4923082971794368] Loss: 24.944257417703728\n",
      "Iteracion: 2893 Gradiente: [0.08645110881364246,-1.4915149402826073] Loss: 24.942023545858035\n",
      "Iteracion: 2894 Gradiente: [0.08640514875062025,-1.490722005158655] Loss: 24.939792048570677\n",
      "Iteracion: 2895 Gradiente: [0.0863592131214072,-1.4899294915833514] Loss: 24.937562923317536\n",
      "Iteracion: 2896 Gradiente: [0.08631330191288764,-1.489137399332596] Loss: 24.93533616757718\n",
      "Iteracion: 2897 Gradiente: [0.08626741511217044,-1.4883457281823942] Loss: 24.93311177883088\n",
      "Iteracion: 2898 Gradiente: [0.08622155270642035,-1.4875544779088687] Loss: 24.930889754562585\n",
      "Iteracion: 2899 Gradiente: [0.0861757146824156,-1.4867636482882824] Loss: 24.92867009225886\n",
      "Iteracion: 2900 Gradiente: [0.08612990102739294,-1.4859732390969924] Loss: 24.926452789409034\n",
      "Iteracion: 2901 Gradiente: [0.08608411172826891,-1.4851832501114919] Loss: 24.92423784350502\n",
      "Iteracion: 2902 Gradiente: [0.08603834677213532,-1.4843936811083858] Loss: 24.92202525204148\n",
      "Iteracion: 2903 Gradiente: [0.08599260614603092,-1.4836045318643982] Loss: 24.91981501251565\n",
      "Iteracion: 2904 Gradiente: [0.08594688983705415,-1.4828158021563704] Loss: 24.91760712242748\n",
      "Iteracion: 2905 Gradiente: [0.08590119783232145,-1.482027491761263] Loss: 24.915401579279585\n",
      "Iteracion: 2906 Gradiente: [0.08585553011885547,-1.4812396004561592] Loss: 24.913198380577196\n",
      "Iteracion: 2907 Gradiente: [0.0858098866837679,-1.4804521280182557] Loss: 24.910997523828247\n",
      "Iteracion: 2908 Gradiente: [0.08576426751400655,-1.4796650742248787] Loss: 24.908799006543234\n",
      "Iteracion: 2909 Gradiente: [0.0857186725969181,-1.4788784388534502] Loss: 24.906602826235435\n",
      "Iteracion: 2910 Gradiente: [0.085673101919456,-1.4780922216815329] Loss: 24.904408980420637\n",
      "Iteracion: 2911 Gradiente: [0.08562755546879448,-1.4773064224867947] Loss: 24.902217466617337\n",
      "Iteracion: 2912 Gradiente: [0.08558203323195622,-1.4765210410470317] Loss: 24.900028282346668\n",
      "Iteracion: 2913 Gradiente: [0.08553653519620638,-1.4757360771401455] Loss: 24.89784142513238\n",
      "Iteracion: 2914 Gradiente: [0.08549106134856857,-1.4749515305441696] Loss: 24.895656892500842\n",
      "Iteracion: 2915 Gradiente: [0.08544561167624257,-1.474167401037243] Loss: 24.893474681981093\n",
      "Iteracion: 2916 Gradiente: [0.08540018616625673,-1.4733836883976374] Loss: 24.891294791104784\n",
      "Iteracion: 2917 Gradiente: [0.08535478480592738,-1.4726003924037232] Loss: 24.88911721740615\n",
      "Iteracion: 2918 Gradiente: [0.0853094075824392,-1.4718175128339974] Loss: 24.886941958422117\n",
      "Iteracion: 2919 Gradiente: [0.08526405448290669,-1.4710350494670792] Loss: 24.884769011692182\n",
      "Iteracion: 2920 Gradiente: [0.08521872549441506,-1.4702530020817088] Loss: 24.882598374758455\n",
      "Iteracion: 2921 Gradiente: [0.08517342060410157,-1.4694713704567397] Loss: 24.88043004516569\n",
      "Iteracion: 2922 Gradiente: [0.08512813979923521,-1.4686901543711341] Loss: 24.878264020461213\n",
      "Iteracion: 2923 Gradiente: [0.08508288306719294,-1.4679093536039707] Loss: 24.876100298195002\n",
      "Iteracion: 2924 Gradiente: [0.08503765039493298,-1.467128967934465] Loss: 24.873938875919574\n",
      "Iteracion: 2925 Gradiente: [0.08499244176978303,-1.4663489971419348] Loss: 24.87177975119013\n",
      "Iteracion: 2926 Gradiente: [0.08494725717888799,-1.4655694410058213] Loss: 24.869622921564417\n",
      "Iteracion: 2927 Gradiente: [0.08490209660956036,-1.4647902993056745] Loss: 24.86746838460276\n",
      "Iteracion: 2928 Gradiente: [0.08485696004896491,-1.464011571821172] Loss: 24.86531613786814\n",
      "Iteracion: 2929 Gradiente: [0.084811847484346,-1.4632332583321033] Loss: 24.863166178926072\n",
      "Iteracion: 2930 Gradiente: [0.0847667589029508,-1.4624553586183746] Loss: 24.861018505344703\n",
      "Iteracion: 2931 Gradiente: [0.08472169429202836,-1.4616778724600124] Loss: 24.858873114694727\n",
      "Iteracion: 2932 Gradiente: [0.08467665363876999,-1.4609007996371595] Loss: 24.856730004549437\n",
      "Iteracion: 2933 Gradiente: [0.08463163693058107,-1.460124139930069] Loss: 24.85458917248472\n",
      "Iteracion: 2934 Gradiente: [0.08458664415467751,-1.4593478931191153] Loss: 24.852450616079004\n",
      "Iteracion: 2935 Gradiente: [0.0845416752981824,-1.4585720589848001] Loss: 24.850314332913317\n",
      "Iteracion: 2936 Gradiente: [0.0844967303486745,-1.4577966373077165] Loss: 24.848180320571263\n",
      "Iteracion: 2937 Gradiente: [0.08445180929320675,-1.4570216278686023] Loss: 24.84604857663899\n",
      "Iteracion: 2938 Gradiente: [0.08440691211919973,-1.4562470304482906] Loss: 24.843919098705236\n",
      "Iteracion: 2939 Gradiente: [0.08436203881376987,-1.4554728448277525] Loss: 24.841791884361307\n",
      "Iteracion: 2940 Gradiente: [0.0843171893645092,-1.4546990707880458] Loss: 24.839666931201027\n",
      "Iteracion: 2941 Gradiente: [0.08427236375853132,-1.4539257081103711] Loss: 24.83754423682082\n",
      "Iteracion: 2942 Gradiente: [0.0842275619832473,-1.453152756576035] Loss: 24.835423798819658\n",
      "Iteracion: 2943 Gradiente: [0.08418278402600474,-1.452380215966456] Loss: 24.833305614799038\n",
      "Iteracion: 2944 Gradiente: [0.08413802987399019,-1.4516080860631848] Loss: 24.83118968236305\n",
      "Iteracion: 2945 Gradiente: [0.0840932995147322,-1.4508363666478672] Loss: 24.829075999118277\n",
      "Iteracion: 2946 Gradiente: [0.08404859293547418,-1.450065057502279] Loss: 24.82696456267392\n",
      "Iteracion: 2947 Gradiente: [0.08400391012367266,-1.4492941584083048] Loss: 24.82485537064163\n",
      "Iteracion: 2948 Gradiente: [0.08395925106652934,-1.448523669147956] Loss: 24.822748420635676\n",
      "Iteracion: 2949 Gradiente: [0.0839146157515709,-1.447753589503345] Loss: 24.820643710272815\n",
      "Iteracion: 2950 Gradiente: [0.08387000416605114,-1.4469839192567144] Loss: 24.818541237172386\n",
      "Iteracion: 2951 Gradiente: [0.08382541629749009,-1.446214658190408] Loss: 24.81644099895619\n",
      "Iteracion: 2952 Gradiente: [0.08378085213308566,-1.4454458060869015] Loss: 24.814342993248598\n",
      "Iteracion: 2953 Gradiente: [0.08373631166059851,-1.4446773627287617] Loss: 24.8122472176765\n",
      "Iteracion: 2954 Gradiente: [0.08369179486709584,-1.443909327898703] Loss: 24.81015366986935\n",
      "Iteracion: 2955 Gradiente: [0.08364730174004649,-1.443141701379538] Loss: 24.80806234745902\n",
      "Iteracion: 2956 Gradiente: [0.08360283226684923,-1.4423744829541951] Loss: 24.80597324808\n",
      "Iteracion: 2957 Gradiente: [0.08355838643508377,-1.4416076724057116] Loss: 24.803886369369238\n",
      "Iteracion: 2958 Gradiente: [0.08351396423201815,-1.4408412695172563] Loss: 24.801801708966227\n",
      "Iteracion: 2959 Gradiente: [0.08346956564510037,-1.440075274072105] Loss: 24.799719264512927\n",
      "Iteracion: 2960 Gradiente: [0.08342519066198027,-1.4393096858536354] Loss: 24.79763903365386\n",
      "Iteracion: 2961 Gradiente: [0.08338083926990218,-1.4385445046453673] Loss: 24.79556101403601\n",
      "Iteracion: 2962 Gradiente: [0.08333651145638991,-1.4377797302309143] Loss: 24.793485203308865\n",
      "Iteracion: 2963 Gradiente: [0.08329220720898718,-1.4370153623940105] Loss: 24.791411599124434\n",
      "Iteracion: 2964 Gradiente: [0.0832479265149876,-1.4362514009185159] Loss: 24.789340199137204\n",
      "Iteracion: 2965 Gradiente: [0.08320366936196422,-1.4354878455883928] Loss: 24.78727100100416\n",
      "Iteracion: 2966 Gradiente: [0.0831594357374731,-1.4347246961877154] Loss: 24.785204002384784\n",
      "Iteracion: 2967 Gradiente: [0.08311522562878224,-1.4339619525006941] Loss: 24.783139200941022\n",
      "Iteracion: 2968 Gradiente: [0.0830710390236201,-1.4331996143116228] Loss: 24.78107659433734\n",
      "Iteracion: 2969 Gradiente: [0.0830268759093921,-1.4324376814049335] Loss: 24.77901618024066\n",
      "Iteracion: 2970 Gradiente: [0.08298273627379255,-1.4316761535651563] Loss: 24.7769579563204\n",
      "Iteracion: 2971 Gradiente: [0.08293862010398433,-1.430915030576965] Loss: 24.77490192024845\n",
      "Iteracion: 2972 Gradiente: [0.08289452738774704,-1.4301543122251135] Loss: 24.772848069699176\n",
      "Iteracion: 2973 Gradiente: [0.08285045811251356,-1.4293939982944903] Loss: 24.770796402349408\n",
      "Iteracion: 2974 Gradiente: [0.08280641226593938,-1.4286340885700832] Loss: 24.768746915878445\n",
      "Iteracion: 2975 Gradiente: [0.08276238983553033,-1.4278745828370105] Loss: 24.766699607968057\n",
      "Iteracion: 2976 Gradiente: [0.08271839080863495,-1.4271154808805073] Loss: 24.764654476302496\n",
      "Iteracion: 2977 Gradiente: [0.08267441517317688,-1.426356782485893] Loss: 24.762611518568473\n",
      "Iteracion: 2978 Gradiente: [0.08263046291636632,-1.4255984874386411] Loss: 24.760570732455108\n",
      "Iteracion: 2979 Gradiente: [0.0825865340258692,-1.4248405955243155] Loss: 24.758532115654035\n",
      "Iteracion: 2980 Gradiente: [0.08254262848944904,-1.4240831065285888] Loss: 24.756495665859322\n",
      "Iteracion: 2981 Gradiente: [0.08249874629454723,-1.4233260202372633] Loss: 24.754461380767477\n",
      "Iteracion: 2982 Gradiente: [0.08245488742866296,-1.4225693364362546] Loss: 24.752429258077463\n",
      "Iteracion: 2983 Gradiente: [0.08241105187955403,-1.4218130549115775] Loss: 24.750399295490727\n",
      "Iteracion: 2984 Gradiente: [0.08236723963479449,-1.4210571754493715] Loss: 24.74837149071108\n",
      "Iteracion: 2985 Gradiente: [0.08232345068195362,-1.4203016978358896] Loss: 24.746345841444846\n",
      "Iteracion: 2986 Gradiente: [0.08227968500873999,-1.4195466218574921] Loss: 24.74432234540075\n",
      "Iteracion: 2987 Gradiente: [0.08223594260258646,-1.418791947300669] Loss: 24.74230100028998\n",
      "Iteracion: 2988 Gradiente: [0.08219222345134179,-1.4180376739519986] Loss: 24.740281803826132\n",
      "Iteracion: 2989 Gradiente: [0.08214852754252698,-1.4172838015981928] Loss: 24.73826475372521\n",
      "Iteracion: 2990 Gradiente: [0.08210485486361278,-1.41653033002608] Loss: 24.736249847705707\n",
      "Iteracion: 2991 Gradiente: [0.08206120540264882,-1.415777259022572] Loss: 24.734237083488512\n",
      "Iteracion: 2992 Gradiente: [0.08201757914705847,-1.415024588374725] Loss: 24.732226458796934\n",
      "Iteracion: 2993 Gradiente: [0.08197397608444987,-1.4142723178697016] Loss: 24.730217971356662\n",
      "Iteracion: 2994 Gradiente: [0.08193039620254959,-1.41352044729477] Loss: 24.728211618895884\n",
      "Iteracion: 2995 Gradiente: [0.08188683948914009,-1.4127689764373104] Loss: 24.726207399145164\n",
      "Iteracion: 2996 Gradiente: [0.08184330593178307,-1.4120179050848274] Loss: 24.724205309837483\n",
      "Iteracion: 2997 Gradiente: [0.0817997955182013,-1.4112672330249283] Loss: 24.72220534870819\n",
      "Iteracion: 2998 Gradiente: [0.08175630823601145,-1.4105169600453429] Loss: 24.72020751349509\n",
      "Iteracion: 2999 Gradiente: [0.08171284407302058,-1.4097670859339013] Loss: 24.718211801938374\n",
      "Iteracion: 3000 Gradiente: [0.08166940301698371,-1.4090176104785512] Loss: 24.716218211780685\n",
      "Iteracion: 3001 Gradiente: [0.08162598505553452,-1.4082685334673577] Loss: 24.71422674076696\n",
      "Iteracion: 3002 Gradiente: [0.08158259017641285,-1.4075198546884973] Loss: 24.71223738664463\n",
      "Iteracion: 3003 Gradiente: [0.08153921836730263,-1.4067715739302573] Loss: 24.710250147163475\n",
      "Iteracion: 3004 Gradiente: [0.08149586961601092,-1.4060236909810346] Loss: 24.708265020075658\n",
      "Iteracion: 3005 Gradiente: [0.08145254391020179,-1.4052762056293464] Loss: 24.706282003135804\n",
      "Iteracion: 3006 Gradiente: [0.0814092412377344,-1.404529117663812] Loss: 24.704301094100806\n",
      "Iteracion: 3007 Gradiente: [0.08136596158625385,-1.4037824268731736] Loss: 24.70232229073004\n",
      "Iteracion: 3008 Gradiente: [0.08132270494359943,-1.403036133046276] Loss: 24.700345590785222\n",
      "Iteracion: 3009 Gradiente: [0.08127947129742191,-1.4022902359720897] Loss: 24.698370992030473\n",
      "Iteracion: 3010 Gradiente: [0.08123626063568282,-1.4015447354396775] Loss: 24.696398492232234\n",
      "Iteracion: 3011 Gradiente: [0.08119307294594856,-1.4007996312382365] Loss: 24.6944280891594\n",
      "Iteracion: 3012 Gradiente: [0.0811499082162129,-1.400054923157055] Loss: 24.69245978058316\n",
      "Iteracion: 3013 Gradiente: [0.08110676643409344,-1.399310610985552] Loss: 24.690493564277133\n",
      "Iteracion: 3014 Gradiente: [0.08106364758750714,-1.3985666945132447] Loss: 24.688529438017284\n",
      "Iteracion: 3015 Gradiente: [0.08102055166424502,-1.3978231735297668] Loss: 24.686567399581918\n",
      "Iteracion: 3016 Gradiente: [0.08097747865207054,-1.3970800478248662] Loss: 24.684607446751727\n",
      "Iteracion: 3017 Gradiente: [0.08093442853885525,-1.3963373171884015] Loss: 24.682649577309782\n",
      "Iteracion: 3018 Gradiente: [0.08089140131237116,-1.395594981410341] Loss: 24.680693789041435\n",
      "Iteracion: 3019 Gradiente: [0.08084839696050684,-1.3948530402807644] Loss: 24.6787400797345\n",
      "Iteracion: 3020 Gradiente: [0.080805415471022,-1.3941114935898695] Loss: 24.676788447179053\n",
      "Iteracion: 3021 Gradiente: [0.08076245683179005,-1.3933703411279583] Loss: 24.67483888916755\n",
      "Iteracion: 3022 Gradiente: [0.08071952103085114,-1.3926295826854367] Loss: 24.672891403494802\n",
      "Iteracion: 3023 Gradiente: [0.08067660805578308,-1.391889218052848] Loss: 24.670945987957957\n",
      "Iteracion: 3024 Gradiente: [0.08063371789448866,-1.391149247020827] Loss: 24.6690026403565\n",
      "Iteracion: 3025 Gradiente: [0.08059085053502978,-1.3904096693801136] Loss: 24.667061358492262\n",
      "Iteracion: 3026 Gradiente: [0.08054800596523345,-1.3896704849215722] Loss: 24.665122140169434\n",
      "Iteracion: 3027 Gradiente: [0.08050518417292854,-1.3889316934361773] Loss: 24.663184983194462\n",
      "Iteracion: 3028 Gradiente: [0.08046238514585108,-1.3881932947150233] Loss: 24.66124988537623\n",
      "Iteracion: 3029 Gradiente: [0.08041960887214733,-1.3874552885492866] Loss: 24.659316844525847\n",
      "Iteracion: 3030 Gradiente: [0.08037685533962152,-1.3867176747302818] Loss: 24.657385858456834\n",
      "Iteracion: 3031 Gradiente: [0.08033412453614612,-1.3859804530494269] Loss: 24.655456924985007\n",
      "Iteracion: 3032 Gradiente: [0.08029141644969778,-1.385243623298244] Loss: 24.653530041928477\n",
      "Iteracion: 3033 Gradiente: [0.08024873106825225,-1.3845071852683708] Loss: 24.65160520710773\n",
      "Iteracion: 3034 Gradiente: [0.08020606837951902,-1.383771138751567] Loss: 24.649682418345485\n",
      "Iteracion: 3035 Gradiente: [0.08016342837170877,-1.3830354835396765] Loss: 24.64776167346686\n",
      "Iteracion: 3036 Gradiente: [0.08012081103254805,-1.3823002194246825] Loss: 24.64584297029922\n",
      "Iteracion: 3037 Gradiente: [0.0800782163500808,-1.3815653461986608] Loss: 24.643926306672316\n",
      "Iteracion: 3038 Gradiente: [0.08003564431229507,-1.3808308636538011] Loss: 24.64201168041813\n",
      "Iteracion: 3039 Gradiente: [0.07999309490715707,-1.3800967715824022] Loss: 24.640099089370974\n",
      "Iteracion: 3040 Gradiente: [0.07995056812256678,-1.379363069776885] Loss: 24.638188531367483\n",
      "Iteracion: 3041 Gradiente: [0.0799080639464241,-1.378629758029772] Loss: 24.636280004246558\n",
      "Iteracion: 3042 Gradiente: [0.07986558236691034,-1.3778968361336876] Loss: 24.63437350584945\n",
      "Iteracion: 3043 Gradiente: [0.07982312337195291,-1.3771643038813763] Loss: 24.6324690340196\n",
      "Iteracion: 3044 Gradiente: [0.07978068694938448,-1.3764321610657033] Loss: 24.63056658660288\n",
      "Iteracion: 3045 Gradiente: [0.07973827308732856,-1.3757004074796226] Loss: 24.628666161447327\n",
      "Iteracion: 3046 Gradiente: [0.07969588177382433,-1.374969042916208] Loss: 24.62676775640337\n",
      "Iteracion: 3047 Gradiente: [0.0796535129967367,-1.37423806716865] Loss: 24.62487136932364\n",
      "Iteracion: 3048 Gradiente: [0.07961116674429812,-1.373507480030231] Loss: 24.622976998063113\n",
      "Iteracion: 3049 Gradiente: [0.07956884300442653,-1.3727772812943588] Loss: 24.62108464047897\n",
      "Iteracion: 3050 Gradiente: [0.07952654176510236,-1.3720474707545522] Loss: 24.619194294430752\n",
      "Iteracion: 3051 Gradiente: [0.0794842630145458,-1.3713180482044218] Loss: 24.617305957780218\n",
      "Iteracion: 3052 Gradiente: [0.07944200674053642,-1.3705890134377166] Loss: 24.615419628391447\n",
      "Iteracion: 3053 Gradiente: [0.07939977293132187,-1.3698603662482656] Loss: 24.613535304130735\n",
      "Iteracion: 3054 Gradiente: [0.07935756157486841,-1.3691321064300297] Loss: 24.61165298286669\n",
      "Iteracion: 3055 Gradiente: [0.07931537265920761,-1.3684042337770694] Loss: 24.609772662470167\n",
      "Iteracion: 3056 Gradiente: [0.0792732061724981,-1.3676767480835528] Loss: 24.607894340814294\n",
      "Iteracion: 3057 Gradiente: [0.07923106210287566,-1.3669496491437554] Loss: 24.60601801577443\n",
      "Iteracion: 3058 Gradiente: [0.07918894043826678,-1.3662229367520746] Loss: 24.60414368522822\n",
      "Iteracion: 3059 Gradiente: [0.0791468411668537,-1.3654966107030078] Loss: 24.60227134705559\n",
      "Iteracion: 3060 Gradiente: [0.07910476427663866,-1.3647706707911664] Loss: 24.600400999138635\n",
      "Iteracion: 3061 Gradiente: [0.0790627097558873,-1.3640451168112595] Loss: 24.598532639361817\n",
      "Iteracion: 3062 Gradiente: [0.07902067759254597,-1.3633199485581222] Loss: 24.59666626561174\n",
      "Iteracion: 3063 Gradiente: [0.07897866777483956,-1.3625951658266855] Loss: 24.59480187577732\n",
      "Iteracion: 3064 Gradiente: [0.07893668029086408,-1.3618707684119937] Loss: 24.59293946774969\n",
      "Iteracion: 3065 Gradiente: [0.0788947151286654,-1.3611467561092065] Loss: 24.591079039422237\n",
      "Iteracion: 3066 Gradiente: [0.07885277227641913,-1.3604231287135826] Loss: 24.589220588690562\n",
      "Iteracion: 3067 Gradiente: [0.07881085172240129,-1.3596998860204883] Loss: 24.587364113452555\n",
      "Iteracion: 3068 Gradiente: [0.07876895345444268,-1.3589770278254192] Loss: 24.585509611608277\n",
      "Iteracion: 3069 Gradiente: [0.07872707746103061,-1.358254553923948] Loss: 24.583657081060064\n",
      "Iteracion: 3070 Gradiente: [0.07868522373020995,-1.3575324641117754] Loss: 24.581806519712487\n",
      "Iteracion: 3071 Gradiente: [0.07864339225005969,-1.356810758184716] Loss: 24.579957925472307\n",
      "Iteracion: 3072 Gradiente: [0.07860158300876302,-1.3560894359386821] Loss: 24.578111296248544\n",
      "Iteracion: 3073 Gradiente: [0.07855979599454485,-1.355368497169695] Loss: 24.576266629952407\n",
      "Iteracion: 3074 Gradiente: [0.07851803119563196,-1.354647941673884] Loss: 24.574423924497374\n",
      "Iteracion: 3075 Gradiente: [0.07847628860017626,-1.3539277692474914] Loss: 24.572583177799093\n",
      "Iteracion: 3076 Gradiente: [0.07843456819636571,-1.3532079796868655] Loss: 24.570744387775445\n",
      "Iteracion: 3077 Gradiente: [0.07839286997237024,-1.352488572788467] Loss: 24.568907552346545\n",
      "Iteracion: 3078 Gradiente: [0.0783511939164356,-1.351769548348856] Loss: 24.56707266943467\n",
      "Iteracion: 3079 Gradiente: [0.07830954001679137,-1.351050906164708] Loss: 24.56523973696437\n",
      "Iteracion: 3080 Gradiente: [0.07826790826161036,-1.3503326460328045] Loss: 24.563408752862337\n",
      "Iteracion: 3081 Gradiente: [0.07822629863913638,-1.3496147677500347] Loss: 24.56157971505754\n",
      "Iteracion: 3082 Gradiente: [0.07818471113763982,-1.348897271113393] Loss: 24.55975262148104\n",
      "Iteracion: 3083 Gradiente: [0.07814314574529249,-1.3481801559199904] Loss: 24.557927470066236\n",
      "Iteracion: 3084 Gradiente: [0.07810160245045855,-1.34746342196703] Loss: 24.556104258748594\n",
      "Iteracion: 3085 Gradiente: [0.07806008124127573,-1.346747069051843] Loss: 24.554282985465857\n",
      "Iteracion: 3086 Gradiente: [0.07801858210606744,-1.346031096971852] Loss: 24.55246364815794\n",
      "Iteracion: 3087 Gradiente: [0.07797710503306518,-1.3453155055245958] Loss: 24.550646244766938\n",
      "Iteracion: 3088 Gradiente: [0.07793565001057155,-1.344600294507717] Loss: 24.548830773237135\n",
      "Iteracion: 3089 Gradiente: [0.07789421702684744,-1.3438854637189666] Loss: 24.547017231514992\n",
      "Iteracion: 3090 Gradiente: [0.07785280607014329,-1.3431710129562078] Loss: 24.545205617549186\n",
      "Iteracion: 3091 Gradiente: [0.07781141712877779,-1.3424569420174028] Loss: 24.543395929290543\n",
      "Iteracion: 3092 Gradiente: [0.07777005019106203,-1.3417432507006264] Loss: 24.541588164692072\n",
      "Iteracion: 3093 Gradiente: [0.07772870524519912,-1.3410299388040658] Loss: 24.539782321708984\n",
      "Iteracion: 3094 Gradiente: [0.07768738227960247,-1.3403170061260032] Loss: 24.53797839829862\n",
      "Iteracion: 3095 Gradiente: [0.07764608128252727,-1.3396044524648378] Loss: 24.53617639242053\n",
      "Iteracion: 3096 Gradiente: [0.07760480224237369,-1.3388922776190673] Loss: 24.534376302036392\n",
      "Iteracion: 3097 Gradiente: [0.07756354514749073,-1.3381804813873002] Loss: 24.532578125110103\n",
      "Iteracion: 3098 Gradiente: [0.0775223099860303,-1.337469063568267] Loss: 24.53078185960771\n",
      "Iteracion: 3099 Gradiente: [0.07748109674654605,-1.3367580239607757] Loss: 24.528987503497365\n",
      "Iteracion: 3100 Gradiente: [0.07743990541725339,-1.336047362363767] Loss: 24.52719505474945\n",
      "Iteracion: 3101 Gradiente: [0.0773987359864994,-1.3353370785762801] Loss: 24.525404511336504\n",
      "Iteracion: 3102 Gradiente: [0.07735758844268711,-1.3346271723974552] Loss: 24.523615871233158\n",
      "Iteracion: 3103 Gradiente: [0.07731646277422707,-1.3339176436265425] Loss: 24.52182913241624\n",
      "Iteracion: 3104 Gradiente: [0.07727535896938396,-1.3332084920629066] Loss: 24.520044292864732\n",
      "Iteracion: 3105 Gradiente: [0.07723427701665647,-1.3324997175060034] Loss: 24.51826135055976\n",
      "Iteracion: 3106 Gradiente: [0.07719321690440589,-1.331791319755408] Loss: 24.51648030348459\n",
      "Iteracion: 3107 Gradiente: [0.07715217862087229,-1.331083298610808] Loss: 24.51470114962463\n",
      "Iteracion: 3108 Gradiente: [0.07711116215444633,-1.3303756538719853] Loss: 24.51292388696741\n",
      "Iteracion: 3109 Gradiente: [0.07707016749376312,-1.3296683853388205] Loss: 24.51114851350267\n",
      "Iteracion: 3110 Gradiente: [0.0770291946270845,-1.3289614928113174] Loss: 24.509375027222195\n",
      "Iteracion: 3111 Gradiente: [0.0769882435428542,-1.3282549760895797] Loss: 24.507603426119974\n",
      "Iteracion: 3112 Gradiente: [0.07694731422940038,-1.3275488349738211] Loss: 24.50583370819209\n",
      "Iteracion: 3113 Gradiente: [0.0769064066752378,-1.3268430692643551] Loss: 24.50406587143676\n",
      "Iteracion: 3114 Gradiente: [0.07686552086888507,-1.3261376787615968] Loss: 24.502299913854365\n",
      "Iteracion: 3115 Gradiente: [0.07682465679856611,-1.3254326632660882] Loss: 24.500535833447355\n",
      "Iteracion: 3116 Gradiente: [0.07678381445293117,-1.3247280225784526] Loss: 24.49877362822034\n",
      "Iteracion: 3117 Gradiente: [0.07674299382032833,-1.324023756499436] Loss: 24.497013296180068\n",
      "Iteracion: 3118 Gradiente: [0.07670219488915772,-1.323319864829888] Loss: 24.495254835335363\n",
      "Iteracion: 3119 Gradiente: [0.07666141764797203,-1.3226163473707577] Loss: 24.493498243697168\n",
      "Iteracion: 3120 Gradiente: [0.07662066208535047,-1.321913203923093] Loss: 24.49174351927858\n",
      "Iteracion: 3121 Gradiente: [0.0765799281895833,-1.3212104342880715] Loss: 24.489990660094783\n",
      "Iteracion: 3122 Gradiente: [0.07653921594904982,-1.3205080382669687] Loss: 24.488239664163036\n",
      "Iteracion: 3123 Gradiente: [0.07649852535256609,-1.3198060156611409] Loss: 24.486490529502813\n",
      "Iteracion: 3124 Gradiente: [0.07645785638843752,-1.3191043662720796] Loss: 24.484743254135548\n",
      "Iteracion: 3125 Gradiente: [0.07641720904499512,-1.3184030899013797] Loss: 24.482997836084873\n",
      "Iteracion: 3126 Gradiente: [0.07637658331106346,-1.3177021863507188] Loss: 24.48125427337654\n",
      "Iteracion: 3127 Gradiente: [0.07633597917493755,-1.3170016554219026] Loss: 24.47951256403832\n",
      "Iteracion: 3128 Gradiente: [0.07629539662541542,-1.3163014969168196] Loss: 24.47777270610014\n",
      "Iteracion: 3129 Gradiente: [0.0762548356507447,-1.3156017106374933] Loss: 24.476034697593978\n",
      "Iteracion: 3130 Gradiente: [0.07621429623941177,-1.31490229638604] Loss: 24.474298536553942\n",
      "Iteracion: 3131 Gradiente: [0.076173778380173,-1.3142032539646658] Loss: 24.472564221016203\n",
      "Iteracion: 3132 Gradiente: [0.07613328206150717,-1.3135045831756968] Loss: 24.470831749019055\n",
      "Iteracion: 3133 Gradiente: [0.07609280727184001,-1.312806283821572] Loss: 24.469101118602836\n",
      "Iteracion: 3134 Gradiente: [0.07605235399982273,-1.3121083557048148] Loss: 24.46737232780998\n",
      "Iteracion: 3135 Gradiente: [0.0760119222340298,-1.3114107986280659] Loss: 24.465645374685018\n",
      "Iteracion: 3136 Gradiente: [0.07597151196297793,-1.3107136123940728] Loss: 24.46392025727455\n",
      "Iteracion: 3137 Gradiente: [0.07593112317532208,-1.3100167968056786] Loss: 24.462196973627247\n",
      "Iteracion: 3138 Gradiente: [0.07589075585957707,-1.3093203516658407] Loss: 24.460475521793846\n",
      "Iteracion: 3139 Gradiente: [0.07585041000436187,-1.308624276777614] Loss: 24.458755899827178\n",
      "Iteracion: 3140 Gradiente: [0.07581008559805014,-1.3079285719441744] Loss: 24.457038105782143\n",
      "Iteracion: 3141 Gradiente: [0.0757697826296436,-1.3072332369687685] Loss: 24.45532213771568\n",
      "Iteracion: 3142 Gradiente: [0.07572950108741926,-1.3065382716547844] Loss: 24.453607993686834\n",
      "Iteracion: 3143 Gradiente: [0.07568924096014579,-1.3058436758056893] Loss: 24.451895671756656\n",
      "Iteracion: 3144 Gradiente: [0.07564900223637684,-1.3051494492250701] Loss: 24.450185169988337\n",
      "Iteracion: 3145 Gradiente: [0.07560878490475413,-1.3044555917166096] Loss: 24.448476486447067\n",
      "Iteracion: 3146 Gradiente: [0.07556858895391372,-1.3037621030840971] Loss: 24.446769619200094\n",
      "Iteracion: 3147 Gradiente: [0.07552841437245851,-1.3030689831314284] Loss: 24.445064566316756\n",
      "Iteracion: 3148 Gradiente: [0.07548826114907001,-1.3023762316626013] Loss: 24.443361325868405\n",
      "Iteracion: 3149 Gradiente: [0.07544812927233788,-1.3016838484817204] Loss: 24.441659895928478\n",
      "Iteracion: 3150 Gradiente: [0.07540801873088962,-1.3009918333929944] Loss: 24.439960274572435\n",
      "Iteracion: 3151 Gradiente: [0.07536792951359152,-1.3003001862007215] Loss: 24.438262459877794\n",
      "Iteracion: 3152 Gradiente: [0.07532786160883423,-1.2996089067093324] Loss: 24.436566449924097\n",
      "Iteracion: 3153 Gradiente: [0.07528781500549257,-1.2989179947233356] Loss: 24.434872242792967\n",
      "Iteracion: 3154 Gradiente: [0.07524778969208133,-1.2982274500473594] Loss: 24.433179836568012\n",
      "Iteracion: 3155 Gradiente: [0.07520778565735403,-1.2975372724861323] Loss: 24.43148922933494\n",
      "Iteracion: 3156 Gradiente: [0.07516780288996945,-1.2968474618444823] Loss: 24.42980041918146\n",
      "Iteracion: 3157 Gradiente: [0.07512784137864324,-1.2961580179273433] Loss: 24.428113404197276\n",
      "Iteracion: 3158 Gradiente: [0.07508790111218673,-1.2954689405397466] Loss: 24.426428182474194\n",
      "Iteracion: 3159 Gradiente: [0.07504798207916205,-1.294780229486843] Loss: 24.42474475210599\n",
      "Iteracion: 3160 Gradiente: [0.07500808426827632,-1.2940918845738778] Loss: 24.42306311118853\n",
      "Iteracion: 3161 Gradiente: [0.07496820766840813,-1.293403905606189] Loss: 24.42138325781964\n",
      "Iteracion: 3162 Gradiente: [0.07492835226806942,-1.2927162923892435] Loss: 24.419705190099194\n",
      "Iteracion: 3163 Gradiente: [0.07488851805603361,-1.2920290447285927] Loss: 24.418028906129088\n",
      "Iteracion: 3164 Gradiente: [0.07484870502125508,-1.2913421624298838] Loss: 24.416354404013237\n",
      "Iteracion: 3165 Gradiente: [0.07480891315217188,-1.2906556452988964] Loss: 24.414681681857573\n",
      "Iteracion: 3166 Gradiente: [0.07476914243768533,-1.2899694931414858] Loss: 24.413010737770023\n",
      "Iteracion: 3167 Gradiente: [0.07472939286652339,-1.2892837057636262] Loss: 24.411341569860546\n",
      "Iteracion: 3168 Gradiente: [0.07468966442755137,-1.2885982829713802] Loss: 24.409674176241094\n",
      "Iteracion: 3169 Gradiente: [0.07464995710937786,-1.2879132245709313] Loss: 24.40800855502565\n",
      "Iteracion: 3170 Gradiente: [0.07461027090075352,-1.287228530368562] Loss: 24.40634470433018\n",
      "Iteracion: 3171 Gradiente: [0.07457060579059961,-1.286544200170643] Loss: 24.404682622272656\n",
      "Iteracion: 3172 Gradiente: [0.07453096176767342,-1.2858602337836602] Loss: 24.403022306973035\n",
      "Iteracion: 3173 Gradiente: [0.07449133882069721,-1.285176631014205] Loss: 24.401363756553316\n",
      "Iteracion: 3174 Gradiente: [0.07445173693851738,-1.2844933916689631] Loss: 24.399706969137476\n",
      "Iteracion: 3175 Gradiente: [0.0744121561098325,-1.2838105155547346] Loss: 24.398051942851474\n",
      "Iteracion: 3176 Gradiente: [0.07437259632354198,-1.2831280024784089] Loss: 24.396398675823228\n",
      "Iteracion: 3177 Gradiente: [0.0743330575685197,-1.2824458522469793] Loss: 24.39474716618274\n",
      "Iteracion: 3178 Gradiente: [0.07429353983346232,-1.2817640646675543] Loss: 24.39309741206191\n",
      "Iteracion: 3179 Gradiente: [0.07425404310725602,-1.2810826395473323] Loss: 24.39144941159466\n",
      "Iteracion: 3180 Gradiente: [0.07421456737870073,-1.2804015766936236] Loss: 24.389803162916905\n",
      "Iteracion: 3181 Gradiente: [0.07417511263667981,-1.2797208759138294] Loss: 24.38815866416654\n",
      "Iteracion: 3182 Gradiente: [0.07413567886999223,-1.2790405370154667] Loss: 24.3865159134834\n",
      "Iteracion: 3183 Gradiente: [0.07409626606754026,-1.2783605598061423] Loss: 24.384874909009337\n",
      "Iteracion: 3184 Gradiente: [0.07405687421819968,-1.2776809440935701] Loss: 24.383235648888178\n",
      "Iteracion: 3185 Gradiente: [0.07401750331072492,-1.2770016896855754] Loss: 24.381598131265726\n",
      "Iteracion: 3186 Gradiente: [0.07397815333399839,-1.2763227963900752] Loss: 24.37996235428971\n",
      "Iteracion: 3187 Gradiente: [0.07393882427683897,-1.2756442640150951] Loss: 24.37832831610985\n",
      "Iteracion: 3188 Gradiente: [0.07389951612837725,-1.274966092368743] Loss: 24.376696014877886\n",
      "Iteracion: 3189 Gradiente: [0.07386022887722656,-1.2742882812592602] Loss: 24.375065448747474\n",
      "Iteracion: 3190 Gradiente: [0.07382096251243221,-1.273610830494966] Loss: 24.373436615874198\n",
      "Iteracion: 3191 Gradiente: [0.07378171702292301,-1.2729337398842886] Loss: 24.37180951441568\n",
      "Iteracion: 3192 Gradiente: [0.07374249239738902,-1.272257009235769] Loss: 24.370184142531436\n",
      "Iteracion: 3193 Gradiente: [0.07370328862497881,-1.271580638358029] Loss: 24.36856049838299\n",
      "Iteracion: 3194 Gradiente: [0.07366410569448002,-1.2709046270598103] Loss: 24.366938580133798\n",
      "Iteracion: 3195 Gradiente: [0.07362494359473146,-1.2702289751499516] Loss: 24.36531838594923\n",
      "Iteracion: 3196 Gradiente: [0.07358580231487413,-1.2695536824373812] Loss: 24.36369991399668\n",
      "Iteracion: 3197 Gradiente: [0.07354668184359998,-1.268878748731152] Loss: 24.362083162445423\n",
      "Iteracion: 3198 Gradiente: [0.07350758217002161,-1.2682041738403946] Loss: 24.360468129466724\n",
      "Iteracion: 3199 Gradiente: [0.0734685032830375,-1.2675299575743557] Loss: 24.358854813233773\n",
      "Iteracion: 3200 Gradiente: [0.07342944517160391,-1.2668560997423763] Loss: 24.357243211921727\n",
      "Iteracion: 3201 Gradiente: [0.07339040782460794,-1.266182600153908] Loss: 24.355633323707625\n",
      "Iteracion: 3202 Gradiente: [0.07335139123119253,-1.2655094586184858] Loss: 24.354025146770503\n",
      "Iteracion: 3203 Gradiente: [0.07331239538011118,-1.264836674945771] Loss: 24.352418679291308\n",
      "Iteracion: 3204 Gradiente: [0.07327342026035713,-1.2641642489455125] Loss: 24.350813919452914\n",
      "Iteracion: 3205 Gradiente: [0.0732344658609918,-1.2634921804275523] Loss: 24.34921086544014\n",
      "Iteracion: 3206 Gradiente: [0.073195532171106,-1.262820469201839] Loss: 24.34760951543974\n",
      "Iteracion: 3207 Gradiente: [0.07315661917949304,-1.262149115078436] Loss: 24.34600986764037\n",
      "Iteracion: 3208 Gradiente: [0.07311772687522287,-1.2614781178674916] Loss: 24.344411920232627\n",
      "Iteracion: 3209 Gradiente: [0.07307885524733516,-1.2608074773792581] Loss: 24.34281567140904\n",
      "Iteracion: 3210 Gradiente: [0.07304000428468574,-1.2601371934241001] Loss: 24.341221119364047\n",
      "Iteracion: 3211 Gradiente: [0.07300117397644498,-1.2594672658124635] Loss: 24.339628262293996\n",
      "Iteracion: 3212 Gradiente: [0.07296236431156634,-1.2587976943549104] Loss: 24.33803709839718\n",
      "Iteracion: 3213 Gradiente: [0.07292357527910838,-1.2581284788620961] Loss: 24.33644762587379\n",
      "Iteracion: 3214 Gradiente: [0.07288480686810127,-1.2574596191447787] Loss: 24.334859842925923\n",
      "Iteracion: 3215 Gradiente: [0.07284605906762635,-1.2567911150138138] Loss: 24.33327374775762\n",
      "Iteracion: 3216 Gradiente: [0.07280733186668348,-1.256122966280164] Loss: 24.331689338574776\n",
      "Iteracion: 3217 Gradiente: [0.07276862525423079,-1.2554551727548948] Loss: 24.330106613585247\n",
      "Iteracion: 3218 Gradiente: [0.07272993921948417,-1.254787734249156] Loss: 24.328525570998746\n",
      "Iteracion: 3219 Gradiente: [0.07269127375127292,-1.2541206505742222] Loss: 24.326946209026946\n",
      "Iteracion: 3220 Gradiente: [0.07265262883893134,-1.25345392154144] Loss: 24.32536852588337\n",
      "Iteracion: 3221 Gradiente: [0.07261400447148673,-1.2527875469622711] Loss: 24.323792519783453\n",
      "Iteracion: 3222 Gradiente: [0.07257540063776939,-1.2521215266482935] Loss: 24.32221818894455\n",
      "Iteracion: 3223 Gradiente: [0.07253681732703399,-1.2514558604111585] Loss: 24.3206455315859\n",
      "Iteracion: 3224 Gradiente: [0.07249825452840734,-1.2507905480626247] Loss: 24.319074545928594\n",
      "Iteracion: 3225 Gradiente: [0.0724597122310693,-1.2501255894145495] Loss: 24.317505230195685\n",
      "Iteracion: 3226 Gradiente: [0.0724211904239553,-1.2494609842789057] Loss: 24.315937582612072\n",
      "Iteracion: 3227 Gradiente: [0.07238268909618645,-1.2487967324677527] Loss: 24.314371601404524\n",
      "Iteracion: 3228 Gradiente: [0.07234420823683081,-1.2481328337932573] Loss: 24.312807284801742\n",
      "Iteracion: 3229 Gradiente: [0.07230574783515067,-1.2474692880676703] Loss: 24.31124463103428\n",
      "Iteracion: 3230 Gradiente: [0.07226730788018566,-1.2468060951033595] Loss: 24.309683638334572\n",
      "Iteracion: 3231 Gradiente: [0.07222888836118292,-1.2461432547127795] Loss: 24.308124304936957\n",
      "Iteracion: 3232 Gradiente: [0.07219048926701722,-1.245480766708506] Loss: 24.306566629077626\n",
      "Iteracion: 3233 Gradiente: [0.07215211058699728,-1.2448186309031906] Loss: 24.30501060899464\n",
      "Iteracion: 3234 Gradiente: [0.07211375231033798,-1.2441568471095876] Loss: 24.303456242927943\n",
      "Iteracion: 3235 Gradiente: [0.07207541442612068,-1.2434954151405635] Loss: 24.301903529119354\n",
      "Iteracion: 3236 Gradiente: [0.07203709692348544,-1.242834334809077] Loss: 24.30035246581257\n",
      "Iteracion: 3237 Gradiente: [0.07199879979164052,-1.242173605928185] Loss: 24.29880305125314\n",
      "Iteracion: 3238 Gradiente: [0.07196052301961231,-1.2415132283110528] Loss: 24.297255283688468\n",
      "Iteracion: 3239 Gradiente: [0.07192226659677298,-1.2408532017709277] Loss: 24.295709161367835\n",
      "Iteracion: 3240 Gradiente: [0.07188403051218681,-1.2401935261211727] Loss: 24.294164682542405\n",
      "Iteracion: 3241 Gradiente: [0.07184581475509902,-1.239534201175241] Loss: 24.29262184546512\n",
      "Iteracion: 3242 Gradiente: [0.07180761931472451,-1.2388752267466847] Loss: 24.29108064839091\n",
      "Iteracion: 3243 Gradiente: [0.07176944418021473,-1.2382166026491632] Loss: 24.289541089576453\n",
      "Iteracion: 3244 Gradiente: [0.07173128934069647,-1.2375583286964316] Loss: 24.288003167280294\n",
      "Iteracion: 3245 Gradiente: [0.07169315478546044,-1.2369004047023393] Loss: 24.28646687976288\n",
      "Iteracion: 3246 Gradiente: [0.07165504050379828,-1.2362428304808333] Loss: 24.284932225286468\n",
      "Iteracion: 3247 Gradiente: [0.07161694648489364,-1.2355856058459656] Loss: 24.28339920211516\n",
      "Iteracion: 3248 Gradiente: [0.07157887271783446,-1.2349287306118921] Loss: 24.2818678085149\n",
      "Iteracion: 3249 Gradiente: [0.07154081919199579,-1.2342722045928545] Loss: 24.28033804275353\n",
      "Iteracion: 3250 Gradiente: [0.07150278589656978,-1.2336160276032004] Loss: 24.278809903100676\n",
      "Iteracion: 3251 Gradiente: [0.07146477282083576,-1.2329601994573751] Loss: 24.2772833878278\n",
      "Iteracion: 3252 Gradiente: [0.07142677995405317,-1.2323047199699197] Loss: 24.275758495208226\n",
      "Iteracion: 3253 Gradiente: [0.07138880728541418,-1.2316495889554808] Loss: 24.27423522351713\n",
      "Iteracion: 3254 Gradiente: [0.07135085480418961,-1.2309948062288] Loss: 24.27271357103148\n",
      "Iteracion: 3255 Gradiente: [0.0713129224996114,-1.230340371604719] Loss: 24.2711935360301\n",
      "Iteracion: 3256 Gradiente: [0.07127501036109152,-1.2296862848981673] Loss: 24.269675116793632\n",
      "Iteracion: 3257 Gradiente: [0.07123711837786004,-1.2290325459241842] Loss: 24.268158311604576\n",
      "Iteracion: 3258 Gradiente: [0.0711992465391802,-1.2283791544979057] Loss: 24.266643118747204\n",
      "Iteracion: 3259 Gradiente: [0.07116139483421761,-1.2277261104345711] Loss: 24.26512953650765\n",
      "Iteracion: 3260 Gradiente: [0.0711235632523587,-1.227073413549507] Loss: 24.263617563173877\n",
      "Iteracion: 3261 Gradiente: [0.07108575178300024,-1.2264210636581372] Loss: 24.262107197035636\n",
      "Iteracion: 3262 Gradiente: [0.07104796041539506,-1.2257690605759928] Loss: 24.26059843638451\n",
      "Iteracion: 3263 Gradiente: [0.07101018913898827,-1.2251174041186905] Loss: 24.259091279513925\n",
      "Iteracion: 3264 Gradiente: [0.07097243794271625,-1.2244660941019778] Loss: 24.257585724719053\n",
      "Iteracion: 3265 Gradiente: [0.0709347068162856,-1.2238151303416522] Loss: 24.25608177029697\n",
      "Iteracion: 3266 Gradiente: [0.07089699574884396,-1.2231645126536463] Loss: 24.254579414546477\n",
      "Iteracion: 3267 Gradiente: [0.07085930472975027,-1.2225142408539738] Loss: 24.25307865576823\n",
      "Iteracion: 3268 Gradiente: [0.07082163374827626,-1.2218643147587542] Loss: 24.251579492264707\n",
      "Iteracion: 3269 Gradiente: [0.07078398279388978,-1.2212147341841946] Loss: 24.250081922340133\n",
      "Iteracion: 3270 Gradiente: [0.07074635185596113,-1.2205654989466042] Loss: 24.24858594430056\n",
      "Iteracion: 3271 Gradiente: [0.07070874092382837,-1.2199166088623916] Loss: 24.247091556453885\n",
      "Iteracion: 3272 Gradiente: [0.070671149986768,-1.2192680637480693] Loss: 24.245598757109754\n",
      "Iteracion: 3273 Gradiente: [0.07063357903418818,-1.2186198634202374] Loss: 24.244107544579617\n",
      "Iteracion: 3274 Gradiente: [0.07059602805551511,-1.2179720076955933] Loss: 24.242617917176705\n",
      "Iteracion: 3275 Gradiente: [0.0705584970400262,-1.2173244963909429] Loss: 24.241129873216096\n",
      "Iteracion: 3276 Gradiente: [0.07052098597725755,-1.2166773293231732] Loss: 24.239643411014605\n",
      "Iteracion: 3277 Gradiente: [0.07048349485644678,-1.2160305063092844] Loss: 24.238158528890846\n",
      "Iteracion: 3278 Gradiente: [0.07044602366714324,-1.2153840271663598] Loss: 24.236675225165246\n",
      "Iteracion: 3279 Gradiente: [0.07040857239860543,-1.2147378917115954] Loss: 24.235193498160022\n",
      "Iteracion: 3280 Gradiente: [0.07037114104032298,-1.2140920997622715] Loss: 24.233713346199117\n",
      "Iteracion: 3281 Gradiente: [0.07033372958170976,-1.2134466511357693] Loss: 24.232234767608308\n",
      "Iteracion: 3282 Gradiente: [0.07029633801217017,-1.2128015456495695] Loss: 24.23075776071513\n",
      "Iteracion: 3283 Gradiente: [0.07025896632117679,-1.2121567831212439] Loss: 24.229282323848885\n",
      "Iteracion: 3284 Gradiente: [0.07022161449810843,-1.2115123633684708] Loss: 24.227808455340714\n",
      "Iteracion: 3285 Gradiente: [0.07018428253248696,-1.210868286209015] Loss: 24.22633615352346\n",
      "Iteracion: 3286 Gradiente: [0.07014697041349886,-1.2102245514607568] Loss: 24.224865416731753\n",
      "Iteracion: 3287 Gradiente: [0.07010967813088959,-1.2095811589416436] Loss: 24.223396243302002\n",
      "Iteracion: 3288 Gradiente: [0.07007240567404173,-1.208938108469741] Loss: 24.221928631572435\n",
      "Iteracion: 3289 Gradiente: [0.07003515303226966,-1.2082953998632138] Loss: 24.22046257988294\n",
      "Iteracion: 3290 Gradiente: [0.06999792019520328,-1.207653032940306] Loss: 24.218998086575244\n",
      "Iteracion: 3291 Gradiente: [0.06996070715224884,-1.2070110075193716] Loss: 24.217535149992845\n",
      "Iteracion: 3292 Gradiente: [0.06992351389297463,-1.2063693234188533] Loss: 24.21607376848095\n",
      "Iteracion: 3293 Gradiente: [0.06988634040667989,-1.205727980457304] Loss: 24.21461394038655\n",
      "Iteracion: 3294 Gradiente: [0.06984918668295184,-1.20508697845336] Loss: 24.21315566405841\n",
      "Iteracion: 3295 Gradiente: [0.06981205271133603,-1.2044463172257522] Loss: 24.211698937847043\n",
      "Iteracion: 3296 Gradiente: [0.06977493848123496,-1.2038059965933217] Loss: 24.210243760104667\n",
      "Iteracion: 3297 Gradiente: [0.06973784398222355,-1.203166016374992] Loss: 24.208790129185342\n",
      "Iteracion: 3298 Gradiente: [0.06970076920374121,-1.2025263763897938] Loss: 24.207338043444807\n",
      "Iteracion: 3299 Gradiente: [0.06966371413547184,-1.2018870764568372] Loss: 24.20588750124056\n",
      "Iteracion: 3300 Gradiente: [0.06962667876665213,-1.2012481163953586] Loss: 24.204438500931875\n",
      "Iteracion: 3301 Gradiente: [0.06958966308699246,-1.200609496024659] Loss: 24.202991040879734\n",
      "Iteracion: 3302 Gradiente: [0.06955266708611609,-1.1999712151641455] Loss: 24.20154511944687\n",
      "Iteracion: 3303 Gradiente: [0.06951569075337906,-1.1993332736333322] Loss: 24.200100734997783\n",
      "Iteracion: 3304 Gradiente: [0.06947873407847946,-1.1986956712518135] Loss: 24.1986578858987\n",
      "Iteracion: 3305 Gradiente: [0.06944179705078189,-1.1980584078392977] Loss: 24.19721657051754\n",
      "Iteracion: 3306 Gradiente: [0.06940487965994653,-1.1974214832155734] Loss: 24.19577678722403\n",
      "Iteracion: 3307 Gradiente: [0.06936798189556725,-1.1967848972005257] Loss: 24.194338534389562\n",
      "Iteracion: 3308 Gradiente: [0.06933110374715454,-1.1961486496141456] Loss: 24.19290181038732\n",
      "Iteracion: 3309 Gradiente: [0.06929424520425963,-1.1955127402765138] Loss: 24.191466613592173\n",
      "Iteracion: 3310 Gradiente: [0.06925740625654177,-1.1948771690078028] Loss: 24.19003294238074\n",
      "Iteracion: 3311 Gradiente: [0.06922058689349815,-1.1942419356282887] Loss: 24.188600795131343\n",
      "Iteracion: 3312 Gradiente: [0.0691837871048468,-1.1936070399583332] Loss: 24.187170170224043\n",
      "Iteracion: 3313 Gradiente: [0.06914700688008016,-1.1929724818184033] Loss: 24.18574106604063\n",
      "Iteracion: 3314 Gradiente: [0.06911024620874287,-1.1923382610290625] Loss: 24.184313480964615\n",
      "Iteracion: 3315 Gradiente: [0.06907350508056614,-1.1917043774109564] Loss: 24.182887413381202\n",
      "Iteracion: 3316 Gradiente: [0.06903678348503679,-1.1910708307848443] Loss: 24.181462861677343\n",
      "Iteracion: 3317 Gradiente: [0.0690000814118908,-1.1904376209715608] Loss: 24.18003982424169\n",
      "Iteracion: 3318 Gradiente: [0.06896339885069457,-1.1898047477920506] Loss: 24.17861829946461\n",
      "Iteracion: 3319 Gradiente: [0.06892673579100499,-1.189172211067351] Loss: 24.17719828573816\n",
      "Iteracion: 3320 Gradiente: [0.06889009222261108,-1.1885400106185868] Loss: 24.17577978145615\n",
      "Iteracion: 3321 Gradiente: [0.0688534681349817,-1.1879081462669896] Loss: 24.174362785014036\n",
      "Iteracion: 3322 Gradiente: [0.06881686351788782,-1.1872766178338745] Loss: 24.17294729480906\n",
      "Iteracion: 3323 Gradiente: [0.0687802783609205,-1.1866454251406595] Loss: 24.171533309240104\n",
      "Iteracion: 3324 Gradiente: [0.06874371265375127,-1.1860145680088543] Loss: 24.17012082670777\n",
      "Iteracion: 3325 Gradiente: [0.06870716638607823,-1.1853840462600609] Loss: 24.168709845614366\n",
      "Iteracion: 3326 Gradiente: [0.06867063954741846,-1.1847538597159906] Loss: 24.167300364363882\n",
      "Iteracion: 3327 Gradiente: [0.06863413212764631,-1.1841240081984248] Loss: 24.165892381362042\n",
      "Iteracion: 3328 Gradiente: [0.06859764411626278,-1.1834944915292631] Loss: 24.16448589501623\n",
      "Iteracion: 3329 Gradiente: [0.06856117550311183,-1.1828653095304797] Loss: 24.163080903735512\n",
      "Iteracion: 3330 Gradiente: [0.06852472627772954,-1.1822364620241639] Loss: 24.161677405930682\n",
      "Iteracion: 3331 Gradiente: [0.06848829642986895,-1.1816079488324855] Loss: 24.16027540001421\n",
      "Iteracion: 3332 Gradiente: [0.0684518859491779,-1.1809797697777151] Loss: 24.15887488440024\n",
      "Iteracion: 3333 Gradiente: [0.06841549482550988,-1.1803519246822063] Loss: 24.157475857504643\n",
      "Iteracion: 3334 Gradiente: [0.06837912304842272,-1.179724413368424] Loss: 24.15607831774489\n",
      "Iteracion: 3335 Gradiente: [0.06834277060778314,-1.179097235658913] Loss: 24.154682263540227\n",
      "Iteracion: 3336 Gradiente: [0.06830643749309218,-1.1784703913763304] Loss: 24.15328769331152\n",
      "Iteracion: 3337 Gradiente: [0.06827012369432642,-1.1778438803434024] Loss: 24.15189460548136\n",
      "Iteracion: 3338 Gradiente: [0.06823382920111574,-1.1772177023829684] Loss: 24.150502998473964\n",
      "Iteracion: 3339 Gradiente: [0.06819755400306728,-1.176591857317964] Loss: 24.14911287071525\n",
      "Iteracion: 3340 Gradiente: [0.06816129809004488,-1.1759663449714068] Loss: 24.147724220632824\n",
      "Iteracion: 3341 Gradiente: [0.06812506145179688,-1.1753411651664094] Loss: 24.14633704665592\n",
      "Iteracion: 3342 Gradiente: [0.06808884407808098,-1.1747163177261868] Loss: 24.14495134721551\n",
      "Iteracion: 3343 Gradiente: [0.06805264595858679,-1.1740918024740457] Loss: 24.14356712074417\n",
      "Iteracion: 3344 Gradiente: [0.06801646708305403,-1.1734676192333855] Loss: 24.142184365676147\n",
      "Iteracion: 3345 Gradiente: [0.06798030744148206,-1.1728437678276862] Loss: 24.14080308044741\n",
      "Iteracion: 3346 Gradiente: [0.06794416702338993,-1.1722202480805481] Loss: 24.139423263495512\n",
      "Iteracion: 3347 Gradiente: [0.0679080458185922,-1.1715970598156507] Loss: 24.138044913259744\n",
      "Iteracion: 3348 Gradiente: [0.0678719438170475,-1.1709742028567556] Loss: 24.136668028180974\n",
      "Iteracion: 3349 Gradiente: [0.06783586100847287,-1.1703516770277373] Loss: 24.135292606701817\n",
      "Iteracion: 3350 Gradiente: [0.06779979738253132,-1.1697294821525641] Loss: 24.133918647266466\n",
      "Iteracion: 3351 Gradiente: [0.06776375292909051,-1.169107618055285] Loss: 24.13254614832082\n",
      "Iteracion: 3352 Gradiente: [0.06772772763804559,-1.168486084560046] Loss: 24.13117510831238\n",
      "Iteracion: 3353 Gradiente: [0.06769172149923293,-1.1678648814910848] Loss: 24.129805525690365\n",
      "Iteracion: 3354 Gradiente: [0.06765573450226062,-1.1672440086727502] Loss: 24.12843739890558\n",
      "Iteracion: 3355 Gradiente: [0.06761976663710242,-1.1666234659294625] Loss: 24.12707072641049\n",
      "Iteracion: 3356 Gradiente: [0.06758381789357486,-1.166003253085745] Loss: 24.125705506659255\n",
      "Iteracion: 3357 Gradiente: [0.06754788826151999,-1.1653833699662097] Loss: 24.124341738107596\n",
      "Iteracion: 3358 Gradiente: [0.06751197773075338,-1.16476381639557] Loss: 24.122979419212957\n",
      "Iteracion: 3359 Gradiente: [0.06747608629114268,-1.1641445921986227] Loss: 24.121618548434355\n",
      "Iteracion: 3360 Gradiente: [0.06744021393253566,-1.1635256972002666] Loss: 24.120259124232465\n",
      "Iteracion: 3361 Gradiente: [0.06740436064474882,-1.1629071312254888] Loss: 24.118901145069653\n",
      "Iteracion: 3362 Gradiente: [0.06736852641764131,-1.1622888940993723] Loss: 24.11754460940984\n",
      "Iteracion: 3363 Gradiente: [0.06733271124115087,-1.1616709856470868] Loss: 24.11618951571862\n",
      "Iteracion: 3364 Gradiente: [0.06729691510513001,-1.1610534056938988] Loss: 24.114835862463206\n",
      "Iteracion: 3365 Gradiente: [0.06726113799934126,-1.1604361540651764] Loss: 24.11348364811245\n",
      "Iteracion: 3366 Gradiente: [0.0672253799137934,-1.1598192305863637] Loss: 24.112132871136815\n",
      "Iteracion: 3367 Gradiente: [0.0671896408383579,-1.1592026350830078] Loss: 24.110783530008426\n",
      "Iteracion: 3368 Gradiente: [0.06715392076294127,-1.1585863673807466] Loss: 24.109435623200984\n",
      "Iteracion: 3369 Gradiente: [0.06711821967745095,-1.157970427305309] Loss: 24.108089149189844\n",
      "Iteracion: 3370 Gradiente: [0.06708253757165987,-1.1573548146825268] Loss: 24.106744106451977\n",
      "Iteracion: 3371 Gradiente: [0.06704687443558158,-1.156739529338312] Loss: 24.105400493465986\n",
      "Iteracion: 3372 Gradiente: [0.06701123025915194,-1.1561245710986683] Loss: 24.104058308712055\n",
      "Iteracion: 3373 Gradiente: [0.06697560503223675,-1.1555099397897033] Loss: 24.102717550671986\n",
      "Iteracion: 3374 Gradiente: [0.06693999874478607,-1.154895635237609] Loss: 24.101378217829264\n",
      "Iteracion: 3375 Gradiente: [0.06690441138676324,-1.154281657268669] Loss: 24.10004030866888\n",
      "Iteracion: 3376 Gradiente: [0.06686884294797721,-1.1536680057092668] Loss: 24.098703821677518\n",
      "Iteracion: 3377 Gradiente: [0.0668332934186329,-1.1530546803858603] Loss: 24.097368755343446\n",
      "Iteracion: 3378 Gradiente: [0.06679776278842174,-1.1524416811250278] Loss: 24.096035108156528\n",
      "Iteracion: 3379 Gradiente: [0.06676225104741604,-1.1518290077534161] Loss: 24.094702878608242\n",
      "Iteracion: 3380 Gradiente: [0.06672675818540294,-1.1512166600977813] Loss: 24.093372065191655\n",
      "Iteracion: 3381 Gradiente: [0.06669128419265083,-1.1506046379849468] Loss: 24.092042666401465\n",
      "Iteracion: 3382 Gradiente: [0.0666558290588739,-1.149992941241859] Loss: 24.09071468073396\n",
      "Iteracion: 3383 Gradiente: [0.06662039277415867,-1.149381569695534] Loss: 24.089388106687007\n",
      "Iteracion: 3384 Gradiente: [0.06658497532845141,-1.1487705231730903] Loss: 24.088062942760082\n",
      "Iteracion: 3385 Gradiente: [0.06654957671172876,-1.1481598015017338] Loss: 24.08673918745427\n",
      "Iteracion: 3386 Gradiente: [0.06651419691405541,-1.1475494045087606] Loss: 24.085416839272217\n",
      "Iteracion: 3387 Gradiente: [0.06647883592535209,-1.1469393320215653] Loss: 24.084095896718203\n",
      "Iteracion: 3388 Gradiente: [0.06644349373566645,-1.146329583867629] Loss: 24.08277635829806\n",
      "Iteracion: 3389 Gradiente: [0.06640817033496281,-1.1457201598745284] Loss: 24.08145822251924\n",
      "Iteracion: 3390 Gradiente: [0.06637286571319786,-1.1451110598699323] Loss: 24.080141487890742\n",
      "Iteracion: 3391 Gradiente: [0.06633757986049223,-1.144502283681592] Loss: 24.07882615292319\n",
      "Iteracion: 3392 Gradiente: [0.06630231276685852,-1.1438938311373588] Loss: 24.077512216128774\n",
      "Iteracion: 3393 Gradiente: [0.06626706442220988,-1.1432857020651785] Loss: 24.07619967602124\n",
      "Iteracion: 3394 Gradiente: [0.06623183481666502,-1.1426778962930793] Loss: 24.07488853111598\n",
      "Iteracion: 3395 Gradiente: [0.06619662394040328,-1.1420704136491768] Loss: 24.07357877992989\n",
      "Iteracion: 3396 Gradiente: [0.06616143178322129,-1.1414632539617007] Loss: 24.072270420981496\n",
      "Iteracion: 3397 Gradiente: [0.06612625833522921,-1.140856417058952] Loss: 24.070963452790874\n",
      "Iteracion: 3398 Gradiente: [0.0660911035865837,-1.1402499027693247] Loss: 24.06965787387967\n",
      "Iteracion: 3399 Gradiente: [0.0660559675272926,-1.139643710921311] Loss: 24.06835368277112\n",
      "Iteracion: 3400 Gradiente: [0.06602085014727853,-1.1390378413434976] Loss: 24.067050877990006\n",
      "Iteracion: 3401 Gradiente: [0.06598575143690369,-1.1384322938645393] Loss: 24.065749458062722\n",
      "Iteracion: 3402 Gradiente: [0.06595067138596657,-1.137827068313215] Loss: 24.06444942151716\n",
      "Iteracion: 3403 Gradiente: [0.06591560998466643,-1.137222164518371] Loss: 24.063150766882845\n",
      "Iteracion: 3404 Gradiente: [0.06588056722315325,-1.1366175823089475] Loss: 24.061853492690798\n",
      "Iteracion: 3405 Gradiente: [0.0658455430914908,-1.1360133215139812] Loss: 24.06055759747367\n",
      "Iteracion: 3406 Gradiente: [0.06581053757965663,-1.1354093819626057] Loss: 24.059263079765643\n",
      "Iteracion: 3407 Gradiente: [0.06577555067782631,-1.1348057634840332] Loss: 24.057969938102435\n",
      "Iteracion: 3408 Gradiente: [0.06574058237612898,-1.1342024659075705] Loss: 24.05667817102135\n",
      "Iteracion: 3409 Gradiente: [0.0657056326646862,-1.133599489062615] Loss: 24.055387777061203\n",
      "Iteracion: 3410 Gradiente: [0.06567070153350016,-1.1329968327786646] Loss: 24.05409875476246\n",
      "Iteracion: 3411 Gradiente: [0.06563578897288949,-1.1323944968852862] Loss: 24.052811102667036\n",
      "Iteracion: 3412 Gradiente: [0.06560089497279382,-1.1317924812121618] Loss: 24.051524819318438\n",
      "Iteracion: 3413 Gradiente: [0.06556601952342665,-1.131190785589047] Loss: 24.0502399032617\n",
      "Iteracion: 3414 Gradiente: [0.0655311626148252,-1.1305894098457996] Loss: 24.048956353043444\n",
      "Iteracion: 3415 Gradiente: [0.0654963242373763,-1.129988353812348] Loss: 24.047674167211817\n",
      "Iteracion: 3416 Gradiente: [0.06546150438097507,-1.12938761731874] Loss: 24.046393344316478\n",
      "Iteracion: 3417 Gradiente: [0.06542670303592692,-1.1287872001950874] Loss: 24.045113882908677\n",
      "Iteracion: 3418 Gradiente: [0.06539192019229745,-1.1281871022716106] Loss: 24.043835781541183\n",
      "Iteracion: 3419 Gradiente: [0.06535715584035794,-1.1275873233786062] Loss: 24.042559038768275\n",
      "Iteracion: 3420 Gradiente: [0.06532240997016364,-1.1269878633464747] Loss: 24.041283653145832\n",
      "Iteracion: 3421 Gradiente: [0.06528768257192515,-1.1263887220056967] Loss: 24.040009623231192\n",
      "Iteracion: 3422 Gradiente: [0.0652529736359052,-1.1257898991868427] Loss: 24.03873694758331\n",
      "Iteracion: 3423 Gradiente: [0.0652182831522623,-1.1251913947205772] Loss: 24.0374656247626\n",
      "Iteracion: 3424 Gradiente: [0.06518361111111139,-1.1245932084376584] Loss: 24.036195653331053\n",
      "Iteracion: 3425 Gradiente: [0.06514895750261758,-1.1239953401689324] Loss: 24.034927031852142\n",
      "Iteracion: 3426 Gradiente: [0.06511432231711088,-1.123397789745326] Loss: 24.033659758890924\n",
      "Iteracion: 3427 Gradiente: [0.06507970554467211,-1.122800556997869] Loss: 24.032393833013952\n",
      "Iteracion: 3428 Gradiente: [0.06504510717564926,-1.1222036417576697] Loss: 24.03112925278929\n",
      "Iteracion: 3429 Gradiente: [0.065010527200072,-1.1216070438559398] Loss: 24.02986601678655\n",
      "Iteracion: 3430 Gradiente: [0.0649759656083944,-1.1210107631239599] Loss: 24.028604123576834\n",
      "Iteracion: 3431 Gradiente: [0.06494142239063004,-1.1204147993931246] Loss: 24.027343571732818\n",
      "Iteracion: 3432 Gradiente: [0.06490689753714965,-1.1198191524949002] Loss: 24.026084359828626\n",
      "Iteracion: 3433 Gradiente: [0.06487239103808899,-1.1192238222608515] Loss: 24.02482648643993\n",
      "Iteracion: 3434 Gradiente: [0.06483790288373256,-1.11862880852263] Loss: 24.023569950143933\n",
      "Iteracion: 3435 Gradiente: [0.06480343306437533,-1.1180341111119723] Loss: 24.022314749519328\n",
      "Iteracion: 3436 Gradiente: [0.0647689815702563,-1.1174397298607122] Loss: 24.02106088314632\n",
      "Iteracion: 3437 Gradiente: [0.06473454839156431,-1.116845664600772] Loss: 24.019808349606652\n",
      "Iteracion: 3438 Gradiente: [0.06470013351868525,-1.1162519151641532] Loss: 24.01855714748352\n",
      "Iteracion: 3439 Gradiente: [0.06466573694167436,-1.1156584813829682] Loss: 24.017307275361656\n",
      "Iteracion: 3440 Gradiente: [0.06463135865108048,-1.1150653630893883] Loss: 24.016058731827318\n",
      "Iteracion: 3441 Gradiente: [0.0645969986370081,-1.1144725601157017] Loss: 24.01481151546823\n",
      "Iteracion: 3442 Gradiente: [0.06456265688968964,-1.113880072294276] Loss: 24.013565624873646\n",
      "Iteracion: 3443 Gradiente: [0.06452833339958298,-1.1132878994575577] Loss: 24.01232105863429\n",
      "Iteracion: 3444 Gradiente: [0.06449402815685043,-1.1126960414380984] Loss: 24.01107781534241\n",
      "Iteracion: 3445 Gradiente: [0.0644597411518409,-1.1121044980685293] Loss: 24.009835893591756\n",
      "Iteracion: 3446 Gradiente: [0.0644254723748323,-1.1115132691815746] Loss: 24.008595291977542\n",
      "Iteracion: 3447 Gradiente: [0.0643912218162645,-1.1109223546100366] Loss: 24.007356009096465\n",
      "Iteracion: 3448 Gradiente: [0.06435698946622116,-1.1103317541868318] Loss: 24.006118043546806\n",
      "Iteracion: 3449 Gradiente: [0.06432277531520564,-1.1097414677449349] Loss: 24.0048813939282\n",
      "Iteracion: 3450 Gradiente: [0.06428857935345889,-1.1091514951174306] Loss: 24.00364605884187\n",
      "Iteracion: 3451 Gradiente: [0.06425440157143687,-1.1085618361374785] Loss: 24.002412036890505\n",
      "Iteracion: 3452 Gradiente: [0.06422024195924697,-1.1079724906383481] Loss: 24.001179326678262\n",
      "Iteracion: 3453 Gradiente: [0.06418610050742093,-1.10738345845337] Loss: 23.999947926810805\n",
      "Iteracion: 3454 Gradiente: [0.06415197720612108,-1.1067947394159878] Loss: 23.998717835895206\n",
      "Iteracion: 3455 Gradiente: [0.0641178720459152,-1.1062063333597127] Loss: 23.997489052540153\n",
      "Iteracion: 3456 Gradiente: [0.06408378501705177,-1.1056182401181576] Loss: 23.996261575355696\n",
      "Iteracion: 3457 Gradiente: [0.06404971610989586,-1.105030459525021] Loss: 23.995035402953427\n",
      "Iteracion: 3458 Gradiente: [0.06401566531483809,-1.1044429914140885] Loss: 23.99381053394637\n",
      "Iteracion: 3459 Gradiente: [0.06398163262220274,-1.1038558356192374] Loss: 23.992586966949034\n",
      "Iteracion: 3460 Gradiente: [0.06394761802237384,-1.103268991974429] Loss: 23.991364700577435\n",
      "Iteracion: 3461 Gradiente: [0.06391362150582343,-1.1026824603137135] Loss: 23.990143733449035\n",
      "Iteracion: 3462 Gradiente: [0.06387964306276786,-1.1020962404712364] Loss: 23.98892406418274\n",
      "Iteracion: 3463 Gradiente: [0.06384568268378246,-1.1015103322812172] Loss: 23.98770569139899\n",
      "Iteracion: 3464 Gradiente: [0.0638117403591347,-1.1009247355779783] Loss: 23.98648861371961\n",
      "Iteracion: 3465 Gradiente: [0.06377781607930899,-1.1003394501959192] Loss: 23.985272829767943\n",
      "Iteracion: 3466 Gradiente: [0.0637439098346609,-1.099754475969536] Loss: 23.984058338168822\n",
      "Iteracion: 3467 Gradiente: [0.06371002161551852,-1.0991698127334104] Loss: 23.982845137548445\n",
      "Iteracion: 3468 Gradiente: [0.06367615141251122,-1.0985854603222016] Loss: 23.981633226534562\n",
      "Iteracion: 3469 Gradiente: [0.06364229921588181,-1.0980014185706746] Loss: 23.980422603756363\n",
      "Iteracion: 3470 Gradiente: [0.06360846501609577,-1.0974176873136714] Loss: 23.97921326784444\n",
      "Iteracion: 3471 Gradiente: [0.06357464880365266,-1.0968342663861195] Loss: 23.978005217430898\n",
      "Iteracion: 3472 Gradiente: [0.0635408505690369,-1.0962511556230345] Loss: 23.976798451149307\n",
      "Iteracion: 3473 Gradiente: [0.06350707030241647,-1.0956683548595407] Loss: 23.975592967634622\n",
      "Iteracion: 3474 Gradiente: [0.06347330799451072,-1.0950858639308172] Loss: 23.974388765523322\n",
      "Iteracion: 3475 Gradiente: [0.06343956363571408,-1.094503682672149] Loss: 23.973185843453287\n",
      "Iteracion: 3476 Gradiente: [0.06340583721644084,-1.093921810918908] Loss: 23.971984200063858\n",
      "Iteracion: 3477 Gradiente: [0.0633721287271328,-1.0933402485065542] Loss: 23.970783833995814\n",
      "Iteracion: 3478 Gradiente: [0.06333843815831036,-1.092758995270629] Loss: 23.969584743891414\n",
      "Iteracion: 3479 Gradiente: [0.06330476550043898,-1.0921780510467642] Loss: 23.96838692839432\n",
      "Iteracion: 3480 Gradiente: [0.06327111074391685,-1.0915974156706845] Loss: 23.967190386149646\n",
      "Iteracion: 3481 Gradiente: [0.06323747387936104,-1.0910170889781887] Loss: 23.965995115803985\n",
      "Iteracion: 3482 Gradiente: [0.06320385489717636,-1.090437070805177] Loss: 23.964801116005308\n",
      "Iteracion: 3483 Gradiente: [0.06317025378795904,-1.0898573609876254] Loss: 23.963608385403028\n",
      "Iteracion: 3484 Gradiente: [0.06313667054204472,-1.08927795936161] Loss: 23.962416922648057\n",
      "Iteracion: 3485 Gradiente: [0.06310310514996426,-1.0886988657632888] Loss: 23.961226726392702\n",
      "Iteracion: 3486 Gradiente: [0.06306955760233848,-1.088120080028896] Loss: 23.96003779529067\n",
      "Iteracion: 3487 Gradiente: [0.06303602788966885,-1.0875416019947608] Loss: 23.958850127997145\n",
      "Iteracion: 3488 Gradiente: [0.0630025160023204,-1.0869634314973107] Loss: 23.957663723168725\n",
      "Iteracion: 3489 Gradiente: [0.06296902193104473,-1.0863855683730377] Loss: 23.956478579463433\n",
      "Iteracion: 3490 Gradiente: [0.06293554566605905,-1.0858080124585494] Loss: 23.95529469554073\n",
      "Iteracion: 3491 Gradiente: [0.06290208719823814,-1.0852307635905045] Loss: 23.954112070061502\n",
      "Iteracion: 3492 Gradiente: [0.06286864651788354,-1.0846538216056802] Loss: 23.952930701688015\n",
      "Iteracion: 3493 Gradiente: [0.06283522361570988,-1.0840771863409195] Loss: 23.95175058908402\n",
      "Iteracion: 3494 Gradiente: [0.06280181848216178,-1.083500857633166] Loss: 23.95057173091467\n",
      "Iteracion: 3495 Gradiente: [0.06276843110782503,-1.082924835319443] Loss: 23.949394125846492\n",
      "Iteracion: 3496 Gradiente: [0.06273506148310351,-1.08234911923687] Loss: 23.9482177725475\n",
      "Iteracion: 3497 Gradiente: [0.06270170959880564,-1.0817737092226325] Loss: 23.947042669687082\n",
      "Iteracion: 3498 Gradiente: [0.06266837544532393,-1.0811986051140252] Loss: 23.94586881593603\n",
      "Iteracion: 3499 Gradiente: [0.06263505901333796,-1.0806238067484117] Loss: 23.944696209966583\n",
      "Iteracion: 3500 Gradiente: [0.06260176029337856,-1.0800493139632537] Loss: 23.943524850452384\n",
      "Iteracion: 3501 Gradiente: [0.06256847927604857,-1.079475126596094] Loss: 23.942354736068467\n",
      "Iteracion: 3502 Gradiente: [0.06253521595192714,-1.078901244484563] Loss: 23.94118586549128\n",
      "Iteracion: 3503 Gradiente: [0.06250197031165498,-1.0783276674663749] Loss: 23.94001823739867\n",
      "Iteracion: 3504 Gradiente: [0.062468742345779066,-1.077754395379337] Loss: 23.938851850469952\n",
      "Iteracion: 3505 Gradiente: [0.062435532044827366,-1.0771814280613405] Loss: 23.937686703385744\n",
      "Iteracion: 3506 Gradiente: [0.0624023393995401,-1.0766087653503549] Loss: 23.93652279482814\n",
      "Iteracion: 3507 Gradiente: [0.06236916440049261,-1.0760364070844446] Loss: 23.935360123480613\n",
      "Iteracion: 3508 Gradiente: [0.06233600703834649,-1.0754643531017518] Loss: 23.93419868802802\n",
      "Iteracion: 3509 Gradiente: [0.06230286730359656,-1.0748926032405208] Loss: 23.933038487156658\n",
      "Iteracion: 3510 Gradiente: [0.06226974518690819,-1.0743211573390672] Loss: 23.931879519554162\n",
      "Iteracion: 3511 Gradiente: [0.0622366406790898,-1.0737500152357882] Loss: 23.930721783909625\n",
      "Iteracion: 3512 Gradiente: [0.062203553770589795,-1.073179176769185] Loss: 23.929565278913483\n",
      "Iteracion: 3513 Gradiente: [0.06217048445205554,-1.0726086417778353] Loss: 23.928410003257575\n",
      "Iteracion: 3514 Gradiente: [0.062137432714197874,-1.0720384101003988] Loss: 23.92725595563515\n",
      "Iteracion: 3515 Gradiente: [0.06210439854771816,-1.071468481575622] Loss: 23.92610313474084\n",
      "Iteracion: 3516 Gradiente: [0.06207138194305723,-1.0708988560423522] Loss: 23.92495153927065\n",
      "Iteracion: 3517 Gradiente: [0.062038382891237615,-1.0703295333394889] Loss: 23.923801167921987\n",
      "Iteracion: 3518 Gradiente: [0.06200540138259782,-1.0697605133060566] Loss: 23.922652019393627\n",
      "Iteracion: 3519 Gradiente: [0.061972437407977546,-1.069191795781137] Loss: 23.92150409238575\n",
      "Iteracion: 3520 Gradiente: [0.06193949095799856,-1.0686233806039127] Loss: 23.920357385599896\n",
      "Iteracion: 3521 Gradiente: [0.06190656202340392,-1.0680552676136408] Loss: 23.919211897739004\n",
      "Iteracion: 3522 Gradiente: [0.06187365059485046,-1.067487456649673] Loss: 23.91806762750736\n",
      "Iteracion: 3523 Gradiente: [0.06184075666300733,-1.0669199475514433] Loss: 23.916924573610686\n",
      "Iteracion: 3524 Gradiente: [0.06180788021855695,-1.0663527401584716] Loss: 23.915782734756007\n",
      "Iteracion: 3525 Gradiente: [0.061775021252242365,-1.0657858343103581] Loss: 23.914642109651773\n",
      "Iteracion: 3526 Gradiente: [0.06174217975480758,-1.0652192298467933] Loss: 23.913502697007793\n",
      "Iteracion: 3527 Gradiente: [0.0617093557169009,-1.0646529266075544] Loss: 23.912364495535268\n",
      "Iteracion: 3528 Gradiente: [0.06167654912924263,-1.064086924432501] Loss: 23.911227503946712\n",
      "Iteracion: 3529 Gradiente: [0.06164375998261183,-1.0635212231615736] Loss: 23.91009172095608\n",
      "Iteracion: 3530 Gradiente: [0.061610988267676706,-1.0629558226348088] Loss: 23.908957145278634\n",
      "Iteracion: 3531 Gradiente: [0.06157823397518314,-1.0623907226923197] Loss: 23.907823775631048\n",
      "Iteracion: 3532 Gradiente: [0.0615454970958145,-1.0618259231743072] Loss: 23.906691610731293\n",
      "Iteracion: 3533 Gradiente: [0.06151277762048058,-1.06126142392105] Loss: 23.905560649298806\n",
      "Iteracion: 3534 Gradiente: [0.06148007553972737,-1.0606972247729287] Loss: 23.904430890054286\n",
      "Iteracion: 3535 Gradiente: [0.061447390844519605,-1.0601333255703846] Loss: 23.903302331719843\n",
      "Iteracion: 3536 Gradiente: [0.06141472352540329,-1.0595697261539727] Loss: 23.902174973018937\n",
      "Iteracion: 3537 Gradiente: [0.06138207357328819,-1.0590064263643064] Loss: 23.901048812676393\n",
      "Iteracion: 3538 Gradiente: [0.061349440978849167,-1.0584434260421012] Loss: 23.89992384941836\n",
      "Iteracion: 3539 Gradiente: [0.06131682573283399,-1.057880725028152] Loss: 23.898800081972375\n",
      "Iteracion: 3540 Gradiente: [0.06128422782621025,-1.057318323163327] Loss: 23.89767750906732\n",
      "Iteracion: 3541 Gradiente: [0.06125164724956278,-1.0567562202886] Loss: 23.896556129433414\n",
      "Iteracion: 3542 Gradiente: [0.0612190839937701,-1.0561944162450134] Loss: 23.895435941802237\n",
      "Iteracion: 3543 Gradiente: [0.06118653804967001,-1.0556329108736964] Loss: 23.89431694490673\n",
      "Iteracion: 3544 Gradiente: [0.06115400940790702,-1.0550717040158764] Loss: 23.893199137481144\n",
      "Iteracion: 3545 Gradiente: [0.061121498059367244,-1.054510795512847] Loss: 23.8920825182611\n",
      "Iteracion: 3546 Gradiente: [0.0610890039949273,-1.0539501852059916] Loss: 23.890967085983583\n",
      "Iteracion: 3547 Gradiente: [0.061056527205321724,-1.053389872936783] Loss: 23.889852839386876\n",
      "Iteracion: 3548 Gradiente: [0.06102406768137409,-1.052829858546775] Loss: 23.888739777210617\n",
      "Iteracion: 3549 Gradiente: [0.06099162541384165,-1.052270141877611] Loss: 23.887627898195824\n",
      "Iteracion: 3550 Gradiente: [0.060959200393729894,-1.0517107227710019] Loss: 23.88651720108481\n",
      "Iteracion: 3551 Gradiente: [0.06092679261165965,-1.051151601068766] Loss: 23.885407684621224\n",
      "Iteracion: 3552 Gradiente: [0.060894402058474385,-1.050592776612794] Loss: 23.884299347550076\n",
      "Iteracion: 3553 Gradiente: [0.06086202872526485,-1.0500342492450472] Loss: 23.883192188617702\n",
      "Iteracion: 3554 Gradiente: [0.06082967260263198,-1.0494760188075962] Loss: 23.88208620657173\n",
      "Iteracion: 3555 Gradiente: [0.06079733368159168,-1.0489180851425768] Loss: 23.880981400161197\n",
      "Iteracion: 3556 Gradiente: [0.060765011952759095,-1.0483604480922282] Loss: 23.87987776813641\n",
      "Iteracion: 3557 Gradiente: [0.0607327074071956,-1.0478031074988494] Loss: 23.87877530924902\n",
      "Iteracion: 3558 Gradiente: [0.060700420035774985,-1.047246063204834] Loss: 23.877674022252013\n",
      "Iteracion: 3559 Gradiente: [0.06066814982933692,-1.0466893150526617] Loss: 23.87657390589967\n",
      "Iteracion: 3560 Gradiente: [0.060635896778659534,-1.0461328628849025] Loss: 23.875474958947652\n",
      "Iteracion: 3561 Gradiente: [0.06060366087473123,-1.0455767065441914] Loss: 23.87437718015292\n",
      "Iteracion: 3562 Gradiente: [0.06057144210833106,-1.0450208458732673] Loss: 23.873280568273707\n",
      "Iteracion: 3563 Gradiente: [0.06053924047057061,-1.0444652807149286] Loss: 23.872185122069638\n",
      "Iteracion: 3564 Gradiente: [0.06050705595206883,-1.0439100109120878] Loss: 23.871090840301598\n",
      "Iteracion: 3565 Gradiente: [0.06047488854387003,-1.043355036307716] Loss: 23.86999772173186\n",
      "Iteracion: 3566 Gradiente: [0.06044273823688684,-1.0428003567448778] Loss: 23.86890576512392\n",
      "Iteracion: 3567 Gradiente: [0.06041060502194569,-1.0422459720667234] Loss: 23.867814969242673\n",
      "Iteracion: 3568 Gradiente: [0.060378488890054885,-1.041691882116478] Loss: 23.86672533285427\n",
      "Iteracion: 3569 Gradiente: [0.06034638983210432,-1.0411380867374567] Loss: 23.865636854726198\n",
      "Iteracion: 3570 Gradiente: [0.060314307838943176,-1.0405845857730605] Loss: 23.864549533627265\n",
      "Iteracion: 3571 Gradiente: [0.060282242901522905,-1.040031379066769] Loss: 23.86346336832755\n",
      "Iteracion: 3572 Gradiente: [0.0602501950108253,-1.0394784664621406] Loss: 23.862378357598494\n",
      "Iteracion: 3573 Gradiente: [0.06021816415777911,-1.038925847802826] Loss: 23.861294500212782\n",
      "Iteracion: 3574 Gradiente: [0.06018615033332727,-1.038373522932551] Loss: 23.86021179494445\n",
      "Iteracion: 3575 Gradiente: [0.06015415352844495,-1.0378214916951296] Loss: 23.859130240568838\n",
      "Iteracion: 3576 Gradiente: [0.06012217373400593,-1.0372697539344595] Loss: 23.858049835862527\n",
      "Iteracion: 3577 Gradiente: [0.06009021094105644,-1.036718309494517] Loss: 23.856970579603487\n",
      "Iteracion: 3578 Gradiente: [0.06005826514043709,-1.0361671582193677] Loss: 23.855892470570918\n",
      "Iteracion: 3579 Gradiente: [0.060026336323193166,-1.0356162999531544] Loss: 23.854815507545357\n",
      "Iteracion: 3580 Gradiente: [0.05999442448027707,-1.035065734540105] Loss: 23.853739689308615\n",
      "Iteracion: 3581 Gradiente: [0.059962529602670615,-1.034515461824528] Loss: 23.852665014643794\n",
      "Iteracion: 3582 Gradiente: [0.05993065168141243,-1.0339654816508141] Loss: 23.851591482335326\n",
      "Iteracion: 3583 Gradiente: [0.059898790707494716,-1.0334157938634374] Loss: 23.850519091168884\n",
      "Iteracion: 3584 Gradiente: [0.05986694667180738,-1.0328663983069666] Loss: 23.849447839931486\n",
      "Iteracion: 3585 Gradiente: [0.05983511956531231,-1.0323172948260388] Loss: 23.848377727411382\n",
      "Iteracion: 3586 Gradiente: [0.059803309379089835,-1.0317684832653773] Loss: 23.847308752398177\n",
      "Iteracion: 3587 Gradiente: [0.05977151610422406,-1.0312199634697816] Loss: 23.84624091368269\n",
      "Iteracion: 3588 Gradiente: [0.0597397397315973,-1.0306717352841486] Loss: 23.84517421005709\n",
      "Iteracion: 3589 Gradiente: [0.05970798025234198,-1.0301237985534428] Loss: 23.844108640314776\n",
      "Iteracion: 3590 Gradiente: [0.059676237657278836,-1.029576153122727] Loss: 23.843044203250454\n",
      "Iteracion: 3591 Gradiente: [0.05964451193756872,-1.029028798837132] Loss: 23.84198089766013\n",
      "Iteracion: 3592 Gradiente: [0.05961280308424269,-1.0284817355418752] Loss: 23.84091872234107\n",
      "Iteracion: 3593 Gradiente: [0.05958111108825316,-1.0279349630822623] Loss: 23.839857676091803\n",
      "Iteracion: 3594 Gradiente: [0.059549435940823514,-1.0273884813036644] Loss: 23.83879775771215\n",
      "Iteracion: 3595 Gradiente: [0.059517777632810485,-1.0268422900515588] Loss: 23.837738966003247\n",
      "Iteracion: 3596 Gradiente: [0.05948613615536639,-1.0262963891714842] Loss: 23.836681299767417\n",
      "Iteracion: 3597 Gradiente: [0.059454511499482506,-1.0257507785090758] Loss: 23.835624757808336\n",
      "Iteracion: 3598 Gradiente: [0.05942290365616903,-1.025205457910046] Loss: 23.834569338930912\n",
      "Iteracion: 3599 Gradiente: [0.05939131261665504,-1.02466042722018] Loss: 23.83351504194134\n",
      "Iteracion: 3600 Gradiente: [0.05935973837185126,-1.0241156862853615] Loss: 23.832461865647076\n",
      "Iteracion: 3601 Gradiente: [0.0593281809129157,-1.023571234951543] Loss: 23.831409808856844\n",
      "Iteracion: 3602 Gradiente: [0.05929664023085005,-1.0230270730647686] Loss: 23.83035887038061\n",
      "Iteracion: 3603 Gradiente: [0.05926511631682653,-1.0224832004711515] Loss: 23.829309049029675\n",
      "Iteracion: 3604 Gradiente: [0.0592336091618004,-1.021939617016905] Loss: 23.828260343616527\n",
      "Iteracion: 3605 Gradiente: [0.05920211875701019,-1.0213963225483056] Loss: 23.827212752954964\n",
      "Iteracion: 3606 Gradiente: [0.05917064509346517,-1.0208533169117222] Loss: 23.82616627586002\n",
      "Iteracion: 3607 Gradiente: [0.059139188162305345,-1.020310599953601] Loss: 23.825120911148\n",
      "Iteracion: 3608 Gradiente: [0.05910774795459209,-1.0197681715204756] Loss: 23.82407665763646\n",
      "Iteracion: 3609 Gradiente: [0.0590763244615232,-1.01922603145895] Loss: 23.82303351414422\n",
      "Iteracion: 3610 Gradiente: [0.05904491767418184,-1.0186841796157202] Loss: 23.821991479491352\n",
      "Iteracion: 3611 Gradiente: [0.05901352758350906,-1.0181426158375713] Loss: 23.820950552499188\n",
      "Iteracion: 3612 Gradiente: [0.05898215418085613,-1.0176013399713442] Loss: 23.819910731990312\n",
      "Iteracion: 3613 Gradiente: [0.05895079745731001,-1.0170603518639802] Loss: 23.818872016788543\n",
      "Iteracion: 3614 Gradiente: [0.058919457403859114,-1.0165196513625048] Loss: 23.817834405718962\n",
      "Iteracion: 3615 Gradiente: [0.05888813401169652,-1.0159792383140167] Loss: 23.816797897607916\n",
      "Iteracion: 3616 Gradiente: [0.058856827272103375,-1.0154391125656874] Loss: 23.815762491282978\n",
      "Iteracion: 3617 Gradiente: [0.058825537176140114,-1.0148992739647833] Loss: 23.814728185572964\n",
      "Iteracion: 3618 Gradiente: [0.058794263715016844,-1.0143597223586467] Loss: 23.81369497930795\n",
      "Iteracion: 3619 Gradiente: [0.05876300687980347,-1.0138204575947067] Loss: 23.812662871319258\n",
      "Iteracion: 3620 Gradiente: [0.05873176666162199,-1.0132814795204703] Loss: 23.811631860439427\n",
      "Iteracion: 3621 Gradiente: [0.058700543051724216,-1.0127427879835216] Loss: 23.81060194550226\n",
      "Iteracion: 3622 Gradiente: [0.05866933604119045,-1.0122043828315317] Loss: 23.809573125342794\n",
      "Iteracion: 3623 Gradiente: [0.058638145621357766,-1.0116662639122425] Loss: 23.8085453987973\n",
      "Iteracion: 3624 Gradiente: [0.05860697178319564,-1.0111284310734945] Loss: 23.807518764703293\n",
      "Iteracion: 3625 Gradiente: [0.058575814518077134,-1.0105908841631868] Loss: 23.80649322189952\n",
      "Iteracion: 3626 Gradiente: [0.05854467381707688,-1.0100536230293202] Loss: 23.80546876922597\n",
      "Iteracion: 3627 Gradiente: [0.05851354967149594,-1.0095166475199586] Loss: 23.80444540552385\n",
      "Iteracion: 3628 Gradiente: [0.05848244207232085,-1.00897995748327] Loss: 23.8034231296356\n",
      "Iteracion: 3629 Gradiente: [0.058451351011005195,-1.0084435527674722] Loss: 23.802401940404927\n",
      "Iteracion: 3630 Gradiente: [0.058420276478762884,-1.0079074332208795] Loss: 23.801381836676693\n",
      "Iteracion: 3631 Gradiente: [0.058389218466539695,-1.007371598691902] Loss: 23.80036281729706\n",
      "Iteracion: 3632 Gradiente: [0.0583581769658205,-1.006836049029001] Loss: 23.799344881113377\n",
      "Iteracion: 3633 Gradiente: [0.058327151967685606,-1.006300784080741] Loss: 23.79832802697422\n",
      "Iteracion: 3634 Gradiente: [0.058296143463368824,-1.0057658036957595] Loss: 23.797312253729455\n",
      "Iteracion: 3635 Gradiente: [0.05826515144401393,-1.0052311077227771] Loss: 23.796297560230034\n",
      "Iteracion: 3636 Gradiente: [0.05823417590112854,-1.0046966960105783] Loss: 23.795283945328276\n",
      "Iteracion: 3637 Gradiente: [0.05820321682567927,-1.0041625684080553] Loss: 23.794271407877606\n",
      "Iteracion: 3638 Gradiente: [0.05817227420909224,-1.003628724764156] Loss: 23.79325994673275\n",
      "Iteracion: 3639 Gradiente: [0.05814134804257757,-1.0030951649279238] Loss: 23.7922495607496\n",
      "Iteracion: 3640 Gradiente: [0.05811043831737,-1.002561888748477] Loss: 23.7912402487853\n",
      "Iteracion: 3641 Gradiente: [0.058079545024657855,-1.00202889607502] Loss: 23.79023200969817\n",
      "Iteracion: 3642 Gradiente: [0.05804866815576304,-1.0014961867568286] Loss: 23.789224842347778\n",
      "Iteracion: 3643 Gradiente: [0.05801780770191935,-1.0009637606432642] Loss: 23.788218745594857\n",
      "Iteracion: 3644 Gradiente: [0.057986963654582266,-1.000431617583756] Loss: 23.78721371830145\n",
      "Iteracion: 3645 Gradiente: [0.05795613600476486,-0.9998997574278391] Loss: 23.78620975933069\n",
      "Iteracion: 3646 Gradiente: [0.05792532474394155,-0.9993681800251034] Loss: 23.785206867547004\n",
      "Iteracion: 3647 Gradiente: [0.05789452986330161,-0.998836885225232] Loss: 23.78420504181596\n",
      "Iteracion: 3648 Gradiente: [0.05786375135414611,-0.9983058728779857] Loss: 23.783204281004412\n",
      "Iteracion: 3649 Gradiente: [0.057832989207814954,-0.9977751428332013] Loss: 23.782204583980324\n",
      "Iteracion: 3650 Gradiente: [0.05780224341558646,-0.9972446949407996] Loss: 23.781205949612954\n",
      "Iteracion: 3651 Gradiente: [0.05777151396875411,-0.9967145290507792] Loss: 23.78020837677269\n",
      "Iteracion: 3652 Gradiente: [0.057740800858685284,-0.996184645013218] Loss: 23.77921186433118\n",
      "Iteracion: 3653 Gradiente: [0.05771010407657684,-0.9956550426782788] Loss: 23.778216411161225\n",
      "Iteracion: 3654 Gradiente: [0.05767942361383405,-0.9951257218961973] Loss: 23.777222016136843\n",
      "Iteracion: 3655 Gradiente: [0.05764875946177881,-0.9945966825172888] Loss: 23.776228678133247\n",
      "Iteracion: 3656 Gradiente: [0.05761811161180219,-0.9940679243919495] Loss: 23.775236396026845\n",
      "Iteracion: 3657 Gradiente: [0.05758748005507357,-0.9935394473706651] Loss: 23.774245168695252\n",
      "Iteracion: 3658 Gradiente: [0.05755686478304275,-0.993011251303984] Loss: 23.773254995017243\n",
      "Iteracion: 3659 Gradiente: [0.05752626578694826,-0.9924833360425519] Loss: 23.77226587387285\n",
      "Iteracion: 3660 Gradiente: [0.05749568305834221,-0.9919557014370675] Loss: 23.7712778041432\n",
      "Iteracion: 3661 Gradiente: [0.05746511658843853,-0.9914283473383368] Loss: 23.770290784710678\n",
      "Iteracion: 3662 Gradiente: [0.057434566368631104,-0.9909012735972299] Loss: 23.76930481445886\n",
      "Iteracion: 3663 Gradiente: [0.057404032390182165,-0.9903744800647075] Loss: 23.768319892272476\n",
      "Iteracion: 3664 Gradiente: [0.05737351464455287,-0.9898479665917951] Loss: 23.76733601703745\n",
      "Iteracion: 3665 Gradiente: [0.05734301312308598,-0.9893217330296075] Loss: 23.766353187640902\n",
      "Iteracion: 3666 Gradiente: [0.057312527817218024,-0.9887957792293297] Loss: 23.765371402971127\n",
      "Iteracion: 3667 Gradiente: [0.05728205871825007,-0.9882701050422391] Loss: 23.764390661917606\n",
      "Iteracion: 3668 Gradiente: [0.0572516058174898,-0.9877447103196871] Loss: 23.76341096337098\n",
      "Iteracion: 3669 Gradiente: [0.05722116910651683,-0.9872195949130907] Loss: 23.7624323062231\n",
      "Iteracion: 3670 Gradiente: [0.057190748576622735,-0.9866947586739627] Loss: 23.761454689366985\n",
      "Iteracion: 3671 Gradiente: [0.057160344219275315,-0.9861702014538878] Loss: 23.760478111696834\n",
      "Iteracion: 3672 Gradiente: [0.05712995602576238,-0.9856459231045339] Loss: 23.759502572107984\n",
      "Iteracion: 3673 Gradiente: [0.057099583987557406,-0.9851219234776427] Loss: 23.758528069497004\n",
      "Iteracion: 3674 Gradiente: [0.05706922809608462,-0.9845982024250362] Loss: 23.757554602761594\n",
      "Iteracion: 3675 Gradiente: [0.05703888834267161,-0.9840747597986206] Loss: 23.756582170800648\n",
      "Iteracion: 3676 Gradiente: [0.05700856471880419,-0.983551595450371] Loss: 23.755610772514213\n",
      "Iteracion: 3677 Gradiente: [0.05697825721588761,-0.9830287092323498] Loss: 23.75464040680352\n",
      "Iteracion: 3678 Gradiente: [0.056947965825370754,-0.9825061009966907] Loss: 23.75367107257098\n",
      "Iteracion: 3679 Gradiente: [0.05691769053868446,-0.9819837705956128] Loss: 23.75270276872014\n",
      "Iteracion: 3680 Gradiente: [0.05688743134740264,-0.9814617178814004] Loss: 23.751735494155717\n",
      "Iteracion: 3681 Gradiente: [0.056857188242760044,-0.9809399427064387] Loss: 23.7507692477836\n",
      "Iteracion: 3682 Gradiente: [0.056826961216340996,-0.9804184449231726] Loss: 23.74980402851087\n",
      "Iteracion: 3683 Gradiente: [0.056796750259448456,-0.9798972243841414] Loss: 23.74883983524572\n",
      "Iteracion: 3684 Gradiente: [0.05676655536365445,-0.9793762809419441] Loss: 23.747876666897525\n",
      "Iteracion: 3685 Gradiente: [0.05673637652040213,-0.9788556144492726] Loss: 23.746914522376823\n",
      "Iteracion: 3686 Gradiente: [0.05670621372107121,-0.9783352247588943] Loss: 23.745953400595326\n",
      "Iteracion: 3687 Gradiente: [0.05667606695725548,-0.9778151117236457] Loss: 23.74499330046585\n",
      "Iteracion: 3688 Gradiente: [0.05664593622031759,-0.9772952751964558] Loss: 23.744034220902417\n",
      "Iteracion: 3689 Gradiente: [0.056615821501955564,-0.9767757150303102] Loss: 23.743076160820195\n",
      "Iteracion: 3690 Gradiente: [0.056585722793526354,-0.9762564310782963] Loss: 23.742119119135484\n",
      "Iteracion: 3691 Gradiente: [0.05655564008643618,-0.9757374231935728] Loss: 23.74116309476575\n",
      "Iteracion: 3692 Gradiente: [0.05652557337216327,-0.9752186912293769] Loss: 23.740208086629632\n",
      "Iteracion: 3693 Gradiente: [0.05649552264228153,-0.9747002350390147] Loss: 23.739254093646856\n",
      "Iteracion: 3694 Gradiente: [0.05646548788833457,-0.974182054475875] Loss: 23.738301114738366\n",
      "Iteracion: 3695 Gradiente: [0.05643546910181859,-0.9736641493934263] Loss: 23.73734914882622\n",
      "Iteracion: 3696 Gradiente: [0.05640546627417583,-0.9731465196452177] Loss: 23.7363981948336\n",
      "Iteracion: 3697 Gradiente: [0.056375479396953664,-0.9726291650848734] Loss: 23.735448251684893\n",
      "Iteracion: 3698 Gradiente: [0.05634550846166064,-0.9721120855660925] Loss: 23.73449931830557\n",
      "Iteracion: 3699 Gradiente: [0.056315553459851724,-0.9715952809426551] Loss: 23.733551393622257\n",
      "Iteracion: 3700 Gradiente: [0.0562856143830345,-0.9710787510684197] Loss: 23.732604476562756\n",
      "Iteracion: 3701 Gradiente: [0.056255691222693825,-0.9705624957973236] Loss: 23.73165856605598\n",
      "Iteracion: 3702 Gradiente: [0.05622578397051067,-0.9700465149833699] Loss: 23.730713661031967\n",
      "Iteracion: 3703 Gradiente: [0.0561958926179481,-0.9695308084806553] Loss: 23.729769760421945\n",
      "Iteracion: 3704 Gradiente: [0.056166017156526964,-0.9690153761433474] Loss: 23.728826863158204\n",
      "Iteracion: 3705 Gradiente: [0.056136157577803186,-0.9685002178256927] Loss: 23.727884968174234\n",
      "Iteracion: 3706 Gradiente: [0.056106313873370554,-0.9679853333820105] Loss: 23.726944074404624\n",
      "Iteracion: 3707 Gradiente: [0.056076486034757525,-0.9674707226667032] Loss: 23.7260041807851\n",
      "Iteracion: 3708 Gradiente: [0.05604667405355504,-0.9669563855342472] Loss: 23.72506528625252\n",
      "Iteracion: 3709 Gradiente: [0.05601687792126029,-0.9664423218392014] Loss: 23.7241273897449\n",
      "Iteracion: 3710 Gradiente: [0.05598709762946801,-0.9659285314361966] Loss: 23.72319049020134\n",
      "Iteracion: 3711 Gradiente: [0.055957333169835505,-0.9654150141799394] Loss: 23.722254586562105\n",
      "Iteracion: 3712 Gradiente: [0.05592758453394613,-0.9649017699252148] Loss: 23.721319677768545\n",
      "Iteracion: 3713 Gradiente: [0.05589785171334446,-0.9643887985268904] Loss: 23.720385762763176\n",
      "Iteracion: 3714 Gradiente: [0.055868134699617635,-0.9638760998399062] Loss: 23.719452840489616\n",
      "Iteracion: 3715 Gradiente: [0.05583843348437843,-0.9633636737192811] Loss: 23.71852090989261\n",
      "Iteracion: 3716 Gradiente: [0.05580874805918654,-0.9628515200201133] Loss: 23.717589969918045\n",
      "Iteracion: 3717 Gradiente: [0.05577907841569545,-0.9623396385975702] Loss: 23.71666001951291\n",
      "Iteracion: 3718 Gradiente: [0.05574942454545351,-0.961828029306906] Loss: 23.715731057625273\n",
      "Iteracion: 3719 Gradiente: [0.055719786440130294,-0.9613166920034437] Loss: 23.714803083204405\n",
      "Iteracion: 3720 Gradiente: [0.05569016409134804,-0.9608056265425879] Loss: 23.71387609520064\n",
      "Iteracion: 3721 Gradiente: [0.05566055749071192,-0.960294832779817] Loss: 23.712950092565432\n",
      "Iteracion: 3722 Gradiente: [0.05563096662987258,-0.9597843105706881] Loss: 23.712025074251343\n",
      "Iteracion: 3723 Gradiente: [0.05560139150039826,-0.9592740597708395] Loss: 23.711101039212075\n",
      "Iteracion: 3724 Gradiente: [0.05557183209405802,-0.9587640802359723] Loss: 23.710177986402446\n",
      "Iteracion: 3725 Gradiente: [0.05554228840235472,-0.9582543718218824] Loss: 23.709255914778325\n",
      "Iteracion: 3726 Gradiente: [0.05551276041697785,-0.9577449343844319] Loss: 23.708334823296763\n",
      "Iteracion: 3727 Gradiente: [0.05548324812960459,-0.957235767779558] Loss: 23.707414710915877\n",
      "Iteracion: 3728 Gradiente: [0.055453751531829694,-0.9567268718632824] Loss: 23.7064955765949\n",
      "Iteracion: 3729 Gradiente: [0.05542427061536349,-0.9562182464916961] Loss: 23.70557741929419\n",
      "Iteracion: 3730 Gradiente: [0.05539480537180357,-0.9557098915209724] Loss: 23.704660237975194\n",
      "Iteracion: 3731 Gradiente: [0.05536535579297303,-0.9552018068073499] Loss: 23.703744031600444\n",
      "Iteracion: 3732 Gradiente: [0.055335921870478914,-0.954693992207154] Loss: 23.702828799133624\n",
      "Iteracion: 3733 Gradiente: [0.055306503595863886,-0.9541864475767916] Loss: 23.70191453953947\n",
      "Iteracion: 3734 Gradiente: [0.055277100960977536,-0.9536791727727286] Loss: 23.70100125178383\n",
      "Iteracion: 3735 Gradiente: [0.055247713957364414,-0.9531721676515289] Loss: 23.70008893483369\n",
      "Iteracion: 3736 Gradiente: [0.0552183425768438,-0.95266543206981] Loss: 23.699177587657083\n",
      "Iteracion: 3737 Gradiente: [0.05518898681101424,-0.9521589658842818] Loss: 23.698267209223165\n",
      "Iteracion: 3738 Gradiente: [0.055159646651571845,-0.9516527689517263] Loss: 23.697357798502182\n",
      "Iteracion: 3739 Gradiente: [0.0551303220902876,-0.9511468411289958] Loss: 23.696449354465493\n",
      "Iteracion: 3740 Gradiente: [0.05510101311880646,-0.9506411822730281] Loss: 23.695541876085507\n",
      "Iteracion: 3741 Gradiente: [0.05507171972887288,-0.9501357922408279] Loss: 23.69463536233576\n",
      "Iteracion: 3742 Gradiente: [0.05504244191227201,-0.9496306708894787] Loss: 23.693729812190877\n",
      "Iteracion: 3743 Gradiente: [0.05501317966060905,-0.9491258180761456] Loss: 23.69282522462656\n",
      "Iteracion: 3744 Gradiente: [0.05498393296575633,-0.9486212336580612] Loss: 23.691921598619615\n",
      "Iteracion: 3745 Gradiente: [0.05495470181924229,-0.948116917492546] Loss: 23.69101893314791\n",
      "Iteracion: 3746 Gradiente: [0.054925486212981885,-0.947612869436981] Loss: 23.69011722719042\n",
      "Iteracion: 3747 Gradiente: [0.054896286138596415,-0.9471090893488352] Loss: 23.689216479727243\n",
      "Iteracion: 3748 Gradiente: [0.05486710158783789,-0.9466055770856487] Loss: 23.688316689739473\n",
      "Iteracion: 3749 Gradiente: [0.054837932552508545,-0.9461023325050358] Loss: 23.687417856209333\n",
      "Iteracion: 3750 Gradiente: [0.05480877902435661,-0.9455993554646867] Loss: 23.686519978120145\n",
      "Iteracion: 3751 Gradiente: [0.054779640995055466,-0.9450966458223756] Loss: 23.685623054456308\n",
      "Iteracion: 3752 Gradiente: [0.05475051845640924,-0.9445942034359411] Loss: 23.684727084203264\n",
      "Iteracion: 3753 Gradiente: [0.05472141140018228,-0.9440920281633036] Loss: 23.683832066347556\n",
      "Iteracion: 3754 Gradiente: [0.05469231981830187,-0.9435901198624463] Loss: 23.682937999876838\n",
      "Iteracion: 3755 Gradiente: [0.05466324370229643,-0.943088478391454] Loss: 23.682044883779785\n",
      "Iteracion: 3756 Gradiente: [0.054634183044039254,-0.9425871036084649] Loss: 23.681152717046142\n",
      "Iteracion: 3757 Gradiente: [0.05460513783535911,-0.9420859953716985] Loss: 23.680261498666816\n",
      "Iteracion: 3758 Gradiente: [0.0545761080680118,-0.9415851535394524] Loss: 23.67937122763369\n",
      "Iteracion: 3759 Gradiente: [0.05454709373380903,-0.9410845779700945] Loss: 23.67848190293976\n",
      "Iteracion: 3760 Gradiente: [0.054518094824438396,-0.940584268522079] Loss: 23.677593523579084\n",
      "Iteracion: 3761 Gradiente: [0.05448911133184424,-0.9400842250539199] Loss: 23.67670608854682\n",
      "Iteracion: 3762 Gradiente: [0.0544601432477009,-0.9395844474242208] Loss: 23.67581959683911\n",
      "Iteracion: 3763 Gradiente: [0.0544311905639385,-0.939084935491646] Loss: 23.67493404745328\n",
      "Iteracion: 3764 Gradiente: [0.05440225327229768,-0.9385856891149473] Loss: 23.674049439387613\n",
      "Iteracion: 3765 Gradiente: [0.05437333136458259,-0.9380867081529493] Loss: 23.673165771641543\n",
      "Iteracion: 3766 Gradiente: [0.0543444248327044,-0.9375879924645432] Loss: 23.67228304321552\n",
      "Iteracion: 3767 Gradiente: [0.05431553366834312,-0.9370895419087096] Loss: 23.671401253111064\n",
      "Iteracion: 3768 Gradiente: [0.05428665786344406,-0.9365913563444894] Loss: 23.670520400330755\n",
      "Iteracion: 3769 Gradiente: [0.05425779740980564,-0.9360934356310078] Loss: 23.669640483878247\n",
      "Iteracion: 3770 Gradiente: [0.05422895229922631,-0.935595779627463] Loss: 23.668761502758226\n",
      "Iteracion: 3771 Gradiente: [0.05420012252365325,-0.9350983881931214] Loss: 23.667883455976476\n",
      "Iteracion: 3772 Gradiente: [0.054171308074883955,-0.934601261187333] Loss: 23.66700634253979\n",
      "Iteracion: 3773 Gradiente: [0.05414250894477656,-0.9341043984695195] Loss: 23.666130161456056\n",
      "Iteracion: 3774 Gradiente: [0.05411372512503097,-0.9336077998991859] Loss: 23.665254911734227\n",
      "Iteracion: 3775 Gradiente: [0.05408495660773459,-0.9331114653358891] Loss: 23.664380592384227\n",
      "Iteracion: 3776 Gradiente: [0.05405620338469438,-0.93261539463928] Loss: 23.663507202417147\n",
      "Iteracion: 3777 Gradiente: [0.05402746544774762,-0.9321195876690813] Loss: 23.662634740845043\n",
      "Iteracion: 3778 Gradiente: [0.05399874278882256,-0.9316240442850824] Loss: 23.66176320668105\n",
      "Iteracion: 3779 Gradiente: [0.053970035399656995,-0.931128764347163] Loss: 23.66089259893937\n",
      "Iteracion: 3780 Gradiente: [0.053941343272200966,-0.930633747715262] Loss: 23.66002291663523\n",
      "Iteracion: 3781 Gradiente: [0.05391266639834005,-0.9301389942493979] Loss: 23.659154158784904\n",
      "Iteracion: 3782 Gradiente: [0.05388400477013041,-0.9296445038096538] Loss: 23.65828632440572\n",
      "Iteracion: 3783 Gradiente: [0.053855358379241616,-0.9291502762562099] Loss: 23.657419412516038\n",
      "Iteracion: 3784 Gradiente: [0.053826727217602864,-0.9286563114493077] Loss: 23.656553422135282\n",
      "Iteracion: 3785 Gradiente: [0.053798111277205865,-0.9281626092492559] Loss: 23.655688352283903\n",
      "Iteracion: 3786 Gradiente: [0.053769510550000635,-0.9276691695164442] Loss: 23.654824201983413\n",
      "Iteracion: 3787 Gradiente: [0.0537409250277193,-0.9271759921113449] Loss: 23.653960970256318\n",
      "Iteracion: 3788 Gradiente: [0.053712354702408524,-0.926683076894492] Loss: 23.653098656126232\n",
      "Iteracion: 3789 Gradiente: [0.053683799565976645,-0.926190423726497] Loss: 23.652237258617745\n",
      "Iteracion: 3790 Gradiente: [0.05365525961032536,-0.9256980324680474] Loss: 23.651376776756493\n",
      "Iteracion: 3791 Gradiente: [0.05362673482732229,-0.925205902979909] Loss: 23.6505172095692\n",
      "Iteracion: 3792 Gradiente: [0.053598225209088926,-0.9247140351229047] Loss: 23.649658556083565\n",
      "Iteracion: 3793 Gradiente: [0.05356973074739244,-0.9242224287579556] Loss: 23.648800815328343\n",
      "Iteracion: 3794 Gradiente: [0.05354125143423971,-0.9237310837460384] Loss: 23.647943986333342\n",
      "Iteracion: 3795 Gradiente: [0.05351278726150876,-0.9232399999482136] Loss: 23.647088068129335\n",
      "Iteracion: 3796 Gradiente: [0.0534843382213315,-0.9227491772256026] Loss: 23.646233059748216\n",
      "Iteracion: 3797 Gradiente: [0.053455904305384175,-0.922258615439425] Loss: 23.64537896022283\n",
      "Iteracion: 3798 Gradiente: [0.05342748550582807,-0.9217683144509484] Loss: 23.644525768587105\n",
      "Iteracion: 3799 Gradiente: [0.05339908181459237,-0.9212782741215262] Loss: 23.643673483875965\n",
      "Iteracion: 3800 Gradiente: [0.053370693223648874,-0.9207884943125831] Loss: 23.642822105125372\n",
      "Iteracion: 3801 Gradiente: [0.053342319724851,-0.9202989748856273] Loss: 23.641971631372297\n",
      "Iteracion: 3802 Gradiente: [0.0533139613103297,-0.9198097157022199] Loss: 23.641122061654762\n",
      "Iteracion: 3803 Gradiente: [0.05328561797207101,-0.9193207166240095] Loss: 23.640273395011786\n",
      "Iteracion: 3804 Gradiente: [0.053257289701980425,-0.9188319775127198] Loss: 23.639425630483437\n",
      "Iteracion: 3805 Gradiente: [0.05322897649202408,-0.9183434982301458] Loss: 23.638578767110765\n",
      "Iteracion: 3806 Gradiente: [0.05320067833422494,-0.9178552786381535] Loss: 23.637732803935865\n",
      "Iteracion: 3807 Gradiente: [0.05317239522068557,-0.9173673185986763] Loss: 23.636887740001853\n",
      "Iteracion: 3808 Gradiente: [0.0531441271432963,-0.9168796179737361] Loss: 23.636043574352865\n",
      "Iteracion: 3809 Gradiente: [0.05311587409406968,-0.9163921766254182] Loss: 23.63520030603401\n",
      "Iteracion: 3810 Gradiente: [0.05308763606500027,-0.9159049944158847] Loss: 23.634357934091486\n",
      "Iteracion: 3811 Gradiente: [0.05305941304816978,-0.9154180712073648] Loss: 23.63351645757243\n",
      "Iteracion: 3812 Gradiente: [0.05303120503556234,-0.9149314068621694] Loss: 23.632675875525038\n",
      "Iteracion: 3813 Gradiente: [0.05300301201921798,-0.9144450012426776] Loss: 23.63183618699851\n",
      "Iteracion: 3814 Gradiente: [0.05297483399109808,-0.9139588542113444] Loss: 23.63099739104305\n",
      "Iteracion: 3815 Gradiente: [0.05294667094330426,-0.9134729656306952] Loss: 23.630159486709868\n",
      "Iteracion: 3816 Gradiente: [0.05291852286777707,-0.9129873353633341] Loss: 23.629322473051218\n",
      "Iteracion: 3817 Gradiente: [0.052890389756775374,-0.9125019632719209] Loss: 23.628486349120287\n",
      "Iteracion: 3818 Gradiente: [0.052862271602111835,-0.9120168492192146] Loss: 23.627651113971343\n",
      "Iteracion: 3819 Gradiente: [0.05283416839588805,-0.911531993068031] Loss: 23.626816766659594\n",
      "Iteracion: 3820 Gradiente: [0.052806080130270064,-0.9110473946812564] Loss: 23.625983306241338\n",
      "Iteracion: 3821 Gradiente: [0.05277800679713115,-0.9105630539218638] Loss: 23.62515073177381\n",
      "Iteracion: 3822 Gradiente: [0.05274994838867239,-0.9100789706528841] Loss: 23.624319042315236\n",
      "Iteracion: 3823 Gradiente: [0.05272190489702287,-0.9095951447374233] Loss: 23.623488236924903\n",
      "Iteracion: 3824 Gradiente: [0.052693876314141144,-0.9091115760386707] Loss: 23.622658314663042\n",
      "Iteracion: 3825 Gradiente: [0.05266586263208429,-0.908628264419883] Loss: 23.621829274590926\n",
      "Iteracion: 3826 Gradiente: [0.05263786384288664,-0.9081452097443911] Loss: 23.62100111577079\n",
      "Iteracion: 3827 Gradiente: [0.052609879938807086,-0.9076624118755874] Loss: 23.620173837265902\n",
      "Iteracion: 3828 Gradiente: [0.0525819109118089,-0.9071798706769502] Loss: 23.619347438140473\n",
      "Iteracion: 3829 Gradiente: [0.05255395675395012,-0.906697586012029] Loss: 23.618521917459773\n",
      "Iteracion: 3830 Gradiente: [0.05252601745740056,-0.9062155577444372] Loss: 23.617697274290013\n",
      "Iteracion: 3831 Gradiente: [0.05249809301427983,-0.9057337857378666] Loss: 23.616873507698426\n",
      "Iteracion: 3832 Gradiente: [0.0524701834165351,-0.905252269856089] Loss: 23.616050616753228\n",
      "Iteracion: 3833 Gradiente: [0.052442288656457475,-0.9047710099629306] Loss: 23.615228600523633\n",
      "Iteracion: 3834 Gradiente: [0.05241440872614476,-0.9042900059223004] Loss: 23.614407458079818\n",
      "Iteracion: 3835 Gradiente: [0.05238654361756024,-0.9038092575981875] Loss: 23.613587188492982\n",
      "Iteracion: 3836 Gradiente: [0.052358693323013954,-0.9033287648546349] Loss: 23.6127677908353\n",
      "Iteracion: 3837 Gradiente: [0.05233085783459425,-0.9028485275557704] Loss: 23.611949264179916\n",
      "Iteracion: 3838 Gradiente: [0.05230303714419051,-0.9023685455658061] Loss: 23.61113160760099\n",
      "Iteracion: 3839 Gradiente: [0.0522752312441933,-0.9018888187489958] Loss: 23.610314820173624\n",
      "Iteracion: 3840 Gradiente: [0.05224744012674023,-0.9014093469696829] Loss: 23.60949890097395\n",
      "Iteracion: 3841 Gradiente: [0.05221966378382963,-0.9009301300922881] Loss: 23.608683849079046\n",
      "Iteracion: 3842 Gradiente: [0.05219190220771092,-0.9004511679812935] Loss: 23.607869663567\n",
      "Iteracion: 3843 Gradiente: [0.05216415539046011,-0.8999724605012609] Loss: 23.607056343516874\n",
      "Iteracion: 3844 Gradiente: [0.05213642332432281,-0.8994940075168145] Loss: 23.606243888008667\n",
      "Iteracion: 3845 Gradiente: [0.052108706001323904,-0.899015808892666] Loss: 23.605432296123393\n",
      "Iteracion: 3846 Gradiente: [0.05208100341378573,-0.8985378644935795] Loss: 23.604621566943084\n",
      "Iteracion: 3847 Gradiente: [0.05205331555378147,-0.8980601741844062] Loss: 23.603811699550665\n",
      "Iteracion: 3848 Gradiente: [0.05202564241351411,-0.8975827378300626] Loss: 23.60300269303007\n",
      "Iteracion: 3849 Gradiente: [0.051997983985055875,-0.8971055552955448] Loss: 23.60219454646622\n",
      "Iteracion: 3850 Gradiente: [0.0519703402607244,-0.896628626445906] Loss: 23.601387258945014\n",
      "Iteracion: 3851 Gradiente: [0.05194271123253032,-0.8961519511462896] Loss: 23.60058082955329\n",
      "Iteracion: 3852 Gradiente: [0.05191509689290683,-0.8956755292618865] Loss: 23.59977525737888\n",
      "Iteracion: 3853 Gradiente: [0.05188749723391955,-0.8951993606579798] Loss: 23.598970541510578\n",
      "Iteracion: 3854 Gradiente: [0.05185991224766061,-0.8947234451999255] Loss: 23.59816668103816\n",
      "Iteracion: 3855 Gradiente: [0.05183234192645519,-0.8942477827531351] Loss: 23.59736367505234\n",
      "Iteracion: 3856 Gradiente: [0.0518047862624788,-0.8937723731831007] Loss: 23.59656152264482\n",
      "Iteracion: 3857 Gradiente: [0.051777245247999794,-0.8932972163553827] Loss: 23.59576022290828\n",
      "Iteracion: 3858 Gradiente: [0.051749718875173774,-0.8928223121356192] Loss: 23.59495977493632\n",
      "Iteracion: 3859 Gradiente: [0.05172220713612698,-0.8923476603895222] Loss: 23.594160177823575\n",
      "Iteracion: 3860 Gradiente: [0.051694710023214914,-0.8918732609828586] Loss: 23.593361430665563\n",
      "Iteracion: 3861 Gradiente: [0.0516672275285733,-0.8913991137814842] Loss: 23.59256353255882\n",
      "Iteracion: 3862 Gradiente: [0.051639759644528264,-0.8909252186513117] Loss: 23.59176648260082\n",
      "Iteracion: 3863 Gradiente: [0.051612306363244896,-0.8904515754583376] Loss: 23.59097027988998\n",
      "Iteracion: 3864 Gradiente: [0.05158486767698112,-0.8899781840686216] Loss: 23.590174923525726\n",
      "Iteracion: 3865 Gradiente: [0.051557443577928554,-0.8895050443483025] Loss: 23.589380412608374\n",
      "Iteracion: 3866 Gradiente: [0.051530034058371636,-0.8890321561635811] Loss: 23.58858674623927\n",
      "Iteracion: 3867 Gradiente: [0.05150263911050293,-0.8885595193807364] Loss: 23.587793923520653\n",
      "Iteracion: 3868 Gradiente: [0.051475258726670366,-0.8880871338661114] Loss: 23.587001943555727\n",
      "Iteracion: 3869 Gradiente: [0.051447892899176395,-0.8876149994861211] Loss: 23.586210805448694\n",
      "Iteracion: 3870 Gradiente: [0.05142054162013684,-0.8871431161072607] Loss: 23.585420508304665\n",
      "Iteracion: 3871 Gradiente: [0.051393204881849404,-0.8866714835960913] Loss: 23.584631051229696\n",
      "Iteracion: 3872 Gradiente: [0.05136588267664972,-0.8862001018192401] Loss: 23.583842433330855\n",
      "Iteracion: 3873 Gradiente: [0.05133857499678811,-0.8857289706434093] Loss: 23.583054653716076\n",
      "Iteracion: 3874 Gradiente: [0.051311281834525366,-0.885258089935372] Loss: 23.582267711494303\n",
      "Iteracion: 3875 Gradiente: [0.051284003182082455,-0.8847874595619767] Loss: 23.581481605775377\n",
      "Iteracion: 3876 Gradiente: [0.05125673903184804,-0.8843170793901317] Loss: 23.580696335670133\n",
      "Iteracion: 3877 Gradiente: [0.05122948937609901,-0.883846949286824] Loss: 23.57991190029036\n",
      "Iteracion: 3878 Gradiente: [0.05120225420699285,-0.8833770691191141] Loss: 23.579128298748707\n",
      "Iteracion: 3879 Gradiente: [0.05117503351703666,-0.8829074387541188] Loss: 23.578345530158852\n",
      "Iteracion: 3880 Gradiente: [0.05114782729842109,-0.8824380580590431] Loss: 23.57756359363538\n",
      "Iteracion: 3881 Gradiente: [0.05112063554347704,-0.8819689269011496] Loss: 23.576782488293837\n",
      "Iteracion: 3882 Gradiente: [0.05109345824443021,-0.8815000451477848] Loss: 23.576002213250653\n",
      "Iteracion: 3883 Gradiente: [0.05106629539372136,-0.8810314126663494] Loss: 23.575222767623288\n",
      "Iteracion: 3884 Gradiente: [0.05103914698369275,-0.8805630293243222] Loss: 23.574444150530066\n",
      "Iteracion: 3885 Gradiente: [0.05101201300652273,-0.8800948949892591] Loss: 23.573666361090257\n",
      "Iteracion: 3886 Gradiente: [0.05098489345456301,-0.8796270095287809] Loss: 23.57288939842412\n",
      "Iteracion: 3887 Gradiente: [0.05095778832027425,-0.8791593728105708] Loss: 23.57211326165279\n",
      "Iteracion: 3888 Gradiente: [0.050930697595855653,-0.8786919847023957] Loss: 23.571337949898354\n",
      "Iteracion: 3889 Gradiente: [0.05090362127373851,-0.8782248450720839] Loss: 23.570563462283847\n",
      "Iteracion: 3890 Gradiente: [0.05087655934621201,-0.8777579537875385] Loss: 23.569789797933222\n",
      "Iteracion: 3891 Gradiente: [0.05084951180562124,-0.8772913107167322] Loss: 23.56901695597137\n",
      "Iteracion: 3892 Gradiente: [0.050822478644361506,-0.8768249157277045] Loss: 23.56824493552411\n",
      "Iteracion: 3893 Gradiente: [0.05079545985463388,-0.8763587686885757] Loss: 23.567473735718185\n",
      "Iteracion: 3894 Gradiente: [0.05076845542907809,-0.875892869467513] Loss: 23.566703355681266\n",
      "Iteracion: 3895 Gradiente: [0.05074146535982796,-0.8754272179327809] Loss: 23.565933794541976\n",
      "Iteracion: 3896 Gradiente: [0.05071448963927499,-0.8749618139527011] Loss: 23.56516505142981\n",
      "Iteracion: 3897 Gradiente: [0.050687528259902595,-0.8744966573956588] Loss: 23.564397125475264\n",
      "Iteracion: 3898 Gradiente: [0.050660581214028375,-0.87403174813012] Loss: 23.563630015809686\n",
      "Iteracion: 3899 Gradiente: [0.05063364849407795,-0.8735670860246145] Loss: 23.562863721565392\n",
      "Iteracion: 3900 Gradiente: [0.050606730092367984,-0.8731026709477473] Loss: 23.56209824187558\n",
      "Iteracion: 3901 Gradiente: [0.05057982600135062,-0.8726385027681883] Loss: 23.561333575874436\n",
      "Iteracion: 3902 Gradiente: [0.05055293621330463,-0.8721745813546858] Loss: 23.560569722697004\n",
      "Iteracion: 3903 Gradiente: [0.05052606072067931,-0.8717109065760472] Loss: 23.559806681479298\n",
      "Iteracion: 3904 Gradiente: [0.05049919951589269,-0.8712474783011518] Loss: 23.55904445135818\n",
      "Iteracion: 3905 Gradiente: [0.0504723525914225,-0.8707842963989499] Loss: 23.558283031471483\n",
      "Iteracion: 3906 Gradiente: [0.050445519939529504,-0.8703213607384667] Loss: 23.557522420957955\n",
      "Iteracion: 3907 Gradiente: [0.050418701552691424,-0.8698586711887922] Loss: 23.556762618957237\n",
      "Iteracion: 3908 Gradiente: [0.05039189742338692,-0.8693962276190808] Loss: 23.556003624609918\n",
      "Iteracion: 3909 Gradiente: [0.05036510754399141,-0.8689340298985642] Loss: 23.555245437057472\n",
      "Iteracion: 3910 Gradiente: [0.050338331906833865,-0.8684720778965485] Loss: 23.554488055442288\n",
      "Iteracion: 3911 Gradiente: [0.05031157050446022,-0.8680103714823943] Loss: 23.55373147890767\n",
      "Iteracion: 3912 Gradiente: [0.05028482332927713,-0.8675489105255413] Loss: 23.552975706597845\n",
      "Iteracion: 3913 Gradiente: [0.050258090373711185,-0.8670876948954984] Loss: 23.55222073765792\n",
      "Iteracion: 3914 Gradiente: [0.05023137163026945,-0.8666267244618365] Loss: 23.55146657123395\n",
      "Iteracion: 3915 Gradiente: [0.05020466709125534,-0.866165999094212] Loss: 23.55071320647287\n",
      "Iteracion: 3916 Gradiente: [0.05017797674915793,-0.8657055186623362] Loss: 23.5499606425225\n",
      "Iteracion: 3917 Gradiente: [0.05015130059652885,-0.8652452830359885] Loss: 23.549208878531644\n",
      "Iteracion: 3918 Gradiente: [0.050124638625719815,-0.8647852920850295] Loss: 23.548457913649948\n",
      "Iteracion: 3919 Gradiente: [0.05009799082923034,-0.8643255456793794] Loss: 23.547707747027957\n",
      "Iteracion: 3920 Gradiente: [0.05007135719953718,-0.8638660436890303] Loss: 23.546958377817173\n",
      "Iteracion: 3921 Gradiente: [0.05004473772910671,-0.8634067859840419] Loss: 23.54620980516993\n",
      "Iteracion: 3922 Gradiente: [0.05001813241037875,-0.8629477724345482] Loss: 23.545462028239502\n",
      "Iteracion: 3923 Gradiente: [0.04999154123597312,-0.86248900291074] Loss: 23.544715046180055\n",
      "Iteracion: 3924 Gradiente: [0.04996496419817428,-0.8620304772828971] Loss: 23.543968858146695\n",
      "Iteracion: 3925 Gradiente: [0.04993840128960869,-0.8615721954213489] Loss: 23.543223463295366\n",
      "Iteracion: 3926 Gradiente: [0.049911852502665964,-0.8611141571965064] Loss: 23.54247886078291\n",
      "Iteracion: 3927 Gradiente: [0.0498853178298314,-0.8606563624788469] Loss: 23.541735049767137\n",
      "Iteracion: 3928 Gradiente: [0.049858797263607355,-0.860198811138913] Loss: 23.54099202940665\n",
      "Iteracion: 3929 Gradiente: [0.04983229079652176,-0.859741503047316] Loss: 23.540249798861023\n",
      "Iteracion: 3930 Gradiente: [0.04980579842119539,-0.8592844380747309] Loss: 23.539508357290696\n",
      "Iteracion: 3931 Gradiente: [0.04977932012996386,-0.8588276160919186] Loss: 23.53876770385704\n",
      "Iteracion: 3932 Gradiente: [0.04975285591538731,-0.8583710369696954] Loss: 23.538027837722222\n",
      "Iteracion: 3933 Gradiente: [0.04972640577006473,-0.8579147005789456] Loss: 23.537288758049392\n",
      "Iteracion: 3934 Gradiente: [0.04969996968635352,-0.8574586067906333] Loss: 23.536550464002566\n",
      "Iteracion: 3935 Gradiente: [0.04967354765692467,-0.8570027554757762] Loss: 23.535812954746625\n",
      "Iteracion: 3936 Gradiente: [0.049647139674313695,-0.8565471465054662] Loss: 23.535076229447352\n",
      "Iteracion: 3937 Gradiente: [0.049620745730972735,-0.8560917797508713] Loss: 23.534340287271426\n",
      "Iteracion: 3938 Gradiente: [0.049594365819440135,-0.8556366550832218] Loss: 23.5336051273864\n",
      "Iteracion: 3939 Gradiente: [0.04956799993231774,-0.855181772373813] Loss: 23.532870748960722\n",
      "Iteracion: 3940 Gradiente: [0.04954164806205767,-0.854727131494018] Loss: 23.532137151163706\n",
      "Iteracion: 3941 Gradiente: [0.04951531020122672,-0.854272732315273] Loss: 23.53140433316556\n",
      "Iteracion: 3942 Gradiente: [0.04948898634251577,-0.8538185747090713] Loss: 23.53067229413739\n",
      "Iteracion: 3943 Gradiente: [0.049462676478338116,-0.853364658546996] Loss: 23.529941033251134\n",
      "Iteracion: 3944 Gradiente: [0.049436380601225474,-0.85291098370069] Loss: 23.5292105496797\n",
      "Iteracion: 3945 Gradiente: [0.04941009870390758,-0.8524575500418514] Loss: 23.528480842596775\n",
      "Iteracion: 3946 Gradiente: [0.04938383077883752,-0.8520043574422644] Loss: 23.527751911176974\n",
      "Iteracion: 3947 Gradiente: [0.04935757681853185,-0.8515514057737777] Loss: 23.527023754595785\n",
      "Iteracion: 3948 Gradiente: [0.0493313368156701,-0.8510986949083013] Loss: 23.52629637202958\n",
      "Iteracion: 3949 Gradiente: [0.04930511076277734,-0.8506462247178164] Loss: 23.525569762655593\n",
      "Iteracion: 3950 Gradiente: [0.04927889865250374,-0.8501939950743704] Loss: 23.52484392565193\n",
      "Iteracion: 3951 Gradiente: [0.04925270047735068,-0.8497420058500876] Loss: 23.52411886019759\n",
      "Iteracion: 3952 Gradiente: [0.04922651622994276,-0.8492902569171512] Loss: 23.52339456547243\n",
      "Iteracion: 3953 Gradiente: [0.049200345902983146,-0.8488387481478061] Loss: 23.52267104065716\n",
      "Iteracion: 3954 Gradiente: [0.049174189488912626,-0.8483874794143861] Loss: 23.521948284933426\n",
      "Iteracion: 3955 Gradiente: [0.04914804698036145,-0.8479364505892782] Loss: 23.521226297483643\n",
      "Iteracion: 3956 Gradiente: [0.049121918369981664,-0.8474856615449381] Loss: 23.520505077491205\n",
      "Iteracion: 3957 Gradiente: [0.04909580365037603,-0.8470351121538897] Loss: 23.519784624140296\n",
      "Iteracion: 3958 Gradiente: [0.049069702814214605,-0.8465848022887243] Loss: 23.519064936615994\n",
      "Iteracion: 3959 Gradiente: [0.0490436158540831,-0.8461347318221034] Loss: 23.518346014104235\n",
      "Iteracion: 3960 Gradiente: [0.04901754276249335,-0.8456849006267634] Loss: 23.517627855791837\n",
      "Iteracion: 3961 Gradiente: [0.04899148353215897,-0.8452353085754936] Loss: 23.51691046086648\n",
      "Iteracion: 3962 Gradiente: [0.048965438155742426,-0.8447859555411561] Loss: 23.51619382851669\n",
      "Iteracion: 3963 Gradiente: [0.04893940662586734,-0.844336841396684] Loss: 23.515477957931864\n",
      "Iteracion: 3964 Gradiente: [0.04891338893507395,-0.8438879660150813] Loss: 23.514762848302247\n",
      "Iteracion: 3965 Gradiente: [0.04888738507620095,-0.8434393292694023] Loss: 23.514048498818987\n",
      "Iteracion: 3966 Gradiente: [0.04886139504172983,-0.8429909310327914] Loss: 23.513334908674075\n",
      "Iteracion: 3967 Gradiente: [0.048835418824210323,-0.842542771178455] Loss: 23.512622077060293\n",
      "Iteracion: 3968 Gradiente: [0.04880945641662701,-0.8420948495796429] Loss: 23.51191000317139\n",
      "Iteracion: 3969 Gradiente: [0.04878350781138181,-0.8416471661097058] Loss: 23.511198686201908\n",
      "Iteracion: 3970 Gradiente: [0.04875757300129067,-0.8411997206420381] Loss: 23.51048812534727\n",
      "Iteracion: 3971 Gradiente: [0.048731651978833196,-0.8407525130501222] Loss: 23.50977831980371\n",
      "Iteracion: 3972 Gradiente: [0.04870574473681018,-0.8403055432074863] Loss: 23.50906926876836\n",
      "Iteracion: 3973 Gradiente: [0.04867985126784144,-0.8398588109877423] Loss: 23.508360971439206\n",
      "Iteracion: 3974 Gradiente: [0.04865397156472397,-0.8394123162645518] Loss: 23.507653427015065\n",
      "Iteracion: 3975 Gradiente: [0.04862810562002646,-0.8389660589116628] Loss: 23.50694663469562\n",
      "Iteracion: 3976 Gradiente: [0.04860225342645587,-0.8385200388028814] Loss: 23.506240593681405\n",
      "Iteracion: 3977 Gradiente: [0.04857641497669078,-0.8380742558120798] Loss: 23.50553530317377\n",
      "Iteracion: 3978 Gradiente: [0.04855059026346377,-0.8376287098131987] Loss: 23.504830762374972\n",
      "Iteracion: 3979 Gradiente: [0.048524779279502654,-0.8371834006802433] Loss: 23.504126970488088\n",
      "Iteracion: 3980 Gradiente: [0.048498982017383695,-0.8367383282872954] Loss: 23.503423926717016\n",
      "Iteracion: 3981 Gradiente: [0.04847319846982524,-0.8362934925084967] Loss: 23.502721630266542\n",
      "Iteracion: 3982 Gradiente: [0.04844742862969724,-0.8358488932180457] Loss: 23.502020080342287\n",
      "Iteracion: 3983 Gradiente: [0.04842167248951815,-0.8354045302902298] Loss: 23.5013192761507\n",
      "Iteracion: 3984 Gradiente: [0.04839593004213706,-0.8349604035993825] Loss: 23.500619216899068\n",
      "Iteracion: 3985 Gradiente: [0.04837020128021644,-0.8345165130199177] Loss: 23.499919901795547\n",
      "Iteracion: 3986 Gradiente: [0.04834448619655044,-0.8340728584263066] Loss: 23.49922133004914\n",
      "Iteracion: 3987 Gradiente: [0.04831878478375131,-0.8336294396930995] Loss: 23.49852350086966\n",
      "Iteracion: 3988 Gradiente: [0.04829309703463878,-0.8331862566948999] Loss: 23.49782641346777\n",
      "Iteracion: 3989 Gradiente: [0.048267422941884776,-0.832743309306387] Loss: 23.49713006705499\n",
      "Iteracion: 3990 Gradiente: [0.04824176249830809,-0.8323005974023007] Loss: 23.49643446084365\n",
      "Iteracion: 3991 Gradiente: [0.048216115696544645,-0.8318581208574545] Loss: 23.495739594046924\n",
      "Iteracion: 3992 Gradiente: [0.04819048252949377,-0.8314158795467163] Loss: 23.49504546587885\n",
      "Iteracion: 3993 Gradiente: [0.04816486298976201,-0.8309738733450369] Loss: 23.494352075554286\n",
      "Iteracion: 3994 Gradiente: [0.04813925707017385,-0.8305321021274215] Loss: 23.493659422288896\n",
      "Iteracion: 3995 Gradiente: [0.04811366476353195,-0.8300905657689412] Loss: 23.492967505299227\n",
      "Iteracion: 3996 Gradiente: [0.04808808606254047,-0.8296492641447436] Loss: 23.492276323802603\n",
      "Iteracion: 3997 Gradiente: [0.04806252095993007,-0.8292081971300375] Loss: 23.491585877017233\n",
      "Iteracion: 3998 Gradiente: [0.04803696944843144,-0.8287673646000997] Loss: 23.490896164162148\n",
      "Iteracion: 3999 Gradiente: [0.04801143152103388,-0.8283267664302573] Loss: 23.490207184457145\n",
      "Iteracion: 4000 Gradiente: [0.04798590717032975,-0.827886402495929] Loss: 23.48951893712296\n",
      "Iteracion: 4001 Gradiente: [0.04796039638915677,-0.8274462726725855] Loss: 23.488831421381065\n",
      "Iteracion: 4002 Gradiente: [0.04793489917029869,-0.8270063768357658] Loss: 23.488144636453796\n",
      "Iteracion: 4003 Gradiente: [0.04790941550657143,-0.8265667148610738] Loss: 23.487458581564308\n",
      "Iteracion: 4004 Gradiente: [0.0478839453907445,-0.8261272866241828] Loss: 23.48677325593661\n",
      "Iteracion: 4005 Gradiente: [0.047858488815528705,-0.8256880920008351] Loss: 23.48608865879549\n",
      "Iteracion: 4006 Gradiente: [0.04783304577385934,-0.8252491308668283] Loss: 23.48540478936658\n",
      "Iteracion: 4007 Gradiente: [0.047807616258534344,-0.8248104030980314] Loss: 23.484721646876377\n",
      "Iteracion: 4008 Gradiente: [0.04778220026226355,-0.8243719085703863] Loss: 23.484039230552085\n",
      "Iteracion: 4009 Gradiente: [0.047756797777969005,-0.8239336471598875] Loss: 23.483357539621878\n",
      "Iteracion: 4010 Gradiente: [0.04773140879844202,-0.8234956187426065] Loss: 23.482676573314645\n",
      "Iteracion: 4011 Gradiente: [0.0477060333163422,-0.8230578231946839] Loss: 23.48199633086013\n",
      "Iteracion: 4012 Gradiente: [0.047680671324666454,-0.8226202603923115] Loss: 23.48131681148891\n",
      "Iteracion: 4013 Gradiente: [0.047655322816226926,-0.8221829302117546] Loss: 23.480638014432333\n",
      "Iteracion: 4014 Gradiente: [0.047629987783921024,-0.8217458325293389] Loss: 23.47995993892261\n",
      "Iteracion: 4015 Gradiente: [0.0476046662204709,-0.8213089672214697] Loss: 23.479282584192738\n",
      "Iteracion: 4016 Gradiente: [0.047579358118629976,-0.8208723341646141] Loss: 23.478605949476577\n",
      "Iteracion: 4017 Gradiente: [0.04755406347143302,-0.8204359332352898] Loss: 23.477930034008732\n",
      "Iteracion: 4018 Gradiente: [0.04752878227158514,-0.8199997643100976] Loss: 23.477254837024685\n",
      "Iteracion: 4019 Gradiente: [0.04750351451202448,-0.8195638272656927] Loss: 23.476580357760685\n",
      "Iteracion: 4020 Gradiente: [0.04747826018565509,-0.8191281219787977] Loss: 23.475906595453804\n",
      "Iteracion: 4021 Gradiente: [0.047453019285242704,-0.8186926483262081] Loss: 23.475233549341958\n",
      "Iteracion: 4022 Gradiente: [0.04742779180360893,-0.8182574061847822] Loss: 23.474561218663826\n",
      "Iteracion: 4023 Gradiente: [0.0474025777336692,-0.8178223954314395] Loss: 23.47388960265893\n",
      "Iteracion: 4024 Gradiente: [0.04737737706828208,-0.8173876159431662] Loss: 23.473218700567564\n",
      "Iteracion: 4025 Gradiente: [0.047352189800384775,-0.8169530675970132] Loss: 23.472548511630887\n",
      "Iteracion: 4026 Gradiente: [0.04732701592287754,-0.8165187502700948] Loss: 23.4718790350908\n",
      "Iteracion: 4027 Gradiente: [0.047301855428494834,-0.8160846638396025] Loss: 23.471210270190063\n",
      "Iteracion: 4028 Gradiente: [0.047276708310214605,-0.8156508081827821] Loss: 23.470542216172205\n",
      "Iteracion: 4029 Gradiente: [0.04725157456099585,-0.8152171831769394] Loss: 23.469874872281594\n",
      "Iteracion: 4030 Gradiente: [0.047226454173537984,-0.8147837886994668] Loss: 23.469208237763336\n",
      "Iteracion: 4031 Gradiente: [0.047201347140947786,-0.8143506246277952] Loss: 23.468542311863434\n",
      "Iteracion: 4032 Gradiente: [0.04717625345600425,-0.8139176908394413] Loss: 23.46787709382861\n",
      "Iteracion: 4033 Gradiente: [0.0471511731116171,-0.8134849872119797] Loss: 23.467212582906424\n",
      "Iteracion: 4034 Gradiente: [0.047126106100659135,-0.8130525136230512] Loss: 23.46654877834523\n",
      "Iteracion: 4035 Gradiente: [0.04710105241613007,-0.8126202699503546] Loss: 23.465885679394194\n",
      "Iteracion: 4036 Gradiente: [0.047076012050939656,-0.8121882560716607] Loss: 23.465223285303242\n",
      "Iteracion: 4037 Gradiente: [0.04705098499788486,-0.8117564718648099] Loss: 23.464561595323158\n",
      "Iteracion: 4038 Gradiente: [0.04702597125007344,-0.811324917207692] Loss: 23.463900608705455\n",
      "Iteracion: 4039 Gradiente: [0.04700097080026258,-0.8108935919782806] Loss: 23.463240324702486\n",
      "Iteracion: 4040 Gradiente: [0.046975983641555294,-0.8104624960545955] Loss: 23.462580742567393\n",
      "Iteracion: 4041 Gradiente: [0.046951009766786456,-0.8100316293147352] Loss: 23.461921861554092\n",
      "Iteracion: 4042 Gradiente: [0.04692604916886201,-0.8096009916368611] Loss: 23.46126368091731\n",
      "Iteracion: 4043 Gradiente: [0.04690110184070022,-0.8091705828991986] Loss: 23.460606199912583\n",
      "Iteracion: 4044 Gradiente: [0.046876167775330185,-0.8087404029800306] Loss: 23.459949417796167\n",
      "Iteracion: 4045 Gradiente: [0.04685124696565879,-0.8083104517577127] Loss: 23.45929333382522\n",
      "Iteracion: 4046 Gradiente: [0.04682633940468861,-0.80788072911066] Loss: 23.458637947257582\n",
      "Iteracion: 4047 Gradiente: [0.04680144508537107,-0.8074512349173553] Loss: 23.45798325735194\n",
      "Iteracion: 4048 Gradiente: [0.04677656400056947,-0.8070219690563503] Loss: 23.45732926336778\n",
      "Iteracion: 4049 Gradiente: [0.04675169614330533,-0.8065929314062547] Loss: 23.45667596456532\n",
      "Iteracion: 4050 Gradiente: [0.04672684150662102,-0.8061641218457396] Loss: 23.45602336020561\n",
      "Iteracion: 4051 Gradiente: [0.04670200008345091,-0.8057355402535469] Loss: 23.45537144955047\n",
      "Iteracion: 4052 Gradiente: [0.0466771718667502,-0.8053071865084837] Loss: 23.45472023186252\n",
      "Iteracion: 4053 Gradiente: [0.04665235684938788,-0.8048790604894253] Loss: 23.45406970640512\n",
      "Iteracion: 4054 Gradiente: [0.046627555024468845,-0.8044511620752994] Loss: 23.453419872442492\n",
      "Iteracion: 4055 Gradiente: [0.046602766385076204,-0.8040234911450971] Loss: 23.452770729239532\n",
      "Iteracion: 4056 Gradiente: [0.04657799092396052,-0.8035960475778966] Loss: 23.452122276062024\n",
      "Iteracion: 4057 Gradiente: [0.04655322863428637,-0.8031688312528132] Loss: 23.451474512176468\n",
      "Iteracion: 4058 Gradiente: [0.04652847950899665,-0.802741842049043] Loss: 23.450827436850158\n",
      "Iteracion: 4059 Gradiente: [0.04650374354109677,-0.80231507984584] Loss: 23.45018104935119\n",
      "Iteracion: 4060 Gradiente: [0.046479020723597844,-0.8018885445225242] Loss: 23.44953534894839\n",
      "Iteracion: 4061 Gradiente: [0.04645431104955738,-0.8014622359584742] Loss: 23.44889033491141\n",
      "Iteracion: 4062 Gradiente: [0.046429614511832064,-0.8010361540331492] Loss: 23.448246006510644\n",
      "Iteracion: 4063 Gradiente: [0.04640493110368974,-0.8006102986260454] Loss: 23.447602363017268\n",
      "Iteracion: 4064 Gradiente: [0.04638026081788667,-0.8001846696167547] Loss: 23.44695940370326\n",
      "Iteracion: 4065 Gradiente: [0.046355603647586466,-0.799759266884909] Loss: 23.446317127841358\n",
      "Iteracion: 4066 Gradiente: [0.04633095958579266,-0.7993340903102134] Loss: 23.445675534705053\n",
      "Iteracion: 4067 Gradiente: [0.04630632862556278,-0.7989091397724337] Loss: 23.44503462356862\n",
      "Iteracion: 4068 Gradiente: [0.046281710759972346,-0.7984844151513997] Loss: 23.444394393707103\n",
      "Iteracion: 4069 Gradiente: [0.046257105981849615,-0.7980599163270203] Loss: 23.443754844396334\n",
      "Iteracion: 4070 Gradiente: [0.0462325142845098,-0.7976356431792371] Loss: 23.443115974912907\n",
      "Iteracion: 4071 Gradiente: [0.046207935660838945,-0.7972115955880852] Loss: 23.442477784534166\n",
      "Iteracion: 4072 Gradiente: [0.046183370103827316,-0.7967877734336537] Loss: 23.441840272538244\n",
      "Iteracion: 4073 Gradiente: [0.046158817606739906,-0.7963641765960818] Loss: 23.44120343820401\n",
      "Iteracion: 4074 Gradiente: [0.046134278162417296,-0.795940804955597] Loss: 23.440567280811155\n",
      "Iteracion: 4075 Gradiente: [0.046109751764105526,-0.7955176583924649] Loss: 23.43993179964011\n",
      "Iteracion: 4076 Gradiente: [0.04608523840472382,-0.7950947367870385] Loss: 23.439296993972032\n",
      "Iteracion: 4077 Gradiente: [0.04606073807743769,-0.794672040019717] Loss: 23.438662863088926\n",
      "Iteracion: 4078 Gradiente: [0.04603625077524498,-0.7942495679709729] Loss: 23.43802940627345\n",
      "Iteracion: 4079 Gradiente: [0.046011776491253424,-0.7938273205213378] Loss: 23.43739662280912\n",
      "Iteracion: 4080 Gradiente: [0.04598731521850728,-0.7934052975514092] Loss: 23.436764511980176\n",
      "Iteracion: 4081 Gradiente: [0.045962866950185345,-0.7929834989418426] Loss: 23.436133073071623\n",
      "Iteracion: 4082 Gradiente: [0.04593843167928166,-0.7925619245733652] Loss: 23.435502305369194\n",
      "Iteracion: 4083 Gradiente: [0.04591400939890965,-0.7921405743267632] Loss: 23.43487220815945\n",
      "Iteracion: 4084 Gradiente: [0.04588960010217041,-0.7917194480828854] Loss: 23.434242780729654\n",
      "Iteracion: 4085 Gradiente: [0.04586520378230906,-0.7912985457226367] Loss: 23.433614022367838\n",
      "Iteracion: 4086 Gradiente: [0.04584082043217942,-0.790877867127008] Loss: 23.432985932362815\n",
      "Iteracion: 4087 Gradiente: [0.045816450044909134,-0.7904574121770376] Loss: 23.43235851000412\n",
      "Iteracion: 4088 Gradiente: [0.045792092613712046,-0.7900371807538219] Loss: 23.431731754582074\n",
      "Iteracion: 4089 Gradiente: [0.045767748131656086,-0.7896171727385327] Loss: 23.431105665387715\n",
      "Iteracion: 4090 Gradiente: [0.045743416591997745,-0.7891973880123876] Loss: 23.43048024171288\n",
      "Iteracion: 4091 Gradiente: [0.04571909798770169,-0.7887778264566907] Loss: 23.42985548285012\n",
      "Iteracion: 4092 Gradiente: [0.04569479231184725,-0.7883584879527997] Loss: 23.42923138809277\n",
      "Iteracion: 4093 Gradiente: [0.0456704995577032,-0.7879393723821243] Loss: 23.428607956734872\n",
      "Iteracion: 4094 Gradiente: [0.04564621971825318,-0.7875204796261566] Loss: 23.427985188071283\n",
      "Iteracion: 4095 Gradiente: [0.04562195278674892,-0.7871018095664326] Loss: 23.427363081397555\n",
      "Iteracion: 4096 Gradiente: [0.04559769875629627,-0.7866833620845632] Loss: 23.42674163601\n",
      "Iteracion: 4097 Gradiente: [0.045573457620020005,-0.7862651370622208] Loss: 23.426120851205678\n",
      "Iteracion: 4098 Gradiente: [0.04554922937105535,-0.7858471343811376] Loss: 23.425500726282444\n",
      "Iteracion: 4099 Gradiente: [0.045525014002600984,-0.7854293539231091] Loss: 23.424881260538804\n",
      "Iteracion: 4100 Gradiente: [0.0455008115078518,-0.7850117955699915] Loss: 23.424262453274114\n",
      "Iteracion: 4101 Gradiente: [0.04547662187973553,-0.78459445920372] Loss: 23.423644303788386\n",
      "Iteracion: 4102 Gradiente: [0.04545244511160433,-0.784177344706269] Loss: 23.423026811382428\n",
      "Iteracion: 4103 Gradiente: [0.04542828119660006,-0.7837604519596878] Loss: 23.422409975357798\n",
      "Iteracion: 4104 Gradiente: [0.045404130127946016,-0.7833437808460821] Loss: 23.421793795016747\n",
      "Iteracion: 4105 Gradiente: [0.04537999189874995,-0.7829273312476298] Loss: 23.421178269662303\n",
      "Iteracion: 4106 Gradiente: [0.045355866502148005,-0.7825111030465692] Loss: 23.420563398598254\n",
      "Iteracion: 4107 Gradiente: [0.04533175393135025,-0.7820950961251952] Loss: 23.419949181129073\n",
      "Iteracion: 4108 Gradiente: [0.045307654179539254,-0.7816793103658697] Loss: 23.419335616560033\n",
      "Iteracion: 4109 Gradiente: [0.045283567239871064,-0.7812637456510179] Loss: 23.418722704197073\n",
      "Iteracion: 4110 Gradiente: [0.0452594931055548,-0.7808484018631243] Loss: 23.418110443346947\n",
      "Iteracion: 4111 Gradiente: [0.04523543176978156,-0.7804332788847378] Loss: 23.4174988333171\n",
      "Iteracion: 4112 Gradiente: [0.045211383225788875,-0.7800183765984661] Loss: 23.416887873415718\n",
      "Iteracion: 4113 Gradiente: [0.04518734746665227,-0.7796036948869907] Loss: 23.416277562951738\n",
      "Iteracion: 4114 Gradiente: [0.045163324485704946,-0.7791892336330385] Loss: 23.415667901234823\n",
      "Iteracion: 4115 Gradiente: [0.04513931427608782,-0.7787749927194132] Loss: 23.41505888757535\n",
      "Iteracion: 4116 Gradiente: [0.04511531683102419,-0.7783609720289725] Loss: 23.41445052128447\n",
      "Iteracion: 4117 Gradiente: [0.045091332143849173,-0.7779471714446331] Loss: 23.41384280167403\n",
      "Iteracion: 4118 Gradiente: [0.04506736020759945,-0.7775335908493896] Loss: 23.413235728056623\n",
      "Iteracion: 4119 Gradiente: [0.04504340101554855,-0.7771202301262871] Loss: 23.41262929974557\n",
      "Iteracion: 4120 Gradiente: [0.045019454561005055,-0.7767070891584287] Loss: 23.412023516054948\n",
      "Iteracion: 4121 Gradiente: [0.04499552083718754,-0.776294167828987] Loss: 23.411418376299515\n",
      "Iteracion: 4122 Gradiente: [0.04497159983715259,-0.7758814660212046] Loss: 23.410813879794784\n",
      "Iteracion: 4123 Gradiente: [0.044947691554262786,-0.7754689836183712] Loss: 23.41021002585699\n",
      "Iteracion: 4124 Gradiente: [0.0449237959817727,-0.7750567205038429] Loss: 23.409606813803116\n",
      "Iteracion: 4125 Gradiente: [0.0448999131129123,-0.7746446765610399] Loss: 23.40900424295084\n",
      "Iteracion: 4126 Gradiente: [0.04487604294091909,-0.7742328516734454] Loss: 23.408402312618577\n",
      "Iteracion: 4127 Gradiente: [0.04485218545903346,-0.7738212457246008] Loss: 23.40780102212549\n",
      "Iteracion: 4128 Gradiente: [0.044828340660497665,-0.7734098585981148] Loss: 23.407200370791397\n",
      "Iteracion: 4129 Gradiente: [0.04480450853863639,-0.7729986901776507] Loss: 23.406600357936938\n",
      "Iteracion: 4130 Gradiente: [0.04478068908663128,-0.7725877403469407] Loss: 23.406000982883373\n",
      "Iteracion: 4131 Gradiente: [0.04475688229775017,-0.7721770089897767] Loss: 23.405402244952775\n",
      "Iteracion: 4132 Gradiente: [0.044733088165383114,-0.7717664959900037] Loss: 23.404804143467878\n",
      "Iteracion: 4133 Gradiente: [0.04470930668263217,-0.7713562012315459] Loss: 23.40420667775216\n",
      "Iteracion: 4134 Gradiente: [0.04468553784284287,-0.7709461245983756] Loss: 23.40360984712978\n",
      "Iteracion: 4135 Gradiente: [0.04466178163929726,-0.7705362659745316] Loss: 23.4030136509257\n",
      "Iteracion: 4136 Gradiente: [0.044638038065317194,-0.7701266252441097] Loss: 23.402418088465517\n",
      "Iteracion: 4137 Gradiente: [0.044614307114080515,-0.7697172022912782] Loss: 23.401823159075562\n",
      "Iteracion: 4138 Gradiente: [0.04459058877905401,-0.7693079970002498] Loss: 23.40122886208293\n",
      "Iteracion: 4139 Gradiente: [0.04456688305328763,-0.7688990092553222] Loss: 23.400635196815383\n",
      "Iteracion: 4140 Gradiente: [0.04454318993029555,-0.7684902389408284] Loss: 23.400042162601395\n",
      "Iteracion: 4141 Gradiente: [0.04451950940326223,-0.7680816859411839] Loss: 23.399449758770174\n",
      "Iteracion: 4142 Gradiente: [0.04449584146559194,-0.7676733501408497] Loss: 23.39885798465165\n",
      "Iteracion: 4143 Gradiente: [0.04447218611050895,-0.7672652314243616] Loss: 23.39826683957643\n",
      "Iteracion: 4144 Gradiente: [0.04444854333127353,-0.7668573296763135] Loss: 23.397676322875892\n",
      "Iteracion: 4145 Gradiente: [0.04442491312130699,-0.7664496447813538] Loss: 23.39708643388203\n",
      "Iteracion: 4146 Gradiente: [0.04440129547386296,-0.766042176624199] Loss: 23.396497171927667\n",
      "Iteracion: 4147 Gradiente: [0.0443776903822652,-0.7656349250896246] Loss: 23.395908536346226\n",
      "Iteracion: 4148 Gradiente: [0.04435409783993881,-0.7652278900624623] Loss: 23.395320526471927\n",
      "Iteracion: 4149 Gradiente: [0.04433051784011184,-0.7648210714276154] Loss: 23.394733141639612\n",
      "Iteracion: 4150 Gradiente: [0.04430695037611656,-0.764414469070043] Loss: 23.3941463811849\n",
      "Iteracion: 4151 Gradiente: [0.04428339544126629,-0.7640080828747652] Loss: 23.39356024444409\n",
      "Iteracion: 4152 Gradiente: [0.04425985302902499,-0.76360191272686] Loss: 23.392974730754194\n",
      "Iteracion: 4153 Gradiente: [0.04423632313260839,-0.7631959585114747] Loss: 23.392389839452903\n",
      "Iteracion: 4154 Gradiente: [0.04421280574541034,-0.7627902201138106] Loss: 23.391805569878656\n",
      "Iteracion: 4155 Gradiente: [0.04418930086076785,-0.7623846974191337] Loss: 23.39122192137054\n",
      "Iteracion: 4156 Gradiente: [0.04416580847206149,-0.7619793903127676] Loss: 23.390638893268395\n",
      "Iteracion: 4157 Gradiente: [0.04414232857264437,-0.7615742986800986] Loss: 23.390056484912733\n",
      "Iteracion: 4158 Gradiente: [0.04411886115587436,-0.7611694224065756] Loss: 23.3894746956448\n",
      "Iteracion: 4159 Gradiente: [0.04409540621496054,-0.7607647613777142] Loss: 23.38889352480648\n",
      "Iteracion: 4160 Gradiente: [0.0440719637435772,-0.7603603154790683] Loss: 23.388312971740415\n",
      "Iteracion: 4161 Gradiente: [0.044048533734882274,-0.7599560845962804] Loss: 23.38773303578994\n",
      "Iteracion: 4162 Gradiente: [0.04402511618221278,-0.7595520686150413] Loss: 23.387153716299057\n",
      "Iteracion: 4163 Gradiente: [0.0440017110791473,-0.7591482674210923] Loss: 23.38657501261248\n",
      "Iteracion: 4164 Gradiente: [0.043978318418863675,-0.7587446809002582] Loss: 23.385996924075645\n",
      "Iteracion: 4165 Gradiente: [0.043954938194887445,-0.7583413089384033] Loss: 23.385419450034632\n",
      "Iteracion: 4166 Gradiente: [0.043931570400540444,-0.7579381514214667] Loss: 23.384842589836264\n",
      "Iteracion: 4167 Gradiente: [0.043908215029223166,-0.7575352082354397] Loss: 23.384266342828035\n",
      "Iteracion: 4168 Gradiente: [0.04388487207443556,-0.7571324792663741] Loss: 23.383690708358134\n",
      "Iteracion: 4169 Gradiente: [0.04386154152937915,-0.7567299644003955] Loss: 23.383115685775447\n",
      "Iteracion: 4170 Gradiente: [0.04383822338759179,-0.7563276635236728] Loss: 23.382541274429556\n",
      "Iteracion: 4171 Gradiente: [0.043814917642412375,-0.7559255765224459] Loss: 23.38196747367071\n",
      "Iteracion: 4172 Gradiente: [0.043791624287278334,-0.7555237032830117] Loss: 23.381394282849882\n",
      "Iteracion: 4173 Gradiente: [0.043768343315710466,-0.755122043691721] Loss: 23.380821701318727\n",
      "Iteracion: 4174 Gradiente: [0.04374507472095672,-0.7547205976350007] Loss: 23.380249728429575\n",
      "Iteracion: 4175 Gradiente: [0.043721818496463054,-0.7543193649993304] Loss: 23.37967836353547\n",
      "Iteracion: 4176 Gradiente: [0.04369857463570762,-0.7539183456712429] Loss: 23.379107605990097\n",
      "Iteracion: 4177 Gradiente: [0.043675343132101335,-0.7535175395373402] Loss: 23.37853745514787\n",
      "Iteracion: 4178 Gradiente: [0.04365212397911004,-0.7531169464842776] Loss: 23.377967910363882\n",
      "Iteracion: 4179 Gradiente: [0.04362891717012758,-0.7527165663987793] Loss: 23.37739897099389\n",
      "Iteracion: 4180 Gradiente: [0.043605722698591386,-0.7523163991676246] Loss: 23.37683063639437\n",
      "Iteracion: 4181 Gradiente: [0.04358254055793509,-0.7519164446776526] Loss: 23.376262905922463\n",
      "Iteracion: 4182 Gradiente: [0.04355937074157528,-0.7515167028157677] Loss: 23.375695778935988\n",
      "Iteracion: 4183 Gradiente: [0.043536213243064024,-0.751117173468923] Loss: 23.375129254793443\n",
      "Iteracion: 4184 Gradiente: [0.043513068055726,-0.7507178565241462] Loss: 23.374563332854045\n",
      "Iteracion: 4185 Gradiente: [0.043489935173147386,-0.7503187518685103] Loss: 23.37399801247763\n",
      "Iteracion: 4186 Gradiente: [0.04346681458874949,-0.7499198593891571] Loss: 23.373433293024778\n",
      "Iteracion: 4187 Gradiente: [0.04344370629589302,-0.7495211789732946] Loss: 23.3728691738567\n",
      "Iteracion: 4188 Gradiente: [0.043420610288063696,-0.7491227105081805] Loss: 23.372305654335314\n",
      "Iteracion: 4189 Gradiente: [0.043397526558830654,-0.7487244538811325] Loss: 23.371742733823215\n",
      "Iteracion: 4190 Gradiente: [0.04337445510174689,-0.7483264089795223] Loss: 23.371180411683664\n",
      "Iteracion: 4191 Gradiente: [0.0433513959099713,-0.7479285756908117] Loss: 23.37061868728061\n",
      "Iteracion: 4192 Gradiente: [0.04332834897728522,-0.7475309539024816] Loss: 23.37005755997865\n",
      "Iteracion: 4193 Gradiente: [0.04330531429696028,-0.7471335435021047] Loss: 23.36949702914309\n",
      "Iteracion: 4194 Gradiente: [0.04328229186268307,-0.7467363443772892] Loss: 23.36893709413992\n",
      "Iteracion: 4195 Gradiente: [0.0432592816677186,-0.7463393564157269] Loss: 23.368377754335746\n",
      "Iteracion: 4196 Gradiente: [0.04323628370582545,-0.7459425795051425] Loss: 23.367819009097914\n",
      "Iteracion: 4197 Gradiente: [0.04321329797030084,-0.745546013533346] Loss: 23.367260857794403\n",
      "Iteracion: 4198 Gradiente: [0.043190324454735675,-0.7451496583881903] Loss: 23.36670329979386\n",
      "Iteracion: 4199 Gradiente: [0.0431673631525598,-0.7447535139575969] Loss: 23.366146334465643\n",
      "Iteracion: 4200 Gradiente: [0.04314441405729592,-0.7443575801295442] Loss: 23.365589961179715\n",
      "Iteracion: 4201 Gradiente: [0.04312147716242218,-0.7439618567920706] Loss: 23.365034179306797\n",
      "Iteracion: 4202 Gradiente: [0.0430985524615636,-0.743566343833267] Loss: 23.364478988218192\n",
      "Iteracion: 4203 Gradiente: [0.043075639948167085,-0.743171041141293] Loss: 23.36392438728591\n",
      "Iteracion: 4204 Gradiente: [0.043052739615845326,-0.7427759486043589] Loss: 23.363370375882656\n",
      "Iteracion: 4205 Gradiente: [0.043029851457932485,-0.7423810661107517] Loss: 23.362816953381756\n",
      "Iteracion: 4206 Gradiente: [0.04300697546804978,-0.7419863935487991] Loss: 23.362264119157206\n",
      "Iteracion: 4207 Gradiente: [0.04298411163984686,-0.7415919308068887] Loss: 23.3617118725837\n",
      "Iteracion: 4208 Gradiente: [0.04296125996666925,-0.7411976777734869] Loss: 23.36116021303657\n",
      "Iteracion: 4209 Gradiente: [0.04293842044219976,-0.7408036343370941] Loss: 23.36060913989184\n",
      "Iteracion: 4210 Gradiente: [0.04291559305996297,-0.7404098003862853] Loss: 23.36005865252612\n",
      "Iteracion: 4211 Gradiente: [0.04289277781337262,-0.7400161758096993] Loss: 23.35950875031681\n",
      "Iteracion: 4212 Gradiente: [0.04286997469611435,-0.7396227604960169] Loss: 23.358959432641857\n",
      "Iteracion: 4213 Gradiente: [0.04284718370167203,-0.7392295543339917] Loss: 23.35841069887991\n",
      "Iteracion: 4214 Gradiente: [0.042824404823576856,-0.7388365572124348] Loss: 23.357862548410303\n",
      "Iteracion: 4215 Gradiente: [0.04280163805548985,-0.738443769020207] Loss: 23.35731498061301\n",
      "Iteracion: 4216 Gradiente: [0.04277888339082096,-0.7380511896462456] Loss: 23.35676799486865\n",
      "Iteracion: 4217 Gradiente: [0.04275614082342827,-0.7376588189795167] Loss: 23.35622159055852\n",
      "Iteracion: 4218 Gradiente: [0.042733410346611815,-0.7372666569090828] Loss: 23.355675767064568\n",
      "Iteracion: 4219 Gradiente: [0.042710691953974825,-0.7368747033240446] Loss: 23.355130523769382\n",
      "Iteracion: 4220 Gradiente: [0.042687985639105364,-0.7364829581135652] Loss: 23.354585860056243\n",
      "Iteracion: 4221 Gradiente: [0.04266529139570233,-0.7360914211668579] Loss: 23.35404177530906\n",
      "Iteracion: 4222 Gradiente: [0.04264260921719843,-0.735700092373213] Loss: 23.353498268912414\n",
      "Iteracion: 4223 Gradiente: [0.04261993909721961,-0.7353089716219685] Loss: 23.352955340251523\n",
      "Iteracion: 4224 Gradiente: [0.042597281029323614,-0.7349180588025225] Loss: 23.35241298871226\n",
      "Iteracion: 4225 Gradiente: [0.04257463500720178,-0.734527353804328] Loss: 23.351871213681168\n",
      "Iteracion: 4226 Gradiente: [0.042552001024410896,-0.7341368565169034] Loss: 23.351330014545407\n",
      "Iteracion: 4227 Gradiente: [0.04252937907461861,-0.7337465668298176] Loss: 23.35078939069286\n",
      "Iteracion: 4228 Gradiente: [0.042506769151226344,-0.7333564846327162] Loss: 23.35024934151197\n",
      "Iteracion: 4229 Gradiente: [0.04248417124802302,-0.7329666098152814] Loss: 23.349709866391883\n",
      "Iteracion: 4230 Gradiente: [0.042461585358510474,-0.7325769422672704] Loss: 23.349170964722397\n",
      "Iteracion: 4231 Gradiente: [0.042439011476409404,-0.732187481878483] Loss: 23.348632635893935\n",
      "Iteracion: 4232 Gradiente: [0.04241644959525578,-0.7317982285387953] Loss: 23.348094879297598\n",
      "Iteracion: 4233 Gradiente: [0.04239389970863006,-0.7314091821381337] Loss: 23.347557694325076\n",
      "Iteracion: 4234 Gradiente: [0.04237136181030223,-0.7310203425664753] Loss: 23.347021080368776\n",
      "Iteracion: 4235 Gradiente: [0.042348835893830976,-0.7306317097138686] Loss: 23.34648503682171\n",
      "Iteracion: 4236 Gradiente: [0.04232632195280909,-0.7302432834704153] Loss: 23.345949563077554\n",
      "Iteracion: 4237 Gradiente: [0.0423038199807858,-0.7298550637262821] Loss: 23.34541465853062\n",
      "Iteracion: 4238 Gradiente: [0.04228132997156517,-0.729467050371677] Loss: 23.34488032257585\n",
      "Iteracion: 4239 Gradiente: [0.04225885191873715,-0.7290792432968817] Loss: 23.344346554608855\n",
      "Iteracion: 4240 Gradiente: [0.04223638581597792,-0.7286916423922285] Loss: 23.34381335402586\n",
      "Iteracion: 4241 Gradiente: [0.0422139316568727,-0.7283042475481142] Loss: 23.343280720223763\n",
      "Iteracion: 4242 Gradiente: [0.0421914894350247,-0.7279170586549927] Loss: 23.342748652600086\n",
      "Iteracion: 4243 Gradiente: [0.04216905914411579,-0.7275300756033743] Loss: 23.342217150552987\n",
      "Iteracion: 4244 Gradiente: [0.042146640778005916,-0.727143298283815] Loss: 23.34168621348129\n",
      "Iteracion: 4245 Gradiente: [0.04212423432998662,-0.7267567265869613] Loss: 23.3411558407844\n",
      "Iteracion: 4246 Gradiente: [0.04210183979406755,-0.72637036040348] Loss: 23.34062603186244\n",
      "Iteracion: 4247 Gradiente: [0.04207945716376097,-0.7259841996241209] Loss: 23.340096786116092\n",
      "Iteracion: 4248 Gradiente: [0.04205708643264738,-0.7255982441396914] Loss: 23.33956810294673\n",
      "Iteracion: 4249 Gradiente: [0.042034727594644515,-0.7252124938410366] Loss: 23.33903998175637\n",
      "Iteracion: 4250 Gradiente: [0.04201238064319549,-0.7248269486190859] Loss: 23.338512421947602\n",
      "Iteracion: 4251 Gradiente: [0.04199004557203712,-0.724441608364811] Loss: 23.337985422923705\n",
      "Iteracion: 4252 Gradiente: [0.04196772237499052,-0.7240564729692385] Loss: 23.337458984088574\n",
      "Iteracion: 4253 Gradiente: [0.04194541104550732,-0.7236715423234709] Loss: 23.336933104846754\n",
      "Iteracion: 4254 Gradiente: [0.04192311157757066,-0.723286816318641] Loss: 23.33640778460337\n",
      "Iteracion: 4255 Gradiente: [0.041900823964673845,-0.7229022948459654] Loss: 23.33588302276427\n",
      "Iteracion: 4256 Gradiente: [0.041878548200469365,-0.7225179777967138] Loss: 23.335358818735862\n",
      "Iteracion: 4257 Gradiente: [0.041856284278859825,-0.7221338650621952] Loss: 23.33483517192519\n",
      "Iteracion: 4258 Gradiente: [0.041834032193360335,-0.7217499565337998] Loss: 23.334312081739938\n",
      "Iteracion: 4259 Gradiente: [0.041811791937807166,-0.7213662521029602] Loss: 23.333789547588474\n",
      "Iteracion: 4260 Gradiente: [0.04178956350574102,-0.7209827516611795] Loss: 23.333267568879695\n",
      "Iteracion: 4261 Gradiente: [0.04176734689106828,-0.720599455100002] Loss: 23.332746145023204\n",
      "Iteracion: 4262 Gradiente: [0.04174514208739026,-0.7202163623110432] Loss: 23.33222527542919\n",
      "Iteracion: 4263 Gradiente: [0.04172294908851389,-0.7198334731859667] Loss: 23.331704959508492\n",
      "Iteracion: 4264 Gradiente: [0.04170076788807838,-0.7194507876165048] Loss: 23.33118519667256\n",
      "Iteracion: 4265 Gradiente: [0.04167859847985558,-0.719068305494437] Loss: 23.33066598633349\n",
      "Iteracion: 4266 Gradiente: [0.04165644085762968,-0.7186860267116022] Loss: 23.33014732790399\n",
      "Iteracion: 4267 Gradiente: [0.041634295014993467,-0.7183039511599074] Loss: 23.329629220797365\n",
      "Iteracion: 4268 Gradiente: [0.041612160945914904,-0.717922078731295] Loss: 23.329111664427597\n",
      "Iteracion: 4269 Gradiente: [0.04159003864385037,-0.7175404093177955] Loss: 23.328594658209248\n",
      "Iteracion: 4270 Gradiente: [0.0415679281026352,-0.7171589428114741] Loss: 23.32807820155755\n",
      "Iteracion: 4271 Gradiente: [0.041545829316071564,-0.7167776791044557] Loss: 23.327562293888278\n",
      "Iteracion: 4272 Gradiente: [0.04152374227797206,-0.7163966180889225] Loss: 23.327046934617915\n",
      "Iteracion: 4273 Gradiente: [0.04150166698203369,-0.716015759657121] Loss: 23.32653212316351\n",
      "Iteracion: 4274 Gradiente: [0.041479603421979996,-0.7156351037013536] Loss: 23.32601785894273\n",
      "Iteracion: 4275 Gradiente: [0.04145755159157526,-0.7152546501139772] Loss: 23.325504141373905\n",
      "Iteracion: 4276 Gradiente: [0.04143551148461976,-0.7148743987874056] Loss: 23.324990969875948\n",
      "Iteracion: 4277 Gradiente: [0.04141348309488156,-0.7144943496141084] Loss: 23.32447834386837\n",
      "Iteracion: 4278 Gradiente: [0.041391466416147674,-0.7141145024866158] Loss: 23.32396626277137\n",
      "Iteracion: 4279 Gradiente: [0.04136946144207438,-0.71373485729752] Loss: 23.323454726005693\n",
      "Iteracion: 4280 Gradiente: [0.04134746816659553,-0.7133554139394559] Loss: 23.322943732992737\n",
      "Iteracion: 4281 Gradiente: [0.04132548658330961,-0.7129761723051329] Loss: 23.322433283154496\n",
      "Iteracion: 4282 Gradiente: [0.0413035166862528,-0.7125971322872962] Loss: 23.321923375913578\n",
      "Iteracion: 4283 Gradiente: [0.04128155846899138,-0.7122182937787718] Loss: 23.321414010693257\n",
      "Iteracion: 4284 Gradiente: [0.041259611925385305,-0.7118396566724262] Loss: 23.32090518691732\n",
      "Iteracion: 4285 Gradiente: [0.04123767704916948,-0.7114612208611936] Loss: 23.32039690401025\n",
      "Iteracion: 4286 Gradiente: [0.041215753834273984,-0.7110829862380491] Loss: 23.31988916139711\n",
      "Iteracion: 4287 Gradiente: [0.04119384227439108,-0.7107049526960438] Loss: 23.319381958503588\n",
      "Iteracion: 4288 Gradiente: [0.04117194236345843,-0.7103271201282672] Loss: 23.318875294755966\n",
      "Iteracion: 4289 Gradiente: [0.041150054095129465,-0.709949488427885] Loss: 23.31836916958115\n",
      "Iteracion: 4290 Gradiente: [0.04112817746330298,-0.7095720574881038] Loss: 23.317863582406645\n",
      "Iteracion: 4291 Gradiente: [0.04110631246173947,-0.7091948272021971] Loss: 23.317358532660574\n",
      "Iteracion: 4292 Gradiente: [0.04108445908428943,-0.7088177974634895] Loss: 23.316854019771636\n",
      "Iteracion: 4293 Gradiente: [0.04106261732469344,-0.7084409681653673] Loss: 23.316350043169187\n",
      "Iteracion: 4294 Gradiente: [0.04104078717693653,-0.7080643392012628] Loss: 23.31584660228317\n",
      "Iteracion: 4295 Gradiente: [0.04101896863478771,-0.7076879104646733] Loss: 23.315343696544122\n",
      "Iteracion: 4296 Gradiente: [0.0409971616919582,-0.7073116818491589] Loss: 23.314841325383192\n",
      "Iteracion: 4297 Gradiente: [0.04097536634235629,-0.7069356532483266] Loss: 23.314339488232115\n",
      "Iteracion: 4298 Gradiente: [0.04095358257992435,-0.7065598245558355] Loss: 23.313838184523288\n",
      "Iteracion: 4299 Gradiente: [0.0409318103983793,-0.7061841956654137] Loss: 23.31333741368966\n",
      "Iteracion: 4300 Gradiente: [0.04091004979156594,-0.7058087664708419] Loss: 23.312837175164773\n",
      "Iteracion: 4301 Gradiente: [0.040888300753347075,-0.7054335368659541] Loss: 23.312337468382825\n",
      "Iteracion: 4302 Gradiente: [0.0408665632776452,-0.7050585067446391] Loss: 23.311838292778575\n",
      "Iteracion: 4303 Gradiente: [0.04084483735818859,-0.7046836760008517] Loss: 23.31133964778738\n",
      "Iteracion: 4304 Gradiente: [0.040823122988945214,-0.7043090445285898] Loss: 23.31084153284521\n",
      "Iteracion: 4305 Gradiente: [0.04080142016374945,-0.7039346122219172] Loss: 23.310343947388663\n",
      "Iteracion: 4306 Gradiente: [0.04077972887631726,-0.7035603789749584] Loss: 23.309846890854864\n",
      "Iteracion: 4307 Gradiente: [0.040758049120709454,-0.7031863446818768] Loss: 23.309350362681627\n",
      "Iteracion: 4308 Gradiente: [0.04073638089080589,-0.7028125092369019] Loss: 23.308854362307276\n",
      "Iteracion: 4309 Gradiente: [0.040714724180332,-0.7024388725343281] Loss: 23.308358889170776\n",
      "Iteracion: 4310 Gradiente: [0.04069307898326997,-0.7020654344684911] Loss: 23.307863942711723\n",
      "Iteracion: 4311 Gradiente: [0.04067144529336133,-0.7016921949337986] Loss: 23.30736952237023\n",
      "Iteracion: 4312 Gradiente: [0.04064982310463752,-0.7013191538246969] Loss: 23.30687562758705\n",
      "Iteracion: 4313 Gradiente: [0.04062821241087041,-0.7009463110357018] Loss: 23.30638225780354\n",
      "Iteracion: 4314 Gradiente: [0.040606613206163426,-0.7005736664613702] Loss: 23.305889412461635\n",
      "Iteracion: 4315 Gradiente: [0.0405850254841719,-0.700201219996337] Loss: 23.305397091003858\n",
      "Iteracion: 4316 Gradiente: [0.04056344923889412,-0.6998289715352755] Loss: 23.304905292873336\n",
      "Iteracion: 4317 Gradiente: [0.04054188446419952,-0.6994569209729219] Loss: 23.304414017513775\n",
      "Iteracion: 4318 Gradiente: [0.04052033115397838,-0.6990850682040687] Loss: 23.303923264369473\n",
      "Iteracion: 4319 Gradiente: [0.04049878930215224,-0.698713413123563] Loss: 23.303433032885348\n",
      "Iteracion: 4320 Gradiente: [0.040477258902727915,-0.6983419556262999] Loss: 23.302943322506895\n",
      "Iteracion: 4321 Gradiente: [0.04045573994944505,-0.6979706956072481] Loss: 23.302454132680154\n",
      "Iteracion: 4322 Gradiente: [0.040434232436328446,-0.6975996329614171] Loss: 23.301965462851808\n",
      "Iteracion: 4323 Gradiente: [0.040412736357218176,-0.6972287675838812] Loss: 23.301477312469114\n",
      "Iteracion: 4324 Gradiente: [0.04039125170610873,-0.6968580993697622] Loss: 23.300989680979914\n",
      "Iteracion: 4325 Gradiente: [0.04036977847682124,-0.6964876282142477] Loss: 23.30050256783261\n",
      "Iteracion: 4326 Gradiente: [0.04034831666350556,-0.6961173540125626] Loss: 23.30001597247624\n",
      "Iteracion: 4327 Gradiente: [0.04032686625983691,-0.6957472766600152] Loss: 23.299529894360408\n",
      "Iteracion: 4328 Gradiente: [0.040305427259941476,-0.6953773960519435] Loss: 23.299044332935292\n",
      "Iteracion: 4329 Gradiente: [0.04028399965764511,-0.6950077120837579] Loss: 23.298559287651635\n",
      "Iteracion: 4330 Gradiente: [0.040262583446894006,-0.6946382246509178] Loss: 23.298074757960816\n",
      "Iteracion: 4331 Gradiente: [0.04024117862173095,-0.6942689336489347] Loss: 23.29759074331478\n",
      "Iteracion: 4332 Gradiente: [0.04021978517595623,-0.6938998389733873] Loss: 23.29710724316601\n",
      "Iteracion: 4333 Gradiente: [0.040198403103672335,-0.6935309405198953] Loss: 23.296624256967664\n",
      "Iteracion: 4334 Gradiente: [0.04017703239867672,-0.6931622381841486] Loss: 23.296141784173365\n",
      "Iteracion: 4335 Gradiente: [0.04015567305493732,-0.6927937318618841] Loss: 23.2956598242374\n",
      "Iteracion: 4336 Gradiente: [0.04013432506657561,-0.6924254214488873] Loss: 23.2951783766146\n",
      "Iteracion: 4337 Gradiente: [0.04011298842738332,-0.692057306841016] Loss: 23.29469744076041\n",
      "Iteracion: 4338 Gradiente: [0.040091663131621165,-0.691689387934156] Loss: 23.29421701613081\n",
      "Iteracion: 4339 Gradiente: [0.04007034917292742,-0.6913216646242853] Loss: 23.293737102182387\n",
      "Iteracion: 4340 Gradiente: [0.040049046545269106,-0.690954136807422] Loss: 23.293257698372297\n",
      "Iteracion: 4341 Gradiente: [0.04002775524284819,-0.6905868043796208] Loss: 23.292778804158267\n",
      "Iteracion: 4342 Gradiente: [0.04000647525954927,-0.6902196672370119] Loss: 23.292300418998625\n",
      "Iteracion: 4343 Gradiente: [0.039985206589358314,-0.6898527252757769] Loss: 23.29182254235225\n",
      "Iteracion: 4344 Gradiente: [0.0399639492261997,-0.689485978392153] Loss: 23.291345173678582\n",
      "Iteracion: 4345 Gradiente: [0.03994270316413898,-0.6891194264824291] Loss: 23.2908683124377\n",
      "Iteracion: 4346 Gradiente: [0.039921468397112865,-0.6887530694429533] Loss: 23.29039195809016\n",
      "Iteracion: 4347 Gradiente: [0.03990024491911773,-0.6883869071701255] Loss: 23.28991611009719\n",
      "Iteracion: 4348 Gradiente: [0.03987903272413386,-0.6880209395604052] Loss: 23.28944076792052\n",
      "Iteracion: 4349 Gradiente: [0.0398578318062647,-0.6876551665102951] Loss: 23.288965931022485\n",
      "Iteracion: 4350 Gradiente: [0.03983664215946779,-0.6872895879163641] Loss: 23.288491598866\n",
      "Iteracion: 4351 Gradiente: [0.03981546377770826,-0.6869242036752382] Loss: 23.28801777091452\n",
      "Iteracion: 4352 Gradiente: [0.03979429665500049,-0.6865590136835907] Loss: 23.287544446632076\n",
      "Iteracion: 4353 Gradiente: [0.03977314078538446,-0.6861940178381528] Loss: 23.287071625483293\n",
      "Iteracion: 4354 Gradiente: [0.03975199616300055,-0.6858292160357025] Loss: 23.28659930693335\n",
      "Iteracion: 4355 Gradiente: [0.03973086278163104,-0.6854646081730948] Loss: 23.286127490448\n",
      "Iteracion: 4356 Gradiente: [0.03970974063548264,-0.6851001941472135] Loss: 23.285656175493532\n",
      "Iteracion: 4357 Gradiente: [0.039688629718433316,-0.6847359738550196] Loss: 23.28518536153688\n",
      "Iteracion: 4358 Gradiente: [0.03966753002464714,-0.6843719471935094] Loss: 23.28471504804543\n",
      "Iteracion: 4359 Gradiente: [0.03964644154805039,-0.6840081140597493] Loss: 23.284245234487244\n",
      "Iteracion: 4360 Gradiente: [0.03962536428284788,-0.6836444743508452] Loss: 23.283775920330893\n",
      "Iteracion: 4361 Gradiente: [0.03960429822286358,-0.6832810279639768] Loss: 23.28330710504552\n",
      "Iteracion: 4362 Gradiente: [0.03958324336229377,-0.6829177747963607] Loss: 23.28283878810083\n",
      "Iteracion: 4363 Gradiente: [0.039562199695059044,-0.6825547147452835] Loss: 23.28237096896711\n",
      "Iteracion: 4364 Gradiente: [0.03954116721533107,-0.682191847708072] Loss: 23.281903647115207\n",
      "Iteracion: 4365 Gradiente: [0.03952014591716875,-0.681829173582111] Loss: 23.281436822016495\n",
      "Iteracion: 4366 Gradiente: [0.039499135794556155,-0.6814666922648489] Loss: 23.280970493142934\n",
      "Iteracion: 4367 Gradiente: [0.039478136841567375,-0.6811044036537803] Loss: 23.28050465996707\n",
      "Iteracion: 4368 Gradiente: [0.0394571490522452,-0.6807423076464595] Loss: 23.280039321961983\n",
      "Iteracion: 4369 Gradiente: [0.039436172420675084,-0.6803804041404924] Loss: 23.279574478601297\n",
      "Iteracion: 4370 Gradiente: [0.03941520694091404,-0.6800186930335365] Loss: 23.279110129359243\n",
      "Iteracion: 4371 Gradiente: [0.03939425260706173,-0.6796571742233074] Loss: 23.278646273710567\n",
      "Iteracion: 4372 Gradiente: [0.0393733094131837,-0.6792958476075749] Loss: 23.278182911130582\n",
      "Iteracion: 4373 Gradiente: [0.03935237735332559,-0.6789347130841626] Loss: 23.277720041095183\n",
      "Iteracion: 4374 Gradiente: [0.03933145642158422,-0.678573770550948] Loss: 23.277257663080796\n",
      "Iteracion: 4375 Gradiente: [0.03931054661219567,-0.6782130199058553] Loss: 23.276795776564434\n",
      "Iteracion: 4376 Gradiente: [0.03928964791898295,-0.677852461046885] Loss: 23.276334381023602\n",
      "Iteracion: 4377 Gradiente: [0.0392687603362333,-0.6774920938720677] Loss: 23.275873475936457\n",
      "Iteracion: 4378 Gradiente: [0.03924788385782847,-0.6771319182795091] Loss: 23.27541306078161\n",
      "Iteracion: 4379 Gradiente: [0.039227018478124855,-0.6767719341673446] Loss: 23.274953135038302\n",
      "Iteracion: 4380 Gradiente: [0.03920616419095211,-0.6764121414337921] Loss: 23.274493698186273\n",
      "Iteracion: 4381 Gradiente: [0.03918532099063441,-0.6760525399770956] Loss: 23.274034749705866\n",
      "Iteracion: 4382 Gradiente: [0.039164488871258146,-0.6756931296955689] Loss: 23.27357628907795\n",
      "Iteracion: 4383 Gradiente: [0.03914366782671171,-0.6753339104875898] Loss: 23.273118315783947\n",
      "Iteracion: 4384 Gradiente: [0.03912285785149645,-0.6749748822515538] Loss: 23.272660829305828\n",
      "Iteracion: 4385 Gradiente: [0.03910205893934062,-0.6746160448859584] Loss: 23.272203829126116\n",
      "Iteracion: 4386 Gradiente: [0.03908127108458359,-0.6742573982893165] Loss: 23.271747314727875\n",
      "Iteracion: 4387 Gradiente: [0.039060494281254896,-0.6738989423602161] Loss: 23.271291285594746\n",
      "Iteracion: 4388 Gradiente: [0.03903972852356977,-0.6735406769972883] Loss: 23.270835741210913\n",
      "Iteracion: 4389 Gradiente: [0.03901897380561081,-0.673182602099223] Loss: 23.270380681061063\n",
      "Iteracion: 4390 Gradiente: [0.03899823012149284,-0.672824717564764] Loss: 23.269926104630507\n",
      "Iteracion: 4391 Gradiente: [0.03897749746534487,-0.67246702329271] Loss: 23.26947201140503\n",
      "Iteracion: 4392 Gradiente: [0.03895677583124666,-0.6721095191819136] Loss: 23.269018400871005\n",
      "Iteracion: 4393 Gradiente: [0.03893606521342576,-0.6717522051312758] Loss: 23.268565272515335\n",
      "Iteracion: 4394 Gradiente: [0.03891536560603773,-0.6713950810397525] Loss: 23.268112625825495\n",
      "Iteracion: 4395 Gradiente: [0.03889467700324,-0.6710381468063557] Loss: 23.267660460289466\n",
      "Iteracion: 4396 Gradiente: [0.038873999399095284,-0.6706814023301556] Loss: 23.267208775395797\n",
      "Iteracion: 4397 Gradiente: [0.0388533327877686,-0.6703248475102711] Loss: 23.266757570633583\n",
      "Iteracion: 4398 Gradiente: [0.038832677163430654,-0.6699684822458738] Loss: 23.26630684549244\n",
      "Iteracion: 4399 Gradiente: [0.038812032520274896,-0.6696123064361896] Loss: 23.26585659946254\n",
      "Iteracion: 4400 Gradiente: [0.038791398852452134,-0.6692563199804977] Loss: 23.265406832034618\n",
      "Iteracion: 4401 Gradiente: [0.03877077615414635,-0.6689005227781302] Loss: 23.26495754269991\n",
      "Iteracion: 4402 Gradiente: [0.038750164419430654,-0.6685449147284821] Loss: 23.26450873095024\n",
      "Iteracion: 4403 Gradiente: [0.038729563642581864,-0.6681894957309865] Loss: 23.26406039627791\n",
      "Iteracion: 4404 Gradiente: [0.038708973817735644,-0.6678342656851389] Loss: 23.263612538175813\n",
      "Iteracion: 4405 Gradiente: [0.038688394939182066,-0.6674792244904802] Loss: 23.263165156137365\n",
      "Iteracion: 4406 Gradiente: [0.03866782700086446,-0.6671243720466258] Loss: 23.262718249656515\n",
      "Iteracion: 4407 Gradiente: [0.038647269997175236,-0.6667697082532188] Loss: 23.26227181822777\n",
      "Iteracion: 4408 Gradiente: [0.03862672392217329,-0.6664152330099727] Loss: 23.261825861346146\n",
      "Iteracion: 4409 Gradiente: [0.0386061887701563,-0.6660609462166437] Loss: 23.261380378507223\n",
      "Iteracion: 4410 Gradiente: [0.038585664535153795,-0.6657068477730557] Loss: 23.260935369207097\n",
      "Iteracion: 4411 Gradiente: [0.03856515121149944,-0.6653529375790657] Loss: 23.260490832942384\n",
      "Iteracion: 4412 Gradiente: [0.038544648793334586,-0.6649992155345998] Loss: 23.26004676921031\n",
      "Iteracion: 4413 Gradiente: [0.03852415727486307,-0.6646456815396324] Loss: 23.25960317750853\n",
      "Iteracion: 4414 Gradiente: [0.038503676650374054,-0.664292335494184] Loss: 23.2591600573353\n",
      "Iteracion: 4415 Gradiente: [0.03848320691407328,-0.6639391772983356] Loss: 23.25871740818942\n",
      "Iteracion: 4416 Gradiente: [0.03846274806007652,-0.6635862068522248] Loss: 23.258275229570156\n",
      "Iteracion: 4417 Gradiente: [0.038442300082513724,-0.6632334240560472] Loss: 23.257833520977396\n",
      "Iteracion: 4418 Gradiente: [0.038421862975817095,-0.6628808288100257] Loss: 23.25739228191147\n",
      "Iteracion: 4419 Gradiente: [0.038401436734074916,-0.6625284210144627] Loss: 23.256951511873304\n",
      "Iteracion: 4420 Gradiente: [0.03838102135159526,-0.6621762005696975] Loss: 23.25651121036432\n",
      "Iteracion: 4421 Gradiente: [0.03836061682257916,-0.661824167376132] Loss: 23.256071376886492\n",
      "Iteracion: 4422 Gradiente: [0.03834022314115373,-0.6614723213342226] Loss: 23.25563201094231\n",
      "Iteracion: 4423 Gradiente: [0.03831984030174453,-0.6611206623444617] Loss: 23.255193112034803\n",
      "Iteracion: 4424 Gradiente: [0.0382994682984408,-0.660769190307416] Loss: 23.254754679667506\n",
      "Iteracion: 4425 Gradiente: [0.03827910712551083,-0.6604179051236938] Loss: 23.254316713344497\n",
      "Iteracion: 4426 Gradiente: [0.038258756777154686,-0.6600668066939612] Loss: 23.25387921257038\n",
      "Iteracion: 4427 Gradiente: [0.03823841724774013,-0.6597158949189269] Loss: 23.25344217685031\n",
      "Iteracion: 4428 Gradiente: [0.03821808853141609,-0.6593651696993651] Loss: 23.253005605689914\n",
      "Iteracion: 4429 Gradiente: [0.03819777062244043,-0.6590146309360958] Loss: 23.25256949859539\n",
      "Iteracion: 4430 Gradiente: [0.038177463515052065,-0.658664278529996] Loss: 23.25213385507345\n",
      "Iteracion: 4431 Gradiente: [0.03815716720367088,-0.658314112381981] Loss: 23.251698674631317\n",
      "Iteracion: 4432 Gradiente: [0.03813688168233872,-0.6579641323930439] Loss: 23.251263956776754\n",
      "Iteracion: 4433 Gradiente: [0.03811660694541198,-0.6576143384642116] Loss: 23.250829701018006\n",
      "Iteracion: 4434 Gradiente: [0.038096342987224335,-0.6572647304965646] Loss: 23.250395906863943\n",
      "Iteracion: 4435 Gradiente: [0.03807608980198817,-0.6569153083912435] Loss: 23.249962573823836\n",
      "Iteracion: 4436 Gradiente: [0.038055847383893136,-0.6565660720494437] Loss: 23.24952970140755\n",
      "Iteracion: 4437 Gradiente: [0.03803561572733732,-0.6562170213724017] Loss: 23.249097289125437\n",
      "Iteracion: 4438 Gradiente: [0.03801539482653027,-0.6558681562614136] Loss: 23.248665336488408\n",
      "Iteracion: 4439 Gradiente: [0.03799518467575354,-0.6555194766178284] Loss: 23.248233843007856\n",
      "Iteracion: 4440 Gradiente: [0.03797498526924699,-0.6551709823430472] Loss: 23.247802808195708\n",
      "Iteracion: 4441 Gradiente: [0.037954796601458916,-0.6548226733385147] Loss: 23.247372231564402\n",
      "Iteracion: 4442 Gradiente: [0.03793461866658561,-0.6544745495057404] Loss: 23.24694211262691\n",
      "Iteracion: 4443 Gradiente: [0.0379144514589342,-0.6541266107462801] Loss: 23.24651245089672\n",
      "Iteracion: 4444 Gradiente: [0.03789429497281655,-0.6537788569617431] Loss: 23.24608324588781\n",
      "Iteracion: 4445 Gradiente: [0.03787414920237874,-0.6534312880538004] Loss: 23.245654497114725\n",
      "Iteracion: 4446 Gradiente: [0.037854014142102224,-0.6530839039241537] Loss: 23.245226204092464\n",
      "Iteracion: 4447 Gradiente: [0.0378338897862335,-0.6527367044745744] Loss: 23.244798366336592\n",
      "Iteracion: 4448 Gradiente: [0.037813776129132746,-0.6523896896068765] Loss: 23.24437098336317\n",
      "Iteracion: 4449 Gradiente: [0.03779367316502089,-0.6520428592229365] Loss: 23.243944054688765\n",
      "Iteracion: 4450 Gradiente: [0.03777358088831211,-0.6516962132246717] Loss: 23.243517579830474\n",
      "Iteracion: 4451 Gradiente: [0.037753499293329655,-0.6513497515140566] Loss: 23.243091558305913\n",
      "Iteracion: 4452 Gradiente: [0.03773342837427454,-0.6510034739931247] Loss: 23.242665989633146\n",
      "Iteracion: 4453 Gradiente: [0.03771336812550885,-0.6506573805639533] Loss: 23.242240873330882\n",
      "Iteracion: 4454 Gradiente: [0.037693318541447716,-0.6503114711286694] Loss: 23.241816208918195\n",
      "Iteracion: 4455 Gradiente: [0.03767327961623721,-0.6499657455894661] Loss: 23.24139199591474\n",
      "Iteracion: 4456 Gradiente: [0.037653251344374895,-0.6496202038485682] Loss: 23.240968233840736\n",
      "Iteracion: 4457 Gradiente: [0.03763323372022474,-0.6492748458082632] Loss: 23.240544922216774\n",
      "Iteracion: 4458 Gradiente: [0.03761322673805883,-0.6489296713708934] Loss: 23.240122060564108\n",
      "Iteracion: 4459 Gradiente: [0.03759323039219756,-0.6485846804388503] Loss: 23.239699648404393\n",
      "Iteracion: 4460 Gradiente: [0.03757324467700869,-0.6482398729145757] Loss: 23.23927768525983\n",
      "Iteracion: 4461 Gradiente: [0.03755326958689883,-0.6478952487005607] Loss: 23.238856170653108\n",
      "Iteracion: 4462 Gradiente: [0.03753330511609268,-0.6475508076993606] Loss: 23.238435104107474\n",
      "Iteracion: 4463 Gradiente: [0.037513351258944755,-0.6472065498135727] Loss: 23.238014485146632\n",
      "Iteracion: 4464 Gradiente: [0.037493408010095666,-0.6468624749458317] Loss: 23.23759431329481\n",
      "Iteracion: 4465 Gradiente: [0.037473475363491576,-0.6465185829988632] Loss: 23.23717458807675\n",
      "Iteracion: 4466 Gradiente: [0.03745355331376648,-0.6461748738754061] Loss: 23.236755309017674\n",
      "Iteracion: 4467 Gradiente: [0.03743364185513561,-0.6458313474782732] Loss: 23.236336475643334\n",
      "Iteracion: 4468 Gradiente: [0.03741374098207852,-0.6454880037103142] Loss: 23.235918087479963\n",
      "Iteracion: 4469 Gradiente: [0.03739385068901318,-0.6451448424744377] Loss: 23.23550014405433\n",
      "Iteracion: 4470 Gradiente: [0.03737397097021547,-0.6448018636736095] Loss: 23.235082644893676\n",
      "Iteracion: 4471 Gradiente: [0.037354101820026624,-0.6444590672108413] Loss: 23.234665589525765\n",
      "Iteracion: 4472 Gradiente: [0.03733424323293377,-0.644116452989194] Loss: 23.23424897747886\n",
      "Iteracion: 4473 Gradiente: [0.037314395203299947,-0.6437740209117815] Loss: 23.233832808281708\n",
      "Iteracion: 4474 Gradiente: [0.037294557725401016,-0.6434317708817763] Loss: 23.233417081463553\n",
      "Iteracion: 4475 Gradiente: [0.03727473079376485,-0.6430897028023904] Loss: 23.233001796554205\n",
      "Iteracion: 4476 Gradiente: [0.03725491440278006,-0.642747816576892] Loss: 23.232586953083885\n",
      "Iteracion: 4477 Gradiente: [0.03723510854675567,-0.6424061121086069] Loss: 23.23217255058334\n",
      "Iteracion: 4478 Gradiente: [0.037215313220134286,-0.6420645893009068] Loss: 23.231758588583872\n",
      "Iteracion: 4479 Gradiente: [0.03719552841724389,-0.6417232480572181] Loss: 23.231345066617216\n",
      "Iteracion: 4480 Gradiente: [0.03717575413260192,-0.6413820882810095] Loss: 23.230931984215623\n",
      "Iteracion: 4481 Gradiente: [0.037155990360610264,-0.6410411098758075] Loss: 23.230519340911847\n",
      "Iteracion: 4482 Gradiente: [0.03713623709562626,-0.6407003127451939] Loss: 23.230107136239138\n",
      "Iteracion: 4483 Gradiente: [0.037116494332119034,-0.6403596967927925] Loss: 23.22969536973123\n",
      "Iteracion: 4484 Gradiente: [0.037096762064492354,-0.6400192619222864] Loss: 23.229284040922398\n",
      "Iteracion: 4485 Gradiente: [0.03707704028712821,-0.6396790080374072] Loss: 23.228873149347347\n",
      "Iteracion: 4486 Gradiente: [0.03705732899443508,-0.6393389350419395] Loss: 23.22846269454129\n",
      "Iteracion: 4487 Gradiente: [0.03703762818082434,-0.6389990428397176] Loss: 23.22805267603999\n",
      "Iteracion: 4488 Gradiente: [0.03701793784077457,-0.6386593313346213] Loss: 23.227643093379633\n",
      "Iteracion: 4489 Gradiente: [0.03699825796882692,-0.6383198004305837] Loss: 23.227233946096952\n",
      "Iteracion: 4490 Gradiente: [0.03697858855918525,-0.6379804500316032] Loss: 23.226825233729148\n",
      "Iteracion: 4491 Gradiente: [0.03695892960650156,-0.6376412800417053] Loss: 23.22641695581391\n",
      "Iteracion: 4492 Gradiente: [0.0369392811050678,-0.6373022903649873] Loss: 23.226009111889418\n",
      "Iteracion: 4493 Gradiente: [0.0369196430493929,-0.636963480905587] Loss: 23.22560170149436\n",
      "Iteracion: 4494 Gradiente: [0.036900015433904323,-0.6366248515676939] Loss: 23.225194724167885\n",
      "Iteracion: 4495 Gradiente: [0.03688039825311288,-0.6362864022555492] Loss: 23.22478817944967\n",
      "Iteracion: 4496 Gradiente: [0.036860791501292546,-0.6359481328734551] Loss: 23.22438206687986\n",
      "Iteracion: 4497 Gradiente: [0.03684119517313699,-0.6356100433257403] Loss: 23.223976385999077\n",
      "Iteracion: 4498 Gradiente: [0.03682160926288513,-0.6352721335168122] Loss: 23.22357113634845\n",
      "Iteracion: 4499 Gradiente: [0.03680203376515389,-0.6349344033511078] Loss: 23.223166317469627\n",
      "Iteracion: 4500 Gradiente: [0.03678246867444083,-0.6345968527331228] Loss: 23.22276192890464\n",
      "Iteracion: 4501 Gradiente: [0.03676291398505687,-0.6342594815674102] Loss: 23.222357970196104\n",
      "Iteracion: 4502 Gradiente: [0.03674336969155926,-0.633922289758564] Loss: 23.221954440887107\n",
      "Iteracion: 4503 Gradiente: [0.03672383578842376,-0.633585277211233] Loss: 23.22155134052119\n",
      "Iteracion: 4504 Gradiente: [0.03670431227010245,-0.6332484438301175] Loss: 23.2211486686424\n",
      "Iteracion: 4505 Gradiente: [0.03668479913102469,-0.6329117895199692] Loss: 23.22074642479527\n",
      "Iteracion: 4506 Gradiente: [0.036665296365766646,-0.6325753141855832] Loss: 23.220344608524798\n",
      "Iteracion: 4507 Gradiente: [0.0366458039688401,-0.6322390177318103] Loss: 23.219943219376493\n",
      "Iteracion: 4508 Gradiente: [0.03662632193456356,-0.63190290006356] Loss: 23.21954225689633\n",
      "Iteracion: 4509 Gradiente: [0.03660685025769226,-0.6315669610857704] Loss: 23.21914172063076\n",
      "Iteracion: 4510 Gradiente: [0.036587388932386486,-0.6312312007034627] Loss: 23.218741610126717\n",
      "Iteracion: 4511 Gradiente: [0.0365679379534356,-0.6308956188216742] Loss: 23.218341924931657\n",
      "Iteracion: 4512 Gradiente: [0.03654849731512968,-0.63056021534552] Loss: 23.217942664593455\n",
      "Iteracion: 4513 Gradiente: [0.03652906701213018,-0.6302249901801446] Loss: 23.21754382866053\n",
      "Iteracion: 4514 Gradiente: [0.03650964703886075,-0.6298899432307572] Loss: 23.21714541668172\n",
      "Iteracion: 4515 Gradiente: [0.03649023738980001,-0.6295550744026158] Loss: 23.216747428206382\n",
      "Iteracion: 4516 Gradiente: [0.03647083805958194,-0.6292203836010176] Loss: 23.216349862784337\n",
      "Iteracion: 4517 Gradiente: [0.03645144904261031,-0.6288858707313243] Loss: 23.2159527199659\n",
      "Iteracion: 4518 Gradiente: [0.0364320703334793,-0.6285515356989381] Loss: 23.215555999301834\n",
      "Iteracion: 4519 Gradiente: [0.036412701926638154,-0.6282173784093187] Loss: 23.215159700343413\n",
      "Iteracion: 4520 Gradiente: [0.036393343816593905,-0.6278833987679733] Loss: 23.214763822642364\n",
      "Iteracion: 4521 Gradiente: [0.036373995997941694,-0.6275495966804563] Loss: 23.21436836575091\n",
      "Iteracion: 4522 Gradiente: [0.036354658465162025,-0.6272159720523756] Loss: 23.213973329221723\n",
      "Iteracion: 4523 Gradiente: [0.036335331212793184,-0.6268825247893889] Loss: 23.213578712607962\n",
      "Iteracion: 4524 Gradiente: [0.036316014235448314,-0.6265492547971986] Loss: 23.213184515463297\n",
      "Iteracion: 4525 Gradiente: [0.03629670752757382,-0.6262161619815669] Loss: 23.21279073734182\n",
      "Iteracion: 4526 Gradiente: [0.036277411083688094,-0.6258832462483033] Loss: 23.212397377798123\n",
      "Iteracion: 4527 Gradiente: [0.03625812489847533,-0.6255505075032578] Loss: 23.212004436387264\n",
      "Iteracion: 4528 Gradiente: [0.03623884896628624,-0.6252179456523462] Loss: 23.21161191266477\n",
      "Iteracion: 4529 Gradiente: [0.0362195832818666,-0.6248855606015198] Loss: 23.211219806186662\n",
      "Iteracion: 4530 Gradiente: [0.036200327839571854,-0.6245533522567944] Loss: 23.210828116509422\n",
      "Iteracion: 4531 Gradiente: [0.03618108263414494,-0.6242213205242166] Loss: 23.210436843189985\n",
      "Iteracion: 4532 Gradiente: [0.03616184766008246,-0.6238894653098981] Loss: 23.21004598578576\n",
      "Iteracion: 4533 Gradiente: [0.03614262291182134,-0.6235577865200033] Loss: 23.20965554385465\n",
      "Iteracion: 4534 Gradiente: [0.03612340838412725,-0.6232262840607291] Loss: 23.209265516955046\n",
      "Iteracion: 4535 Gradiente: [0.03610420407142764,-0.6228949578383388] Loss: 23.208875904645723\n",
      "Iteracion: 4536 Gradiente: [0.0360850099683366,-0.6225638077591398] Loss: 23.208486706486045\n",
      "Iteracion: 4537 Gradiente: [0.03606582606940189,-0.6222328337294871] Loss: 23.20809792203571\n",
      "Iteracion: 4538 Gradiente: [0.03604665236925844,-0.6219020356557863] Loss: 23.207709550854993\n",
      "Iteracion: 4539 Gradiente: [0.03602748886244456,-0.6215714134444942] Loss: 23.207321592504613\n",
      "Iteracion: 4540 Gradiente: [0.036008335543508,-0.6212409670021191] Loss: 23.20693404654569\n",
      "Iteracion: 4541 Gradiente: [0.03598919240705148,-0.6209106962352173] Loss: 23.206546912539913\n",
      "Iteracion: 4542 Gradiente: [0.035970059447631306,-0.6205806010503956] Loss: 23.206160190049328\n",
      "Iteracion: 4543 Gradiente: [0.0359509366598985,-0.6202506813543055] Loss: 23.20577387863656\n",
      "Iteracion: 4544 Gradiente: [0.03593182403853158,-0.6199209370536465] Loss: 23.205387977864625\n",
      "Iteracion: 4545 Gradiente: [0.035912721577925785,-0.6195913680551847] Loss: 23.205002487297005\n",
      "Iteracion: 4546 Gradiente: [0.035893629272860036,-0.6192619742657125] Loss: 23.204617406497665\n",
      "Iteracion: 4547 Gradiente: [0.035874547117777905,-0.618932755592094] Loss: 23.204232735031034\n",
      "Iteracion: 4548 Gradiente: [0.03585547510727736,-0.6186037119412323] Loss: 23.203848472461992\n",
      "Iteracion: 4549 Gradiente: [0.03583641323626523,-0.6182748432200625] Loss: 23.203464618355916\n",
      "Iteracion: 4550 Gradiente: [0.03581736149900034,-0.6179461493356072] Loss: 23.203081172278605\n",
      "Iteracion: 4551 Gradiente: [0.03579831989020287,-0.6176176301949118] Loss: 23.202698133796325\n",
      "Iteracion: 4552 Gradiente: [0.03577928840449355,-0.617289285705075] Loss: 23.202315502475802\n",
      "Iteracion: 4553 Gradiente: [0.0357602670365291,-0.6169611157732459] Loss: 23.201933277884255\n",
      "Iteracion: 4554 Gradiente: [0.03574125578098896,-0.6166331203066233] Loss: 23.20155145958933\n",
      "Iteracion: 4555 Gradiente: [0.03572225463233849,-0.6163052992124605] Loss: 23.20117004715914\n",
      "Iteracion: 4556 Gradiente: [0.03570326358528556,-0.615977652398054] Loss: 23.200789040162263\n",
      "Iteracion: 4557 Gradiente: [0.035684282634379845,-0.6156501797707563] Loss: 23.20040843816772\n",
      "Iteracion: 4558 Gradiente: [0.035665311774478896,-0.6153228812379495] Loss: 23.200028240745034\n",
      "Iteracion: 4559 Gradiente: [0.03564635099997228,-0.6149957567070954] Loss: 23.199648447464117\n",
      "Iteracion: 4560 Gradiente: [0.03562740030561429,-0.6146688060856815] Loss: 23.199269057895393\n",
      "Iteracion: 4561 Gradiente: [0.0356084596859688,-0.6143420292812569] Loss: 23.19889007160972\n",
      "Iteracion: 4562 Gradiente: [0.03558952913585832,-0.614015426201404] Loss: 23.198511488178433\n",
      "Iteracion: 4563 Gradiente: [0.035570608649739675,-0.6136889967537783] Loss: 23.19813330717327\n",
      "Iteracion: 4564 Gradiente: [0.03555169822242306,-0.6133627408460617] Loss: 23.197755528166496\n",
      "Iteracion: 4565 Gradiente: [0.03553279784842402,-0.6130366583860026] Loss: 23.19737815073078\n",
      "Iteracion: 4566 Gradiente: [0.03551390752232256,-0.6127107492813944] Loss: 23.197001174439254\n",
      "Iteracion: 4567 Gradiente: [0.03549502723901033,-0.6123850134400636] Loss: 23.19662459886552\n",
      "Iteracion: 4568 Gradiente: [0.03547615699306732,-0.6120594507699034] Loss: 23.19624842358363\n",
      "Iteracion: 4569 Gradiente: [0.035457296779040345,-0.6117340611788563] Loss: 23.19587264816807\n",
      "Iteracion: 4570 Gradiente: [0.03543844659167803,-0.6114088445749037] Loss: 23.19549727219378\n",
      "Iteracion: 4571 Gradiente: [0.035419606425728035,-0.6110838008660759] Loss: 23.195122295236203\n",
      "Iteracion: 4572 Gradiente: [0.035400776275727706,-0.6107589299604645] Loss: 23.194747716871163\n",
      "Iteracion: 4573 Gradiente: [0.03538195613646735,-0.6104342317661959] Loss: 23.19437353667495\n",
      "Iteracion: 4574 Gradiente: [0.03536314600255537,-0.6101097061914537] Loss: 23.193999754224354\n",
      "Iteracion: 4575 Gradiente: [0.03534434586863995,-0.609785353144472] Loss: 23.19362636909655\n",
      "Iteracion: 4576 Gradiente: [0.03532555572953508,-0.6094611725335208] Loss: 23.193253380869212\n",
      "Iteracion: 4577 Gradiente: [0.035306775579800845,-0.6091371642669349] Loss: 23.19288078912043\n",
      "Iteracion: 4578 Gradiente: [0.035288005414175434,-0.60881332825309] Loss: 23.192508593428773\n",
      "Iteracion: 4579 Gradiente: [0.03526924522727673,-0.6084896644004137] Loss: 23.192136793373233\n",
      "Iteracion: 4580 Gradiente: [0.03525049501402009,-0.6081661726173668] Loss: 23.191765388533256\n",
      "Iteracion: 4581 Gradiente: [0.03523175476882822,-0.6078428528124898] Loss: 23.191394378488717\n",
      "Iteracion: 4582 Gradiente: [0.03521302448649806,-0.6075197048943458] Loss: 23.191023762819984\n",
      "Iteracion: 4583 Gradiente: [0.035194304161877694,-0.6071967287715476] Loss: 23.19065354110785\n",
      "Iteracion: 4584 Gradiente: [0.03517559378951489,-0.6068739243527717] Loss: 23.190283712933496\n",
      "Iteracion: 4585 Gradiente: [0.035156893364185746,-0.6065512915467316] Loss: 23.18991427787866\n",
      "Iteracion: 4586 Gradiente: [0.03513820288056024,-0.6062288302621965] Loss: 23.18954523552544\n",
      "Iteracion: 4587 Gradiente: [0.03511952233340973,-0.6059065404079753] Loss: 23.18917658545639\n",
      "Iteracion: 4588 Gradiente: [0.03510085171733029,-0.6055844218929385] Loss: 23.188808327254552\n",
      "Iteracion: 4589 Gradiente: [0.03508219102706297,-0.6052624746259956] Loss: 23.188440460503337\n",
      "Iteracion: 4590 Gradiente: [0.03506354025743879,-0.6049406985160978] Loss: 23.188072984786686\n",
      "Iteracion: 4591 Gradiente: [0.03504489940307849,-0.6046190934722601] Loss: 23.187705899688904\n",
      "Iteracion: 4592 Gradiente: [0.03502626845896278,-0.604297659403525] Loss: 23.187339204794778\n",
      "Iteracion: 4593 Gradiente: [0.035007647419574066,-0.6039763962190096] Loss: 23.186972899689543\n",
      "Iteracion: 4594 Gradiente: [0.03498903627962117,-0.6036553038278697] Loss: 23.186606983958864\n",
      "Iteracion: 4595 Gradiente: [0.034970435033931345,-0.6033343821392999] Loss: 23.186241457188824\n",
      "Iteracion: 4596 Gradiente: [0.03495184367728257,-0.6030136310625475] Loss: 23.18587631896597\n",
      "Iteracion: 4597 Gradiente: [0.03493326220436093,-0.6026930505069127] Loss: 23.185511568877295\n",
      "Iteracion: 4598 Gradiente: [0.03491469060991979,-0.6023726403817423] Loss: 23.18514720651023\n",
      "Iteracion: 4599 Gradiente: [0.034896128888651866,-0.6020524005964328] Loss: 23.184783231452606\n",
      "Iteracion: 4600 Gradiente: [0.034877577035387235,-0.6017323310604207] Loss: 23.184419643292742\n",
      "Iteracion: 4601 Gradiente: [0.03485903504492285,-0.6014124316831949] Loss: 23.184056441619372\n",
      "Iteracion: 4602 Gradiente: [0.034840502911888886,-0.6010927023743006] Loss: 23.183693626021654\n",
      "Iteracion: 4603 Gradiente: [0.0348219806312026,-0.6007731430433156] Loss: 23.18333119608922\n",
      "Iteracion: 4604 Gradiente: [0.034803468197361555,-0.6004537535998895] Loss: 23.18296915141211\n",
      "Iteracion: 4605 Gradiente: [0.034784965605418466,-0.6001345339536871] Loss: 23.182607491580782\n",
      "Iteracion: 4606 Gradiente: [0.034766472849982695,-0.5998154840144498] Loss: 23.182246216186183\n",
      "Iteracion: 4607 Gradiente: [0.034747989925872955,-0.599496603691954] Loss: 23.181885324819653\n",
      "Iteracion: 4608 Gradiente: [0.03472951682784545,-0.5991778928960249] Loss: 23.181524817072994\n",
      "Iteracion: 4609 Gradiente: [0.03471105355064878,-0.5988593515365399] Loss: 23.181164692538392\n",
      "Iteracion: 4610 Gradiente: [0.03469260008914811,-0.5985409795234169] Loss: 23.180804950808525\n",
      "Iteracion: 4611 Gradiente: [0.03467415643806836,-0.5982227767666285] Loss: 23.180445591476495\n",
      "Iteracion: 4612 Gradiente: [0.03465572259214771,-0.5979047431761956] Loss: 23.180086614135785\n",
      "Iteracion: 4613 Gradiente: [0.03463729854627881,-0.5975868786621786] Loss: 23.17972801838037\n",
      "Iteracion: 4614 Gradiente: [0.03461888429525857,-0.5972691831346892] Loss: 23.179369803804605\n",
      "Iteracion: 4615 Gradiente: [0.03460047983369444,-0.5969516565039025] Loss: 23.179011970003337\n",
      "Iteracion: 4616 Gradiente: [0.034582085156611694,-0.5966342986800119] Loss: 23.178654516571815\n",
      "Iteracion: 4617 Gradiente: [0.034563700258604516,-0.5963171095732889] Loss: 23.178297443105684\n",
      "Iteracion: 4618 Gradiente: [0.03454532513466025,-0.5960000890940246] Loss: 23.177940749201046\n",
      "Iteracion: 4619 Gradiente: [0.03452695977948584,-0.5956832371525792] Loss: 23.177584434454488\n",
      "Iteracion: 4620 Gradiente: [0.03450860418784221,-0.595366553659354] Loss: 23.1772284984629\n",
      "Iteracion: 4621 Gradiente: [0.034490258354631466,-0.595050038524793] Loss: 23.176872940823742\n",
      "Iteracion: 4622 Gradiente: [0.034471922274603155,-0.5947336916593943] Loss: 23.176517761134786\n",
      "Iteracion: 4623 Gradiente: [0.034453595942597795,-0.5944175129736993] Loss: 23.17616295899429\n",
      "Iteracion: 4624 Gradiente: [0.034435279353393376,-0.594101502378301] Loss: 23.175808534000943\n",
      "Iteracion: 4625 Gradiente: [0.0344169725019763,-0.5937856597838286] Loss: 23.175454485753832\n",
      "Iteracion: 4626 Gradiente: [0.03439867538295971,-0.593469985100977] Loss: 23.175100813852477\n",
      "Iteracion: 4627 Gradiente: [0.034380387991268434,-0.5931544782404766] Loss: 23.174747517896854\n",
      "Iteracion: 4628 Gradiente: [0.034362110321737305,-0.5928391391131065] Loss: 23.17439459748733\n",
      "Iteracion: 4629 Gradiente: [0.03434384236913199,-0.5925239676296989] Loss: 23.174042052224703\n",
      "Iteracion: 4630 Gradiente: [0.03432558412831478,-0.5922089637011254] Loss: 23.17368988171019\n",
      "Iteracion: 4631 Gradiente: [0.03430733559415936,-0.5918941272383099] Loss: 23.17333808554545\n",
      "Iteracion: 4632 Gradiente: [0.03428909676153561,-0.5915794581522188] Loss: 23.172986663332573\n",
      "Iteracion: 4633 Gradiente: [0.034270867625211095,-0.5912649563538721] Loss: 23.172635614674036\n",
      "Iteracion: 4634 Gradiente: [0.03425264817999031,-0.5909506217543402] Loss: 23.172284939172776\n",
      "Iteracion: 4635 Gradiente: [0.03423443842087484,-0.5906364542647243] Loss: 23.17193463643212\n",
      "Iteracion: 4636 Gradiente: [0.03421623834265404,-0.5903224537961885] Loss: 23.17158470605584\n",
      "Iteracion: 4637 Gradiente: [0.03419804794006609,-0.5900086202599435] Loss: 23.171235147648115\n",
      "Iteracion: 4638 Gradiente: [0.03417986720807183,-0.5896949535672389] Loss: 23.170885960813543\n",
      "Iteracion: 4639 Gradiente: [0.03416169614147672,-0.5893814536293782] Loss: 23.170537145157176\n",
      "Iteracion: 4640 Gradiente: [0.034143534735279486,-0.5890681203577017] Loss: 23.170188700284427\n",
      "Iteracion: 4641 Gradiente: [0.03412538298411694,-0.5887549536636176] Loss: 23.169840625801182\n",
      "Iteracion: 4642 Gradiente: [0.034107240883144145,-0.5884419534585507] Loss: 23.169492921313708\n",
      "Iteracion: 4643 Gradiente: [0.03408910842696287,-0.5881291196540059] Loss: 23.169145586428744\n",
      "Iteracion: 4644 Gradiente: [0.03407098561057277,-0.5878164521615138] Loss: 23.16879862075336\n",
      "Iteracion: 4645 Gradiente: [0.034052872428839955,-0.5875039508926563] Loss: 23.168452023895124\n",
      "Iteracion: 4646 Gradiente: [0.03403476887664378,-0.5871916157590652] Loss: 23.168105795461997\n",
      "Iteracion: 4647 Gradiente: [0.03401667494880769,-0.5868794466724209] Loss: 23.167759935062342\n",
      "Iteracion: 4648 Gradiente: [0.03399859064021958,-0.5865674435444473] Loss: 23.167414442304928\n",
      "Iteracion: 4649 Gradiente: [0.033980515945866804,-0.5862556062869103] Loss: 23.167069316798976\n",
      "Iteracion: 4650 Gradiente: [0.03396245086055482,-0.5859439348116311] Loss: 23.166724558154126\n",
      "Iteracion: 4651 Gradiente: [0.03394439537923593,-0.5856324290304736] Loss: 23.166380165980367\n",
      "Iteracion: 4652 Gradiente: [0.0339263494967516,-0.5853210888553515] Loss: 23.1660361398882\n",
      "Iteracion: 4653 Gradiente: [0.033908313207978345,-0.5850099141982248] Loss: 23.165692479488452\n",
      "Iteracion: 4654 Gradiente: [0.0338902865078533,-0.5846989049710966] Loss: 23.1653491843924\n",
      "Iteracion: 4655 Gradiente: [0.03387226939132309,-0.584388061086018] Loss: 23.165006254211736\n",
      "Iteracion: 4656 Gradiente: [0.033854261853177074,-0.5840773824550943] Loss: 23.1646636885586\n",
      "Iteracion: 4657 Gradiente: [0.033836263888401656,-0.5837668689904646] Loss: 23.164321487045434\n",
      "Iteracion: 4658 Gradiente: [0.03381827549198609,-0.5834565206043204] Loss: 23.16397964928524\n",
      "Iteracion: 4659 Gradiente: [0.03380029665864962,-0.58314633720891] Loss: 23.16363817489132\n",
      "Iteracion: 4660 Gradiente: [0.033782327383499934,-0.5828363187165072] Loss: 23.163297063477426\n",
      "Iteracion: 4661 Gradiente: [0.03376436766131027,-0.5825264650394552] Loss: 23.162956314657713\n",
      "Iteracion: 4662 Gradiente: [0.03374641748706608,-0.582216776090129] Loss: 23.162615928046765\n",
      "Iteracion: 4663 Gradiente: [0.033728476855667586,-0.5819072517809547] Loss: 23.162275903259548\n",
      "Iteracion: 4664 Gradiente: [0.03371054576208034,-0.5815978920244018] Loss: 23.161936239911462\n",
      "Iteracion: 4665 Gradiente: [0.03369262420117517,-0.5812886967329957] Loss: 23.161596937618295\n",
      "Iteracion: 4666 Gradiente: [0.033674712167937555,-0.5809796658192954] Loss: 23.161257995996262\n",
      "Iteracion: 4667 Gradiente: [0.03365680965736147,-0.5806707991959087] Loss: 23.160919414661972\n",
      "Iteracion: 4668 Gradiente: [0.03363891666429879,-0.5803620967755019] Loss: 23.160581193232456\n",
      "Iteracion: 4669 Gradiente: [0.03362103318367152,-0.5800535584707786] Loss: 23.160243331325116\n",
      "Iteracion: 4670 Gradiente: [0.0336031592103808,-0.5797451841944924] Loss: 23.159905828557818\n",
      "Iteracion: 4671 Gradiente: [0.03358529473958261,-0.5794369738594284] Loss: 23.159568684548788\n",
      "Iteracion: 4672 Gradiente: [0.033567439766010426,-0.5791289273784438] Loss: 23.159231898916683\n",
      "Iteracion: 4673 Gradiente: [0.03354959428469044,-0.5788210446644233] Loss: 23.15889547128054\n",
      "Iteracion: 4674 Gradiente: [0.033531758290547484,-0.5785133256303067] Loss: 23.158559401259854\n",
      "Iteracion: 4675 Gradiente: [0.0335139317785566,-0.578205770189074] Loss: 23.158223688474436\n",
      "Iteracion: 4676 Gradiente: [0.03349611474369756,-0.577898378253754] Loss: 23.157888332544587\n",
      "Iteracion: 4677 Gradiente: [0.03347830718096721,-0.5775911497374193] Loss: 23.15755333309097\n",
      "Iteracion: 4678 Gradiente: [0.033460509085273316,-0.5772840845531952] Loss: 23.157218689734677\n",
      "Iteracion: 4679 Gradiente: [0.033442720451575765,-0.5769771826142492] Loss: 23.156884402097134\n",
      "Iteracion: 4680 Gradiente: [0.033424941274858115,-0.5766704438337936] Loss: 23.156550469800248\n",
      "Iteracion: 4681 Gradiente: [0.03340717155012764,-0.576363868125087] Loss: 23.156216892466297\n",
      "Iteracion: 4682 Gradiente: [0.03338941127226936,-0.5760574554014416] Loss: 23.155883669717987\n",
      "Iteracion: 4683 Gradiente: [0.03337166043637486,-0.5757512055762021] Loss: 23.155550801178375\n",
      "Iteracion: 4684 Gradiente: [0.03335391903730548,-0.5754451185627739] Loss: 23.15521828647094\n",
      "Iteracion: 4685 Gradiente: [0.03333618707018028,-0.575139194274593] Loss: 23.154886125219583\n",
      "Iteracion: 4686 Gradiente: [0.03331846452987861,-0.5748334326251562] Loss: 23.154554317048575\n",
      "Iteracion: 4687 Gradiente: [0.03330075141145509,-0.5745278335279953] Loss: 23.154222861582607\n",
      "Iteracion: 4688 Gradiente: [0.03328304770982318,-0.5742223968966973] Loss: 23.15389175844677\n",
      "Iteracion: 4689 Gradiente: [0.033265353419999616,-0.5739171226448894] Loss: 23.15356100726652\n",
      "Iteracion: 4690 Gradiente: [0.033247668537096806,-0.5736120106862385] Loss: 23.15323060766776\n",
      "Iteracion: 4691 Gradiente: [0.03322999305596473,-0.5733070609344736] Loss: 23.152900559276745\n",
      "Iteracion: 4692 Gradiente: [0.03321232697169118,-0.5730022733033545] Loss: 23.152570861720157\n",
      "Iteracion: 4693 Gradiente: [0.033194670279208555,-0.5726976477066995] Loss: 23.152241514625082\n",
      "Iteracion: 4694 Gradiente: [0.03317702297358475,-0.57239318405836] Loss: 23.151912517618964\n",
      "Iteracion: 4695 Gradiente: [0.03315938504980428,-0.5720888822722422] Loss: 23.151583870329663\n",
      "Iteracion: 4696 Gradiente: [0.03314175650293407,-0.571784742262291] Loss: 23.15125557238546\n",
      "Iteracion: 4697 Gradiente: [0.03312413732793213,-0.5714807639425051] Loss: 23.15092762341501\n",
      "Iteracion: 4698 Gradiente: [0.03310652751980854,-0.5711769472269258] Loss: 23.150600023047346\n",
      "Iteracion: 4699 Gradiente: [0.033088927073508974,-0.570873292029643] Loss: 23.15027277091189\n",
      "Iteracion: 4700 Gradiente: [0.03307133598429459,-0.5705697982647748] Loss: 23.14994586663852\n",
      "Iteracion: 4701 Gradiente: [0.03305375424689695,-0.5702664658465157] Loss: 23.149619309857467\n",
      "Iteracion: 4702 Gradiente: [0.03303618185650521,-0.5699632946890809] Loss: 23.149293100199298\n",
      "Iteracion: 4703 Gradiente: [0.033018618808237456,-0.5696602847067325] Loss: 23.14896723729509\n",
      "Iteracion: 4704 Gradiente: [0.03300106509699106,-0.5693574358137946] Loss: 23.148641720776215\n",
      "Iteracion: 4705 Gradiente: [0.03298352071774199,-0.5690547479246296] Loss: 23.148316550274505\n",
      "Iteracion: 4706 Gradiente: [0.03296598566572584,-0.5687522209536319] Loss: 23.14799172542212\n",
      "Iteracion: 4707 Gradiente: [0.03294845993589869,-0.5684498548152559] Loss: 23.147667245851654\n",
      "Iteracion: 4708 Gradiente: [0.032930943523257386,-0.5681476494240021] Loss: 23.147343111196065\n",
      "Iteracion: 4709 Gradiente: [0.032913436422950325,-0.5678456046944064] Loss: 23.147019321088752\n",
      "Iteracion: 4710 Gradiente: [0.03289593862983793,-0.5675437205410662] Loss: 23.146695875163434\n",
      "Iteracion: 4711 Gradiente: [0.03287845013915008,-0.5672419968786061] Loss: 23.14637277305426\n",
      "Iteracion: 4712 Gradiente: [0.03286097094582298,-0.5669404336217096] Loss: 23.146050014395772\n",
      "Iteracion: 4713 Gradiente: [0.03284350104502399,-0.5666390306850931] Loss: 23.145727598822884\n",
      "Iteracion: 4714 Gradiente: [0.032826040431713936,-0.5663377879835316] Loss: 23.145405525970897\n",
      "Iteracion: 4715 Gradiente: [0.03280858910105924,-0.5660367054318338] Loss: 23.145083795475504\n",
      "Iteracion: 4716 Gradiente: [0.032791147048082316,-0.5657357829448609] Loss: 23.144762406972795\n",
      "Iteracion: 4717 Gradiente: [0.032773714267793255,-0.565435020437522] Loss: 23.144441360099247\n",
      "Iteracion: 4718 Gradiente: [0.032756290755285515,-0.5651344178247645] Loss: 23.1441206544917\n",
      "Iteracion: 4719 Gradiente: [0.03273887650558815,-0.5648339750215864] Loss: 23.143800289787407\n",
      "Iteracion: 4720 Gradiente: [0.03272147151396325,-0.5645336919430181] Loss: 23.14348026562398\n",
      "Iteracion: 4721 Gradiente: [0.03270407577537166,-0.5642335685041506] Loss: 23.14316058163945\n",
      "Iteracion: 4722 Gradiente: [0.03268668928487083,-0.5639336046201168] Loss: 23.14284123747221\n",
      "Iteracion: 4723 Gradiente: [0.03266931203758266,-0.5636338002060896] Loss: 23.14252223276103\n",
      "Iteracion: 4724 Gradiente: [0.032651944028496395,-0.5633341551772962] Loss: 23.14220356714508\n",
      "Iteracion: 4725 Gradiente: [0.032634585252775614,-0.563034669448997] Loss: 23.141885240263903\n",
      "Iteracion: 4726 Gradiente: [0.032617235705513774,-0.562735342936503] Loss: 23.141567251757454\n",
      "Iteracion: 4727 Gradiente: [0.032599895381861185,-0.5624361755551668] Loss: 23.14124960126602\n",
      "Iteracion: 4728 Gradiente: [0.032582564276829845,-0.5621371672203982] Loss: 23.140932288430314\n",
      "Iteracion: 4729 Gradiente: [0.03256524238553122,-0.561838317847638] Loss: 23.14061531289142\n",
      "Iteracion: 4730 Gradiente: [0.03254792970306823,-0.5615396273523766] Loss: 23.140298674290786\n",
      "Iteracion: 4731 Gradiente: [0.03253062622465374,-0.5612410956501473] Loss: 23.13998237227026\n",
      "Iteracion: 4732 Gradiente: [0.03251333194521635,-0.5609427226565382] Loss: 23.139666406472053\n",
      "Iteracion: 4733 Gradiente: [0.03249604686000301,-0.560644508287168] Loss: 23.13935077653878\n",
      "Iteracion: 4734 Gradiente: [0.032478770963957494,-0.5603464524577165] Loss: 23.139035482113425\n",
      "Iteracion: 4735 Gradiente: [0.032461504252378856,-0.560048555083889] Loss: 23.138720522839364\n",
      "Iteracion: 4736 Gradiente: [0.03244424672035298,-0.5597508160814473] Loss: 23.1384058983603\n",
      "Iteracion: 4737 Gradiente: [0.03242699836292028,-0.559453235366199] Loss: 23.13809160832038\n",
      "Iteracion: 4738 Gradiente: [0.032409759175223485,-0.5591558128539956] Loss: 23.13777765236409\n",
      "Iteracion: 4739 Gradiente: [0.03239252915243564,-0.5588585484607269] Loss: 23.13746403013632\n",
      "Iteracion: 4740 Gradiente: [0.032375308289715576,-0.5585614421023315] Loss: 23.13715074128233\n",
      "Iteracion: 4741 Gradiente: [0.032358096581983395,-0.5582644936948048] Loss: 23.13683778544771\n",
      "Iteracion: 4742 Gradiente: [0.03234089402461298,-0.5579677031541604] Loss: 23.136525162278513\n",
      "Iteracion: 4743 Gradiente: [0.03232370061269686,-0.5576710703964752] Loss: 23.13621287142109\n",
      "Iteracion: 4744 Gradiente: [0.03230651634126218,-0.5573745953378721] Loss: 23.135900912522235\n",
      "Iteracion: 4745 Gradiente: [0.03228934120557199,-0.557078277894507] Loss: 23.135589285229045\n",
      "Iteracion: 4746 Gradiente: [0.03227217520073869,-0.5567821179825891] Loss: 23.13527798918905\n",
      "Iteracion: 4747 Gradiente: [0.03225501832180081,-0.5564861155183742] Loss: 23.134967024050137\n",
      "Iteracion: 4748 Gradiente: [0.03223787056400245,-0.5561902704181553] Loss: 23.134656389460563\n",
      "Iteracion: 4749 Gradiente: [0.03222073192248255,-0.5558945825982723] Loss: 23.13434608506895\n",
      "Iteracion: 4750 Gradiente: [0.03220360239231563,-0.5555990519751142] Loss: 23.134036110524338\n",
      "Iteracion: 4751 Gradiente: [0.0321864819687903,-0.5553036784651046] Loss: 23.13372646547605\n",
      "Iteracion: 4752 Gradiente: [0.03216937064704931,-0.5550084619847155] Loss: 23.13341714957389\n",
      "Iteracion: 4753 Gradiente: [0.03215226842214728,-0.5547134024504726] Loss: 23.133108162467956\n",
      "Iteracion: 4754 Gradiente: [0.03213517528943536,-0.5544184997789282] Loss: 23.13279950380876\n",
      "Iteracion: 4755 Gradiente: [0.03211809124382133,-0.5541237538867047] Loss: 23.13249117324716\n",
      "Iteracion: 4756 Gradiente: [0.03210101628059192,-0.5538291646904452] Loss: 23.1321831704344\n",
      "Iteracion: 4757 Gradiente: [0.03208395039506892,-0.5535347321068375] Loss: 23.131875495022076\n",
      "Iteracion: 4758 Gradiente: [0.03206689358222832,-0.5532404560526314] Loss: 23.131568146662197\n",
      "Iteracion: 4759 Gradiente: [0.03204984583729716,-0.5529463364446092] Loss: 23.131261125007075\n",
      "Iteracion: 4760 Gradiente: [0.03203280715554513,-0.5526523731995956] Loss: 23.130954429709465\n",
      "Iteracion: 4761 Gradiente: [0.0320157775320259,-0.5523585662344684] Loss: 23.13064806042245\n",
      "Iteracion: 4762 Gradiente: [0.03199875696199683,-0.5520649154661413] Loss: 23.130342016799464\n",
      "Iteracion: 4763 Gradiente: [0.0319817454405874,-0.5517714208115791] Loss: 23.130036298494353\n",
      "Iteracion: 4764 Gradiente: [0.031964742963001905,-0.5514780821877843] Loss: 23.129730905161306\n",
      "Iteracion: 4765 Gradiente: [0.031947749524466454,-0.5511848995118063] Loss: 23.12942583645488\n",
      "Iteracion: 4766 Gradiente: [0.03193076512017872,-0.5508918727007363] Loss: 23.129121092030022\n",
      "Iteracion: 4767 Gradiente: [0.03191378974532502,-0.5505990016717139] Loss: 23.12881667154201\n",
      "Iteracion: 4768 Gradiente: [0.03189682339516366,-0.5503062863419158] Loss: 23.128512574646507\n",
      "Iteracion: 4769 Gradiente: [0.03187986606472274,-0.5500137266285781] Loss: 23.128208800999534\n",
      "Iteracion: 4770 Gradiente: [0.03186291774937236,-0.5497213224489618] Loss: 23.1279053502575\n",
      "Iteracion: 4771 Gradiente: [0.03184597844426662,-0.5494290737203824] Loss: 23.127602222077165\n",
      "Iteracion: 4772 Gradiente: [0.031829048144640146,-0.5491369803601968] Loss: 23.127299416115658\n",
      "Iteracion: 4773 Gradiente: [0.031812126845665034,-0.548845042285807] Loss: 23.126996932030433\n",
      "Iteracion: 4774 Gradiente: [0.031795214542546545,-0.5485532594146615] Loss: 23.126694769479375\n",
      "Iteracion: 4775 Gradiente: [0.03177831123060362,-0.5482616316642409] Loss: 23.126392928120698\n",
      "Iteracion: 4776 Gradiente: [0.03176141690487195,-0.5479701589520907] Loss: 23.126091407612968\n",
      "Iteracion: 4777 Gradiente: [0.03174453156071024,-0.5476788411957788] Loss: 23.125790207615132\n",
      "Iteracion: 4778 Gradiente: [0.03172765519332851,-0.5473876783129277] Loss: 23.125489327786497\n",
      "Iteracion: 4779 Gradiente: [0.031710787797998324,-0.5470966702211986] Loss: 23.125188767786724\n",
      "Iteracion: 4780 Gradiente: [0.031693929369831154,-0.5468058168383078] Loss: 23.12488852727585\n",
      "Iteracion: 4781 Gradiente: [0.03167707990418952,-0.5465151180819982] Loss: 23.124588605914262\n",
      "Iteracion: 4782 Gradiente: [0.03166023939615457,-0.5462245738700763] Loss: 23.124289003362712\n",
      "Iteracion: 4783 Gradiente: [0.031643407841122934,-0.5459341841203703] Loss: 23.123989719282314\n",
      "Iteracion: 4784 Gradiente: [0.031626585234261026,-0.5456439487507697] Loss: 23.123690753334532\n",
      "Iteracion: 4785 Gradiente: [0.031609771570754217,-0.5453538676792031] Loss: 23.123392105181193\n",
      "Iteracion: 4786 Gradiente: [0.031592966845958394,-0.5450639408236362] Loss: 23.123093774484502\n",
      "Iteracion: 4787 Gradiente: [0.0315761710549926,-0.5447741681020886] Loss: 23.12279576090701\n",
      "Iteracion: 4788 Gradiente: [0.031559384193264844,-0.5444845494326106] Loss: 23.12249806411161\n",
      "Iteracion: 4789 Gradiente: [0.03154260625589889,-0.5441950847333089] Loss: 23.122200683761594\n",
      "Iteracion: 4790 Gradiente: [0.03152583723827907,-0.5439057739223219] Loss: 23.121903619520573\n",
      "Iteracion: 4791 Gradiente: [0.03150907713549505,-0.5436166169178474] Loss: 23.121606871052514\n",
      "Iteracion: 4792 Gradiente: [0.03149232594294441,-0.5433276136381101] Loss: 23.12131043802179\n",
      "Iteracion: 4793 Gradiente: [0.03147558365577367,-0.5430387640013904] Loss: 23.121014320093074\n",
      "Iteracion: 4794 Gradiente: [0.031458850269429676,-0.542750067925997] Loss: 23.120718516931426\n",
      "Iteracion: 4795 Gradiente: [0.03144212577895189,-0.5424615253303059] Loss: 23.120423028202254\n",
      "Iteracion: 4796 Gradiente: [0.03142541017980894,-0.5421731361327108] Loss: 23.120127853571322\n",
      "Iteracion: 4797 Gradiente: [0.03140870346711324,-0.5418849002516694] Loss: 23.11983299270475\n",
      "Iteracion: 4798 Gradiente: [0.03139200563623585,-0.541596817605669] Loss: 23.119538445269022\n",
      "Iteracion: 4799 Gradiente: [0.03137531668241233,-0.5413088881132481] Loss: 23.119244210930972\n",
      "Iteracion: 4800 Gradiente: [0.031358636601046895,-0.5410211116929758] Loss: 23.118950289357755\n",
      "Iteracion: 4801 Gradiente: [0.03134196538738555,-0.5407334882634786] Loss: 23.11865668021694\n",
      "Iteracion: 4802 Gradiente: [0.03132530303648385,-0.5404460177434355] Loss: 23.118363383176398\n",
      "Iteracion: 4803 Gradiente: [0.03130864954391086,-0.5401587000515382] Loss: 23.118070397904383\n",
      "Iteracion: 4804 Gradiente: [0.031292004904852885,-0.5398715351065453] Loss: 23.117777724069487\n",
      "Iteracion: 4805 Gradiente: [0.03127536911453224,-0.5395845228272539] Loss: 23.11748536134066\n",
      "Iteracion: 4806 Gradiente: [0.03125874216830577,-0.5392976631325016] Loss: 23.117193309387197\n",
      "Iteracion: 4807 Gradiente: [0.031242124061510404,-0.5390109559411641] Loss: 23.116901567878763\n",
      "Iteracion: 4808 Gradiente: [0.031225514789339097,-0.5387244011721751] Loss: 23.116610136485363\n",
      "Iteracion: 4809 Gradiente: [0.031208914347239634,-0.5384379987444933] Loss: 23.116319014877345\n",
      "Iteracion: 4810 Gradiente: [0.03119232273036611,-0.5381517485771379] Loss: 23.116028202725406\n",
      "Iteracion: 4811 Gradiente: [0.031175739934190004,-0.5378656505891517] Loss: 23.115737699700603\n",
      "Iteracion: 4812 Gradiente: [0.031159165953876786,-0.5375797046996417] Loss: 23.115447505474343\n",
      "Iteracion: 4813 Gradiente: [0.031142600784860027,-0.5372939108277398] Loss: 23.1151576197184\n",
      "Iteracion: 4814 Gradiente: [0.03112604442238383,-0.537008268892634] Loss: 23.114868042104835\n",
      "Iteracion: 4815 Gradiente: [0.03110949686181357,-0.5367227788135451] Loss: 23.11457877230613\n",
      "Iteracion: 4816 Gradiente: [0.031092958098251227,-0.5364374405097577] Loss: 23.114289809995082\n",
      "Iteracion: 4817 Gradiente: [0.03107642812738997,-0.5361522539005605] Loss: 23.114001154844836\n",
      "Iteracion: 4818 Gradiente: [0.03105990694427779,-0.5358672189053226] Loss: 23.11371280652886\n",
      "Iteracion: 4819 Gradiente: [0.031043394544435426,-0.5355823354434311] Loss: 23.113424764721042\n",
      "Iteracion: 4820 Gradiente: [0.031026890922998027,-0.5352976034343384] Loss: 23.11313702909553\n",
      "Iteracion: 4821 Gradiente: [0.031010396075430435,-0.5350130227975186] Loss: 23.112849599326896\n",
      "Iteracion: 4822 Gradiente: [0.03099390999704781,-0.534728593452499] Loss: 23.112562475089984\n",
      "Iteracion: 4823 Gradiente: [0.03097743268316909,-0.5344443153188517] Loss: 23.11227565606004\n",
      "Iteracion: 4824 Gradiente: [0.03096096412907817,-0.534160188316188] Loss: 23.11198914191264\n",
      "Iteracion: 4825 Gradiente: [0.03094450433014136,-0.5338762123641623] Loss: 23.111702932323677\n",
      "Iteracion: 4826 Gradiente: [0.03092805328187372,-0.5335923873824621] Loss: 23.111417026969445\n",
      "Iteracion: 4827 Gradiente: [0.03091161097938766,-0.5333087132908393] Loss: 23.111131425526533\n",
      "Iteracion: 4828 Gradiente: [0.030895177418189708,-0.5330251900090686] Loss: 23.110846127671877\n",
      "Iteracion: 4829 Gradiente: [0.03087875259354291,-0.5327418174569785] Loss: 23.110561133082783\n",
      "Iteracion: 4830 Gradiente: [0.030862336500869485,-0.5324585955544326] Loss: 23.110276441436884\n",
      "Iteracion: 4831 Gradiente: [0.03084592913547605,-0.5321755242213466] Loss: 23.109992052412153\n",
      "Iteracion: 4832 Gradiente: [0.030829530492743135,-0.5318926033776683] Loss: 23.109707965686926\n",
      "Iteracion: 4833 Gradiente: [0.03081314056804937,-0.5316098329433937] Loss: 23.109424180939843\n",
      "Iteracion: 4834 Gradiente: [0.03079675935677623,-0.5313272128385597] Loss: 23.10914069784992\n",
      "Iteracion: 4835 Gradiente: [0.03078038685419339,-0.5310447429832528] Loss: 23.108857516096503\n",
      "Iteracion: 4836 Gradiente: [0.030764023055708852,-0.5307624232975924] Loss: 23.10857463535928\n",
      "Iteracion: 4837 Gradiente: [0.030747667956716403,-0.530480253701743] Loss: 23.108292055318262\n",
      "Iteracion: 4838 Gradiente: [0.03073132155265815,-0.5301982341159087] Loss: 23.10800977565383\n",
      "Iteracion: 4839 Gradiente: [0.030714983838841668,-0.5299163644603425] Loss: 23.107727796046674\n",
      "Iteracion: 4840 Gradiente: [0.03069865481067874,-0.529634644655336] Loss: 23.107446116177865\n",
      "Iteracion: 4841 Gradiente: [0.030682334463455162,-0.5293530746212299] Loss: 23.107164735728762\n",
      "Iteracion: 4842 Gradiente: [0.03066602279269072,-0.5290716542783932] Loss: 23.10688365438111\n",
      "Iteracion: 4843 Gradiente: [0.030649719793673095,-0.52879038354725] Loss: 23.106602871816943\n",
      "Iteracion: 4844 Gradiente: [0.030633425461854814,-0.5285092623482599] Loss: 23.106322387718702\n",
      "Iteracion: 4845 Gradiente: [0.030617139792585134,-0.5282282906019289] Loss: 23.106042201769082\n",
      "Iteracion: 4846 Gradiente: [0.030600862781255955,-0.5279474682288048] Loss: 23.105762313651176\n",
      "Iteracion: 4847 Gradiente: [0.030584594423274325,-0.5276667951494746] Loss: 23.105482723048397\n",
      "Iteracion: 4848 Gradiente: [0.030568334714070035,-0.5273862712845676] Loss: 23.105203429644497\n",
      "Iteracion: 4849 Gradiente: [0.03055208364901792,-0.5271058965547571] Loss: 23.10492443312353\n",
      "Iteracion: 4850 Gradiente: [0.030535841223676623,-0.5268256708807512] Loss: 23.104645733169956\n",
      "Iteracion: 4851 Gradiente: [0.030519607433168023,-0.5265455941833239] Loss: 23.104367329468516\n",
      "Iteracion: 4852 Gradiente: [0.030503382273024234,-0.5262656663832661] Loss: 23.104089221704303\n",
      "Iteracion: 4853 Gradiente: [0.030487165738764095,-0.5259858874014133] Loss: 23.103811409562717\n",
      "Iteracion: 4854 Gradiente: [0.03047095782562413,-0.5257062571586605] Loss: 23.10353389272954\n",
      "Iteracion: 4855 Gradiente: [0.030454758529151603,-0.525426775575925] Loss: 23.103256670890865\n",
      "Iteracion: 4856 Gradiente: [0.030438567844725147,-0.5251474425741784] Loss: 23.10297974373312\n",
      "Iteracion: 4857 Gradiente: [0.030422385767736652,-0.5248682580744303] Loss: 23.102703110943057\n",
      "Iteracion: 4858 Gradiente: [0.03040621229369075,-0.5245892219977297] Loss: 23.102426772207775\n",
      "Iteracion: 4859 Gradiente: [0.030390047417840075,-0.5243103342651786] Loss: 23.102150727214696\n",
      "Iteracion: 4860 Gradiente: [0.030373891135837994,-0.5240315947978998] Loss: 23.101874975651587\n",
      "Iteracion: 4861 Gradiente: [0.030357743443026188,-0.5237530035170778] Loss: 23.101599517206527\n",
      "Iteracion: 4862 Gradiente: [0.03034160433477003,-0.5234745603439354] Loss: 23.101324351567953\n",
      "Iteracion: 4863 Gradiente: [0.030325473806586463,-0.523196265199728] Loss: 23.101049478424596\n",
      "Iteracion: 4864 Gradiente: [0.030309351853882543,-0.5229181180057626] Loss: 23.10077489746555\n",
      "Iteracion: 4865 Gradiente: [0.03029323847214395,-0.5226401186833805] Loss: 23.10050060838024\n",
      "Iteracion: 4866 Gradiente: [0.03027713365669437,-0.5223622671539752] Loss: 23.100226610858396\n",
      "Iteracion: 4867 Gradiente: [0.030261037403085802,-0.5220845633389709] Loss: 23.099952904590108\n",
      "Iteracion: 4868 Gradiente: [0.030244949706821938,-0.5218070071598339] Loss: 23.09967948926575\n",
      "Iteracion: 4869 Gradiente: [0.03022887056322645,-0.5215295985380828] Loss: 23.099406364576087\n",
      "Iteracion: 4870 Gradiente: [0.030212799967753768,-0.5212523373952729] Loss: 23.09913353021216\n",
      "Iteracion: 4871 Gradiente: [0.030196737915855466,-0.5209752236530002] Loss: 23.098860985865382\n",
      "Iteracion: 4872 Gradiente: [0.03018068440301723,-0.5206982572329008] Loss: 23.098588731227434\n",
      "Iteracion: 4873 Gradiente: [0.030164639424862118,-0.5204214380566418] Loss: 23.098316765990397\n",
      "Iteracion: 4874 Gradiente: [0.030148602976595384,-0.5201447660459615] Loss: 23.098045089846625\n",
      "Iteracion: 4875 Gradiente: [0.03013257505386567,-0.5198682411226099] Loss: 23.097773702488816\n",
      "Iteracion: 4876 Gradiente: [0.03011655565208476,-0.5195918632083953] Loss: 23.09750260361002\n",
      "Iteracion: 4877 Gradiente: [0.030100544766697605,-0.5193156322251632] Loss: 23.097231792903564\n",
      "Iteracion: 4878 Gradiente: [0.030084542393171886,-0.5190395480948027] Loss: 23.09696127006314\n",
      "Iteracion: 4879 Gradiente: [0.030068548526929816,-0.5187636107392432] Loss: 23.09669103478274\n",
      "Iteracion: 4880 Gradiente: [0.030052563163650348,-0.5184878200804452] Loss: 23.096421086756717\n",
      "Iteracion: 4881 Gradiente: [0.030036586298678003,-0.5182121760404258] Loss: 23.0961514256797\n",
      "Iteracion: 4882 Gradiente: [0.03002061792739994,-0.5179366785412446] Loss: 23.095882051246686\n",
      "Iteracion: 4883 Gradiente: [0.030004658045419318,-0.5176613275049899] Loss: 23.09561296315297\n",
      "Iteracion: 4884 Gradiente: [0.029988706648134668,-0.5173861228538011] Loss: 23.09534416109418\n",
      "Iteracion: 4885 Gradiente: [0.029972763731224934,-0.5171110645098456] Loss: 23.095075644766272\n",
      "Iteracion: 4886 Gradiente: [0.02995682929001191,-0.5168361523953507] Loss: 23.094807413865514\n",
      "Iteracion: 4887 Gradiente: [0.02994090332000686,-0.5165613864325774] Loss: 23.094539468088513\n",
      "Iteracion: 4888 Gradiente: [0.0299249858167542,-0.5162867665438223] Loss: 23.09427180713217\n",
      "Iteracion: 4889 Gradiente: [0.029909076775863734,-0.5160122926514205] Loss: 23.094004430693747\n",
      "Iteracion: 4890 Gradiente: [0.029893176192552082,-0.5157379646777741] Loss: 23.093737338470795\n",
      "Iteracion: 4891 Gradiente: [0.029877284062505775,-0.5154637825452983] Loss: 23.0934705301612\n",
      "Iteracion: 4892 Gradiente: [0.029861400381220923,-0.5151897461764587] Loss: 23.093204005463175\n",
      "Iteracion: 4893 Gradiente: [0.029845525144143417,-0.5149158554937678] Loss: 23.092937764075245\n",
      "Iteracion: 4894 Gradiente: [0.029829658346889686,-0.5146421104197654] Loss: 23.09267180569625\n",
      "Iteracion: 4895 Gradiente: [0.02981379998487815,-0.5143685108770508] Loss: 23.09240613002536\n",
      "Iteracion: 4896 Gradiente: [0.029797950053771654,-0.5140950567882429] Loss: 23.09214073676207\n",
      "Iteracion: 4897 Gradiente: [0.02978210854881714,-0.5138217480760316] Loss: 23.091875625606175\n",
      "Iteracion: 4898 Gradiente: [0.029766275465744722,-0.513548584663119] Loss: 23.091610796257815\n",
      "Iteracion: 4899 Gradiente: [0.029750450800061876,-0.5132755664722587] Loss: 23.091346248417423\n",
      "Iteracion: 4900 Gradiente: [0.029734634547294074,-0.5130026934262466] Loss: 23.09108198178577\n",
      "Iteracion: 4901 Gradiente: [0.029718826702861634,-0.5127299654479255] Loss: 23.090817996063926\n",
      "Iteracion: 4902 Gradiente: [0.0297030272623734,-0.5124573824601681] Loss: 23.090554290953303\n",
      "Iteracion: 4903 Gradiente: [0.029687236221302746,-0.5121849443858971] Loss: 23.090290866155605\n",
      "Iteracion: 4904 Gradiente: [0.029671453575288827,-0.5119126511480663] Loss: 23.09002772137287\n",
      "Iteracion: 4905 Gradiente: [0.029655679319875124,-0.5116405026696745] Loss: 23.089764856307468\n",
      "Iteracion: 4906 Gradiente: [0.029639913450469634,-0.5113684988737717] Loss: 23.08950227066203\n",
      "Iteracion: 4907 Gradiente: [0.02962415596267268,-0.5110966396834368] Loss: 23.08923996413956\n",
      "Iteracion: 4908 Gradiente: [0.02960840685202489,-0.5108249250217927] Loss: 23.088977936443353\n",
      "Iteracion: 4909 Gradiente: [0.02959266611409343,-0.5105533548120021] Loss: 23.088716187277008\n",
      "Iteracion: 4910 Gradiente: [0.02957693374449756,-0.5102819289772671] Loss: 23.088454716344476\n",
      "Iteracion: 4911 Gradiente: [0.02956120973872013,-0.5100106474408365] Loss: 23.088193523349993\n",
      "Iteracion: 4912 Gradiente: [0.029545494092192825,-0.5097395101260047] Loss: 23.087932607998123\n",
      "Iteracion: 4913 Gradiente: [0.02952978680065712,-0.5094685169560882] Loss: 23.087671969993718\n",
      "Iteracion: 4914 Gradiente: [0.02951408785952007,-0.5091976678544626] Loss: 23.087411609041965\n",
      "Iteracion: 4915 Gradiente: [0.029498397264550627,-0.5089269627445252] Loss: 23.087151524848398\n",
      "Iteracion: 4916 Gradiente: [0.029482715011086687,-0.5086564015497396] Loss: 23.086891717118803\n",
      "Iteracion: 4917 Gradiente: [0.029467041094794883,-0.5083859841935897] Loss: 23.086632185559292\n",
      "Iteracion: 4918 Gradiente: [0.029451375511327645,-0.5081157105996009] Loss: 23.086372929876315\n",
      "Iteracion: 4919 Gradiente: [0.029435718256140338,-0.5078455806913523] Loss: 23.086113949776625\n",
      "Iteracion: 4920 Gradiente: [0.02942006932475181,-0.5075755943924601] Loss: 23.08585524496728\n",
      "Iteracion: 4921 Gradiente: [0.029404428712849533,-0.5073057516265688] Loss: 23.085596815155654\n",
      "Iteracion: 4922 Gradiente: [0.029388796416021514,-0.5070360523173713] Loss: 23.085338660049413\n",
      "Iteracion: 4923 Gradiente: [0.02937317242974397,-0.5067664963886097] Loss: 23.0850807793566\n",
      "Iteracion: 4924 Gradiente: [0.029357556749710058,-0.5064970837640509] Loss: 23.084823172785466\n",
      "Iteracion: 4925 Gradiente: [0.029341949371314513,-0.5062278143675191] Loss: 23.084565840044643\n",
      "Iteracion: 4926 Gradiente: [0.029326350290390716,-0.5059586881228577] Loss: 23.08430878084305\n",
      "Iteracion: 4927 Gradiente: [0.029310759502440456,-0.5056897049539685] Loss: 23.084051994889947\n",
      "Iteracion: 4928 Gradiente: [0.029295177003019528,-0.5054208647847883] Loss: 23.083795481894846\n",
      "Iteracion: 4929 Gradiente: [0.02927960278763256,-0.5051521675392997] Loss: 23.083539241567617\n",
      "Iteracion: 4930 Gradiente: [0.029264036852111985,-0.5048836131415066] Loss: 23.083283273618406\n",
      "Iteracion: 4931 Gradiente: [0.029248479191817486,-0.5046152015154809] Loss: 23.083027577757676\n",
      "Iteracion: 4932 Gradiente: [0.02923292980260612,-0.5043469325853054] Loss: 23.08277215369624\n",
      "Iteracion: 4933 Gradiente: [0.029217388679821474,-0.5040788062751337] Loss: 23.082517001145153\n",
      "Iteracion: 4934 Gradiente: [0.02920185581916428,-0.5038108225091397] Loss: 23.0822621198158\n",
      "Iteracion: 4935 Gradiente: [0.029186331216230123,-0.5035429812115411] Loss: 23.082007509419878\n",
      "Iteracion: 4936 Gradiente: [0.029170814866707435,-0.5032752823065958] Loss: 23.081753169669405\n",
      "Iteracion: 4937 Gradiente: [0.029155306766127372,-0.5030077257186052] Loss: 23.08149910027668\n",
      "Iteracion: 4938 Gradiente: [0.029139806910140464,-0.5027403113719102] Loss: 23.08124530095432\n",
      "Iteracion: 4939 Gradiente: [0.02912431529434893,-0.5024730391908891] Loss: 23.080991771415263\n",
      "Iteracion: 4940 Gradiente: [0.029108831914406132,-0.5022059090999633] Loss: 23.080738511372726\n",
      "Iteracion: 4941 Gradiente: [0.029093356765875454,-0.5019389210235937] Loss: 23.080485520540222\n",
      "Iteracion: 4942 Gradiente: [0.029077889844374264,-0.5016720748862824] Loss: 23.08023279863158\n",
      "Iteracion: 4943 Gradiente: [0.029062431145561618,-0.5014053706125697] Loss: 23.079980345360987\n",
      "Iteracion: 4944 Gradiente: [0.029046980665123102,-0.5011388081270315] Loss: 23.07972816044285\n",
      "Iteracion: 4945 Gradiente: [0.02903153839857756,-0.5008723873542958] Loss: 23.079476243591923\n",
      "Iteracion: 4946 Gradiente: [0.02901610434167689,-0.5006061082190173] Loss: 23.07922459452325\n",
      "Iteracion: 4947 Gradiente: [0.029000678489952254,-0.5003399706459036] Loss: 23.07897321295221\n",
      "Iteracion: 4948 Gradiente: [0.028985260839084502,-0.5000739745596922] Loss: 23.078722098594426\n",
      "Iteracion: 4949 Gradiente: [0.028969851384797116,-0.4998081198851595] Loss: 23.078471251165865\n",
      "Iteracion: 4950 Gradiente: [0.028954450122509457,-0.49954240654714016] Loss: 23.078220670382805\n",
      "Iteracion: 4951 Gradiente: [0.028939057048132592,-0.4992768344704803] Loss: 23.077970355961785\n",
      "Iteracion: 4952 Gradiente: [0.028923672157156945,-0.49901140358009094] Loss: 23.0777203076197\n",
      "Iteracion: 4953 Gradiente: [0.02890829544525957,-0.4987461138009098] Loss: 23.07747052507368\n",
      "Iteracion: 4954 Gradiente: [0.028892926908030366,-0.49848096505792167] Loss: 23.077221008041192\n",
      "Iteracion: 4955 Gradiente: [0.02887756654116629,-0.49821595727614465] Loss: 23.076971756239992\n",
      "Iteracion: 4956 Gradiente: [0.028862214340426816,-0.4979510903806349] Loss: 23.076722769388194\n",
      "Iteracion: 4957 Gradiente: [0.028846870301302376,-0.4976863642965013] Loss: 23.076474047204126\n",
      "Iteracion: 4958 Gradiente: [0.028831534419630125,-0.4974217789488749] Loss: 23.07622558940643\n",
      "Iteracion: 4959 Gradiente: [0.02881620669092039,-0.4971573342629443] Loss: 23.075977395714116\n",
      "Iteracion: 4960 Gradiente: [0.028800887111020756,-0.49689303016391845] Loss: 23.075729465846432\n",
      "Iteracion: 4961 Gradiente: [0.028785575675363852,-0.49662886657707167] Loss: 23.075481799522912\n",
      "Iteracion: 4962 Gradiente: [0.028770272379803903,-0.49636484342769205] Loss: 23.075234396463454\n",
      "Iteracion: 4963 Gradiente: [0.02875497721992038,-0.4961009606411237] Loss: 23.074987256388184\n",
      "Iteracion: 4964 Gradiente: [0.028739690191352453,-0.4958372181427484] Loss: 23.074740379017562\n",
      "Iteracion: 4965 Gradiente: [0.02872441128988612,-0.49557361585797843] Loss: 23.07449376407234\n",
      "Iteracion: 4966 Gradiente: [0.02870914051107813,-0.49531015371227777] Loss: 23.074247411273568\n",
      "Iteracion: 4967 Gradiente: [0.02869387785080638,-0.49504683163113455] Loss: 23.07400132034259\n",
      "Iteracion: 4968 Gradiente: [0.02867862330456224,-0.49478364954009585] Loss: 23.073755491001034\n",
      "Iteracion: 4969 Gradiente: [0.02866337686815541,-0.49452060736473213] Loss: 23.073509922970857\n",
      "Iteracion: 4970 Gradiente: [0.028648138537210835,-0.4942577050306635] Loss: 23.073264615974278\n",
      "Iteracion: 4971 Gradiente: [0.02863290830740368,-0.49399494246354664] Loss: 23.07301956973382\n",
      "Iteracion: 4972 Gradiente: [0.028617686174478272,-0.49373231958907515] Loss: 23.07277478397231\n",
      "Iteracion: 4973 Gradiente: [0.028602472134062397,-0.4934698363329871] Loss: 23.072530258412876\n",
      "Iteracion: 4974 Gradiente: [0.02858726618192217,-0.49320749262105373] Loss: 23.072285992778905\n",
      "Iteracion: 4975 Gradiente: [0.02857206831382276,-0.4929452883790842] Loss: 23.072041986794137\n",
      "Iteracion: 4976 Gradiente: [0.028556878525311427,-0.4926832235329428] Loss: 23.07179824018253\n",
      "Iteracion: 4977 Gradiente: [0.028541696812178923,-0.49242129800851586] Loss: 23.071554752668398\n",
      "Iteracion: 4978 Gradiente: [0.028526523170034086,-0.49215951173174116] Loss: 23.07131152397632\n",
      "Iteracion: 4979 Gradiente: [0.028511357594683772,-0.49189786462858615] Loss: 23.071068553831175\n",
      "Iteracion: 4980 Gradiente: [0.028496200081860934,-0.4916363566250594] Loss: 23.070825841958122\n",
      "Iteracion: 4981 Gradiente: [0.028481050627197153,-0.49137498764721743] Loss: 23.07058338808264\n",
      "Iteracion: 4982 Gradiente: [0.028465909226417806,-0.49111375762114806] Loss: 23.07034119193049\n",
      "Iteracion: 4983 Gradiente: [0.02845077587533543,-0.4908526664729758] Loss: 23.070099253227692\n",
      "Iteracion: 4984 Gradiente: [0.028435650569564548,-0.4905917141288739] Loss: 23.069857571700602\n",
      "Iteracion: 4985 Gradiente: [0.028420533304945175,-0.4903309005150432] Loss: 23.069616147075838\n",
      "Iteracion: 4986 Gradiente: [0.02840542407708142,-0.49007022555773905] Loss: 23.069374979080333\n",
      "Iteracion: 4987 Gradiente: [0.02839032288176213,-0.48980968918324214] Loss: 23.069134067441272\n",
      "Iteracion: 4988 Gradiente: [0.02837522971465963,-0.4895492913178808] Loss: 23.068893411886172\n",
      "Iteracion: 4989 Gradiente: [0.028360144571593082,-0.4892890318880146] Loss: 23.06865301214281\n",
      "Iteracion: 4990 Gradiente: [0.02834506744826323,-0.4890289108200509] Loss: 23.06841286793928\n",
      "Iteracion: 4991 Gradiente: [0.028329998340330082,-0.4887689280404339] Loss: 23.06817297900392\n",
      "Iteracion: 4992 Gradiente: [0.028314937243612802,-0.4885090834756428] Loss: 23.0679333450654\n",
      "Iteracion: 4993 Gradiente: [0.0282998841539334,-0.48824937705219207] Loss: 23.067693965852683\n",
      "Iteracion: 4994 Gradiente: [0.028284839066851458,-0.487989808696652] Loss: 23.06745484109498\n",
      "Iteracion: 4995 Gradiente: [0.028269801978151085,-0.4877303783356189] Loss: 23.067215970521808\n",
      "Iteracion: 4996 Gradiente: [0.028254772883679873,-0.48747108589572896] Loss: 23.066977353863006\n",
      "Iteracion: 4997 Gradiente: [0.028239751779127194,-0.48721193130365825] Loss: 23.066738990848613\n",
      "Iteracion: 4998 Gradiente: [0.02822473866023832,-0.4869529144861234] Loss: 23.066500881209056\n",
      "Iteracion: 4999 Gradiente: [0.02820973352283621,-0.48669403536987743] Loss: 23.06626302467498\n",
      "Iteracion: 5000 Gradiente: [0.02819473636255907,-0.48643529388171924] Loss: 23.06602542097737\n",
      "Iteracion: 5001 Gradiente: [0.028179747175317023,-0.48617668994847363] Loss: 23.065788069847414\n",
      "Iteracion: 5002 Gradiente: [0.028164765956790915,-0.48591822349701663] Loss: 23.06555097101668\n",
      "Iteracion: 5003 Gradiente: [0.0281497927026597,-0.4856598944542629] Loss: 23.065314124216968\n",
      "Iteracion: 5004 Gradiente: [0.02813482740887139,-0.48540170274715194] Loss: 23.06507752918037\n",
      "Iteracion: 5005 Gradiente: [0.028119870071020615,-0.48514364830268114] Loss: 23.06484118563927\n",
      "Iteracion: 5006 Gradiente: [0.02810492068485549,-0.48488573104788046] Loss: 23.064605093326335\n",
      "Iteracion: 5007 Gradiente: [0.028089979246360978,-0.48462795090980165] Loss: 23.06436925197452\n",
      "Iteracion: 5008 Gradiente: [0.028075045751156342,-0.4843703078155598] Loss: 23.06413366131704\n",
      "Iteracion: 5009 Gradiente: [0.028060120195110963,-0.4841128016922914] Loss: 23.06389832108743\n",
      "Iteracion: 5010 Gradiente: [0.02804520257389053,-0.483855432467186] Loss: 23.063663231019483\n",
      "Iteracion: 5011 Gradiente: [0.028030292883436423,-0.4835982000674543] Loss: 23.06342839084729\n",
      "Iteracion: 5012 Gradiente: [0.028015391119334748,-0.4833411044203665] Loss: 23.063193800305214\n",
      "Iteracion: 5013 Gradiente: [0.028000497277486148,-0.48308414545321743] Loss: 23.062959459127892\n",
      "Iteracion: 5014 Gradiente: [0.02798561135361221,-0.48282732309334475] Loss: 23.06272536705027\n",
      "Iteracion: 5015 Gradiente: [0.027970733343717788,-0.4825706372681106] Loss: 23.062491523807548\n",
      "Iteracion: 5016 Gradiente: [0.027955863243293303,-0.48231408790494873] Loss: 23.06225792913524\n",
      "Iteracion: 5017 Gradiente: [0.02794100104832277,-0.4820576749312997] Loss: 23.062024582769087\n",
      "Iteracion: 5018 Gradiente: [0.027926146754469982,-0.48180139827466334] Loss: 23.061791484445173\n",
      "Iteracion: 5019 Gradiente: [0.027911300357682953,-0.4815452578625601] Loss: 23.06155863389982\n",
      "Iteracion: 5020 Gradiente: [0.02789646185372779,-0.4812892536225596] Loss: 23.06132603086967\n",
      "Iteracion: 5021 Gradiente: [0.027881631238331768,-0.4810333854822743] Loss: 23.061093675091577\n",
      "Iteracion: 5022 Gradiente: [0.027866808507369,-0.48077765336934525] Loss: 23.06086156630276\n",
      "Iteracion: 5023 Gradiente: [0.02785199365651939,-0.4805220572114628] Loss: 23.06062970424065\n",
      "Iteracion: 5024 Gradiente: [0.02783718668176694,-0.48026659693633983] Loss: 23.06039808864297\n",
      "Iteracion: 5025 Gradiente: [0.027822387578892934,-0.4800112724717375] Loss: 23.060166719247768\n",
      "Iteracion: 5026 Gradiente: [0.027807596343656846,-0.47975608374545914] Loss: 23.05993559579332\n",
      "Iteracion: 5027 Gradiente: [0.027792812971965,-0.4795010306853361] Loss: 23.059704718018182\n",
      "Iteracion: 5028 Gradiente: [0.0277780374593893,-0.4792461132192599] Loss: 23.05947408566122\n",
      "Iteracion: 5029 Gradiente: [0.02776326980211934,-0.4789913312751214] Loss: 23.059243698461557\n",
      "Iteracion: 5030 Gradiente: [0.027748509995715646,-0.4787366847808894] Loss: 23.059013556158593\n",
      "Iteracion: 5031 Gradiente: [0.027733758036027702,-0.47848217366455387] Loss: 23.058783658492008\n",
      "Iteracion: 5032 Gradiente: [0.027719013918855732,-0.4782277978541444] Loss: 23.058554005201756\n",
      "Iteracion: 5033 Gradiente: [0.02770427764034006,-0.47797355727770896] Loss: 23.05832459602808\n",
      "Iteracion: 5034 Gradiente: [0.027689549195899114,-0.47771945186337905] Loss: 23.058095430711475\n",
      "Iteracion: 5035 Gradiente: [0.027674828581641955,-0.4774654815392816] Loss: 23.057866508992735\n",
      "Iteracion: 5036 Gradiente: [0.027660115793283542,-0.4772116462336064] Loss: 23.057637830612926\n",
      "Iteracion: 5037 Gradiente: [0.027645410826683776,-0.47695794587457185] Loss: 23.057409395313375\n",
      "Iteracion: 5038 Gradiente: [0.027630713677692143,-0.47670438039043517] Loss: 23.057181202835693\n",
      "Iteracion: 5039 Gradiente: [0.027616024342239597,-0.47645094970948826] Loss: 23.05695325292176\n",
      "Iteracion: 5040 Gradiente: [0.02760134281603162,-0.47619765376007417] Loss: 23.05672554531375\n",
      "Iteracion: 5041 Gradiente: [0.027586669095063597,-0.47594449247055537] Loss: 23.056498079754103\n",
      "Iteracion: 5042 Gradiente: [0.027572003174934898,-0.47569146576935883] Loss: 23.0562708559855\n",
      "Iteracion: 5043 Gradiente: [0.02755734505180101,-0.47543857358491537] Loss: 23.056043873750923\n",
      "Iteracion: 5044 Gradiente: [0.02754269472122246,-0.4751858158457278] Loss: 23.05581713279367\n",
      "Iteracion: 5045 Gradiente: [0.02752805217933864,-0.47493319248030585] Loss: 23.05559063285721\n",
      "Iteracion: 5046 Gradiente: [0.027513417421837973,-0.474680703417221] Loss: 23.055364373685393\n",
      "Iteracion: 5047 Gradiente: [0.027498790444617307,-0.4744283485850726] Loss: 23.055138355022258\n",
      "Iteracion: 5048 Gradiente: [0.027484171243515713,-0.47417612791250113] Loss: 23.05491257661218\n",
      "Iteracion: 5049 Gradiente: [0.027469559814437616,-0.4739240413281795] Loss: 23.054687038199738\n",
      "Iteracion: 5050 Gradiente: [0.02745495615333766,-0.4736720887608192] Loss: 23.054461739529838\n",
      "Iteracion: 5051 Gradiente: [0.027440360255869222,-0.47342027013918486] Loss: 23.054236680347643\n",
      "Iteracion: 5052 Gradiente: [0.027425772118152735,-0.47316858539205114] Loss: 23.054011860398564\n",
      "Iteracion: 5053 Gradiente: [0.02741119173582831,-0.472917034448261] Loss: 23.053787279428327\n",
      "Iteracion: 5054 Gradiente: [0.02739661910494533,-0.47266561723667083] Loss: 23.053562937182893\n",
      "Iteracion: 5055 Gradiente: [0.027382054221266116,-0.47241433368619057] Loss: 23.05333883340849\n",
      "Iteracion: 5056 Gradiente: [0.027367497080709312,-0.4721631837257598] Loss: 23.053114967851666\n",
      "Iteracion: 5057 Gradiente: [0.027352947679243775,-0.4719121672843523] Loss: 23.052891340259148\n",
      "Iteracion: 5058 Gradiente: [0.027338406012664032,-0.47166128429099197] Loss: 23.052667950378016\n",
      "Iteracion: 5059 Gradiente: [0.027323872076840415,-0.4714105346747342] Loss: 23.052444797955598\n",
      "Iteracion: 5060 Gradiente: [0.027309345867715253,-0.471159918364668] Loss: 23.052221882739442\n",
      "Iteracion: 5061 Gradiente: [0.027294827381162654,-0.47090943528992557] Loss: 23.05199920447743\n",
      "Iteracion: 5062 Gradiente: [0.027280316613179898,-0.47065908537966916] Loss: 23.051776762917697\n",
      "Iteracion: 5063 Gradiente: [0.02726581355951794,-0.4704088685631113] Loss: 23.05155455780859\n",
      "Iteracion: 5064 Gradiente: [0.02725131821612005,-0.47015878476949347] Loss: 23.051332588898813\n",
      "Iteracion: 5065 Gradiente: [0.027236830578977824,-0.4699088339280916] Loss: 23.051110855937264\n",
      "Iteracion: 5066 Gradiente: [0.027222350643819483,-0.4696590159682349] Loss: 23.050889358673153\n",
      "Iteracion: 5067 Gradiente: [0.027207878406556082,-0.4694093308192786] Loss: 23.050668096855926\n",
      "Iteracion: 5068 Gradiente: [0.02719341386329006,-0.46915977841060547] Loss: 23.05044707023531\n",
      "Iteracion: 5069 Gradiente: [0.027178957009743005,-0.4689103586716571] Loss: 23.050226278561286\n",
      "Iteracion: 5070 Gradiente: [0.02716450784198609,-0.46866107153189207] Loss: 23.050005721584125\n",
      "Iteracion: 5071 Gradiente: [0.027150066355794897,-0.46841191692082673] Loss: 23.049785399054365\n",
      "Iteracion: 5072 Gradiente: [0.027135632547158176,-0.46816289476799766] Loss: 23.04956531072275\n",
      "Iteracion: 5073 Gradiente: [0.02712120641196615,-0.4679140050029902] Loss: 23.049345456340358\n",
      "Iteracion: 5074 Gradiente: [0.02710678794614978,-0.4676652475554212] Loss: 23.049125835658522\n",
      "Iteracion: 5075 Gradiente: [0.027092377145718653,-0.46741662235494263] Loss: 23.048906448428802\n",
      "Iteracion: 5076 Gradiente: [0.027077974006401936,-0.46716812933125856] Loss: 23.048687294403024\n",
      "Iteracion: 5077 Gradiente: [0.02706357852427838,-0.4669197684140902] Loss: 23.048468373333353\n",
      "Iteracion: 5078 Gradiente: [0.027049190695205994,-0.46667153953321083] Loss: 23.048249684972106\n",
      "Iteracion: 5079 Gradiente: [0.027034810515092052,-0.4664234426184271] Loss: 23.04803122907196\n",
      "Iteracion: 5080 Gradiente: [0.027020437980059833,-0.46617547759956984] Loss: 23.04781300538578\n",
      "Iteracion: 5081 Gradiente: [0.027006073085887768,-0.4659276444065295] Loss: 23.047595013666754\n",
      "Iteracion: 5082 Gradiente: [0.026991715828475548,-0.46567994296922516] Loss: 23.047377253668277\n",
      "Iteracion: 5083 Gradiente: [0.026977366203863085,-0.46543237321760633] Loss: 23.04715972514407\n",
      "Iteracion: 5084 Gradiente: [0.026963024207923545,-0.46518493508166675] Loss: 23.046942427848055\n",
      "Iteracion: 5085 Gradiente: [0.02694868983659641,-0.4649376284914374] Loss: 23.046725361534456\n",
      "Iteracion: 5086 Gradiente: [0.026934363085906434,-0.4646904533769811] Loss: 23.046508525957737\n",
      "Iteracion: 5087 Gradiente: [0.026920043951714472,-0.4644434096684037] Loss: 23.04629192087261\n",
      "Iteracion: 5088 Gradiente: [0.02690573243003674,-0.46419649729584395] Loss: 23.046075546034118\n",
      "Iteracion: 5089 Gradiente: [0.026891428516832624,-0.463949716189479] Loss: 23.045859401197472\n",
      "Iteracion: 5090 Gradiente: [0.026877132207980027,-0.4637030662795262] Loss: 23.04564348611819\n",
      "Iteracion: 5091 Gradiente: [0.026862843499475274,-0.46345654749623943] Loss: 23.045427800552066\n",
      "Iteracion: 5092 Gradiente: [0.026848562387367755,-0.4632101597698989] Loss: 23.04521234425509\n",
      "Iteracion: 5093 Gradiente: [0.02683428886754863,-0.46296390303083484] Loss: 23.044997116983605\n",
      "Iteracion: 5094 Gradiente: [0.026820022935861706,-0.4627177772094205] Loss: 23.044782118494116\n",
      "Iteracion: 5095 Gradiente: [0.02680576458839236,-0.46247178223604546] Loss: 23.044567348543474\n",
      "Iteracion: 5096 Gradiente: [0.026791513821192818,-0.46222591804114593] Loss: 23.044352806888732\n",
      "Iteracion: 5097 Gradiente: [0.026777270629986562,-0.4619801845552053] Loss: 23.0441384932872\n",
      "Iteracion: 5098 Gradiente: [0.02676303501109108,-0.4617345817087183] Loss: 23.043924407496476\n",
      "Iteracion: 5099 Gradiente: [0.026748806960205228,-0.46148910943224736] Loss: 23.043710549274405\n",
      "Iteracion: 5100 Gradiente: [0.0267345864733007,-0.4612437676563791] Loss: 23.043496918379063\n",
      "Iteracion: 5101 Gradiente: [0.026720373546532037,-0.4609985563117238] Loss: 23.043283514568834\n",
      "Iteracion: 5102 Gradiente: [0.02670616817587567,-0.4607534753289412] Loss: 23.043070337602337\n",
      "Iteracion: 5103 Gradiente: [0.026691970357113824,-0.4605085246387393] Loss: 23.042857387238424\n",
      "Iteracion: 5104 Gradiente: [0.026677780086320504,-0.46026370417184403] Loss: 23.04264466323621\n",
      "Iteracion: 5105 Gradiente: [0.026663597359570683,-0.4600190138590207] Loss: 23.04243216535509\n",
      "Iteracion: 5106 Gradiente: [0.026649422172785837,-0.4597744536310781] Loss: 23.04221989335472\n",
      "Iteracion: 5107 Gradiente: [0.026635254521949037,-0.4595300234188604] Loss: 23.042007846994967\n",
      "Iteracion: 5108 Gradiente: [0.026621094403083134,-0.4592857231532463] Loss: 23.041796026036\n",
      "Iteracion: 5109 Gradiente: [0.026606941812149405,-0.45904155276515285] Loss: 23.0415844302382\n",
      "Iteracion: 5110 Gradiente: [0.026592796745214288,-0.4587975121855295] Loss: 23.04137305936224\n",
      "Iteracion: 5111 Gradiente: [0.026578659198266526,-0.4585536013453671] Loss: 23.041161913169038\n",
      "Iteracion: 5112 Gradiente: [0.026564529167209607,-0.4583098201756967] Loss: 23.040950991419756\n",
      "Iteracion: 5113 Gradiente: [0.02655040664805502,-0.45806616860758437] Loss: 23.0407402938758\n",
      "Iteracion: 5114 Gradiente: [0.02653629163697057,-0.45782264657211585] Loss: 23.040529820298868\n",
      "Iteracion: 5115 Gradiente: [0.026522184129732788,-0.4575792540004457] Loss: 23.04031957045088\n",
      "Iteracion: 5116 Gradiente: [0.026508084122552115,-0.45733599082373266] Loss: 23.040109544094012\n",
      "Iteracion: 5117 Gradiente: [0.026493991611368985,-0.45709285697319174] Loss: 23.03989974099071\n",
      "Iteracion: 5118 Gradiente: [0.026479906592168354,-0.45684985238006953] Loss: 23.039690160903625\n",
      "Iteracion: 5119 Gradiente: [0.026465829061090556,-0.4566069769756432] Loss: 23.039480803595737\n",
      "Iteracion: 5120 Gradiente: [0.02645175901392444,-0.4563642306912438] Loss: 23.039271668830207\n",
      "Iteracion: 5121 Gradiente: [0.026437696447001714,-0.45612161345821095] Loss: 23.039062756370516\n",
      "Iteracion: 5122 Gradiente: [0.02642364135613396,-0.45587912520794743] Loss: 23.038854065980317\n",
      "Iteracion: 5123 Gradiente: [0.026409593737335514,-0.45563676587188484] Loss: 23.038645597423564\n",
      "Iteracion: 5124 Gradiente: [0.02639555358666996,-0.45539453538148406] Loss: 23.03843735046446\n",
      "Iteracion: 5125 Gradiente: [0.026381520900155428,-0.4551524336682494] Loss: 23.038229324867455\n",
      "Iteracion: 5126 Gradiente: [0.0263674956738735,-0.4549104606637145] Loss: 23.038021520397248\n",
      "Iteracion: 5127 Gradiente: [0.02635347790378546,-0.45466861629945954] Loss: 23.03781393681876\n",
      "Iteracion: 5128 Gradiente: [0.02633946758604395,-0.45442690050708734] Loss: 23.037606573897218\n",
      "Iteracion: 5129 Gradiente: [0.026325464716535407,-0.4541853132182551] Loss: 23.03739943139806\n",
      "Iteracion: 5130 Gradiente: [0.02631146929141058,-0.4539438543646394] Loss: 23.03719250908698\n",
      "Iteracion: 5131 Gradiente: [0.026297481306745377,-0.45370252387795806] Loss: 23.036985806729906\n",
      "Iteracion: 5132 Gradiente: [0.0262835007584736,-0.45346132168997405] Loss: 23.036779324093057\n",
      "Iteracion: 5133 Gradiente: [0.026269527642737482,-0.45322024773247416] Loss: 23.036573060942867\n",
      "Iteracion: 5134 Gradiente: [0.02625556195550871,-0.4529793019372904] Loss: 23.036367017046015\n",
      "Iteracion: 5135 Gradiente: [0.026241603692894463,-0.4527384842362868] Loss: 23.036161192169466\n",
      "Iteracion: 5136 Gradiente: [0.02622765285083517,-0.4524977945613703] Loss: 23.03595558608038\n",
      "Iteracion: 5137 Gradiente: [0.026213709425530852,-0.4522572328444678] Loss: 23.035750198546193\n",
      "Iteracion: 5138 Gradiente: [0.026199773412925727,-0.4520167990175622] Loss: 23.035545029334596\n",
      "Iteracion: 5139 Gradiente: [0.026185844809204658,-0.45177649301265416] Loss: 23.035340078213512\n",
      "Iteracion: 5140 Gradiente: [0.026171923610337443,-0.45153631476179623] Loss: 23.03513534495113\n",
      "Iteracion: 5141 Gradiente: [0.026158009812453996,-0.4512962641970664] Loss: 23.03493082931584\n",
      "Iteracion: 5142 Gradiente: [0.026144103411528855,-0.4510563412505874] Loss: 23.034726531076334\n",
      "Iteracion: 5143 Gradiente: [0.026130204403620873,-0.4508165458545135] Loss: 23.03452245000151\n",
      "Iteracion: 5144 Gradiente: [0.0261163127849386,-0.4505768779410289] Loss: 23.03431858586054\n",
      "Iteracion: 5145 Gradiente: [0.026102428551375092,-0.4503373374423669] Loss: 23.034114938422817\n",
      "Iteracion: 5146 Gradiente: [0.026088551699209952,-0.45009792429078177] Loss: 23.033911507458004\n",
      "Iteracion: 5147 Gradiente: [0.02607468222436656,-0.4498586384185773] Loss: 23.03370829273597\n",
      "Iteracion: 5148 Gradiente: [0.026060820122969137,-0.4496194797580883] Loss: 23.03350529402688\n",
      "Iteracion: 5149 Gradiente: [0.02604696539101686,-0.44938044824168866] Loss: 23.03330251110109\n",
      "Iteracion: 5150 Gradiente: [0.026033118024697426,-0.4491415438017776] Loss: 23.033099943729262\n",
      "Iteracion: 5151 Gradiente: [0.02601927802010569,-0.44890276637079674] Loss: 23.03289759168224\n",
      "Iteracion: 5152 Gradiente: [0.02600544537323041,-0.44866411588123234] Loss: 23.032695454731144\n",
      "Iteracion: 5153 Gradiente: [0.02599162008027065,-0.44842559226558976] Loss: 23.032493532647344\n",
      "Iteracion: 5154 Gradiente: [0.02597780213724453,-0.44818719545642266] Loss: 23.03229182520242\n",
      "Iteracion: 5155 Gradiente: [0.02596399154021185,-0.44794892538632036] Loss: 23.032090332168227\n",
      "Iteracion: 5156 Gradiente: [0.025950188285280736,-0.4477107819879028] Loss: 23.031889053316856\n",
      "Iteracion: 5157 Gradiente: [0.025936392368618992,-0.4474727651938247] Loss: 23.03168798842062\n",
      "Iteracion: 5158 Gradiente: [0.025922603786322423,-0.44723487493677716] Loss: 23.031487137252093\n",
      "Iteracion: 5159 Gradiente: [0.02590882253445083,-0.4469971111494929] Loss: 23.031286499584102\n",
      "Iteracion: 5160 Gradiente: [0.025895048609123707,-0.4467594737647352] Loss: 23.031086075189666\n",
      "Iteracion: 5161 Gradiente: [0.025881282006410326,-0.4465219627153079] Loss: 23.030885863842137\n",
      "Iteracion: 5162 Gradiente: [0.02586752272255334,-0.44628457793403875] Loss: 23.030685865314997\n",
      "Iteracion: 5163 Gradiente: [0.02585377075351592,-0.44604731935380687] Loss: 23.03048607938206\n",
      "Iteracion: 5164 Gradiente: [0.025840026095377767,-0.4458101869075231] Loss: 23.030286505817312\n",
      "Iteracion: 5165 Gradiente: [0.025826288744414682,-0.4455731805281219] Loss: 23.030087144395033\n",
      "Iteracion: 5166 Gradiente: [0.025812558696497,-0.44533630014859515] Loss: 23.0298879948897\n",
      "Iteracion: 5167 Gradiente: [0.025798835948025574,-0.44509954570194155] Loss: 23.029689057076077\n",
      "Iteracion: 5168 Gradiente: [0.02578512049496074,-0.4448629171212205] Loss: 23.029490330729125\n",
      "Iteracion: 5169 Gradiente: [0.02577141233341725,-0.44462641433951866] Loss: 23.02929181562404\n",
      "Iteracion: 5170 Gradiente: [0.025757711459534486,-0.4443900372899558] Loss: 23.02909351153631\n",
      "Iteracion: 5171 Gradiente: [0.02574401786951815,-0.4441537859056845] Loss: 23.028895418241607\n",
      "Iteracion: 5172 Gradiente: [0.0257303315594451,-0.44391766011989897] Loss: 23.02869753551586\n",
      "Iteracion: 5173 Gradiente: [0.025716652525402613,-0.44368165986583286] Loss: 23.028499863135252\n",
      "Iteracion: 5174 Gradiente: [0.02570298076352439,-0.4434457850767461] Loss: 23.028302400876193\n",
      "Iteracion: 5175 Gradiente: [0.025689316270003815,-0.4432100356859362] Loss: 23.028105148515312\n",
      "Iteracion: 5176 Gradiente: [0.02567565904095943,-0.4429744116267378] Loss: 23.027908105829507\n",
      "Iteracion: 5177 Gradiente: [0.025662009072561887,-0.44273891283251954] Loss: 23.027711272595905\n",
      "Iteracion: 5178 Gradiente: [0.02564836636078856,-0.4425035392366952] Loss: 23.02751464859182\n",
      "Iteracion: 5179 Gradiente: [0.025634730901953163,-0.44226829077269597] Loss: 23.027318233594887\n",
      "Iteracion: 5180 Gradiente: [0.02562110269215907,-0.4420331673740009] Loss: 23.027122027382923\n",
      "Iteracion: 5181 Gradiente: [0.02560748172757504,-0.4417981689741198] Loss: 23.026926029734007\n",
      "Iteracion: 5182 Gradiente: [0.025593868004287403,-0.4415632955066012] Loss: 23.026730240426414\n",
      "Iteracion: 5183 Gradiente: [0.025580261518502803,-0.4413285469050269] Loss: 23.026534659238695\n",
      "Iteracion: 5184 Gradiente: [0.02556666226624884,-0.44109392310301915] Loss: 23.026339285949632\n",
      "Iteracion: 5185 Gradiente: [0.02555307024381269,-0.4408594240342249] Loss: 23.026144120338223\n",
      "Iteracion: 5186 Gradiente: [0.02553948544734131,-0.44062504963233307] Loss: 23.025949162183725\n",
      "Iteracion: 5187 Gradiente: [0.025525907872921985,-0.4403907998310679] Loss: 23.025754411265588\n",
      "Iteracion: 5188 Gradiente: [0.02551233751674715,-0.44015667456418855] Loss: 23.02555986736354\n",
      "Iteracion: 5189 Gradiente: [0.025498774375027247,-0.4399226737654851] Loss: 23.02536553025753\n",
      "Iteracion: 5190 Gradiente: [0.025485218443852393,-0.43968879736879196] Loss: 23.02517139972773\n",
      "Iteracion: 5191 Gradiente: [0.025471669719521136,-0.43945504530796453] Loss: 23.024977475554575\n",
      "Iteracion: 5192 Gradiente: [0.025458128198148227,-0.4392214175169033] Loss: 23.02478375751868\n",
      "Iteracion: 5193 Gradiente: [0.025444593875680727,-0.43898791392955944] Loss: 23.024590245400944\n",
      "Iteracion: 5194 Gradiente: [0.02543106674858393,-0.43875453447988294] Loss: 23.0243969389825\n",
      "Iteracion: 5195 Gradiente: [0.025417546812886372,-0.43852127910188765] Loss: 23.02420383804466\n",
      "Iteracion: 5196 Gradiente: [0.025404034064821227,-0.43828814772960967] Loss: 23.024010942369006\n",
      "Iteracion: 5197 Gradiente: [0.025390528500506094,-0.4380551402971284] Loss: 23.02381825173737\n",
      "Iteracion: 5198 Gradiente: [0.02537703011618362,-0.43782225673854874] Loss: 23.023625765931776\n",
      "Iteracion: 5199 Gradiente: [0.025363538908055715,-0.4375894969880144] Loss: 23.023433484734507\n",
      "Iteracion: 5200 Gradiente: [0.025350054872253242,-0.4373568609797091] Loss: 23.02324140792806\n",
      "Iteracion: 5201 Gradiente: [0.02533657800496864,-0.4371243486478478] Loss: 23.023049535295186\n",
      "Iteracion: 5202 Gradiente: [0.025323108302403814,-0.436891959926677] Loss: 23.022857866618835\n",
      "Iteracion: 5203 Gradiente: [0.02530964576071805,-0.4366596947504868] Loss: 23.022666401682226\n",
      "Iteracion: 5204 Gradiente: [0.02529619037618526,-0.4364275530535899] Loss: 23.022475140268785\n",
      "Iteracion: 5205 Gradiente: [0.025282742144887984,-0.43619553477034806] Loss: 23.02228408216214\n",
      "Iteracion: 5206 Gradiente: [0.02526930106313235,-0.4359636398351442] Loss: 23.022093227146225\n",
      "Iteracion: 5207 Gradiente: [0.02525586712708048,-0.43573186818240545] Loss: 23.021902575005136\n",
      "Iteracion: 5208 Gradiente: [0.025242440332900174,-0.4355002197465937] Loss: 23.02171212552321\n",
      "Iteracion: 5209 Gradiente: [0.02522902067679714,-0.4352686944622004] Loss: 23.021521878485036\n",
      "Iteracion: 5210 Gradiente: [0.025215608154991288,-0.4350372922637561] Loss: 23.021331833675443\n",
      "Iteracion: 5211 Gradiente: [0.02520220276365516,-0.4348060130858258] Loss: 23.02114199087943\n",
      "Iteracion: 5212 Gradiente: [0.025188804499178256,-0.43457485686299685] Loss: 23.02095234988229\n",
      "Iteracion: 5213 Gradiente: [0.025175413357559743,-0.4343438235299145] Loss: 23.02076291046949\n",
      "Iteracion: 5214 Gradiente: [0.025162029335069745,-0.43411291302124544] Loss: 23.020573672426757\n",
      "Iteracion: 5215 Gradiente: [0.025148652427952813,-0.4338821252716895] Loss: 23.02038463554006\n",
      "Iteracion: 5216 Gradiente: [0.025135282632354004,-0.43365146021598816] Loss: 23.020195799595545\n",
      "Iteracion: 5217 Gradiente: [0.025121919944601244,-0.43342091778890696] Loss: 23.02000716437963\n",
      "Iteracion: 5218 Gradiente: [0.02510856436085286,-0.4331904979252579] Loss: 23.019818729678946\n",
      "Iteracion: 5219 Gradiente: [0.025095215877378983,-0.43296020055987866] Loss: 23.019630495280353\n",
      "Iteracion: 5220 Gradiente: [0.025081874490282985,-0.4327300256276526] Loss: 23.019442460970915\n",
      "Iteracion: 5221 Gradiente: [0.02506854019590984,-0.4324999730634842] Loss: 23.019254626537954\n",
      "Iteracion: 5222 Gradiente: [0.025055212990470938,-0.4322700428023192] Loss: 23.01906699176901\n",
      "Iteracion: 5223 Gradiente: [0.025041892870148293,-0.4320402347791414] Loss: 23.018879556451846\n",
      "Iteracion: 5224 Gradiente: [0.02502857983117413,-0.4318105489289637] Loss: 23.018692320374434\n",
      "Iteracion: 5225 Gradiente: [0.025015273869911425,-0.43158098518682936] Loss: 23.018505283325\n",
      "Iteracion: 5226 Gradiente: [0.02500197498249861,-0.431351543487828] Loss: 23.01831844509196\n",
      "Iteracion: 5227 Gradiente: [0.02498868316520581,-0.4311222237670737] Loss: 23.018131805464012\n",
      "Iteracion: 5228 Gradiente: [0.02497539841424062,-0.43089302595972306] Loss: 23.017945364230016\n",
      "Iteracion: 5229 Gradiente: [0.024962120725784114,-0.4306639500009654] Loss: 23.017759121179083\n",
      "Iteracion: 5230 Gradiente: [0.024948850096241888,-0.4304349958260151] Loss: 23.017573076100586\n",
      "Iteracion: 5231 Gradiente: [0.024935586521675646,-0.43020616337013706] Loss: 23.01738722878404\n",
      "Iteracion: 5232 Gradiente: [0.024922329998559197,-0.42997745256861014] Loss: 23.01720157901923\n",
      "Iteracion: 5233 Gradiente: [0.024909080522843397,-0.42974886335677226] Loss: 23.017016126596182\n",
      "Iteracion: 5234 Gradiente: [0.024895838091114796,-0.4295203956699685] Loss: 23.016830871305128\n",
      "Iteracion: 5235 Gradiente: [0.024882602699451202,-0.4292920494436024] Loss: 23.016645812936517\n",
      "Iteracion: 5236 Gradiente: [0.024869374344080104,-0.42906382461310116] Loss: 23.016460951281022\n",
      "Iteracion: 5237 Gradiente: [0.024856153021323733,-0.42883572111392426] Loss: 23.01627628612954\n",
      "Iteracion: 5238 Gradiente: [0.02484293872739253,-0.4286077388815715] Loss: 23.016091817273175\n",
      "Iteracion: 5239 Gradiente: [0.024829731458631463,-0.42837987785156884] Loss: 23.015907544503293\n",
      "Iteracion: 5240 Gradiente: [0.02481653121119886,-0.42815213795948637] Loss: 23.015723467611448\n",
      "Iteracion: 5241 Gradiente: [0.02480333798155338,-0.42792451914091373] Loss: 23.015539586389437\n",
      "Iteracion: 5242 Gradiente: [0.024790151765714086,-0.42769702133149856] Loss: 23.015355900629245\n",
      "Iteracion: 5243 Gradiente: [0.02477697256006195,-0.42746964446690294] Loss: 23.015172410123128\n",
      "Iteracion: 5244 Gradiente: [0.024763800360982677,-0.4272423884828213] Loss: 23.01498911466352\n",
      "Iteracion: 5245 Gradiente: [0.024750635164537963,-0.42701525331500323] Loss: 23.014806014043092\n",
      "Iteracion: 5246 Gradiente: [0.02473747696722436,-0.4267882388992054] Loss: 23.01462310805474\n",
      "Iteracion: 5247 Gradiente: [0.024724325765105467,-0.4265613451712459] Loss: 23.01444039649156\n",
      "Iteracion: 5248 Gradiente: [0.02471118155461056,-0.42633457206695496] Loss: 23.014257879146896\n",
      "Iteracion: 5249 Gradiente: [0.024698044332030614,-0.42610791952220334] Loss: 23.01407555581429\n",
      "Iteracion: 5250 Gradiente: [0.024684914093477537,-0.4258813874729081] Loss: 23.013893426287506\n",
      "Iteracion: 5251 Gradiente: [0.024671790835499034,-0.42565497585499706] Loss: 23.01371149036054\n",
      "Iteracion: 5252 Gradiente: [0.024658674554238282,-0.4254286846044516] Loss: 23.013529747827626\n",
      "Iteracion: 5253 Gradiente: [0.024645565245942674,-0.4252025136572836] Loss: 23.013348198483133\n",
      "Iteracion: 5254 Gradiente: [0.024632462906947695,-0.42497646294953395] Loss: 23.013166842121752\n",
      "Iteracion: 5255 Gradiente: [0.024619367533606842,-0.4247505324172765] Loss: 23.012985678538328\n",
      "Iteracion: 5256 Gradiente: [0.02460627912214098,-0.4245247219966269] Loss: 23.012804707527938\n",
      "Iteracion: 5257 Gradiente: [0.024593197668941493,-0.4242990316237231] Loss: 23.012623928885898\n",
      "Iteracion: 5258 Gradiente: [0.024580123170077666,-0.42407346123476064] Loss: 23.01244334240771\n",
      "Iteracion: 5259 Gradiente: [0.024567055622171097,-0.42384801076593065] Loss: 23.0122629478891\n",
      "Iteracion: 5260 Gradiente: [0.02455399502127117,-0.42362268015349697] Loss: 23.01208274512605\n",
      "Iteracion: 5261 Gradiente: [0.024540941363941707,-0.42339746933372296] Loss: 23.011902733914702\n",
      "Iteracion: 5262 Gradiente: [0.02452789464617524,-0.42317237824294435] Loss: 23.011722914051457\n",
      "Iteracion: 5263 Gradiente: [0.024514854864456957,-0.4229474068174967] Loss: 23.01154328533289\n",
      "Iteracion: 5264 Gradiente: [0.024501822015133713,-0.42272255499376155] Loss: 23.01136384755585\n",
      "Iteracion: 5265 Gradiente: [0.02448879609449174,-0.4224978227081568] Loss: 23.011184600517357\n",
      "Iteracion: 5266 Gradiente: [0.024475777098809697,-0.422273209897135] Loss: 23.011005544014647\n",
      "Iteracion: 5267 Gradiente: [0.024462765024417386,-0.42204871649717773] Loss: 23.0108266778452\n",
      "Iteracion: 5268 Gradiente: [0.02444975986774504,-0.42182434244479683] Loss: 23.010648001806683\n",
      "Iteracion: 5269 Gradiente: [0.02443676162483636,-0.4216000876765597] Loss: 23.010469515697018\n",
      "Iteracion: 5270 Gradiente: [0.02442377029224853,-0.4213759521290376] Loss: 23.010291219314286\n",
      "Iteracion: 5271 Gradiente: [0.024410785866316096,-0.4211519357388492] Loss: 23.010113112456814\n",
      "Iteracion: 5272 Gradiente: [0.02439780834329402,-0.42092803844265025] Loss: 23.00993519492317\n",
      "Iteracion: 5273 Gradiente: [0.024384837719412644,-0.4207042601771319] Loss: 23.009757466512063\n",
      "Iteracion: 5274 Gradiente: [0.024371873991208305,-0.4204806008790036] Loss: 23.009579927022482\n",
      "Iteracion: 5275 Gradiente: [0.024358917154907546,-0.4202570604850241] Loss: 23.009402576253624\n",
      "Iteracion: 5276 Gradiente: [0.02434596720687428,-0.420033638931978] Loss: 23.009225414004863\n",
      "Iteracion: 5277 Gradiente: [0.02433302414342696,-0.4198103361566883] Loss: 23.009048440075794\n",
      "Iteracion: 5278 Gradiente: [0.024320087960884015,-0.41958715209600955] Loss: 23.008871654266258\n",
      "Iteracion: 5279 Gradiente: [0.02430715865566716,-0.41936408668682457] Loss: 23.0086950563763\n",
      "Iteracion: 5280 Gradiente: [0.02429423622402472,-0.41914113986606005] Loss: 23.00851864620613\n",
      "Iteracion: 5281 Gradiente: [0.024281320662348095,-0.41891831157066833] Loss: 23.008342423556236\n",
      "Iteracion: 5282 Gradiente: [0.02426841196695572,-0.41869560173764064] Loss: 23.00816638822725\n",
      "Iteracion: 5283 Gradiente: [0.024255510134276886,-0.4184730103039918] Loss: 23.007990540020106\n",
      "Iteracion: 5284 Gradiente: [0.024242615160581713,-0.4182505372067838] Loss: 23.007814878735857\n",
      "Iteracion: 5285 Gradiente: [0.024229727042198116,-0.4180281823831062] Loss: 23.007639404175833\n",
      "Iteracion: 5286 Gradiente: [0.0242168457754777,-0.4178059457700813] Loss: 23.007464116141524\n",
      "Iteracion: 5287 Gradiente: [0.024203971356917008,-0.41758382730485644] Loss: 23.007289014434686\n",
      "Iteracion: 5288 Gradiente: [0.024191103782842067,-0.417361826924623] Loss: 23.007114098857226\n",
      "Iteracion: 5289 Gradiente: [0.024178243049587423,-0.417139944566604] Loss: 23.00693936921133\n",
      "Iteracion: 5290 Gradiente: [0.02416538915340804,-0.4169181801680625] Loss: 23.006764825299328\n",
      "Iteracion: 5291 Gradiente: [0.02415254209074173,-0.41669653366628256] Loss: 23.006590466923793\n",
      "Iteracion: 5292 Gradiente: [0.024139701858039566,-0.4164750049985827] Loss: 23.006416293887515\n",
      "Iteracion: 5293 Gradiente: [0.024126868451541366,-0.41625359410232504] Loss: 23.006242305993464\n",
      "Iteracion: 5294 Gradiente: [0.024114041867766407,-0.4160323009148923] Loss: 23.006068503044865\n",
      "Iteracion: 5295 Gradiente: [0.024101222102908083,-0.41581112537371406] Loss: 23.005894884845087\n",
      "Iteracion: 5296 Gradiente: [0.024088409153446833,-0.41559006741624244] Loss: 23.00572145119778\n",
      "Iteracion: 5297 Gradiente: [0.024075603015791102,-0.4153691269799639] Loss: 23.005548201906763\n",
      "Iteracion: 5298 Gradiente: [0.024062803686242282,-0.41514830400240366] Loss: 23.005375136776056\n",
      "Iteracion: 5299 Gradiente: [0.024050011161182282,-0.4149275984211183] Loss: 23.005202255609902\n",
      "Iteracion: 5300 Gradiente: [0.02403722543699587,-0.41470701017369543] Loss: 23.005029558212758\n",
      "Iteracion: 5301 Gradiente: [0.02402444651017106,-0.41448653919775197] Loss: 23.004857044389265\n",
      "Iteracion: 5302 Gradiente: [0.024011674377003564,-0.41426618543094673] Loss: 23.004684713944325\n",
      "Iteracion: 5303 Gradiente: [0.023998909033852555,-0.4140459488109714] Loss: 23.004512566682976\n",
      "Iteracion: 5304 Gradiente: [0.023986150477194695,-0.4138258292755405] Loss: 23.00434060241052\n",
      "Iteracion: 5305 Gradiente: [0.023973398703348418,-0.41360582676241187] Loss: 23.004168820932414\n",
      "Iteracion: 5306 Gradiente: [0.023960653708791333,-0.41338594120936883] Loss: 23.003997222054373\n",
      "Iteracion: 5307 Gradiente: [0.023947915489834295,-0.4131661725542372] Loss: 23.00382580558231\n",
      "Iteracion: 5308 Gradiente: [0.023935184042889544,-0.41294652073486804] Loss: 23.003654571322315\n",
      "Iteracion: 5309 Gradiente: [0.02392245936430489,-0.41272698568915456] Loss: 23.003483519080692\n",
      "Iteracion: 5310 Gradiente: [0.023909741450571193,-0.41250756735500765] Loss: 23.003312648663965\n",
      "Iteracion: 5311 Gradiente: [0.02389703029814143,-0.4122882656703786] Loss: 23.00314195987887\n",
      "Iteracion: 5312 Gradiente: [0.023884325903311302,-0.4120690805732585] Loss: 23.002971452532332\n",
      "Iteracion: 5313 Gradiente: [0.023871628262564098,-0.4118500120016616] Loss: 23.002801126431468\n",
      "Iteracion: 5314 Gradiente: [0.023858937372243833,-0.4116310598936421] Loss: 23.00263098138364\n",
      "Iteracion: 5315 Gradiente: [0.023846253228816748,-0.41141222418728385] Loss: 23.002461017196378\n",
      "Iteracion: 5316 Gradiente: [0.023833575828666652,-0.4111935048207043] Loss: 23.002291233677436\n",
      "Iteracion: 5317 Gradiente: [0.0238209051681442,-0.4109749017320568] Loss: 23.002121630634758\n",
      "Iteracion: 5318 Gradiente: [0.023808241243724146,-0.4107564148595222] Loss: 23.001952207876528\n",
      "Iteracion: 5319 Gradiente: [0.02379558405191157,-0.41053804414131023] Loss: 23.00178296521109\n",
      "Iteracion: 5320 Gradiente: [0.02378293358907134,-0.4103197895156749] Loss: 23.001613902447012\n",
      "Iteracion: 5321 Gradiente: [0.023770289851512416,-0.4101016509209019] Loss: 23.00144501939305\n",
      "Iteracion: 5322 Gradiente: [0.023757652835753144,-0.409883628295303] Loss: 23.00127631585821\n",
      "Iteracion: 5323 Gradiente: [0.023745022538393338,-0.4096657215772127] Loss: 23.001107791651613\n",
      "Iteracion: 5324 Gradiente: [0.02373239895551175,-0.40944793070503316] Loss: 23.00093944658268\n",
      "Iteracion: 5325 Gradiente: [0.02371978208376504,-0.40923025561716303] Loss: 23.000771280460988\n",
      "Iteracion: 5326 Gradiente: [0.023707171919487752,-0.4090126962520543] Loss: 23.00060329309631\n",
      "Iteracion: 5327 Gradiente: [0.023694568459210547,-0.40879525254817917] Loss: 23.000435484298624\n",
      "Iteracion: 5328 Gradiente: [0.023681971699310602,-0.40857792444405117] Loss: 23.000267853878128\n",
      "Iteracion: 5329 Gradiente: [0.023669381636366893,-0.40836071187820716] Loss: 23.00010040164523\n",
      "Iteracion: 5330 Gradiente: [0.023656798266492272,-0.40814361478924127] Loss: 22.999933127410465\n",
      "Iteracion: 5331 Gradiente: [0.023644221586350985,-0.40792663311574934] Loss: 22.999766030984667\n",
      "Iteracion: 5332 Gradiente: [0.023631651592435788,-0.4077097667963719] Loss: 22.99959911217882\n",
      "Iteracion: 5333 Gradiente: [0.02361908828105849,-0.4074930157697902] Loss: 22.999432370804143\n",
      "Iteracion: 5334 Gradiente: [0.023606531648810384,-0.4072763799747019] Loss: 22.99926580667197\n",
      "Iteracion: 5335 Gradiente: [0.02359398169197675,-0.4070598593498567] Loss: 22.999099419593957\n",
      "Iteracion: 5336 Gradiente: [0.023581438407092037,-0.40684345383402004] Loss: 22.998933209381864\n",
      "Iteracion: 5337 Gradiente: [0.023568901790659423,-0.4066271633659953] Loss: 22.998767175847693\n",
      "Iteracion: 5338 Gradiente: [0.023556371839011565,-0.40641098788462576] Loss: 22.99860131880364\n",
      "Iteracion: 5339 Gradiente: [0.02354384854872175,-0.40619492732877505] Loss: 22.998435638062105\n",
      "Iteracion: 5340 Gradiente: [0.02353133191612168,-0.4059789816373518] Loss: 22.99827013343568\n",
      "Iteracion: 5341 Gradiente: [0.023518821937804545,-0.40576315074928326] Loss: 22.99810480473716\n",
      "Iteracion: 5342 Gradiente: [0.02350631861012194,-0.405547434603544] Loss: 22.997939651779532\n",
      "Iteracion: 5343 Gradiente: [0.023493821929645265,-0.40533183313912496] Loss: 22.99777467437599\n",
      "Iteracion: 5344 Gradiente: [0.023481331892786745,-0.4051163462950632] Loss: 22.99760987233992\n",
      "Iteracion: 5345 Gradiente: [0.023468848495888515,-0.4049009740104283] Loss: 22.99744524548492\n",
      "Iteracion: 5346 Gradiente: [0.023456371735701018,-0.4046857162243035] Loss: 22.997280793624775\n",
      "Iteracion: 5347 Gradiente: [0.023443901608536065,-0.40447057287582344] Loss: 22.99711651657344\n",
      "Iteracion: 5348 Gradiente: [0.023431438110863685,-0.40425554390415214] Loss: 22.99695241414515\n",
      "Iteracion: 5349 Gradiente: [0.023418981239115056,-0.4040406292484865] Loss: 22.996788486154237\n",
      "Iteracion: 5350 Gradiente: [0.023406530989810412,-0.4038258288480486] Loss: 22.9966247324153\n",
      "Iteracion: 5351 Gradiente: [0.02339408735946241,-0.4036111426420955] Loss: 22.9964611527431\n",
      "Iteracion: 5352 Gradiente: [0.02338165034455623,-0.40339657056991823] Loss: 22.99629774695264\n",
      "Iteracion: 5353 Gradiente: [0.02336921994157232,-0.403182112570839] Loss: 22.996134514859065\n",
      "Iteracion: 5354 Gradiente: [0.023356796146899228,-0.4029677685842196] Loss: 22.99597145627772\n",
      "Iteracion: 5355 Gradiente: [0.02334437895714719,-0.4027535385494385] Loss: 22.995808571024178\n",
      "Iteracion: 5356 Gradiente: [0.023331968368716125,-0.402539422405923] Loss: 22.99564585891421\n",
      "Iteracion: 5357 Gradiente: [0.023319564378206793,-0.40232542009311667] Loss: 22.995483319763768\n",
      "Iteracion: 5358 Gradiente: [0.023307166982041848,-0.4021115315505085] Loss: 22.99532095338897\n",
      "Iteracion: 5359 Gradiente: [0.023294776176702687,-0.4018977567176156] Loss: 22.995158759606205\n",
      "Iteracion: 5360 Gradiente: [0.02328239195861291,-0.40168409553399076] Loss: 22.994996738231958\n",
      "Iteracion: 5361 Gradiente: [0.023270014324414962,-0.40147054793920667] Loss: 22.99483488908301\n",
      "Iteracion: 5362 Gradiente: [0.02325764327051729,-0.4012571138728796] Loss: 22.994673211976274\n",
      "Iteracion: 5363 Gradiente: [0.02324527879342971,-0.40104379327465506] Loss: 22.99451170672885\n",
      "Iteracion: 5364 Gradiente: [0.023232920889712242,-0.40083058608420574] Loss: 22.994350373158074\n",
      "Iteracion: 5365 Gradiente: [0.02322056955585007,-0.4006174922412426] Loss: 22.994189211081487\n",
      "Iteracion: 5366 Gradiente: [0.023208224788300907,-0.40040451168550995] Loss: 22.994028220316725\n",
      "Iteracion: 5367 Gradiente: [0.023195886583608664,-0.4001916443567783] Loss: 22.993867400681737\n",
      "Iteracion: 5368 Gradiente: [0.02318355493834853,-0.3999788901948477] Loss: 22.993706751994615\n",
      "Iteracion: 5369 Gradiente: [0.023171229848988634,-0.3997662491395591] Loss: 22.99354627407364\n",
      "Iteracion: 5370 Gradiente: [0.023158911311947843,-0.3995537211307874] Loss: 22.993385966737264\n",
      "Iteracion: 5371 Gradiente: [0.023146599323815544,-0.3993413061084284] Loss: 22.993225829804192\n",
      "Iteracion: 5372 Gradiente: [0.023134293881052297,-0.3991290040124188] Loss: 22.99306586309329\n",
      "Iteracion: 5373 Gradiente: [0.023121994980401912,-0.3989168147827107] Loss: 22.992906066423583\n",
      "Iteracion: 5374 Gradiente: [0.023109702618122206,-0.3987047383593166] Loss: 22.99274643961436\n",
      "Iteracion: 5375 Gradiente: [0.023097416790830987,-0.3984927746822598] Loss: 22.992586982485037\n",
      "Iteracion: 5376 Gradiente: [0.023085137495120495,-0.3982809236915956] Loss: 22.992427694855262\n",
      "Iteracion: 5377 Gradiente: [0.023072864727470232,-0.398069185327421] Loss: 22.99226857654485\n",
      "Iteracion: 5378 Gradiente: [0.023060598484354006,-0.39785755952986185] Loss: 22.992109627373807\n",
      "Iteracion: 5379 Gradiente: [0.023048338762368795,-0.39764604623907096] Loss: 22.99195084716236\n",
      "Iteracion: 5380 Gradiente: [0.023036085557968513,-0.397434645395242] Loss: 22.991792235730916\n",
      "Iteracion: 5381 Gradiente: [0.02302383886778898,-0.3972233569385889] Loss: 22.991633792900053\n",
      "Iteracion: 5382 Gradiente: [0.023011598688309695,-0.3970121808093655] Loss: 22.99147551849056\n",
      "Iteracion: 5383 Gradiente: [0.02299936501610489,-0.3968011169478531] Loss: 22.991317412323387\n",
      "Iteracion: 5384 Gradiente: [0.022987137847762067,-0.3965901652943639] Loss: 22.991159474219728\n",
      "Iteracion: 5385 Gradiente: [0.02297491717978062,-0.3963793257892475] Loss: 22.99100170400091\n",
      "Iteracion: 5386 Gradiente: [0.02296270300850362,-0.39616859837289387] Loss: 22.990844101488467\n",
      "Iteracion: 5387 Gradiente: [0.022950495330723204,-0.3959579829856993] Loss: 22.99068666650418\n",
      "Iteracion: 5388 Gradiente: [0.022938294143004138,-0.39574747956810347] Loss: 22.990529398869914\n",
      "Iteracion: 5389 Gradiente: [0.022926099441772863,-0.3955370880605899] Loss: 22.990372298407806\n",
      "Iteracion: 5390 Gradiente: [0.022913911223571403,-0.3953268084036612] Loss: 22.990215364940163\n",
      "Iteracion: 5391 Gradiente: [0.02290172948505074,-0.3951166405378495] Loss: 22.990058598289455\n",
      "Iteracion: 5392 Gradiente: [0.02288955422267994,-0.39490658440372833] Loss: 22.989901998278356\n",
      "Iteracion: 5393 Gradiente: [0.022877385433055984,-0.394696639941896] Loss: 22.98974556472977\n",
      "Iteracion: 5394 Gradiente: [0.022865223112709523,-0.3944868070929851] Loss: 22.989589297466686\n",
      "Iteracion: 5395 Gradiente: [0.02285306725829912,-0.394277085797653] Loss: 22.989433196312422\n",
      "Iteracion: 5396 Gradiente: [0.02284091786621616,-0.3940674759966048] Loss: 22.98927726109034\n",
      "Iteracion: 5397 Gradiente: [0.022828774933239517,-0.39385797763055413] Loss: 22.989121491624093\n",
      "Iteracion: 5398 Gradiente: [0.022816638455697102,-0.39364859064027147] Loss: 22.988965887737482\n",
      "Iteracion: 5399 Gradiente: [0.022804508430378217,-0.3934393149665353] Loss: 22.98881044925451\n",
      "Iteracion: 5400 Gradiente: [0.022792384853789826,-0.3932301505501676] Loss: 22.98865517599932\n",
      "Iteracion: 5401 Gradiente: [0.022780267722421855,-0.39302109733202806] Loss: 22.98850006779632\n",
      "Iteracion: 5402 Gradiente: [0.022768157032861797,-0.39281215525299645] Loss: 22.988345124470033\n",
      "Iteracion: 5403 Gradiente: [0.022756052781706632,-0.3926033242539895] Loss: 22.98819034584523\n",
      "Iteracion: 5404 Gradiente: [0.02274395496555049,-0.39239460427594913] Loss: 22.988035731746812\n",
      "Iteracion: 5405 Gradiente: [0.02273186358090129,-0.39218599525986103] Loss: 22.987881281999872\n",
      "Iteracion: 5406 Gradiente: [0.022719778624551167,-0.3919774971467202] Loss: 22.987726996429743\n",
      "Iteracion: 5407 Gradiente: [0.02270770009285267,-0.39176910987758035] Loss: 22.987572874861915\n",
      "Iteracion: 5408 Gradiente: [0.02269562798241793,-0.3915608333935136] Loss: 22.987418917122014\n",
      "Iteracion: 5409 Gradiente: [0.022683562289876134,-0.3913526676356194] Loss: 22.98726512303595\n",
      "Iteracion: 5410 Gradiente: [0.022671503011925626,-0.39114461254502403] Loss: 22.98711149242971\n",
      "Iteracion: 5411 Gradiente: [0.022659450145100853,-0.3909366680629001] Loss: 22.986958025129546\n",
      "Iteracion: 5412 Gradiente: [0.022647403685859522,-0.3907288341304506] Loss: 22.986804720961874\n",
      "Iteracion: 5413 Gradiente: [0.022635363630856394,-0.3905211106889008] Loss: 22.986651579753268\n",
      "Iteracion: 5414 Gradiente: [0.022623329976786028,-0.39031349767950446] Loss: 22.986498601330503\n",
      "Iteracion: 5415 Gradiente: [0.02261130272018761,-0.3901059950435562] Loss: 22.98634578552056\n",
      "Iteracion: 5416 Gradiente: [0.02259928185760979,-0.38989860272238025] Loss: 22.986193132150582\n",
      "Iteracion: 5417 Gradiente: [0.022587267385662812,-0.38969132065732914] Loss: 22.98604064104789\n",
      "Iteracion: 5418 Gradiente: [0.022575259301023227,-0.3894841487897845] Loss: 22.985888312040004\n",
      "Iteracion: 5419 Gradiente: [0.022563257600243483,-0.38927708706116326] Loss: 22.985736144954615\n",
      "Iteracion: 5420 Gradiente: [0.022551262279934765,-0.3890701354129129] Loss: 22.985584139619604\n",
      "Iteracion: 5421 Gradiente: [0.02253927333665236,-0.3888632937865146] Loss: 22.985432295863028\n",
      "Iteracion: 5422 Gradiente: [0.02252729076712967,-0.3886565621234708] Loss: 22.98528061351316\n",
      "Iteracion: 5423 Gradiente: [0.022515314567843347,-0.3884499403653276] Loss: 22.985129092398385\n",
      "Iteracion: 5424 Gradiente: [0.022503344735509738,-0.3882434284536507] Loss: 22.984977732347353\n",
      "Iteracion: 5425 Gradiente: [0.02249138126677792,-0.388037026330041] Loss: 22.98482653318884\n",
      "Iteracion: 5426 Gradiente: [0.022479424158123606,-0.38783073393614026] Loss: 22.984675494751816\n",
      "Iteracion: 5427 Gradiente: [0.02246747340625556,-0.3876245512136061] Loss: 22.984524616865432\n",
      "Iteracion: 5428 Gradiente: [0.022455529007817176,-0.3874184781041341] Loss: 22.98437389935905\n",
      "Iteracion: 5429 Gradiente: [0.02244359095932301,-0.3872125145494562] Loss: 22.984223342062172\n",
      "Iteracion: 5430 Gradiente: [0.022431659257469504,-0.3870066604913249] Loss: 22.984072944804492\n",
      "Iteracion: 5431 Gradiente: [0.02241973389884985,-0.3868009158715316] Loss: 22.983922707415914\n",
      "Iteracion: 5432 Gradiente: [0.022407814880070494,-0.38659528063189597] Loss: 22.983772629726502\n",
      "Iteracion: 5433 Gradiente: [0.022395902197907467,-0.3863897547142591] Loss: 22.983622711566458\n",
      "Iteracion: 5434 Gradiente: [0.022383995848881,-0.38618433806050967] Loss: 22.98347295276624\n",
      "Iteracion: 5435 Gradiente: [0.02237209582961649,-0.38597903061256034] Loss: 22.983323353156454\n",
      "Iteracion: 5436 Gradiente: [0.02236020213674029,-0.38577383231235335] Loss: 22.983173912567874\n",
      "Iteracion: 5437 Gradiente: [0.022348314766910942,-0.3855687431018622] Loss: 22.983024630831476\n",
      "Iteracion: 5438 Gradiente: [0.022336433716783215,-0.385363762923089] Loss: 22.982875507778374\n",
      "Iteracion: 5439 Gradiente: [0.02232455898302514,-0.3851588917180686] Loss: 22.982726543239917\n",
      "Iteracion: 5440 Gradiente: [0.02231269056223179,-0.3849541294288702] Loss: 22.982577737047617\n",
      "Iteracion: 5441 Gradiente: [0.022300828451041827,-0.384749475997591] Loss: 22.98242908903312\n",
      "Iteracion: 5442 Gradiente: [0.022288972646177286,-0.38454493136635304] Loss: 22.982280599028297\n",
      "Iteracion: 5443 Gradiente: [0.022277123144129028,-0.38434049547732496] Loss: 22.98213226686521\n",
      "Iteracion: 5444 Gradiente: [0.02226527994165887,-0.3841361682726895] Loss: 22.981984092376045\n",
      "Iteracion: 5445 Gradiente: [0.022253443035421584,-0.38393194969466615] Loss: 22.981836075393236\n",
      "Iteracion: 5446 Gradiente: [0.022241612422047297,-0.3837278396855075] Loss: 22.98168821574932\n",
      "Iteracion: 5447 Gradiente: [0.022229788098184145,-0.38352383818749575] Loss: 22.98154051327707\n",
      "Iteracion: 5448 Gradiente: [0.022217970060474577,-0.3833199451429428] Loss: 22.981392967809413\n",
      "Iteracion: 5449 Gradiente: [0.02220615830567946,-0.38311616049418584] Loss: 22.981245579179458\n",
      "Iteracion: 5450 Gradiente: [0.02219435283025272,-0.38291248418361107] Loss: 22.981098347220478\n",
      "Iteracion: 5451 Gradiente: [0.02218255363108407,-0.3827089161536085] Loss: 22.980951271765935\n",
      "Iteracion: 5452 Gradiente: [0.022170760704683327,-0.3825054563466203] Loss: 22.980804352649482\n",
      "Iteracion: 5453 Gradiente: [0.022158974047812308,-0.38230210470510995] Loss: 22.980657589704933\n",
      "Iteracion: 5454 Gradiente: [0.022147193657013987,-0.3820988611715773] Loss: 22.980510982766287\n",
      "Iteracion: 5455 Gradiente: [0.022135419529054918,-0.38189572568854435] Loss: 22.98036453166769\n",
      "Iteracion: 5456 Gradiente: [0.022123651660549134,-0.3816926981985723] Loss: 22.9802182362435\n",
      "Iteracion: 5457 Gradiente: [0.022111890048206344,-0.3814897786442458] Loss: 22.980072096328247\n",
      "Iteracion: 5458 Gradiente: [0.02210013468870784,-0.381286966968183] Loss: 22.97992611175661\n",
      "Iteracion: 5459 Gradiente: [0.022088385578788915,-0.38108426311302673] Loss: 22.979780282363475\n",
      "Iteracion: 5460 Gradiente: [0.022076642715003914,-0.3808816670214648] Loss: 22.979634607983883\n",
      "Iteracion: 5461 Gradiente: [0.022064906094148757,-0.3806791786362009] Loss: 22.97948908845307\n",
      "Iteracion: 5462 Gradiente: [0.02205317571275695,-0.3804767978999821] Loss: 22.97934372360641\n",
      "Iteracion: 5463 Gradiente: [0.02204145156768315,-0.38027452475557055] Loss: 22.979198513279517\n",
      "Iteracion: 5464 Gradiente: [0.022029733655466545,-0.38007235914577536] Loss: 22.9790534573081\n",
      "Iteracion: 5465 Gradiente: [0.022018021972869898,-0.3798703010134215] Loss: 22.978908555528104\n",
      "Iteracion: 5466 Gradiente: [0.022006316516575454,-0.3796683503013726] Loss: 22.978763807775636\n",
      "Iteracion: 5467 Gradiente: [0.02199461728322092,-0.37946650695252243] Loss: 22.978619213886926\n",
      "Iteracion: 5468 Gradiente: [0.021982924269556746,-0.37926477090979077] Loss: 22.978474773698487\n",
      "Iteracion: 5469 Gradiente: [0.02197123747222539,-0.3790631421161355] Loss: 22.978330487046875\n",
      "Iteracion: 5470 Gradiente: [0.021959556888000976,-0.3788616205145319] Loss: 22.978186353768915\n",
      "Iteracion: 5471 Gradiente: [0.021947882513489957,-0.3786602060479999] Loss: 22.97804237370156\n",
      "Iteracion: 5472 Gradiente: [0.021936214345498684,-0.3784588986595775] Loss: 22.977898546681953\n",
      "Iteracion: 5473 Gradiente: [0.021924552380644967,-0.3782576982923432] Loss: 22.97775487254743\n",
      "Iteracion: 5474 Gradiente: [0.021912896615573156,-0.3780566048894039] Loss: 22.977611351135454\n",
      "Iteracion: 5475 Gradiente: [0.021901247047102855,-0.3778556183938875] Loss: 22.97746798228369\n",
      "Iteracion: 5476 Gradiente: [0.02188960367190115,-0.37765473874896194] Loss: 22.97732476582997\n",
      "Iteracion: 5477 Gradiente: [0.021877966486749757,-0.3774539658978166] Loss: 22.97718170161229\n",
      "Iteracion: 5478 Gradiente: [0.021866335488289222,-0.37725329978368194] Loss: 22.977038789468843\n",
      "Iteracion: 5479 Gradiente: [0.02185471067311179,-0.3770527403498191] Loss: 22.976896029237977\n",
      "Iteracion: 5480 Gradiente: [0.021843092038046543,-0.3768522875395072] Loss: 22.976753420758186\n",
      "Iteracion: 5481 Gradiente: [0.02183147957989225,-0.3766519412960578] Loss: 22.976610963868193\n",
      "Iteracion: 5482 Gradiente: [0.0218198732952582,-0.3764517015628214] Loss: 22.976468658406848\n",
      "Iteracion: 5483 Gradiente: [0.021808273180771682,-0.37625156828317924] Loss: 22.97632650421317\n",
      "Iteracion: 5484 Gradiente: [0.021796679233342314,-0.376051541400528] Loss: 22.976184501126404\n",
      "Iteracion: 5485 Gradiente: [0.02178509144966938,-0.37585162085830365] Loss: 22.976042648985874\n",
      "Iteracion: 5486 Gradiente: [0.02177350982637923,-0.37565180659997804] Loss: 22.97590094763119\n",
      "Iteracion: 5487 Gradiente: [0.021761934360152207,-0.37545209856904976] Loss: 22.975759396902003\n",
      "Iteracion: 5488 Gradiente: [0.021750365047928235,-0.37525249670903377] Loss: 22.97561799663826\n",
      "Iteracion: 5489 Gradiente: [0.021738801886312824,-0.3750530009634936] Loss: 22.975476746679988\n",
      "Iteracion: 5490 Gradiente: [0.021727244871900097,-0.37485361127602146] Loss: 22.975335646867403\n",
      "Iteracion: 5491 Gradiente: [0.02171569400160346,-0.37465432759022477] Loss: 22.975194697040948\n",
      "Iteracion: 5492 Gradiente: [0.021704149272109893,-0.37445514984975253] Loss: 22.975053897041153\n",
      "Iteracion: 5493 Gradiente: [0.021692610680153733,-0.37425607799828003] Loss: 22.97491324670877\n",
      "Iteracion: 5494 Gradiente: [0.02168107822244849,-0.37405711197951586] Loss: 22.974772745884696\n",
      "Iteracion: 5495 Gradiente: [0.021669551895780615,-0.37385825173719195] Loss: 22.974632394410015\n",
      "Iteracion: 5496 Gradiente: [0.021658031696897714,-0.3736594972150756] Loss: 22.974492192125968\n",
      "Iteracion: 5497 Gradiente: [0.021646517622446973,-0.37346084835696625] Loss: 22.974352138873975\n",
      "Iteracion: 5498 Gradiente: [0.021635009669259374,-0.3732623051066865] Loss: 22.974212234495614\n",
      "Iteracion: 5499 Gradiente: [0.021623507833974525,-0.3730638674080975] Loss: 22.97407247883263\n",
      "Iteracion: 5500 Gradiente: [0.021612012113467926,-0.3728655352050764] Loss: 22.97393287172695\n",
      "Iteracion: 5501 Gradiente: [0.02160052250448814,-0.3726673084415404] Loss: 22.973793413020648\n",
      "Iteracion: 5502 Gradiente: [0.021589039003757193,-0.37246918706143445] Loss: 22.973654102556015\n",
      "Iteracion: 5503 Gradiente: [0.021577561607859744,-0.3722711710087451] Loss: 22.97351494017543\n",
      "Iteracion: 5504 Gradiente: [0.021566090313776463,-0.3720732602274622] Loss: 22.973375925721502\n",
      "Iteracion: 5505 Gradiente: [0.021554625118228424,-0.3718754546616237] Loss: 22.973237059036986\n",
      "Iteracion: 5506 Gradiente: [0.021543166017850505,-0.37167775425530003] Loss: 22.9730983399648\n",
      "Iteracion: 5507 Gradiente: [0.02153171300963758,-0.3714801589525712] Loss: 22.972959768348048\n",
      "Iteracion: 5508 Gradiente: [0.021520266090037883,-0.37128266869757975] Loss: 22.97282134402997\n",
      "Iteracion: 5509 Gradiente: [0.021508825256027344,-0.37108528343446684] Loss: 22.97268306685402\n",
      "Iteracion: 5510 Gradiente: [0.021497390504320417,-0.37088800310741826] Loss: 22.972544936663763\n",
      "Iteracion: 5511 Gradiente: [0.02148596183169597,-0.3706908276606441] Loss: 22.97240695330295\n",
      "Iteracion: 5512 Gradiente: [0.021474539234848558,-0.3704937570383947] Loss: 22.97226911661554\n",
      "Iteracion: 5513 Gradiente: [0.021463122710679274,-0.3702967911849323] Loss: 22.97213142644559\n",
      "Iteracion: 5514 Gradiente: [0.021451712255845714,-0.37009993004456554] Loss: 22.971993882637374\n",
      "Iteracion: 5515 Gradiente: [0.021440307867168447,-0.36990317356162306] Loss: 22.971856485035293\n",
      "Iteracion: 5516 Gradiente: [0.021428909541406447,-0.3697065216804675] Loss: 22.97171923348395\n",
      "Iteracion: 5517 Gradiente: [0.02141751727532532,-0.3695099743454884] Loss: 22.97158212782809\n",
      "Iteracion: 5518 Gradiente: [0.0214061310658091,-0.3693135315011004] Loss: 22.971445167912645\n",
      "Iteracion: 5519 Gradiente: [0.021394750909441502,-0.3691171930917656] Loss: 22.97130835358265\n",
      "Iteracion: 5520 Gradiente: [0.02138337680315677,-0.36892095906195255] Loss: 22.971171684683405\n",
      "Iteracion: 5521 Gradiente: [0.021372008743631453,-0.36872482935617723] Loss: 22.97103516106028\n",
      "Iteracion: 5522 Gradiente: [0.02136064672776854,-0.3685288039189716] Loss: 22.97089878255888\n",
      "Iteracion: 5523 Gradiente: [0.02134929075226353,-0.36833288269490616] Loss: 22.970762549024933\n",
      "Iteracion: 5524 Gradiente: [0.02133794081399761,-0.36813706562857423] Loss: 22.97062646030433\n",
      "Iteracion: 5525 Gradiente: [0.021326596909680497,-0.3679413526646074] Loss: 22.970490516243142\n",
      "Iteracion: 5526 Gradiente: [0.021315259036105278,-0.36774574374766084] Loss: 22.970354716687616\n",
      "Iteracion: 5527 Gradiente: [0.021303927190249775,-0.3675502388224088] Loss: 22.970219061484126\n",
      "Iteracion: 5528 Gradiente: [0.021292601368566012,-0.3673548378335877] Loss: 22.97008355047923\n",
      "Iteracion: 5529 Gradiente: [0.021281281568182445,-0.367159540725921] Loss: 22.969948183519666\n",
      "Iteracion: 5530 Gradiente: [0.021269967785665738,-0.3669643474441962] Loss: 22.96981296045231\n",
      "Iteracion: 5531 Gradiente: [0.02125866001791792,-0.3667692579332113] Loss: 22.969677881124174\n",
      "Iteracion: 5532 Gradiente: [0.021247358261742497,-0.36657427213779764] Loss: 22.96954294538251\n",
      "Iteracion: 5533 Gradiente: [0.02123606251388613,-0.3663793900028197] Loss: 22.969408153074685\n",
      "Iteracion: 5534 Gradiente: [0.02122477277128591,-0.36618461147316134] Loss: 22.969273504048203\n",
      "Iteracion: 5535 Gradiente: [0.021213489030495226,-0.3659899364937586] Loss: 22.969138998150775\n",
      "Iteracion: 5536 Gradiente: [0.02120221128861033,-0.3657953650095426] Loss: 22.96900463523027\n",
      "Iteracion: 5537 Gradiente: [0.021190939542267036,-0.3656008969655021] Loss: 22.9688704151347\n",
      "Iteracion: 5538 Gradiente: [0.021179673788412855,-0.36540653230663905] Loss: 22.96873633771223\n",
      "Iteracion: 5539 Gradiente: [0.021168414023727187,-0.36521227097799724] Loss: 22.968602402811214\n",
      "Iteracion: 5540 Gradiente: [0.021157160245101635,-0.36501811292463926] Loss: 22.968468610280144\n",
      "Iteracion: 5541 Gradiente: [0.021145912449287606,-0.3648240580916648] Loss: 22.96833495996771\n",
      "Iteracion: 5542 Gradiente: [0.021134670633149236,-0.3646301064241951] Loss: 22.968201451722702\n",
      "Iteracion: 5543 Gradiente: [0.021123434793518452,-0.36443625786738554] Loss: 22.968068085394137\n",
      "Iteracion: 5544 Gradiente: [0.021112204927125806,-0.3642425123664236] Loss: 22.96793486083114\n",
      "Iteracion: 5545 Gradiente: [0.021100981030990815,-0.36404886986651047] Loss: 22.96780177788302\n",
      "Iteracion: 5546 Gradiente: [0.021089763101752132,-0.3638553303128967] Loss: 22.96766883639926\n",
      "Iteracion: 5547 Gradiente: [0.021078551136434007,-0.3636618936508437] Loss: 22.96753603622945\n",
      "Iteracion: 5548 Gradiente: [0.021067345131566148,-0.3634685598256675] Loss: 22.967403377223423\n",
      "Iteracion: 5549 Gradiente: [0.02105614508425712,-0.3632753287826819] Loss: 22.9672708592311\n",
      "Iteracion: 5550 Gradiente: [0.02104495099125927,-0.3630822004672492] Loss: 22.967138482102577\n",
      "Iteracion: 5551 Gradiente: [0.02103376284931831,-0.3628891748247599] Loss: 22.967006245688125\n",
      "Iteracion: 5552 Gradiente: [0.02102258065539502,-0.36269625180062365] Loss: 22.966874149838183\n",
      "Iteracion: 5553 Gradiente: [0.021011404406197205,-0.3625034313402928] Loss: 22.966742194403324\n",
      "Iteracion: 5554 Gradiente: [0.021000234098690385,-0.3623107133892353] Loss: 22.966610379234286\n",
      "Iteracion: 5555 Gradiente: [0.020989069729643005,-0.36211809789295585] Loss: 22.966478704181974\n",
      "Iteracion: 5556 Gradiente: [0.020977911295921102,-0.3619255847969863] Loss: 22.966347169097446\n",
      "Iteracion: 5557 Gradiente: [0.02096675879436418,-0.36173317404688893] Loss: 22.96621577383191\n",
      "Iteracion: 5558 Gradiente: [0.020955612221859116,-0.3615408655882489] Loss: 22.966084518236762\n",
      "Iteracion: 5559 Gradiente: [0.020944471575128888,-0.3613486593666933] Loss: 22.965953402163528\n",
      "Iteracion: 5560 Gradiente: [0.020933336851173105,-0.3611565553278616] Loss: 22.965822425463887\n",
      "Iteracion: 5561 Gradiente: [0.02092220804667022,-0.3609645534174394] Loss: 22.965691587989692\n",
      "Iteracion: 5562 Gradiente: [0.020911085158665323,-0.36077265358112137] Loss: 22.965560889592947\n",
      "Iteracion: 5563 Gradiente: [0.020899968184019714,-0.36058085576464227] Loss: 22.965430330125823\n",
      "Iteracion: 5564 Gradiente: [0.020888857119309514,-0.3603891599137801] Loss: 22.96529990944066\n",
      "Iteracion: 5565 Gradiente: [0.020877751961712456,-0.3601975659743084] Loss: 22.965169627389894\n",
      "Iteracion: 5566 Gradiente: [0.02086665270796099,-0.3600060738920553] Loss: 22.965039483826196\n",
      "Iteracion: 5567 Gradiente: [0.020855559354881356,-0.35981468361287305] Loss: 22.96490947860233\n",
      "Iteracion: 5568 Gradiente: [0.020844471899408745,-0.3596233950826363] Loss: 22.96477961157126\n",
      "Iteracion: 5569 Gradiente: [0.020833390338281296,-0.3594322082472592] Loss: 22.964649882586084\n",
      "Iteracion: 5570 Gradiente: [0.020822314668606623,-0.3592411230526617] Loss: 22.96452029150007\n",
      "Iteracion: 5571 Gradiente: [0.020811244886929595,-0.35905013944482855] Loss: 22.964390838166615\n",
      "Iteracion: 5572 Gradiente: [0.020800180990302883,-0.3588592573697434] Loss: 22.96426152243931\n",
      "Iteracion: 5573 Gradiente: [0.020789122975657885,-0.35866847677342534] Loss: 22.964132344171887\n",
      "Iteracion: 5574 Gradiente: [0.020778070839701474,-0.3584777976019324] Loss: 22.964003303218206\n",
      "Iteracion: 5575 Gradiente: [0.02076702457954506,-0.3582872198013315] Loss: 22.963874399432303\n",
      "Iteracion: 5576 Gradiente: [0.020755984191847196,-0.3580967433177429] Loss: 22.963745632668424\n",
      "Iteracion: 5577 Gradiente: [0.020744949673545912,-0.35790636809730064] Loss: 22.963617002780843\n",
      "Iteracion: 5578 Gradiente: [0.020733921021544196,-0.357716094086168] Loss: 22.96348850962412\n",
      "Iteracion: 5579 Gradiente: [0.020722898232653126,-0.35752592123054455] Loss: 22.963360153052886\n",
      "Iteracion: 5580 Gradiente: [0.020711881303866638,-0.3573358494766443] Loss: 22.963231932921975\n",
      "Iteracion: 5581 Gradiente: [0.02070087023204792,-0.35714587877072135] Loss: 22.963103849086316\n",
      "Iteracion: 5582 Gradiente: [0.020689865014099952,-0.3569560090590541] Loss: 22.962975901401077\n",
      "Iteracion: 5583 Gradiente: [0.02067886564674287,-0.3567662402879609] Loss: 22.96284808972149\n",
      "Iteracion: 5584 Gradiente: [0.020667872127023656,-0.3565765724037698] Loss: 22.962720413903014\n",
      "Iteracion: 5585 Gradiente: [0.020656884451754346,-0.35638700535284895] Loss: 22.962592873801213\n",
      "Iteracion: 5586 Gradiente: [0.020645902617889077,-0.35619753908159096] Loss: 22.96246546927185\n",
      "Iteracion: 5587 Gradiente: [0.020634926622337464,-0.35600817353641584] Loss: 22.962338200170784\n",
      "Iteracion: 5588 Gradiente: [0.02062395646200438,-0.3558189086637744] Loss: 22.96221106635406\n",
      "Iteracion: 5589 Gradiente: [0.02061299213376344,-0.35562974441014794] Loss: 22.962084067677885\n",
      "Iteracion: 5590 Gradiente: [0.0206020336343812,-0.355440680722052] Loss: 22.961957203998626\n",
      "Iteracion: 5591 Gradiente: [0.020591080960937803,-0.35525171754601137] Loss: 22.96183047517274\n",
      "Iteracion: 5592 Gradiente: [0.02058013411023391,-0.3550628548285985] Loss: 22.961703881056906\n",
      "Iteracion: 5593 Gradiente: [0.02056919307925682,-0.35487409251640106] Loss: 22.96157742150793\n",
      "Iteracion: 5594 Gradiente: [0.020558257864881094,-0.3546854305560399] Loss: 22.961451096382778\n",
      "Iteracion: 5595 Gradiente: [0.020547328463930134,-0.354496868894174] Loss: 22.96132490553855\n",
      "Iteracion: 5596 Gradiente: [0.020536404873423446,-0.35430840747747266] Loss: 22.961198848832495\n",
      "Iteracion: 5597 Gradiente: [0.020525487090273485,-0.35412004625264226] Loss: 22.961072926122064\n",
      "Iteracion: 5598 Gradiente: [0.020514575111310288,-0.35393178516642426] Loss: 22.96094713726479\n",
      "Iteracion: 5599 Gradiente: [0.020503668933464305,-0.35374362416557925] Loss: 22.960821482118376\n",
      "Iteracion: 5600 Gradiente: [0.020492768553723787,-0.3535555631968952] Loss: 22.96069596054073\n",
      "Iteracion: 5601 Gradiente: [0.020481873969027713,-0.35336760220718955] Loss: 22.960570572389866\n",
      "Iteracion: 5602 Gradiente: [0.020470985176067793,-0.3531797411433254] Loss: 22.960445317523927\n",
      "Iteracion: 5603 Gradiente: [0.020460102171957335,-0.35299197995216763] Loss: 22.960320195801255\n",
      "Iteracion: 5604 Gradiente: [0.020449224953640054,-0.3528043185806196] Loss: 22.960195207080325\n",
      "Iteracion: 5605 Gradiente: [0.020438353517981987,-0.3526167569756153] Loss: 22.960070351219738\n",
      "Iteracion: 5606 Gradiente: [0.02042748786182642,-0.3524292950841242] Loss: 22.959945628078287\n",
      "Iteracion: 5607 Gradiente: [0.020416627982258243,-0.35224193285312366] Loss: 22.959821037514878\n",
      "Iteracion: 5608 Gradiente: [0.020405773876190855,-0.3520546702296336] Loss: 22.9596965793886\n",
      "Iteracion: 5609 Gradiente: [0.020394925540479867,-0.35186750716070186] Loss: 22.959572253558658\n",
      "Iteracion: 5610 Gradiente: [0.020384082972138156,-0.3516804435933992] Loss: 22.95944805988444\n",
      "Iteracion: 5611 Gradiente: [0.02037324616779112,-0.35149347947484666] Loss: 22.959323998225443\n",
      "Iteracion: 5612 Gradiente: [0.020362415124766168,-0.35130661475214947] Loss: 22.959200068441383\n",
      "Iteracion: 5613 Gradiente: [0.02035158983991797,-0.35111984937247226] Loss: 22.95907627039203\n",
      "Iteracion: 5614 Gradiente: [0.020340770310125815,-0.35093318328300105] Loss: 22.95895260393739\n",
      "Iteracion: 5615 Gradiente: [0.020329956532248162,-0.3507466164309581] Loss: 22.95882906893754\n",
      "Iteracion: 5616 Gradiente: [0.02031914850331115,-0.3505601487635815] Loss: 22.958705665252793\n",
      "Iteracion: 5617 Gradiente: [0.02030834622028787,-0.350373780228137] Loss: 22.958582392743526\n",
      "Iteracion: 5618 Gradiente: [0.020297549680064246,-0.35018751077192917] Loss: 22.958459251270327\n",
      "Iteracion: 5619 Gradiente: [0.020286758879611472,-0.35000134034228075] Loss: 22.958336240693903\n",
      "Iteracion: 5620 Gradiente: [0.020275973815860954,-0.3498152688865481] Loss: 22.958213360875114\n",
      "Iteracion: 5621 Gradiente: [0.020265194485784833,-0.3496292963521125] Loss: 22.958090611674933\n",
      "Iteracion: 5622 Gradiente: [0.020254420886383667,-0.3494434226863825] Loss: 22.95796799295457\n",
      "Iteracion: 5623 Gradiente: [0.0202436530145197,-0.34925764783679925] Loss: 22.957845504575285\n",
      "Iteracion: 5624 Gradiente: [0.020232890867239915,-0.34907197175082605] Loss: 22.95772314639856\n",
      "Iteracion: 5625 Gradiente: [0.020222134441456773,-0.3488863943759598] Loss: 22.957600918285962\n",
      "Iteracion: 5626 Gradiente: [0.02021138373405999,-0.3487009156597253] Loss: 22.95747882009926\n",
      "Iteracion: 5627 Gradiente: [0.02020063874213444,-0.34851553554966525] Loss: 22.957356851700354\n",
      "Iteracion: 5628 Gradiente: [0.020189899462463738,-0.34833025399336925] Loss: 22.95723501295125\n",
      "Iteracion: 5629 Gradiente: [0.020179165892189606,-0.34814507093843133] Loss: 22.95711330371415\n",
      "Iteracion: 5630 Gradiente: [0.020168438028179973,-0.34795998633249103] Loss: 22.956991723851367\n",
      "Iteracion: 5631 Gradiente: [0.020157715867516874,-0.3477750001232042] Loss: 22.956870273225416\n",
      "Iteracion: 5632 Gradiente: [0.020146999406941292,-0.34759011225827313] Loss: 22.956748951698895\n",
      "Iteracion: 5633 Gradiente: [0.020136288643665998,-0.3474053226853988] Loss: 22.956627759134573\n",
      "Iteracion: 5634 Gradiente: [0.020125583574586395,-0.34722063135233333] Loss: 22.95650669539538\n",
      "Iteracion: 5635 Gradiente: [0.020114884196628206,-0.34703603820684953] Loss: 22.956385760344368\n",
      "Iteracion: 5636 Gradiente: [0.020104190506707673,-0.3468515431967528] Loss: 22.956264953844748\n",
      "Iteracion: 5637 Gradiente: [0.02009350250194283,-0.34666714626986406] Loss: 22.956144275759872\n",
      "Iteracion: 5638 Gradiente: [0.02008282017925372,-0.3464828473740408] Loss: 22.956023725953234\n",
      "Iteracion: 5639 Gradiente: [0.020072143535600162,-0.34629864645716907] Loss: 22.955903304288487\n",
      "Iteracion: 5640 Gradiente: [0.020061472568025353,-0.3461145434671548] Loss: 22.955783010629393\n",
      "Iteracion: 5641 Gradiente: [0.02005080727340953,-0.3459305383519448] Loss: 22.95566284483993\n",
      "Iteracion: 5642 Gradiente: [0.02004014764882811,-0.3457466310595006] Loss: 22.955542806784134\n",
      "Iteracion: 5643 Gradiente: [0.020029493691185014,-0.345562821537819] Loss: 22.95542289632624\n",
      "Iteracion: 5644 Gradiente: [0.020018845397575546,-0.3453791097349183] Loss: 22.95530311333061\n",
      "Iteracion: 5645 Gradiente: [0.020008202764830684,-0.34519549559885665] Loss: 22.95518345766176\n",
      "Iteracion: 5646 Gradiente: [0.01999756579017079,-0.3450119790776978] Loss: 22.955063929184366\n",
      "Iteracion: 5647 Gradiente: [0.019986934470375674,-0.3448285601195588] Loss: 22.954944527763192\n",
      "Iteracion: 5648 Gradiente: [0.01997630880254159,-0.34464523867256663] Loss: 22.954825253263184\n",
      "Iteracion: 5649 Gradiente: [0.019965688783607525,-0.3444620146848833] Loss: 22.954706105549437\n",
      "Iteracion: 5650 Gradiente: [0.019955074410569297,-0.3442788881047006] Loss: 22.954587084487194\n",
      "Iteracion: 5651 Gradiente: [0.01994446568043789,-0.3440958588802284] Loss: 22.954468189941807\n",
      "Iteracion: 5652 Gradiente: [0.019933862590237557,-0.34391292695971] Loss: 22.954349421778787\n",
      "Iteracion: 5653 Gradiente: [0.019923265137154544,-0.3437300922914054] Loss: 22.954230779863806\n",
      "Iteracion: 5654 Gradiente: [0.01991267331780004,-0.3435473548236325] Loss: 22.954112264062662\n",
      "Iteracion: 5655 Gradiente: [0.01990208712945029,-0.34336471450470385] Loss: 22.953993874241288\n",
      "Iteracion: 5656 Gradiente: [0.01989150656908502,-0.34318217128297246] Loss: 22.953875610265804\n",
      "Iteracion: 5657 Gradiente: [0.01988093163365837,-0.3429997251068206] Loss: 22.953757472002387\n",
      "Iteracion: 5658 Gradiente: [0.019870362320143425,-0.34281737592465983] Loss: 22.95363945931745\n",
      "Iteracion: 5659 Gradiente: [0.019859798625589065,-0.34263512368492205] Loss: 22.953521572077484\n",
      "Iteracion: 5660 Gradiente: [0.019849240547049854,-0.3424529683360666] Loss: 22.953403810149137\n",
      "Iteracion: 5661 Gradiente: [0.019838688081541513,-0.3422709098265846] Loss: 22.95328617339924\n",
      "Iteracion: 5662 Gradiente: [0.01982814122592534,-0.34208894810500207] Loss: 22.9531686616947\n",
      "Iteracion: 5663 Gradiente: [0.0198175999774179,-0.34190708311984996] Loss: 22.95305127490261\n",
      "Iteracion: 5664 Gradiente: [0.0198070643330027,-0.3417253148197049] Loss: 22.952934012890186\n",
      "Iteracion: 5665 Gradiente: [0.019796534289709674,-0.3415436431531641] Loss: 22.952816875524785\n",
      "Iteracion: 5666 Gradiente: [0.019786009844408644,-0.3413620680688646] Loss: 22.952699862673917\n",
      "Iteracion: 5667 Gradiente: [0.01977549099425744,-0.34118058951545105] Loss: 22.952582974205225\n",
      "Iteracion: 5668 Gradiente: [0.019764977736231988,-0.34099920744160755] Loss: 22.952466209986497\n",
      "Iteracion: 5669 Gradiente: [0.01975447006737075,-0.3408179217960429] Loss: 22.952349569885648\n",
      "Iteracion: 5670 Gradiente: [0.019743967984736818,-0.3406367325274901] Loss: 22.952233053770748\n",
      "Iteracion: 5671 Gradiente: [0.019733471485280537,-0.3404556395847165] Loss: 22.952116661509997\n",
      "Iteracion: 5672 Gradiente: [0.019722980566122792,-0.3402746429165081] Loss: 22.95200039297176\n",
      "Iteracion: 5673 Gradiente: [0.01971249522430867,-0.3400937424716824] Loss: 22.951884248024516\n",
      "Iteracion: 5674 Gradiente: [0.01970201545670136,-0.33991293819909185] Loss: 22.95176822653686\n",
      "Iteracion: 5675 Gradiente: [0.01969154126058091,-0.33973223004759645] Loss: 22.951652328377588\n",
      "Iteracion: 5676 Gradiente: [0.019681072632760296,-0.33955161796610617] Loss: 22.95153655341562\n",
      "Iteracion: 5677 Gradiente: [0.01967060957049398,-0.33937110190353614] Loss: 22.951420901519953\n",
      "Iteracion: 5678 Gradiente: [0.01966015207065368,-0.3391906818088492] Loss: 22.95130537255979\n",
      "Iteracion: 5679 Gradiente: [0.019649700130369752,-0.3390103576310208] Loss: 22.951189966404478\n",
      "Iteracion: 5680 Gradiente: [0.019639253746660756,-0.3388301293190595] Loss: 22.95107468292344\n",
      "Iteracion: 5681 Gradiente: [0.01962881291656894,-0.3386499968220024] Loss: 22.95095952198631\n",
      "Iteracion: 5682 Gradiente: [0.019618377637085398,-0.33846996008891267] Loss: 22.950844483462806\n",
      "Iteracion: 5683 Gradiente: [0.019607947905349952,-0.33829001906887357] Loss: 22.950729567222798\n",
      "Iteracion: 5684 Gradiente: [0.019597523718399165,-0.33811017371100255] Loss: 22.95061477313631\n",
      "Iteracion: 5685 Gradiente: [0.019587105073208023,-0.3379304239644493] Loss: 22.950500101073505\n",
      "Iteracion: 5686 Gradiente: [0.01957669196693814,-0.3377507697783757] Loss: 22.950385550904674\n",
      "Iteracion: 5687 Gradiente: [0.019566284396631772,-0.3375712111019786] Loss: 22.95027112250023\n",
      "Iteracion: 5688 Gradiente: [0.019555882359210842,-0.3373917478844915] Loss: 22.950156815730747\n",
      "Iteracion: 5689 Gradiente: [0.019545485851846442,-0.3372123800751589] Loss: 22.95004263046695\n",
      "Iteracion: 5690 Gradiente: [0.0195350948716604,-0.33703310762325484] Loss: 22.94992856657966\n",
      "Iteracion: 5691 Gradiente: [0.0195247094155737,-0.33685393047809253] Loss: 22.949814623939844\n",
      "Iteracion: 5692 Gradiente: [0.019514329480807648,-0.33667484858899627] Loss: 22.94970080241864\n",
      "Iteracion: 5693 Gradiente: [0.019503955064262377,-0.3364958619053317] Loss: 22.94958710188729\n",
      "Iteracion: 5694 Gradiente: [0.01949358616299719,-0.3363169703764875] Loss: 22.9494735222172\n",
      "Iteracion: 5695 Gradiente: [0.019483222774271287,-0.3361381739518638] Loss: 22.949360063279862\n",
      "Iteracion: 5696 Gradiente: [0.019472864895037863,-0.3359594725809076] Loss: 22.949246724947\n",
      "Iteracion: 5697 Gradiente: [0.01946251252238653,-0.3357808662130851] Loss: 22.949133507090345\n",
      "Iteracion: 5698 Gradiente: [0.019452165653252487,-0.33560235479789724] Loss: 22.949020409581898\n",
      "Iteracion: 5699 Gradiente: [0.019441824284881667,-0.33542393828485295] Loss: 22.948907432293687\n",
      "Iteracion: 5700 Gradiente: [0.019431488414396843,-0.3352456166234988] Loss: 22.948794575097928\n",
      "Iteracion: 5701 Gradiente: [0.019421158038733212,-0.33506738976341477] Loss: 22.948681837866967\n",
      "Iteracion: 5702 Gradiente: [0.01941083315500407,-0.3348892576542024] Loss: 22.948569220473306\n",
      "Iteracion: 5703 Gradiente: [0.01940051376027346,-0.33471122024548805] Loss: 22.948456722789537\n",
      "Iteracion: 5704 Gradiente: [0.019390199851609207,-0.3345332774869262] Loss: 22.948344344688408\n",
      "Iteracion: 5705 Gradiente: [0.019379891426253456,-0.3343554293281915] Loss: 22.94823208604282\n",
      "Iteracion: 5706 Gradiente: [0.019369588481075083,-0.334177675719002] Loss: 22.948119946725782\n",
      "Iteracion: 5707 Gradiente: [0.019359291013256553,-0.33400001660908835] Loss: 22.948007926610465\n",
      "Iteracion: 5708 Gradiente: [0.0193489990200239,-0.33382245194820115] Loss: 22.94789602557013\n",
      "Iteracion: 5709 Gradiente: [0.019338712498156004,-0.333644981686149] Loss: 22.947784243478228\n",
      "Iteracion: 5710 Gradiente: [0.01932843144489406,-0.333467605772737] Loss: 22.947672580208337\n",
      "Iteracion: 5711 Gradiente: [0.019318155857520006,-0.3332903241577961] Loss: 22.947561035634124\n",
      "Iteracion: 5712 Gradiente: [0.019307885732969036,-0.33311313679120264] Loss: 22.94744960962941\n",
      "Iteracion: 5713 Gradiente: [0.01929762106835824,-0.3329360436228505] Loss: 22.947338302068196\n",
      "Iteracion: 5714 Gradiente: [0.019287361860558388,-0.3327590446026729] Loss: 22.947227112824546\n",
      "Iteracion: 5715 Gradiente: [0.01927710810698026,-0.33258213968060024] Loss: 22.94711604177268\n",
      "Iteracion: 5716 Gradiente: [0.01926685980465758,-0.33240532880661416] Loss: 22.947005088786998\n",
      "Iteracion: 5717 Gradiente: [0.019256616950585226,-0.33222861193071745] Loss: 22.94689425374197\n",
      "Iteracion: 5718 Gradiente: [0.01924637954197313,-0.3320519890029343] Loss: 22.946783536512267\n",
      "Iteracion: 5719 Gradiente: [0.0192361475758626,-0.33187545997332146] Loss: 22.9466729369726\n",
      "Iteracion: 5720 Gradiente: [0.01922592104927124,-0.33169902479196645] Loss: 22.946562454997906\n",
      "Iteracion: 5721 Gradiente: [0.019215699959569104,-0.33152268340896013] Loss: 22.946452090463204\n",
      "Iteracion: 5722 Gradiente: [0.019205484303563482,-0.33134643577445416] Loss: 22.946341843243665\n",
      "Iteracion: 5723 Gradiente: [0.01919527407864147,-0.33117028183859415] Loss: 22.946231713214576\n",
      "Iteracion: 5724 Gradiente: [0.01918506928172974,-0.33099422155157543] Loss: 22.94612170025136\n",
      "Iteracion: 5725 Gradiente: [0.019174869910079904,-0.3308182548636064] Loss: 22.9460118042296\n",
      "Iteracion: 5726 Gradiente: [0.01916467596059306,-0.3306423817249369] Loss: 22.945902025024957\n",
      "Iteracion: 5727 Gradiente: [0.019154487430580503,-0.3304666020858241] Loss: 22.945792362513295\n",
      "Iteracion: 5728 Gradiente: [0.019144304317117646,-0.33029091589656046] Loss: 22.945682816570557\n",
      "Iteracion: 5729 Gradiente: [0.019134126617314944,-0.3301153231074688] Loss: 22.945573387072805\n",
      "Iteracion: 5730 Gradiente: [0.019123954328277175,-0.3299398236688938] Loss: 22.9454640738963\n",
      "Iteracion: 5731 Gradiente: [0.019113787447109114,-0.3297644175312096] Loss: 22.945354876917385\n",
      "Iteracion: 5732 Gradiente: [0.019103625971103117,-0.32958910464480456] Loss: 22.945245796012543\n",
      "Iteracion: 5733 Gradiente: [0.019093469897133748,-0.3294138849601168] Loss: 22.945136831058367\n",
      "Iteracion: 5734 Gradiente: [0.019083319222492415,-0.32923875842758965] Loss: 22.945027981931634\n",
      "Iteracion: 5735 Gradiente: [0.019073173944253577,-0.32906372499770253] Loss: 22.9449192485092\n",
      "Iteracion: 5736 Gradiente: [0.019063034059519167,-0.3288887846209615] Loss: 22.944810630668098\n",
      "Iteracion: 5737 Gradiente: [0.019052899565454594,-0.32871393724789527] Loss: 22.944702128285435\n",
      "Iteracion: 5738 Gradiente: [0.019042770459274532,-0.32853918282905475] Loss: 22.944593741238528\n",
      "Iteracion: 5739 Gradiente: [0.019032646738035435,-0.32836452131502797] Loss: 22.944485469404736\n",
      "Iteracion: 5740 Gradiente: [0.01902252839883166,-0.3281899526564244] Loss: 22.944377312661597\n",
      "Iteracion: 5741 Gradiente: [0.01901241543873008,-0.3280154768038861] Loss: 22.944269270886775\n",
      "Iteracion: 5742 Gradiente: [0.019002307855158544,-0.32784109370805664] Loss: 22.94416134395808\n",
      "Iteracion: 5743 Gradiente: [0.018992205645004864,-0.3276668033196383] Loss: 22.94405353175341\n",
      "Iteracion: 5744 Gradiente: [0.01898210880550645,-0.3274926055893394] Loss: 22.943945834150828\n",
      "Iteracion: 5745 Gradiente: [0.01897201733380124,-0.32731850046790073] Loss: 22.943838251028502\n",
      "Iteracion: 5746 Gradiente: [0.01896193122704138,-0.3271444879060889] Loss: 22.94373078226476\n",
      "Iteracion: 5747 Gradiente: [0.018951850482403644,-0.3269705678546927] Loss: 22.943623427738018\n",
      "Iteracion: 5748 Gradiente: [0.018941775096941647,-0.3267967402645386] Loss: 22.943516187326868\n",
      "Iteracion: 5749 Gradiente: [0.018931705067871955,-0.3266230050864665] Loss: 22.943409060909996\n",
      "Iteracion: 5750 Gradiente: [0.018921640392420613,-0.3264493622713425] Loss: 22.943302048366228\n",
      "Iteracion: 5751 Gradiente: [0.018911581067514286,-0.3262758117700777] Loss: 22.943195149574525\n",
      "Iteracion: 5752 Gradiente: [0.018901527090483226,-0.3261023535335853] Loss: 22.943088364413953\n",
      "Iteracion: 5753 Gradiente: [0.01889147845857148,-0.32592898751280935] Loss: 22.942981692763755\n",
      "Iteracion: 5754 Gradiente: [0.01888143516871613,-0.32575571365873646] Loss: 22.94287513450324\n",
      "Iteracion: 5755 Gradiente: [0.018871397218269217,-0.32558253192235787] Loss: 22.9427686895119\n",
      "Iteracion: 5756 Gradiente: [0.018861364604226574,-0.3254094422547117] Loss: 22.942662357669338\n",
      "Iteracion: 5757 Gradiente: [0.01885133732389382,-0.32523644460684115] Loss: 22.94255613885525\n",
      "Iteracion: 5758 Gradiente: [0.018841315374351096,-0.32506353892983253] Loss: 22.942450032949502\n",
      "Iteracion: 5759 Gradiente: [0.01883129875278371,-0.32489072517478756] Loss: 22.942344039832072\n",
      "Iteracion: 5760 Gradiente: [0.018821287456304958,-0.324718003292843] Loss: 22.94223815938309\n",
      "Iteracion: 5761 Gradiente: [0.018811281482170254,-0.3245453732351506] Loss: 22.942132391482787\n",
      "Iteracion: 5762 Gradiente: [0.018801280827606584,-0.32437283495289065] Loss: 22.942026736011496\n",
      "Iteracion: 5763 Gradiente: [0.018791285489609778,-0.3242003883972823] Loss: 22.941921192849726\n",
      "Iteracion: 5764 Gradiente: [0.01878129546538787,-0.32402803351956017] Loss: 22.941815761878107\n",
      "Iteracion: 5765 Gradiente: [0.01877131075227775,-0.32385577027097495] Loss: 22.941710442977357\n",
      "Iteracion: 5766 Gradiente: [0.01876133134732072,-0.3236835986028216] Loss: 22.94160523602837\n",
      "Iteracion: 5767 Gradiente: [0.018751357247594077,-0.32351151846641835] Loss: 22.941500140912137\n",
      "Iteracion: 5768 Gradiente: [0.018741388450570183,-0.32333952981309] Loss: 22.941395157509778\n",
      "Iteracion: 5769 Gradiente: [0.018731424953246763,-0.3231676325942085] Loss: 22.941290285702546\n",
      "Iteracion: 5770 Gradiente: [0.018721466752685007,-0.32299582676117305] Loss: 22.94118552537181\n",
      "Iteracion: 5771 Gradiente: [0.018711513846335494,-0.3228241122653851] Loss: 22.941080876399095\n",
      "Iteracion: 5772 Gradiente: [0.018701566231220566,-0.32265248905829663] Loss: 22.940976338665998\n",
      "Iteracion: 5773 Gradiente: [0.018691623904640173,-0.3224809570913667] Loss: 22.94087191205432\n",
      "Iteracion: 5774 Gradiente: [0.01868168686360718,-0.3223095163161021] Loss: 22.940767596445866\n",
      "Iteracion: 5775 Gradiente: [0.018671755105361853,-0.3221381666840183] Loss: 22.940663391722715\n",
      "Iteracion: 5776 Gradiente: [0.01866182862720033,-0.32196690814665724] Loss: 22.940559297766978\n",
      "Iteracion: 5777 Gradiente: [0.018651907426295602,-0.3217957406555871] Loss: 22.940455314460902\n",
      "Iteracion: 5778 Gradiente: [0.018641991499793183,-0.3216246641624104] Loss: 22.940351441686854\n",
      "Iteracion: 5779 Gradiente: [0.018632080844895427,-0.3214536786187503] Loss: 22.940247679327385\n",
      "Iteracion: 5780 Gradiente: [0.01862217545890322,-0.321282783976246] Loss: 22.940144027265085\n",
      "Iteracion: 5781 Gradiente: [0.01861227533875649,-0.32111198018659004] Loss: 22.940040485382717\n",
      "Iteracion: 5782 Gradiente: [0.01860238048188876,-0.3209412672014667] Loss: 22.939937053563185\n",
      "Iteracion: 5783 Gradiente: [0.018592490885529854,-0.32077064497260027] Loss: 22.939833731689472\n",
      "Iteracion: 5784 Gradiente: [0.01858260654663866,-0.3206001134517565] Loss: 22.939730519644737\n",
      "Iteracion: 5785 Gradiente: [0.01857272746261742,-0.3204296725907002] Loss: 22.939627417312174\n",
      "Iteracion: 5786 Gradiente: [0.018562853630614502,-0.3202593223412381] Loss: 22.93952442457521\n",
      "Iteracion: 5787 Gradiente: [0.018552985047818993,-0.32008906265519776] Loss: 22.93942154131737\n",
      "Iteracion: 5788 Gradiente: [0.01854312171146451,-0.3199188934844347] Loss: 22.93931876742223\n",
      "Iteracion: 5789 Gradiente: [0.018533263618833947,-0.31974881478082057] Loss: 22.93921610277355\n",
      "Iteracion: 5790 Gradiente: [0.018523410766975227,-0.319578826496272] Loss: 22.939113547255207\n",
      "Iteracion: 5791 Gradiente: [0.01851356315321482,-0.31940892858271064] Loss: 22.939011100751205\n",
      "Iteracion: 5792 Gradiente: [0.01850372077482992,-0.3192391209920922] Loss: 22.938908763145673\n",
      "Iteracion: 5793 Gradiente: [0.01849388362895373,-0.3190694036764] Loss: 22.938806534322833\n",
      "Iteracion: 5794 Gradiente: [0.018484051712751654,-0.3188997765876458] Loss: 22.938704414167066\n",
      "Iteracion: 5795 Gradiente: [0.018474225023450686,-0.3187302396778611] Loss: 22.93860240256284\n",
      "Iteracion: 5796 Gradiente: [0.018464403558310968,-0.31856079289910194] Loss: 22.9385004993948\n",
      "Iteracion: 5797 Gradiente: [0.018454587314612542,-0.3183914362034501] Loss: 22.93839870454765\n",
      "Iteracion: 5798 Gradiente: [0.018444776289595666,-0.3182221695430118] Loss: 22.938297017906276\n",
      "Iteracion: 5799 Gradiente: [0.018434970480305422,-0.3180529928699316] Loss: 22.938195439355617\n",
      "Iteracion: 5800 Gradiente: [0.018425169884172494,-0.3178839061363579] Loss: 22.938093968780823\n",
      "Iteracion: 5801 Gradiente: [0.018415374498312077,-0.3177149092944847] Loss: 22.937992606067095\n",
      "Iteracion: 5802 Gradiente: [0.018405584320028843,-0.31754600229651636] Loss: 22.93789135109976\n",
      "Iteracion: 5803 Gradiente: [0.01839579934641904,-0.3173771850946977] Loss: 22.937790203764298\n",
      "Iteracion: 5804 Gradiente: [0.018386019574831873,-0.3172084576412839] Loss: 22.937689163946338\n",
      "Iteracion: 5805 Gradiente: [0.018376245002430854,-0.31703981988856555] Loss: 22.937588231531528\n",
      "Iteracion: 5806 Gradiente: [0.01836647562658982,-0.3168712717888476] Loss: 22.937487406405754\n",
      "Iteracion: 5807 Gradiente: [0.018356711444408803,-0.3167028132944752] Loss: 22.937386688454943\n",
      "Iteracion: 5808 Gradiente: [0.018346952453099637,-0.3165344443578128] Loss: 22.93728607756517\n",
      "Iteracion: 5809 Gradiente: [0.018337198650019104,-0.3163661649312425] Loss: 22.93718557362263\n",
      "Iteracion: 5810 Gradiente: [0.018327450032385666,-0.3161979749671792] Loss: 22.93708517651366\n",
      "Iteracion: 5811 Gradiente: [0.01831770659733915,-0.3160298744180673] Loss: 22.93698488612467\n",
      "Iteracion: 5812 Gradiente: [0.01830796834227139,-0.31586186323636434] Loss: 22.936884702342244\n",
      "Iteracion: 5813 Gradiente: [0.018298235264281478,-0.3156939413745654] Loss: 22.936784625053033\n",
      "Iteracion: 5814 Gradiente: [0.018288507360805777,-0.3155261087851783] Loss: 22.93668465414387\n",
      "Iteracion: 5815 Gradiente: [0.01827878462898601,-0.315358365420746] Loss: 22.93658478950166\n",
      "Iteracion: 5816 Gradiente: [0.01826906706599042,-0.31519071123383935] Loss: 22.936485031013436\n",
      "Iteracion: 5817 Gradiente: [0.01825935466916159,-0.31502314617704313] Loss: 22.936385378566378\n",
      "Iteracion: 5818 Gradiente: [0.01824964743579566,-0.3148556702029725] Loss: 22.936285832047744\n",
      "Iteracion: 5819 Gradiente: [0.018239945363042884,-0.3146882832642723] Loss: 22.936186391344936\n",
      "Iteracion: 5820 Gradiente: [0.018230248448238246,-0.3145209853136061] Loss: 22.936087056345492\n",
      "Iteracion: 5821 Gradiente: [0.01822055668857369,-0.314353776303667] Loss: 22.935987826937044\n",
      "Iteracion: 5822 Gradiente: [0.018210870081358622,-0.3141866561871693] Loss: 22.935888703007354\n",
      "Iteracion: 5823 Gradiente: [0.018201188623869293,-0.3140196249168543] Loss: 22.935789684444288\n",
      "Iteracion: 5824 Gradiente: [0.018191512313296698,-0.3138526824454922] Loss: 22.935690771135853\n",
      "Iteracion: 5825 Gradiente: [0.018181841147025087,-0.31368582872586626] Loss: 22.935591962970154\n",
      "Iteracion: 5826 Gradiente: [0.018172175122092918,-0.31351906371080956] Loss: 22.935493259835457\n",
      "Iteracion: 5827 Gradiente: [0.01816251423605119,-0.31335238735314447] Loss: 22.935394661620084\n",
      "Iteracion: 5828 Gradiente: [0.018152858485913727,-0.31318579960575416] Loss: 22.93529616821252\n",
      "Iteracion: 5829 Gradiente: [0.018143207869174678,-0.31301930042151854] Loss: 22.93519777950136\n",
      "Iteracion: 5830 Gradiente: [0.01813356238290851,-0.31285288975336495] Loss: 22.935099495375308\n",
      "Iteracion: 5831 Gradiente: [0.018123922024519363,-0.3126865675542285] Loss: 22.93500131572319\n",
      "Iteracion: 5832 Gradiente: [0.018114286791309078,-0.3125203337770737] Loss: 22.934903240433965\n",
      "Iteracion: 5833 Gradiente: [0.018104656680489484,-0.31235418837489726] Loss: 22.934805269396687\n",
      "Iteracion: 5834 Gradiente: [0.018095031689283777,-0.31218813130071915] Loss: 22.934707402500525\n",
      "Iteracion: 5835 Gradiente: [0.01808541181498716,-0.3120221625075811] Loss: 22.93460963963481\n",
      "Iteracion: 5836 Gradiente: [0.01807579705492325,-0.31185628194854664] Loss: 22.934511980688953\n",
      "Iteracion: 5837 Gradiente: [0.018066187406406205,-0.31169048957670653] Loss: 22.934414425552472\n",
      "Iteracion: 5838 Gradiente: [0.01805658286666869,-0.3115247853451801] Loss: 22.93431697411502\n",
      "Iteracion: 5839 Gradiente: [0.01804698343296233,-0.3113591692071117] Loss: 22.934219626266387\n",
      "Iteracion: 5840 Gradiente: [0.018037389102574745,-0.3111936411156685] Loss: 22.93412238189645\n",
      "Iteracion: 5841 Gradiente: [0.018027799872909138,-0.311028201024034] Loss: 22.934025240895213\n",
      "Iteracion: 5842 Gradiente: [0.018018215741097757,-0.31086284888543586] Loss: 22.933928203152785\n",
      "Iteracion: 5843 Gradiente: [0.01800863670457981,-0.3106975846531051] Loss: 22.93383126855942\n",
      "Iteracion: 5844 Gradiente: [0.017999062760567123,-0.3105324082803121] Loss: 22.933734437005473\n",
      "Iteracion: 5845 Gradiente: [0.01798949390626774,-0.31036731972035553] Loss: 22.9336377083814\n",
      "Iteracion: 5846 Gradiente: [0.01797993013909623,-0.3102023189265425] Loss: 22.933541082577822\n",
      "Iteracion: 5847 Gradiente: [0.01797037145633548,-0.31003740585221423] Loss: 22.933444559485398\n",
      "Iteracion: 5848 Gradiente: [0.01796081785532048,-0.3098725804507361] Loss: 22.933348138994983\n",
      "Iteracion: 5849 Gradiente: [0.01795126933331801,-0.30970784267549983] Loss: 22.933251820997494\n",
      "Iteracion: 5850 Gradiente: [0.017941725887514318,-0.3095431924799263] Loss: 22.93315560538401\n",
      "Iteracion: 5851 Gradiente: [0.01793218751533819,-0.30937862981744796] Loss: 22.93305949204566\n",
      "Iteracion: 5852 Gradiente: [0.017922654213955034,-0.30921415464153756] Loss: 22.93296348087376\n",
      "Iteracion: 5853 Gradiente: [0.01791312598090542,-0.3090497669056711] Loss: 22.932867571759676\n",
      "Iteracion: 5854 Gradiente: [0.017903602813236337,-0.30888546656337834] Loss: 22.93277176459496\n",
      "Iteracion: 5855 Gradiente: [0.01789408470844099,-0.30872125356818725] Loss: 22.932676059271223\n",
      "Iteracion: 5856 Gradiente: [0.01788457166378521,-0.30855712787366385] Loss: 22.932580455680213\n",
      "Iteracion: 5857 Gradiente: [0.01787506367642209,-0.308393089433404] Loss: 22.932484953713786\n",
      "Iteracion: 5858 Gradiente: [0.01786556074398978,-0.3082291382010035] Loss: 22.932389553263917\n",
      "Iteracion: 5859 Gradiente: [0.017856062863522006,-0.3080652741301151] Loss: 22.932294254222693\n",
      "Iteracion: 5860 Gradiente: [0.01784657003245703,-0.3079014971743951] Loss: 22.932199056482354\n",
      "Iteracion: 5861 Gradiente: [0.01783708224801804,-0.3077378072875347] Loss: 22.93210395993516\n",
      "Iteracion: 5862 Gradiente: [0.017827599507511612,-0.3075742044232463] Loss: 22.932008964473592\n",
      "Iteracion: 5863 Gradiente: [0.017818121808395895,-0.30741068853525666] Loss: 22.931914069990178\n",
      "Iteracion: 5864 Gradiente: [0.017808649147885564,-0.30724725957733556] Loss: 22.931819276377578\n",
      "Iteracion: 5865 Gradiente: [0.017799181523463402,-0.30708391750325686] Loss: 22.931724583528595\n",
      "Iteracion: 5866 Gradiente: [0.017789718932172606,-0.30692066226684694] Loss: 22.931629991336074\n",
      "Iteracion: 5867 Gradiente: [0.017780261371531953,-0.3067574938219284] Loss: 22.931535499693055\n",
      "Iteracion: 5868 Gradiente: [0.01777080883885181,-0.3065944121223637] Loss: 22.93144110849265\n",
      "Iteracion: 5869 Gradiente: [0.017761361331455797,-0.3064314171220323] Loss: 22.931346817628075\n",
      "Iteracion: 5870 Gradiente: [0.01775191884656806,-0.30626850877485184] Loss: 22.931252626992702\n",
      "Iteracion: 5871 Gradiente: [0.017742481381549168,-0.306105687034752] Loss: 22.931158536479966\n",
      "Iteracion: 5872 Gradiente: [0.01773304893381938,-0.3059429518556846] Loss: 22.93106454598346\n",
      "Iteracion: 5873 Gradiente: [0.01772362150076958,-0.30578030319162697] Loss: 22.930970655396827\n",
      "Iteracion: 5874 Gradiente: [0.017714199079543393,-0.3056177409965972] Loss: 22.930876864613897\n",
      "Iteracion: 5875 Gradiente: [0.01770478166754496,-0.3054552652246211] Loss: 22.930783173528596\n",
      "Iteracion: 5876 Gradiente: [0.01769536926211496,-0.3052928758297535] Loss: 22.930689582034912\n",
      "Iteracion: 5877 Gradiente: [0.0176859618607106,-0.3051305727660671] Loss: 22.930596090027006\n",
      "Iteracion: 5878 Gradiente: [0.01767655946048213,-0.30496835598767674] Loss: 22.930502697399124\n",
      "Iteracion: 5879 Gradiente: [0.017667162058969173,-0.30480622544869823] Loss: 22.93040940404561\n",
      "Iteracion: 5880 Gradiente: [0.017657769653422407,-0.30464418110329006] Loss: 22.930316209860948\n",
      "Iteracion: 5881 Gradiente: [0.017648382241029027,-0.30448222290563637] Loss: 22.93022311473973\n",
      "Iteracion: 5882 Gradiente: [0.017638999819320135,-0.30432035080992986] Loss: 22.93013011857665\n",
      "Iteracion: 5883 Gradiente: [0.017629622385625984,-0.3041585647703967] Loss: 22.930037221266517\n",
      "Iteracion: 5884 Gradiente: [0.017620249937150827,-0.30399686474129467] Loss: 22.929944422704253\n",
      "Iteracion: 5885 Gradiente: [0.017610882471418184,-0.3038352506768874] Loss: 22.929851722784893\n",
      "Iteracion: 5886 Gradiente: [0.01760151998571852,-0.30367372253147734] Loss: 22.929759121403546\n",
      "Iteracion: 5887 Gradiente: [0.0175921624774143,-0.3035122802593868] Loss: 22.929666618455535\n",
      "Iteracion: 5888 Gradiente: [0.01758280994386704,-0.30335092381496304] Loss: 22.929574213836194\n",
      "Iteracion: 5889 Gradiente: [0.017573462382445844,-0.3031896531525759] Loss: 22.929481907440973\n",
      "Iteracion: 5890 Gradiente: [0.01756411979041559,-0.30302846822662644] Loss: 22.92938969916553\n",
      "Iteracion: 5891 Gradiente: [0.017554782165243903,-0.3028673689915267] Loss: 22.929297588905488\n",
      "Iteracion: 5892 Gradiente: [0.017545449504211774,-0.3027063554017284] Loss: 22.929205576556704\n",
      "Iteracion: 5893 Gradiente: [0.01753612180472809,-0.30254542741169516] Loss: 22.929113662015098\n",
      "Iteracion: 5894 Gradiente: [0.01752679906408427,-0.30238458497592485] Loss: 22.929021845176692\n",
      "Iteracion: 5895 Gradiente: [0.017517481279717608,-0.30222382804892856] Loss: 22.928930125937637\n",
      "Iteracion: 5896 Gradiente: [0.017508168448928055,-0.3020631565852509] Loss: 22.928838504194182\n",
      "Iteracion: 5897 Gradiente: [0.017498860569127337,-0.3019025705394563] Loss: 22.92874697984271\n",
      "Iteracion: 5898 Gradiente: [0.017489557637700652,-0.301742069866131] Loss: 22.92865555277968\n",
      "Iteracion: 5899 Gradiente: [0.017480259652044576,-0.3015816545198877] Loss: 22.928564222901652\n",
      "Iteracion: 5900 Gradiente: [0.01747096660952252,-0.3014213244553641] Loss: 22.928472990105362\n",
      "Iteracion: 5901 Gradiente: [0.01746167850739937,-0.3012610796272275] Loss: 22.928381854287576\n",
      "Iteracion: 5902 Gradiente: [0.017452395343176854,-0.3011009199901572] Loss: 22.92829081534527\n",
      "Iteracion: 5903 Gradiente: [0.017443117114091442,-0.30094084549886946] Loss: 22.928199873175384\n",
      "Iteracion: 5904 Gradiente: [0.017433843817642014,-0.30078085610809363] Loss: 22.92810902767512\n",
      "Iteracion: 5905 Gradiente: [0.01742457545111241,-0.3006209517725914] Loss: 22.92801827874167\n",
      "Iteracion: 5906 Gradiente: [0.01741531201190109,-0.300461132447143] Loss: 22.927927626272425\n",
      "Iteracion: 5907 Gradiente: [0.017406053497518316,-0.3003013980865485] Loss: 22.927837070164834\n",
      "Iteracion: 5908 Gradiente: [0.017396799905244127,-0.30014174864564336] Loss: 22.927746610316436\n",
      "Iteracion: 5909 Gradiente: [0.017387551232359518,-0.2999821840792869] Loss: 22.92765624662497\n",
      "Iteracion: 5910 Gradiente: [0.017378307476454323,-0.2998227043423463] Loss: 22.92756597898817\n",
      "Iteracion: 5911 Gradiente: [0.017369068634741323,-0.2996633093897324] Loss: 22.92747580730394\n",
      "Iteracion: 5912 Gradiente: [0.017359834704735514,-0.2995039991763663] Loss: 22.927385731470302\n",
      "Iteracion: 5913 Gradiente: [0.017350605683806936,-0.2993447736571961] Loss: 22.927295751385362\n",
      "Iteracion: 5914 Gradiente: [0.01734138156924511,-0.2991856327872038] Loss: 22.927205866947354\n",
      "Iteracion: 5915 Gradiente: [0.01733216235851671,-0.29902657652138165] Loss: 22.927116078054578\n",
      "Iteracion: 5916 Gradiente: [0.017322948048999365,-0.29886760481475344] Loss: 22.927026384605494\n",
      "Iteracion: 5917 Gradiente: [0.017313738638150274,-0.29870871762236023] Loss: 22.926936786498647\n",
      "Iteracion: 5918 Gradiente: [0.01730453412315285,-0.2985499148992832] Loss: 22.926847283632682\n",
      "Iteracion: 5919 Gradiente: [0.017295334501619665,-0.2983911966006075] Loss: 22.926757875906343\n",
      "Iteracion: 5920 Gradiente: [0.017286139770927397,-0.2982325626814473] Loss: 22.92666856321855\n",
      "Iteracion: 5921 Gradiente: [0.017276949928427144,-0.2980740130969481] Loss: 22.926579345468234\n",
      "Iteracion: 5922 Gradiente: [0.01726776497151358,-0.2979155478022753] Loss: 22.92649022255448\n",
      "Iteracion: 5923 Gradiente: [0.017258584897664755,-0.2977571667526141] Loss: 22.92640119437651\n",
      "Iteracion: 5924 Gradiente: [0.017249409704163554,-0.297598869903184] Loss: 22.926312260833576\n",
      "Iteracion: 5925 Gradiente: [0.0172402393885316,-0.2974406572092156] Loss: 22.926223421825124\n",
      "Iteracion: 5926 Gradiente: [0.017231073948083993,-0.297282528625973] Loss: 22.92613467725065\n",
      "Iteracion: 5927 Gradiente: [0.01722191338021446,-0.2971244841087406] Loss: 22.926046027009775\n",
      "Iteracion: 5928 Gradiente: [0.017212757682426628,-0.2969665236128236] Loss: 22.92595747100222\n",
      "Iteracion: 5929 Gradiente: [0.01720360685209338,-0.29680864709355437] Loss: 22.925869009127837\n",
      "Iteracion: 5930 Gradiente: [0.01719446088658761,-0.29665085450629114] Loss: 22.925780641286526\n",
      "Iteracion: 5931 Gradiente: [0.01718531978343189,-0.29649314580640623] Loss: 22.92569236737836\n",
      "Iteracion: 5932 Gradiente: [0.01717618353997731,-0.2963355209493071] Loss: 22.925604187303488\n",
      "Iteracion: 5933 Gradiente: [0.017167052153567397,-0.29617797989042255] Loss: 22.925516100962167\n",
      "Iteracion: 5934 Gradiente: [0.017157925621726616,-0.2960205225851986] Loss: 22.92542810825474\n",
      "Iteracion: 5935 Gradiente: [0.017148803941879957,-0.29586314898910876] Loss: 22.92534020908169\n",
      "Iteracion: 5936 Gradiente: [0.017139687111254413,-0.2957058590576602] Loss: 22.9252524033436\n",
      "Iteracion: 5937 Gradiente: [0.01713057512757056,-0.29554865274635905] Loss: 22.92516469094114\n",
      "Iteracion: 5938 Gradiente: [0.017121467987966335,-0.2953915300107661] Loss: 22.925077071775114\n",
      "Iteracion: 5939 Gradiente: [0.01711236569000031,-0.2952344908064415] Loss: 22.924989545746403\n",
      "Iteracion: 5940 Gradiente: [0.01710326823117043,-0.2950775350889738] Loss: 22.924902112755987\n",
      "Iteracion: 5941 Gradiente: [0.017094175608769056,-0.29492066281398766] Loss: 22.924814772704984\n",
      "Iteracion: 5942 Gradiente: [0.017085087820304542,-0.2947638739371166] Loss: 22.924727525494607\n",
      "Iteracion: 5943 Gradiente: [0.01707600486321894,-0.294607168414023] Loss: 22.92464037102616\n",
      "Iteracion: 5944 Gradiente: [0.01706692673489556,-0.29445054620039707] Loss: 22.924553309201062\n",
      "Iteracion: 5945 Gradiente: [0.017057853432715812,-0.2942940072519509] Loss: 22.924466339920833\n",
      "Iteracion: 5946 Gradiente: [0.017048784954332062,-0.2941375515244058] Loss: 22.92437946308711\n",
      "Iteracion: 5947 Gradiente: [0.017039721296927723,-0.29398117897353326] Loss: 22.924292678601578\n",
      "Iteracion: 5948 Gradiente: [0.017030662458149475,-0.29382488955510433] Loss: 22.92420598636615\n",
      "Iteracion: 5949 Gradiente: [0.017021608435205355,-0.29366868322493206] Loss: 22.924119386282715\n",
      "Iteracion: 5950 Gradiente: [0.017012559225676682,-0.29351255993884057] Loss: 22.924032878253335\n",
      "Iteracion: 5951 Gradiente: [0.017003514826983708,-0.29335651965267845] Loss: 22.923946462180144\n",
      "Iteracion: 5952 Gradiente: [0.016994475236520163,-0.29320056232232605] Loss: 22.923860137965413\n",
      "Iteracion: 5953 Gradiente: [0.016985440451797253,-0.2930446879036767] Loss: 22.92377390551146\n",
      "Iteracion: 5954 Gradiente: [0.016976410470381134,-0.2928888963526443] Loss: 22.923687764720814\n",
      "Iteracion: 5955 Gradiente: [0.016967385289519635,-0.29273318762518663] Loss: 22.923601715495977\n",
      "Iteracion: 5956 Gradiente: [0.016958364906663328,-0.29257756167727084] Loss: 22.923515757739654\n",
      "Iteracion: 5957 Gradiente: [0.016949349319353737,-0.2924220184648844] Loss: 22.923429891354584\n",
      "Iteracion: 5958 Gradiente: [0.01694033852495617,-0.29226655794404655] Loss: 22.92334411624366\n",
      "Iteracion: 5959 Gradiente: [0.01693133252098183,-0.2921111800707928] Loss: 22.923258432309858\n",
      "Iteracion: 5960 Gradiente: [0.01692233130497224,-0.29195588480118023] Loss: 22.923172839456264\n",
      "Iteracion: 5961 Gradiente: [0.01691333487425292,-0.2918006720913014] Loss: 22.923087337586036\n",
      "Iteracion: 5962 Gradiente: [0.016904343226264017,-0.29164554189726616] Loss: 22.923001926602506\n",
      "Iteracion: 5963 Gradiente: [0.016895356358521477,-0.29149049417520284] Loss: 22.92291660640902\n",
      "Iteracion: 5964 Gradiente: [0.016886374268432293,-0.29133552888127084] Loss: 22.922831376909105\n",
      "Iteracion: 5965 Gradiente: [0.016877396953566402,-0.29118064597164106] Loss: 22.922746238006315\n",
      "Iteracion: 5966 Gradiente: [0.016868424411292913,-0.2910258454025211] Loss: 22.922661189604373\n",
      "Iteracion: 5967 Gradiente: [0.01685945663912681,-0.29087112713013474] Loss: 22.92257623160707\n",
      "Iteracion: 5968 Gradiente: [0.016850493634442878,-0.29071649111073417] Loss: 22.922491363918347\n",
      "Iteracion: 5969 Gradiente: [0.016841535394851802,-0.2905619373005813] Loss: 22.922406586442147\n",
      "Iteracion: 5970 Gradiente: [0.016832581917646885,-0.29040746565598624] Loss: 22.922321899082615\n",
      "Iteracion: 5971 Gradiente: [0.01682363320042555,-0.2902530761332552] Loss: 22.922237301743927\n",
      "Iteracion: 5972 Gradiente: [0.016814689240696148,-0.29009876868872897] Loss: 22.92215279433044\n",
      "Iteracion: 5973 Gradiente: [0.01680575003571126,-0.28994454327878405] Loss: 22.92206837674652\n",
      "Iteracion: 5974 Gradiente: [0.016796815583121353,-0.2897903998597991] Loss: 22.9219840488967\n",
      "Iteracion: 5975 Gradiente: [0.016787885880441424,-0.2896363383881829] Loss: 22.92189981068561\n",
      "Iteracion: 5976 Gradiente: [0.016778960925036775,-0.2894823588203744] Loss: 22.921815662017938\n",
      "Iteracion: 5977 Gradiente: [0.016770040714300195,-0.28932846111283844] Loss: 22.921731602798506\n",
      "Iteracion: 5978 Gradiente: [0.01676112524590773,-0.2891746452220417] Loss: 22.92164763293225\n",
      "Iteracion: 5979 Gradiente: [0.01675221451722469,-0.2890209111044964] Loss: 22.92156375232417\n",
      "Iteracion: 5980 Gradiente: [0.016743308525822915,-0.2888672587167227] Loss: 22.921479960879388\n",
      "Iteracion: 5981 Gradiente: [0.01673440726913308,-0.2887136880152731] Loss: 22.921396258503155\n",
      "Iteracion: 5982 Gradiente: [0.01672551074447881,-0.28856019895673163] Loss: 22.921312645100762\n",
      "Iteracion: 5983 Gradiente: [0.016716618949624264,-0.28840679149767695] Loss: 22.921229120577625\n",
      "Iteracion: 5984 Gradiente: [0.016707731881896847,-0.2882534655947371] Loss: 22.92114568483929\n",
      "Iteracion: 5985 Gradiente: [0.016698849538755667,-0.28810022120455725] Loss: 22.921062337791376\n",
      "Iteracion: 5986 Gradiente: [0.01668997191780856,-0.28794705828379513] Loss: 22.920979079339595\n",
      "Iteracion: 5987 Gradiente: [0.016681099016441672,-0.2877939767891442] Loss: 22.920895909389785\n",
      "Iteracion: 5988 Gradiente: [0.016672230832098952,-0.28764097667732014] Loss: 22.92081282784787\n",
      "Iteracion: 5989 Gradiente: [0.016663367362474445,-0.28748805790504456] Loss: 22.92072983461987\n",
      "Iteracion: 5990 Gradiente: [0.0166545086049941,-0.2873352204290794] Loss: 22.920646929611905\n",
      "Iteracion: 5991 Gradiente: [0.016645654557025106,-0.2871824642062117] Loss: 22.920564112730197\n",
      "Iteracion: 5992 Gradiente: [0.016636805216106155,-0.28702978919324346] Loss: 22.92048138388108\n",
      "Iteracion: 5993 Gradiente: [0.01662796057977687,-0.28687719534699774] Loss: 22.920398742970974\n",
      "Iteracion: 5994 Gradiente: [0.01661912064548119,-0.2867246826243273] Loss: 22.92031618990638\n",
      "Iteracion: 5995 Gradiente: [0.016610285410816536,-0.2865722509820981] Loss: 22.920233724593967\n",
      "Iteracion: 5996 Gradiente: [0.01660145487325148,-0.28641990037720894] Loss: 22.920151346940408\n",
      "Iteracion: 5997 Gradiente: [0.016592629030269753,-0.28626763076657746] Loss: 22.92006905685256\n",
      "Iteracion: 5998 Gradiente: [0.016583807879371193,-0.28611544210714507] Loss: 22.919986854237322\n",
      "Iteracion: 5999 Gradiente: [0.01657499141804332,-0.28596333435587645] Loss: 22.91990473900173\n",
      "Iteracion: 6000 Gradiente: [0.016566179643839028,-0.28581130746975575] Loss: 22.91982271105288\n",
      "Iteracion: 6001 Gradiente: [0.01655737255424583,-0.28565936140579556] Loss: 22.91974077029799\n",
      "Iteracion: 6002 Gradiente: [0.016548570146757886,-0.285507496121026] Loss: 22.91965891664439\n",
      "Iteracion: 6003 Gradiente: [0.016539772418984928,-0.2853557115724993] Loss: 22.91957714999948\n",
      "Iteracion: 6004 Gradiente: [0.016530979368262897,-0.2852040077173025] Loss: 22.91949547027078\n",
      "Iteracion: 6005 Gradiente: [0.016522190992210046,-0.2850523845125308] Loss: 22.919413877365894\n",
      "Iteracion: 6006 Gradiente: [0.016513407288347062,-0.28490084191530785] Loss: 22.91933237119254\n",
      "Iteracion: 6007 Gradiente: [0.016504628254182308,-0.2847493798827802] Loss: 22.919250951658515\n",
      "Iteracion: 6008 Gradiente: [0.016495853887248776,-0.28459799837211647] Loss: 22.91916961867172\n",
      "Iteracion: 6009 Gradiente: [0.0164870841848663,-0.28444669734052064] Loss: 22.919088372140155\n",
      "Iteracion: 6010 Gradiente: [0.016478319144956306,-0.28429547674518374] Loss: 22.91900721197194\n",
      "Iteracion: 6011 Gradiente: [0.01646955876466715,-0.28414433654336524] Loss: 22.918926138075264\n",
      "Iteracion: 6012 Gradiente: [0.016460803041655935,-0.2839932766923186] Loss: 22.918845150358415\n",
      "Iteracion: 6013 Gradiente: [0.016452051973457553,-0.283842297149325] Loss: 22.918764248729765\n",
      "Iteracion: 6014 Gradiente: [0.016443305557675102,-0.2836913978716866] Loss: 22.91868343309786\n",
      "Iteracion: 6015 Gradiente: [0.01643456379169663,-0.28354057881673855] Loss: 22.918602703371242\n",
      "Iteracion: 6016 Gradiente: [0.01642582667321051,-0.2833898399418248] Loss: 22.918522059458603\n",
      "Iteracion: 6017 Gradiente: [0.016417094199474982,-0.2832391812043325] Loss: 22.91844150126874\n",
      "Iteracion: 6018 Gradiente: [0.016408366368316742,-0.2830886025616427] Loss: 22.918361028710518\n",
      "Iteracion: 6019 Gradiente: [0.016399643177127624,-0.2829381039711821] Loss: 22.918280641692913\n",
      "Iteracion: 6020 Gradiente: [0.01639092462341599,-0.28278768539039306] Loss: 22.918200340125004\n",
      "Iteracion: 6021 Gradiente: [0.016382210704786834,-0.2826373467767379] Loss: 22.918120123915955\n",
      "Iteracion: 6022 Gradiente: [0.016373501418641467,-0.28248708808771] Loss: 22.91803999297503\n",
      "Iteracion: 6023 Gradiente: [0.016364796762709945,-0.2823369092808086] Loss: 22.917959947211603\n",
      "Iteracion: 6024 Gradiente: [0.016356096734436203,-0.28218681031356996] Loss: 22.91787998653511\n",
      "Iteracion: 6025 Gradiente: [0.016347401331422397,-0.28203679114354674] Loss: 22.91780011085512\n",
      "Iteracion: 6026 Gradiente: [0.016338710551040473,-0.2818868517283255] Loss: 22.91772032008128\n",
      "Iteracion: 6027 Gradiente: [0.01633002439108964,-0.2817369920254913] Loss: 22.917640614123336\n",
      "Iteracion: 6028 Gradiente: [0.016321342848823406,-0.2815872119926829] Loss: 22.91756099289113\n",
      "Iteracion: 6029 Gradiente: [0.016312665921982255,-0.28143751158753516] Loss: 22.917481456294627\n",
      "Iteracion: 6030 Gradiente: [0.016303993608049912,-0.28128789076771843] Loss: 22.9174020042438\n",
      "Iteracion: 6031 Gradiente: [0.01629532590463422,-0.28113834949091987] Loss: 22.917322636648848\n",
      "Iteracion: 6032 Gradiente: [0.016286662809184803,-0.2809888877148554] Loss: 22.91724335341995\n",
      "Iteracion: 6033 Gradiente: [0.016278004319312346,-0.28083950539726066] Loss: 22.917164154467443\n",
      "Iteracion: 6034 Gradiente: [0.016269350432508153,-0.28069020249589216] Loss: 22.917085039701725\n",
      "Iteracion: 6035 Gradiente: [0.016260701146501334,-0.2805409789685226] Loss: 22.917006009033347\n",
      "Iteracion: 6036 Gradiente: [0.0162520564587093,-0.28039183477296065] Loss: 22.91692706237287\n",
      "Iteracion: 6037 Gradiente: [0.01624341636668305,-0.2802427698670318] Loss: 22.916848199631026\n",
      "Iteracion: 6038 Gradiente: [0.016234780867913894,-0.28009378420858805] Loss: 22.916769420718595\n",
      "Iteracion: 6039 Gradiente: [0.01622614996008167,-0.2799448777554897] Loss: 22.91669072554649\n",
      "Iteracion: 6040 Gradiente: [0.016217523640684326,-0.2797960504656345] Loss: 22.91661211402566\n",
      "Iteracion: 6041 Gradiente: [0.016208901907314537,-0.2796473022969347] Loss: 22.916533586067224\n",
      "Iteracion: 6042 Gradiente: [0.016200284757511934,-0.27949863320732754] Loss: 22.916455141582333\n",
      "Iteracion: 6043 Gradiente: [0.016191672188883406,-0.27935004315476974] Loss: 22.91637678048227\n",
      "Iteracion: 6044 Gradiente: [0.016183064198972374,-0.27920153209724463] Loss: 22.91629850267838\n",
      "Iteracion: 6045 Gradiente: [0.016174460785284357,-0.2790530999927583] Loss: 22.916220308082142\n",
      "Iteracion: 6046 Gradiente: [0.016165861945434776,-0.2789047467993345] Loss: 22.91614219660509\n",
      "Iteracion: 6047 Gradiente: [0.016157267676988833,-0.27875647247502233] Loss: 22.91606416815888\n",
      "Iteracion: 6048 Gradiente: [0.016148677977622584,-0.2786082769778858] Loss: 22.915986222655256\n",
      "Iteracion: 6049 Gradiente: [0.016140092844667227,-0.27846016026603365] Loss: 22.915908360006053\n",
      "Iteracion: 6050 Gradiente: [0.016131512275896397,-0.2783121222975678] Loss: 22.915830580123174\n",
      "Iteracion: 6051 Gradiente: [0.016122936268783406,-0.27816416303063213] Loss: 22.91575288291868\n",
      "Iteracion: 6052 Gradiente: [0.016114364821060198,-0.27801628242337617] Loss: 22.915675268304636\n",
      "Iteracion: 6053 Gradiente: [0.016105797930134714,-0.2778684804339933] Loss: 22.915597736193305\n",
      "Iteracion: 6054 Gradiente: [0.016097235593552264,-0.27772075702068916] Loss: 22.915520286496943\n",
      "Iteracion: 6055 Gradiente: [0.016088677808996484,-0.2775731121416849] Loss: 22.91544291912798\n",
      "Iteracion: 6056 Gradiente: [0.016080124574073314,-0.27742554575522715] Loss: 22.91536563399887\n",
      "Iteracion: 6057 Gradiente: [0.01607157588623238,-0.2772780578195945] Loss: 22.91528843102221\n",
      "Iteracion: 6058 Gradiente: [0.01606303174325395,-0.2771306482930696] Loss: 22.915211310110696\n",
      "Iteracion: 6059 Gradiente: [0.01605449214250901,-0.276983317133978] Loss: 22.91513427117706\n",
      "Iteracion: 6060 Gradiente: [0.016045957081633824,-0.27683606430065644] Loss: 22.91505731413418\n",
      "Iteracion: 6061 Gradiente: [0.01603742655827318,-0.2766888897514594] Loss: 22.914980438894993\n",
      "Iteracion: 6062 Gradiente: [0.016028900570072818,-0.276541793444767] Loss: 22.914903645372554\n",
      "Iteracion: 6063 Gradiente: [0.01602037911450888,-0.2763947753389875] Loss: 22.91482693348\n",
      "Iteracion: 6064 Gradiente: [0.016011862189297215,-0.2762478353925412] Loss: 22.91475030313056\n",
      "Iteracion: 6065 Gradiente: [0.016003349791884603,-0.2761009735638852] Loss: 22.91467375423755\n",
      "Iteracion: 6066 Gradiente: [0.01599484191999636,-0.2759541898114807] Loss: 22.91459728671439\n",
      "Iteracion: 6067 Gradiente: [0.015986338570966534,-0.2758074840938329] Loss: 22.91452090047458\n",
      "Iteracion: 6068 Gradiente: [0.015977839742690018,-0.2756608563694434] Loss: 22.914444595431707\n",
      "Iteracion: 6069 Gradiente: [0.015969345432679916,-0.2755143065968507] Loss: 22.9143683714995\n",
      "Iteracion: 6070 Gradiente: [0.01596085563848722,-0.2753678347346167] Loss: 22.9142922285917\n",
      "Iteracion: 6071 Gradiente: [0.01595237035771409,-0.2752214407413207] Loss: 22.914216166622204\n",
      "Iteracion: 6072 Gradiente: [0.01594388958793805,-0.2750751245755681] Loss: 22.91414018550495\n",
      "Iteracion: 6073 Gradiente: [0.015935413326832303,-0.27492888619597833] Loss: 22.91406428515402\n",
      "Iteracion: 6074 Gradiente: [0.015926941571952586,-0.27478272556120104] Loss: 22.91398846548355\n",
      "Iteracion: 6075 Gradiente: [0.015918474320919054,-0.2746366426299025] Loss: 22.913912726407762\n",
      "Iteracion: 6076 Gradiente: [0.01591001157136134,-0.2744906373607743] Loss: 22.913837067841026\n",
      "Iteracion: 6077 Gradiente: [0.01590155332092138,-0.2743447097125253] Loss: 22.91376148969773\n",
      "Iteracion: 6078 Gradiente: [0.015893099567034597,-0.2741988596438978] Loss: 22.913685991892404\n",
      "Iteracion: 6079 Gradiente: [0.0158846503074642,-0.27405308711364224] Loss: 22.913610574339618\n",
      "Iteracion: 6080 Gradiente: [0.015876205539768762,-0.27390739208053816] Loss: 22.913535236954115\n",
      "Iteracion: 6081 Gradiente: [0.015867765261585495,-0.27376177450338685] Loss: 22.913459979650632\n",
      "Iteracion: 6082 Gradiente: [0.015859329470541184,-0.27361623434100674] Loss: 22.913384802344076\n",
      "Iteracion: 6083 Gradiente: [0.015850898164230406,-0.2734707715522429] Loss: 22.913309704949405\n",
      "Iteracion: 6084 Gradiente: [0.01584247134015963,-0.27332538609596946] Loss: 22.91323468738165\n",
      "Iteracion: 6085 Gradiente: [0.015834048996198172,-0.273180077931058] Loss: 22.913159749555998\n",
      "Iteracion: 6086 Gradiente: [0.01582563112972745,-0.27303484701643205] Loss: 22.91308489138765\n",
      "Iteracion: 6087 Gradiente: [0.015817217738400776,-0.2728896933110215] Loss: 22.91301011279196\n",
      "Iteracion: 6088 Gradiente: [0.015808808819925465,-0.27274461677377354] Loss: 22.91293541368431\n",
      "Iteracion: 6089 Gradiente: [0.01580040437193778,-0.27259961736366345] Loss: 22.912860793980233\n",
      "Iteracion: 6090 Gradiente: [0.01579200439197071,-0.2724546950396923] Loss: 22.912786253595307\n",
      "Iteracion: 6091 Gradiente: [0.015783608877732527,-0.27230984976087586] Loss: 22.912711792445236\n",
      "Iteracion: 6092 Gradiente: [0.01577521782679791,-0.27216508148625507] Loss: 22.91263741044579\n",
      "Iteracion: 6093 Gradiente: [0.01576683123683627,-0.2720203901748913] Loss: 22.912563107512817\n",
      "Iteracion: 6094 Gradiente: [0.015758449105333246,-0.27187577578587585] Loss: 22.91248888356229\n",
      "Iteracion: 6095 Gradiente: [0.015750071430083303,-0.27173123827830614] Loss: 22.912414738510243\n",
      "Iteracion: 6096 Gradiente: [0.015741698208637446,-0.2715867776113131] Loss: 22.912340672272816\n",
      "Iteracion: 6097 Gradiente: [0.01573332943868119,-0.27144239374404305] Loss: 22.912266684766227\n",
      "Iteracion: 6098 Gradiente: [0.015724965117862175,-0.27129808663566535] Loss: 22.912192775906775\n",
      "Iteracion: 6099 Gradiente: [0.01571660524374655,-0.2711538562453787] Loss: 22.912118945610892\n",
      "Iteracion: 6100 Gradiente: [0.015708249813961097,-0.2710097025323967] Loss: 22.91204519379502\n",
      "Iteracion: 6101 Gradiente: [0.015699898826151564,-0.2708656254559548] Loss: 22.911971520375776\n",
      "Iteracion: 6102 Gradiente: [0.015691552278094417,-0.2707216249753028] Loss: 22.911897925269805\n",
      "Iteracion: 6103 Gradiente: [0.01568321016729423,-0.2705777010497278] Loss: 22.911824408393883\n",
      "Iteracion: 6104 Gradiente: [0.015674872491395795,-0.2704338536385303] Loss: 22.911750969664823\n",
      "Iteracion: 6105 Gradiente: [0.015666539248005998,-0.27029008270103494] Loss: 22.911677608999582\n",
      "Iteracion: 6106 Gradiente: [0.015658210434793318,-0.2701463881965849] Loss: 22.911604326315157\n",
      "Iteracion: 6107 Gradiente: [0.01564988604955791,-0.27000277008453694] Loss: 22.911531121528686\n",
      "Iteracion: 6108 Gradiente: [0.015641566089745613,-0.2698592283242893] Loss: 22.91145799455733\n",
      "Iteracion: 6109 Gradiente: [0.0156332505530969,-0.2697157628752483] Loss: 22.911384945318407\n",
      "Iteracion: 6110 Gradiente: [0.015624939437212977,-0.2695723736968431] Loss: 22.91131197372927\n",
      "Iteracion: 6111 Gradiente: [0.015616632739788845,-0.269429060748527] Loss: 22.91123907970738\n",
      "Iteracion: 6112 Gradiente: [0.015608330458463607,-0.26928582398977147] Loss: 22.91116626317028\n",
      "Iteracion: 6113 Gradiente: [0.015600032590885842,-0.2691426633800756] Loss: 22.911093524035618\n",
      "Iteracion: 6114 Gradiente: [0.015591739134832968,-0.2689995788789459] Loss: 22.911020862221104\n",
      "Iteracion: 6115 Gradiente: [0.015583450087630506,-0.2688565704459413] Loss: 22.91094827764456\n",
      "Iteracion: 6116 Gradiente: [0.015575165447231143,-0.26871363804060333] Loss: 22.91087577022389\n",
      "Iteracion: 6117 Gradiente: [0.015566885211287247,-0.2685707816225142] Loss: 22.910803339877056\n",
      "Iteracion: 6118 Gradiente: [0.015558609377256971,-0.26842800115128823] Loss: 22.91073098652214\n",
      "Iteracion: 6119 Gradiente: [0.015550337942912999,-0.2682852965865442] Loss: 22.910658710077314\n",
      "Iteracion: 6120 Gradiente: [0.01554207090598349,-0.26814266788792324] Loss: 22.910586510460806\n",
      "Iteracion: 6121 Gradiente: [0.015533808264018489,-0.26800011501509974] Loss: 22.91051438759096\n",
      "Iteracion: 6122 Gradiente: [0.015525550014780265,-0.2678576379277543] Loss: 22.910442341386194\n",
      "Iteracion: 6123 Gradiente: [0.015517296155852022,-0.267715236585604] Loss: 22.910370371765\n",
      "Iteracion: 6124 Gradiente: [0.01550904668490413,-0.26757291094838026] Loss: 22.910298478646006\n",
      "Iteracion: 6125 Gradiente: [0.01550080159962685,-0.26743066097583407] Loss: 22.910226661947863\n",
      "Iteracion: 6126 Gradiente: [0.015492560897659284,-0.2672884866277409] Loss: 22.910154921589335\n",
      "Iteracion: 6127 Gradiente: [0.015484324576758014,-0.26714638786389244] Loss: 22.91008325748931\n",
      "Iteracion: 6128 Gradiente: [0.015476092634501507,-0.26700436464410976] Loss: 22.910011669566657\n",
      "Iteracion: 6129 Gradiente: [0.015467865068643505,-0.2668624169282277] Loss: 22.909940157740483\n",
      "Iteracion: 6130 Gradiente: [0.015459641876839214,-0.2667205446761064] Loss: 22.909868721929843\n",
      "Iteracion: 6131 Gradiente: [0.015451423056655736,-0.2665787478476345] Loss: 22.909797362053954\n",
      "Iteracion: 6132 Gradiente: [0.01544320860587239,-0.2664370264027079] Loss: 22.90972607803211\n",
      "Iteracion: 6133 Gradiente: [0.015434998522155751,-0.2662953803012507] Loss: 22.909654869783648\n",
      "Iteracion: 6134 Gradiente: [0.015426792803131662,-0.26615380950321127] Loss: 22.90958373722805\n",
      "Iteracion: 6135 Gradiente: [0.015418591446557646,-0.26601231396855074] Loss: 22.90951268028486\n",
      "Iteracion: 6136 Gradiente: [0.01541039445006523,-0.2658708936572605] Loss: 22.90944169887367\n",
      "Iteracion: 6137 Gradiente: [0.015402201811362677,-0.265729548529349] Loss: 22.909370792914228\n",
      "Iteracion: 6138 Gradiente: [0.015394013528144039,-0.2655882785448431] Loss: 22.90929996232631\n",
      "Iteracion: 6139 Gradiente: [0.015385829598026627,-0.26544708366379993] Loss: 22.9092292070298\n",
      "Iteracion: 6140 Gradiente: [0.015377650018788814,-0.26530596384628663] Loss: 22.90915852694467\n",
      "Iteracion: 6141 Gradiente: [0.015369474788057384,-0.2651649190524007] Loss: 22.909087921990952\n",
      "Iteracion: 6142 Gradiente: [0.015361303903530181,-0.2650239492422552] Loss: 22.909017392088835\n",
      "Iteracion: 6143 Gradiente: [0.015353137362831148,-0.26488305437599446] Loss: 22.908946937158486\n",
      "Iteracion: 6144 Gradiente: [0.015344975163678025,-0.2647422344137681] Loss: 22.90887655712022\n",
      "Iteracion: 6145 Gradiente: [0.015336817303888969,-0.2646014893157532] Loss: 22.90880625189447\n",
      "Iteracion: 6146 Gradiente: [0.01532866378102066,-0.26446081904215374] Loss: 22.908736021401648\n",
      "Iteracion: 6147 Gradiente: [0.015320514592932948,-0.26432022355318335] Loss: 22.908665865562355\n",
      "Iteracion: 6148 Gradiente: [0.015312369737067873,-0.2641797028090983] Loss: 22.90859578429724\n",
      "Iteracion: 6149 Gradiente: [0.015304229211288127,-0.2640392567701528] Loss: 22.90852577752703\n",
      "Iteracion: 6150 Gradiente: [0.015296093013271653,-0.2638988853966325] Loss: 22.90845584517252\n",
      "Iteracion: 6151 Gradiente: [0.015287961140709664,-0.2637585886488418] Loss: 22.90838598715461\n",
      "Iteracion: 6152 Gradiente: [0.015279833591232734,-0.26361836648711295] Loss: 22.90831620339429\n",
      "Iteracion: 6153 Gradiente: [0.015271710362702607,-0.26347821887178413] Loss: 22.908246493812637\n",
      "Iteracion: 6154 Gradiente: [0.015263591452673116,-0.26333814576323306] Loss: 22.908176858330776\n",
      "Iteracion: 6155 Gradiente: [0.015255476858951057,-0.2631981471218437] Loss: 22.908107296869968\n",
      "Iteracion: 6156 Gradiente: [0.01524736657924753,-0.26305822290802516] Loss: 22.908037809351494\n",
      "Iteracion: 6157 Gradiente: [0.015239260611142906,-0.2629183730822182] Loss: 22.907968395696795\n",
      "Iteracion: 6158 Gradiente: [0.015231158952436393,-0.26277859760487166] Loss: 22.90789905582733\n",
      "Iteracion: 6159 Gradiente: [0.015223061600815413,-0.2626388964364575] Loss: 22.907829789664675\n",
      "Iteracion: 6160 Gradiente: [0.01521496855401665,-0.2624992695374723] Loss: 22.90776059713047\n",
      "Iteracion: 6161 Gradiente: [0.015206879809742684,-0.2623597168684316] Loss: 22.907691478146475\n",
      "Iteracion: 6162 Gradiente: [0.015198795365665773,-0.2622202383898749] Loss: 22.907622432634476\n",
      "Iteracion: 6163 Gradiente: [0.015190715219555765,-0.2620808340623573] Loss: 22.9075534605164\n",
      "Iteracion: 6164 Gradiente: [0.015182639369097235,-0.2619415038464582] Loss: 22.90748456171421\n",
      "Iteracion: 6165 Gradiente: [0.015174567811927393,-0.26180224770278476] Loss: 22.907415736150003\n",
      "Iteracion: 6166 Gradiente: [0.01516650054595156,-0.26166306559194497] Loss: 22.90734698374589\n",
      "Iteracion: 6167 Gradiente: [0.015158437568732096,-0.26152395747458995] Loss: 22.907278304424132\n",
      "Iteracion: 6168 Gradiente: [0.01515037887801327,-0.2613849233113825] Loss: 22.90720969810702\n",
      "Iteracion: 6169 Gradiente: [0.015142324471473028,-0.26124596306300657] Loss: 22.90714116471698\n",
      "Iteracion: 6170 Gradiente: [0.015134274347081108,-0.2611070766901546] Loss: 22.907072704176475\n",
      "Iteracion: 6171 Gradiente: [0.015126228502307982,-0.2609682641535649] Loss: 22.907004316408063\n",
      "Iteracion: 6172 Gradiente: [0.015118186934872331,-0.26082952541398574] Loss: 22.90693600133441\n",
      "Iteracion: 6173 Gradiente: [0.015110149642726849,-0.2606908604321704] Loss: 22.90686775887822\n",
      "Iteracion: 6174 Gradiente: [0.015102116623416843,-0.2605522691689174] Loss: 22.90679958896231\n",
      "Iteracion: 6175 Gradiente: [0.015094087874559629,-0.26041375158504393] Loss: 22.906731491509564\n",
      "Iteracion: 6176 Gradiente: [0.01508606339416474,-0.2602753076413605] Loss: 22.90666346644299\n",
      "Iteracion: 6177 Gradiente: [0.015078043179813486,-0.2601369372987296] Loss: 22.9065955136856\n",
      "Iteracion: 6178 Gradiente: [0.015070027229202764,-0.25999864051802246] Loss: 22.906527633160547\n",
      "Iteracion: 6179 Gradiente: [0.01506201554018105,-0.25986041726012643] Loss: 22.906459824791053\n",
      "Iteracion: 6180 Gradiente: [0.015054008110405448,-0.25972226748595456] Loss: 22.906392088500414\n",
      "Iteracion: 6181 Gradiente: [0.015046004937651484,-0.25958419115644393] Loss: 22.906324424212016\n",
      "Iteracion: 6182 Gradiente: [0.01503800601954121,-0.2594461882325535] Loss: 22.906256831849323\n",
      "Iteracion: 6183 Gradiente: [0.015030011353924995,-0.2593082586752511] Loss: 22.90618931133587\n",
      "Iteracion: 6184 Gradiente: [0.01502202093854237,-0.25917040244553413] Loss: 22.90612186259528\n",
      "Iteracion: 6185 Gradiente: [0.015014034771199173,-0.2590326195044141] Loss: 22.906054485551284\n",
      "Iteracion: 6186 Gradiente: [0.01500605284939051,-0.25889490981294483] Loss: 22.90598718012764\n",
      "Iteracion: 6187 Gradiente: [0.014998075171110752,-0.25875727333217025] Loss: 22.905919946248243\n",
      "Iteracion: 6188 Gradiente: [0.014990101733928896,-0.2586197100231761] Loss: 22.905852783837023\n",
      "Iteracion: 6189 Gradiente: [0.014982132535759736,-0.25848221984705533] Loss: 22.905785692818036\n",
      "Iteracion: 6190 Gradiente: [0.014974167574231955,-0.25834480276493454] Loss: 22.90571867311537\n",
      "Iteracion: 6191 Gradiente: [0.014966206847188347,-0.25820745873794804] Loss: 22.905651724653207\n",
      "Iteracion: 6192 Gradiente: [0.01495825035225001,-0.2580701877272664] Loss: 22.905584847355854\n",
      "Iteracion: 6193 Gradiente: [0.014950298087128052,-0.25793298969407263] Loss: 22.90551804114765\n",
      "Iteracion: 6194 Gradiente: [0.014942350049826322,-0.2577958645995565] Loss: 22.905451305953022\n",
      "Iteracion: 6195 Gradiente: [0.014934406237926131,-0.25765881240494937] Loss: 22.905384641696475\n",
      "Iteracion: 6196 Gradiente: [0.014926466649178375,-0.25752183307149823] Loss: 22.905318048302643\n",
      "Iteracion: 6197 Gradiente: [0.014918531281328267,-0.25738492656046597] Loss: 22.905251525696148\n",
      "Iteracion: 6198 Gradiente: [0.014910600132109646,-0.25724809283314026] Loss: 22.905185073801782\n",
      "Iteracion: 6199 Gradiente: [0.014902673199397516,-0.25711133185082163] Loss: 22.905118692544374\n",
      "Iteracion: 6200 Gradiente: [0.014894750480859404,-0.25697464357483923] Loss: 22.905052381848815\n",
      "Iteracion: 6201 Gradiente: [0.01488683197438263,-0.25683802796653493] Loss: 22.904986141640133\n",
      "Iteracion: 6202 Gradiente: [0.014878917677515346,-0.2567014849872879] Loss: 22.90491997184338\n",
      "Iteracion: 6203 Gradiente: [0.0148710075881894,-0.2565650145984763] Loss: 22.904853872383715\n",
      "Iteracion: 6204 Gradiente: [0.01486310170409979,-0.25642861676151274] Loss: 22.904787843186384\n",
      "Iteracion: 6205 Gradiente: [0.01485520002304194,-0.2562922914378253] Loss: 22.90472188417667\n",
      "Iteracion: 6206 Gradiente: [0.014847302542720323,-0.25615603858886593] Loss: 22.90465599527998\n",
      "Iteracion: 6207 Gradiente: [0.014839409260930362,-0.2560198581761051] Loss: 22.904590176421816\n",
      "Iteracion: 6208 Gradiente: [0.014831520175464637,-0.2558837501610303] Loss: 22.904524427527676\n",
      "Iteracion: 6209 Gradiente: [0.01482363528402099,-0.2557477145051583] Loss: 22.90445874852321\n",
      "Iteracion: 6210 Gradiente: [0.01481575458455969,-0.2556117511700081] Loss: 22.904393139334143\n",
      "Iteracion: 6211 Gradiente: [0.014807878074687626,-0.255475860117143] Loss: 22.904327599886244\n",
      "Iteracion: 6212 Gradiente: [0.014800005752206857,-0.2553400413081313] Loss: 22.90426213010539\n",
      "Iteracion: 6213 Gradiente: [0.014792137614815222,-0.25520429470457273] Loss: 22.904196729917512\n",
      "Iteracion: 6214 Gradiente: [0.014784273660420883,-0.2550686202680725] Loss: 22.904131399248662\n",
      "Iteracion: 6215 Gradiente: [0.014776413886723579,-0.2549330179602689] Loss: 22.904066138024916\n",
      "Iteracion: 6216 Gradiente: [0.014768558291573678,-0.2547974877428107] Loss: 22.904000946172463\n",
      "Iteracion: 6217 Gradiente: [0.014760706872670918,-0.25466202957737827] Loss: 22.903935823617566\n",
      "Iteracion: 6218 Gradiente: [0.014752859627812616,-0.25452664342566395] Loss: 22.903870770286566\n",
      "Iteracion: 6219 Gradiente: [0.014745016554907881,-0.2543913292493775] Loss: 22.903805786105856\n",
      "Iteracion: 6220 Gradiente: [0.014737177651481186,-0.254256087010269] Loss: 22.903740871001972\n",
      "Iteracion: 6221 Gradiente: [0.014729342915451108,-0.2541209166700848] Loss: 22.903676024901454\n",
      "Iteracion: 6222 Gradiente: [0.01472151234466234,-0.2539858181906008] Loss: 22.90361124773096\n",
      "Iteracion: 6223 Gradiente: [0.014713685936784297,-0.2538507915336188] Loss: 22.903546539417253\n",
      "Iteracion: 6224 Gradiente: [0.014705863689723249,-0.25371583666094855] Loss: 22.90348189988708\n",
      "Iteracion: 6225 Gradiente: [0.014698045601242408,-0.25358095353442905] Loss: 22.90341732906734\n",
      "Iteracion: 6226 Gradiente: [0.01469023166897235,-0.25344614211592675] Loss: 22.903352826885055\n",
      "Iteracion: 6227 Gradiente: [0.014682421890952924,-0.25331140236730576] Loss: 22.903288393267182\n",
      "Iteracion: 6228 Gradiente: [0.014674616264797654,-0.2531767342504737] Loss: 22.903224028140873\n",
      "Iteracion: 6229 Gradiente: [0.014666814788334174,-0.25304213772734746] Loss: 22.903159731433348\n",
      "Iteracion: 6230 Gradiente: [0.014659017459388224,-0.2529076127598637] Loss: 22.90309550307185\n",
      "Iteracion: 6231 Gradiente: [0.014651224275716383,-0.2527731593099837] Loss: 22.903031342983738\n",
      "Iteracion: 6232 Gradiente: [0.01464343523518797,-0.25263877733968115] Loss: 22.90296725109643\n",
      "Iteracion: 6233 Gradiente: [0.014635650335461985,-0.25250446681096494] Loss: 22.902903227337447\n",
      "Iteracion: 6234 Gradiente: [0.014627869574610486,-0.25237022768583706] Loss: 22.902839271634356\n",
      "Iteracion: 6235 Gradiente: [0.01462009295020626,-0.2522360599263505] Loss: 22.902775383914822\n",
      "Iteracion: 6236 Gradiente: [0.014612320459947151,-0.2521019634945725] Loss: 22.902711564106564\n",
      "Iteracion: 6237 Gradiente: [0.014604552101923218,-0.2519679383525651] Loss: 22.90264781213741\n",
      "Iteracion: 6238 Gradiente: [0.014596787873740406,-0.25183398446244115] Loss: 22.90258412793525\n",
      "Iteracion: 6239 Gradiente: [0.01458902777324435,-0.25170010178631763] Loss: 22.90252051142805\n",
      "Iteracion: 6240 Gradiente: [0.01458127179829584,-0.25156629028633204] Loss: 22.90245696254384\n",
      "Iteracion: 6241 Gradiente: [0.014573519946646721,-0.2514325499246477] Loss: 22.90239348121074\n",
      "Iteracion: 6242 Gradiente: [0.01456577221609905,-0.25129888066344486] Loss: 22.902330067356957\n",
      "Iteracion: 6243 Gradiente: [0.014558028604464349,-0.25116528246492714] Loss: 22.902266720910752\n",
      "Iteracion: 6244 Gradiente: [0.014550289109646997,-0.2510317552913072] Loss: 22.902203441800467\n",
      "Iteracion: 6245 Gradiente: [0.014542553729339148,-0.2508982991048337] Loss: 22.902140229954526\n",
      "Iteracion: 6246 Gradiente: [0.01453482246141012,-0.2507649138677638] Loss: 22.902077085301435\n",
      "Iteracion: 6247 Gradiente: [0.01452709530365818,-0.250631599542382] Loss: 22.902014007769782\n",
      "Iteracion: 6248 Gradiente: [0.014519372253888226,-0.2504983560909873] Loss: 22.901950997288218\n",
      "Iteracion: 6249 Gradiente: [0.014511653309928837,-0.2503651834759009] Loss: 22.90188805378544\n",
      "Iteracion: 6250 Gradiente: [0.014503938469616173,-0.2502320816594654] Loss: 22.90182517719028\n",
      "Iteracion: 6251 Gradiente: [0.014496227730686921,-0.25009905060404247] Loss: 22.901762367431576\n",
      "Iteracion: 6252 Gradiente: [0.014488521091023662,-0.24996609027201291] Loss: 22.901699624438333\n",
      "Iteracion: 6253 Gradiente: [0.014480818548474873,-0.249833200625775] Loss: 22.90163694813955\n",
      "Iteracion: 6254 Gradiente: [0.014473120100846396,-0.2497003816277516] Loss: 22.901574338464357\n",
      "Iteracion: 6255 Gradiente: [0.01446542574598008,-0.24956763324038272] Loss: 22.901511795341904\n",
      "Iteracion: 6256 Gradiente: [0.014457735481645766,-0.24943495542613217] Loss: 22.901449318701466\n",
      "Iteracion: 6257 Gradiente: [0.01445004930566256,-0.2493023481474818] Loss: 22.901386908472364\n",
      "Iteracion: 6258 Gradiente: [0.014442367215927258,-0.24916981136692964] Loss: 22.90132456458402\n",
      "Iteracion: 6259 Gradiente: [0.01443468921020307,-0.24903734504699884] Loss: 22.901262286965895\n",
      "Iteracion: 6260 Gradiente: [0.014427015286329947,-0.24890494915023129] Loss: 22.90120007554756\n",
      "Iteracion: 6261 Gradiente: [0.014419345442134576,-0.24877262363918765] Loss: 22.901137930258642\n",
      "Iteracion: 6262 Gradiente: [0.014411679675529854,-0.24864036847644294] Loss: 22.901075851028857\n",
      "Iteracion: 6263 Gradiente: [0.014404017984261941,-0.24850818362460306] Loss: 22.90101383778796\n",
      "Iteracion: 6264 Gradiente: [0.014396360366109206,-0.24837606904629214] Loss: 22.900951890465826\n",
      "Iteracion: 6265 Gradiente: [0.014388706819135184,-0.2482440247041371] Loss: 22.900890008992388\n",
      "Iteracion: 6266 Gradiente: [0.014381057340924031,-0.24811205056081356] Loss: 22.900828193297617\n",
      "Iteracion: 6267 Gradiente: [0.014373411929316642,-0.247980146579001] Loss: 22.90076644331164\n",
      "Iteracion: 6268 Gradiente: [0.01436577058242771,-0.24784831272138383] Loss: 22.900704758964586\n",
      "Iteracion: 6269 Gradiente: [0.014358133297811075,-0.24771654895069684] Loss: 22.900643140186666\n",
      "Iteracion: 6270 Gradiente: [0.01435050007333795,-0.24758485522967913] Loss: 22.900581586908196\n",
      "Iteracion: 6271 Gradiente: [0.014342870907062396,-0.24745323152107837] Loss: 22.900520099059577\n",
      "Iteracion: 6272 Gradiente: [0.014335245796710675,-0.24732167778767847] Loss: 22.90045867657121\n",
      "Iteracion: 6273 Gradiente: [0.014327624740082949,-0.24719019399228362] Loss: 22.90039731937366\n",
      "Iteracion: 6274 Gradiente: [0.01432000773493011,-0.2470587800977165] Loss: 22.900336027397504\n",
      "Iteracion: 6275 Gradiente: [0.014312394779237063,-0.24692743606680773] Loss: 22.90027480057342\n",
      "Iteracion: 6276 Gradiente: [0.014304785870861755,-0.24679616186241482] Loss: 22.90021363883215\n",
      "Iteracion: 6277 Gradiente: [0.01429718100753424,-0.24666495744742414] Loss: 22.900152542104504\n",
      "Iteracion: 6278 Gradiente: [0.014289580187350263,-0.24653382278471767] Loss: 22.900091510321385\n",
      "Iteracion: 6279 Gradiente: [0.014281983407875032,-0.24640275783723004] Loss: 22.900030543413752\n",
      "Iteracion: 6280 Gradiente: [0.014274390667089657,-0.2462717625678909] Loss: 22.899969641312644\n",
      "Iteracion: 6281 Gradiente: [0.014266801962941144,-0.2461408369396517] Loss: 22.89990880394919\n",
      "Iteracion: 6282 Gradiente: [0.014259217293075228,-0.2460099809155018] Loss: 22.899848031254557\n",
      "Iteracion: 6283 Gradiente: [0.014251636655474915,-0.24587919445842832] Loss: 22.89978732316\n",
      "Iteracion: 6284 Gradiente: [0.014244060048006683,-0.24574847753144885] Loss: 22.899726679596856\n",
      "Iteracion: 6285 Gradiente: [0.014236487468535113,-0.24561783009759813] Loss: 22.899666100496535\n",
      "Iteracion: 6286 Gradiente: [0.014228918914810151,-0.24548725211993686] Loss: 22.899605585790514\n",
      "Iteracion: 6287 Gradiente: [0.014221354384663225,-0.2453567435615425] Loss: 22.899545135410335\n",
      "Iteracion: 6288 Gradiente: [0.014213793876230814,-0.2452263043854939] Loss: 22.89948474928764\n",
      "Iteracion: 6289 Gradiente: [0.014206237387010863,-0.24509593455492543] Loss: 22.8994244273541\n",
      "Iteracion: 6290 Gradiente: [0.014198684915210908,-0.2449656340329516] Loss: 22.89936416954149\n",
      "Iteracion: 6291 Gradiente: [0.014191136458534478,-0.24483540278273447] Loss: 22.899303975781653\n",
      "Iteracion: 6292 Gradiente: [0.014183592014702148,-0.24470524076745667] Loss: 22.899243846006513\n",
      "Iteracion: 6293 Gradiente: [0.01417605158186177,-0.24457514795029311] Loss: 22.899183780148043\n",
      "Iteracion: 6294 Gradiente: [0.01416851515771403,-0.24444512429446555] Loss: 22.899123778138303\n",
      "Iteracion: 6295 Gradiente: [0.014160982740143406,-0.24431516976320644] Loss: 22.899063839909434\n",
      "Iteracion: 6296 Gradiente: [0.014153454327026794,-0.24418528431976572] Loss: 22.89900396539361\n",
      "Iteracion: 6297 Gradiente: [0.014145929916282777,-0.244055467927411] Loss: 22.898944154523143\n",
      "Iteracion: 6298 Gradiente: [0.014138409505713411,-0.2439257205494368] Loss: 22.89888440723036\n",
      "Iteracion: 6299 Gradiente: [0.014130893093242019,-0.24379604214915082] Loss: 22.89882472344767\n",
      "Iteracion: 6300 Gradiente: [0.014123380676777705,-0.24366643268988022] Loss: 22.898765103107575\n",
      "Iteracion: 6301 Gradiente: [0.014115872254112104,-0.2435368921349775] Loss: 22.898705546142644\n",
      "Iteracion: 6302 Gradiente: [0.01410836782310696,-0.24340742044781294] Loss: 22.8986460524855\n",
      "Iteracion: 6303 Gradiente: [0.014100867381697905,-0.2432780175917717] Loss: 22.898586622068834\n",
      "Iteracion: 6304 Gradiente: [0.014093370927762786,-0.2431486835302594] Loss: 22.898527254825446\n",
      "Iteracion: 6305 Gradiente: [0.014085878459214503,-0.24301941822670112] Loss: 22.8984679506882\n",
      "Iteracion: 6306 Gradiente: [0.01407838997386932,-0.24289022164454888] Loss: 22.898408709589955\n",
      "Iteracion: 6307 Gradiente: [0.014070905469625927,-0.24276109374726443] Loss: 22.89834953146376\n",
      "Iteracion: 6308 Gradiente: [0.01406342494439059,-0.24263203449833312] Loss: 22.898290416242666\n",
      "Iteracion: 6309 Gradiente: [0.014055948396014628,-0.2425030438612609] Loss: 22.89823136385978\n",
      "Iteracion: 6310 Gradiente: [0.014048475822357886,-0.24237412179957415] Loss: 22.89817237424833\n",
      "Iteracion: 6311 Gradiente: [0.014041007221428951,-0.24224526827680665] Loss: 22.898113447341576\n",
      "Iteracion: 6312 Gradiente: [0.014033542591019455,-0.24211648325652851] Loss: 22.898054583072874\n",
      "Iteracion: 6313 Gradiente: [0.014026081929002506,-0.24198776670232233] Loss: 22.897995781375638\n",
      "Iteracion: 6314 Gradiente: [0.01401862523334311,-0.2418591185777854] Loss: 22.897937042183365\n",
      "Iteracion: 6315 Gradiente: [0.014011172501870799,-0.24173053884653986] Loss: 22.8978783654296\n",
      "Iteracion: 6316 Gradiente: [0.01400372373253731,-0.2416020274722256] Loss: 22.897819751047972\n",
      "Iteracion: 6317 Gradiente: [0.013996278923116279,-0.24147358441850741] Loss: 22.897761198972198\n",
      "Iteracion: 6318 Gradiente: [0.013988838071600184,-0.24134520964905745] Loss: 22.897702709136038\n",
      "Iteracion: 6319 Gradiente: [0.013981401175855505,-0.24121690312757713] Loss: 22.897644281473326\n",
      "Iteracion: 6320 Gradiente: [0.013973968233840613,-0.24108866481777808] Loss: 22.89758591591798\n",
      "Iteracion: 6321 Gradiente: [0.013966539243419144,-0.2409604946834] Loss: 22.897527612403977\n",
      "Iteracion: 6322 Gradiente: [0.013959114202486944,-0.24083239268819956] Loss: 22.897469370865362\n",
      "Iteracion: 6323 Gradiente: [0.013951693108889647,-0.24070435879595492] Loss: 22.897411191236287\n",
      "Iteracion: 6324 Gradiente: [0.013944275960613102,-0.240576392970454] Loss: 22.897353073450898\n",
      "Iteracion: 6325 Gradiente: [0.013936862755545575,-0.24044849517551312] Loss: 22.897295017443504\n",
      "Iteracion: 6326 Gradiente: [0.013929453491467332,-0.24032066537497046] Loss: 22.897237023148385\n",
      "Iteracion: 6327 Gradiente: [0.01392204816636422,-0.2401929035326757] Loss: 22.89717909049999\n",
      "Iteracion: 6328 Gradiente: [0.013914646778284615,-0.2400652096124902] Loss: 22.897121219432776\n",
      "Iteracion: 6329 Gradiente: [0.013907249324901727,-0.23993758357831893] Loss: 22.897063409881262\n",
      "Iteracion: 6330 Gradiente: [0.01389985580430751,-0.23981002539406002] Loss: 22.897005661780078\n",
      "Iteracion: 6331 Gradiente: [0.013892466214350445,-0.23968253502364595] Loss: 22.896947975063892\n",
      "Iteracion: 6332 Gradiente: [0.013885080552885636,-0.23955511243102912] Loss: 22.89689034966747\n",
      "Iteracion: 6333 Gradiente: [0.013877698817860088,-0.2394277575801734] Loss: 22.896832785525614\n",
      "Iteracion: 6334 Gradiente: [0.013870321007210388,-0.239300470435064] Loss: 22.89677528257322\n",
      "Iteracion: 6335 Gradiente: [0.01386294711875659,-0.23917325095971337] Loss: 22.896717840745247\n",
      "Iteracion: 6336 Gradiente: [0.013855577150592543,-0.2390460991181345] Loss: 22.896660459976715\n",
      "Iteracion: 6337 Gradiente: [0.013848211100511775,-0.23891901487437875] Loss: 22.896603140202718\n",
      "Iteracion: 6338 Gradiente: [0.013840848966397819,-0.2387919981925099] Loss: 22.896545881358424\n",
      "Iteracion: 6339 Gradiente: [0.01383349074621094,-0.2386650490366095] Loss: 22.89648868337907\n",
      "Iteracion: 6340 Gradiente: [0.013826136437917096,-0.2385381673707741] Loss: 22.896431546199953\n",
      "Iteracion: 6341 Gradiente: [0.01381878603939602,-0.23841135315912668] Loss: 22.896374469756434\n",
      "Iteracion: 6342 Gradiente: [0.013811439548562513,-0.23828460636580834] Loss: 22.89631745398397\n",
      "Iteracion: 6343 Gradiente: [0.013804096963427999,-0.23815792695497084] Loss: 22.89626049881807\n",
      "Iteracion: 6344 Gradiente: [0.013796758281718743,-0.2380313148908037] Loss: 22.89620360419428\n",
      "Iteracion: 6345 Gradiente: [0.013789423501612911,-0.23790477013748798] Loss: 22.896146770048283\n",
      "Iteracion: 6346 Gradiente: [0.013782092620729713,-0.23777829265925612] Loss: 22.896089996315762\n",
      "Iteracion: 6347 Gradiente: [0.013774765637285214,-0.23765188242032867] Loss: 22.89603328293252\n",
      "Iteracion: 6348 Gradiente: [0.013767442549066308,-0.2375255393849649] Loss: 22.895976629834415\n",
      "Iteracion: 6349 Gradiente: [0.013760123354049369,-0.23739926351743593] Loss: 22.895920036957325\n",
      "Iteracion: 6350 Gradiente: [0.013752808050018455,-0.23727305478204208] Loss: 22.895863504237276\n",
      "Iteracion: 6351 Gradiente: [0.013745496635190571,-0.237146913143075] Loss: 22.895807031610293\n",
      "Iteracion: 6352 Gradiente: [0.01373818910730904,-0.23702083856487743] Loss: 22.895750619012535\n",
      "Iteracion: 6353 Gradiente: [0.013730885464291494,-0.23689483101179826] Loss: 22.895694266380158\n",
      "Iteracion: 6354 Gradiente: [0.01372358570412473,-0.23676889044820026] Loss: 22.895637973649432\n",
      "Iteracion: 6355 Gradiente: [0.013716289824700804,-0.23664301683847513] Loss: 22.89558174075668\n",
      "Iteracion: 6356 Gradiente: [0.013708997824011249,-0.23651721014702218] Loss: 22.895525567638302\n",
      "Iteracion: 6357 Gradiente: [0.013701709699968015,-0.23639147033826818] Loss: 22.89546945423077\n",
      "Iteracion: 6358 Gradiente: [0.013694425450510531,-0.23626579737665712] Loss: 22.895413400470563\n",
      "Iteracion: 6359 Gradiente: [0.013687145073545064,-0.23614019122665178] Loss: 22.89535740629434\n",
      "Iteracion: 6360 Gradiente: [0.01367986856711146,-0.23601465185272766] Loss: 22.895301471638728\n",
      "Iteracion: 6361 Gradiente: [0.013672595929118831,-0.23588917921938696] Loss: 22.895245596440475\n",
      "Iteracion: 6362 Gradiente: [0.013665327157378707,-0.2357637732911551] Loss: 22.895189780636365\n",
      "Iteracion: 6363 Gradiente: [0.013658062250004831,-0.23563843403256043] Loss: 22.895134024163262\n",
      "Iteracion: 6364 Gradiente: [0.013650801204875052,-0.23551316140816353] Loss: 22.89507832695811\n",
      "Iteracion: 6365 Gradiente: [0.013643544019935424,-0.2353879553825389] Loss: 22.89502268895791\n",
      "Iteracion: 6366 Gradiente: [0.013636290693222955,-0.23526281592027515] Loss: 22.89496711009972\n",
      "Iteracion: 6367 Gradiente: [0.013629041222461068,-0.23513774298599893] Loss: 22.894911590320667\n",
      "Iteracion: 6368 Gradiente: [0.01362179560584688,-0.23501273654432828] Loss: 22.89485612955798\n",
      "Iteracion: 6369 Gradiente: [0.013614553841129388,-0.23488779655992384] Loss: 22.894800727748883\n",
      "Iteracion: 6370 Gradiente: [0.01360731592642234,-0.23476292299744633] Loss: 22.89474538483075\n",
      "Iteracion: 6371 Gradiente: [0.01360008185957137,-0.2346381158215916] Loss: 22.894690100740963\n",
      "Iteracion: 6372 Gradiente: [0.01359285163866275,-0.23451337499705527] Loss: 22.894634875416966\n",
      "Iteracion: 6373 Gradiente: [0.013585625261472955,-0.23438870048857594] Loss: 22.89457970879634\n",
      "Iteracion: 6374 Gradiente: [0.013578402726154574,-0.2342640922608858] Loss: 22.894524600816656\n",
      "Iteracion: 6375 Gradiente: [0.013571184030424396,-0.23413955027876174] Loss: 22.89446955141557\n",
      "Iteracion: 6376 Gradiente: [0.013563969172512695,-0.2340150745069702] Loss: 22.894414560530837\n",
      "Iteracion: 6377 Gradiente: [0.013556758150164683,-0.23389066491032498] Loss: 22.894359628100247\n",
      "Iteracion: 6378 Gradiente: [0.013549550961409787,-0.2337663214536396] Loss: 22.89430475406166\n",
      "Iteracion: 6379 Gradiente: [0.013542347604195962,-0.2336420441017547] Loss: 22.894249938353028\n",
      "Iteracion: 6380 Gradiente: [0.01353514807649295,-0.2335178328195249] Loss: 22.89419518091231\n",
      "Iteracion: 6381 Gradiente: [0.01352795237629607,-0.23339368757182502] Loss: 22.89414048167758\n",
      "Iteracion: 6382 Gradiente: [0.013520760501555174,-0.23326960832354876] Loss: 22.894085840587003\n",
      "Iteracion: 6383 Gradiente: [0.01351357245021063,-0.2331455950396118] Loss: 22.894031257578725\n",
      "Iteracion: 6384 Gradiente: [0.013506388220375243,-0.23302164768493558] Loss: 22.893976732591025\n",
      "Iteracion: 6385 Gradiente: [0.013499207809837798,-0.2328977662244815] Loss: 22.89392226556224\n",
      "Iteracion: 6386 Gradiente: [0.013492031216585095,-0.23277395062321585] Loss: 22.893867856430745\n",
      "Iteracion: 6387 Gradiente: [0.013484858438742246,-0.23265020084611882] Loss: 22.89381350513499\n",
      "Iteracion: 6388 Gradiente: [0.013477689474052567,-0.23252651685820755] Loss: 22.893759211613528\n",
      "Iteracion: 6389 Gradiente: [0.013470524320649702,-0.2324028986244978] Loss: 22.893704975804916\n",
      "Iteracion: 6390 Gradiente: [0.013463362976428547,-0.23227934611003828] Loss: 22.893650797647815\n",
      "Iteracion: 6391 Gradiente: [0.013456205439358845,-0.23215585927989021] Loss: 22.893596677080946\n",
      "Iteracion: 6392 Gradiente: [0.013449051707467182,-0.2320324380991309] Loss: 22.89354261404309\n",
      "Iteracion: 6393 Gradiente: [0.01344190177883604,-0.23190908253285253] Loss: 22.893488608473103\n",
      "Iteracion: 6394 Gradiente: [0.013434755651236212,-0.23178579254618442] Loss: 22.89343466030986\n",
      "Iteracion: 6395 Gradiente: [0.01342761332277007,-0.23166256810425534] Loss: 22.893380769492396\n",
      "Iteracion: 6396 Gradiente: [0.013420474791362835,-0.23153940917222388] Loss: 22.89332693595973\n",
      "Iteracion: 6397 Gradiente: [0.013413340055025931,-0.23141631571526097] Loss: 22.893273159650953\n",
      "Iteracion: 6398 Gradiente: [0.013406209111685522,-0.23129328769855975] Loss: 22.893219440505252\n",
      "Iteracion: 6399 Gradiente: [0.013399081959454407,-0.23117032508732424] Loss: 22.89316577846187\n",
      "Iteracion: 6400 Gradiente: [0.013391958596142218,-0.23104742784679289] Loss: 22.893112173460086\n",
      "Iteracion: 6401 Gradiente: [0.01338483901993944,-0.23092459594220144] Loss: 22.893058625439277\n",
      "Iteracion: 6402 Gradiente: [0.013377723228630126,-0.2308018293388263] Loss: 22.89300513433889\n",
      "Iteracion: 6403 Gradiente: [0.013370611220341288,-0.23067912800194154] Loss: 22.892951700098408\n",
      "Iteracion: 6404 Gradiente: [0.013363502993007615,-0.23055649189685426] Loss: 22.892898322657388\n",
      "Iteracion: 6405 Gradiente: [0.013356398544562846,-0.23043392098888874] Loss: 22.892845001955433\n",
      "Iteracion: 6406 Gradiente: [0.013349297873237258,-0.23031141524337062] Loss: 22.892791737932278\n",
      "Iteracion: 6407 Gradiente: [0.013342200976712586,-0.2301889746256742] Loss: 22.892738530527637\n",
      "Iteracion: 6408 Gradiente: [0.01333510785315563,-0.23006659910116506] Loss: 22.892685379681335\n",
      "Iteracion: 6409 Gradiente: [0.01332801850055887,-0.22994428863524044] Loss: 22.892632285333264\n",
      "Iteracion: 6410 Gradiente: [0.013320932916801097,-0.2298220431933172] Loss: 22.89257924742335\n",
      "Iteracion: 6411 Gradiente: [0.013313851099996062,-0.22969986274082146] Loss: 22.892526265891608\n",
      "Iteracion: 6412 Gradiente: [0.013306773048083188,-0.2295777472432056] Loss: 22.892473340678098\n",
      "Iteracion: 6413 Gradiente: [0.013299698759013268,-0.22945569666594068] Loss: 22.89242047172298\n",
      "Iteracion: 6414 Gradiente: [0.013292628230966367,-0.22933371097450259] Loss: 22.892367658966442\n",
      "Iteracion: 6415 Gradiente: [0.013285561461757803,-0.2292117901344067] Loss: 22.892314902348726\n",
      "Iteracion: 6416 Gradiente: [0.0132784984494909,-0.2290899341111686] Loss: 22.892262201810187\n",
      "Iteracion: 6417 Gradiente: [0.013271439192160983,-0.22896814287033276] Loss: 22.8922095572912\n",
      "Iteracion: 6418 Gradiente: [0.01326438368772737,-0.22884641637745975] Loss: 22.892156968732213\n",
      "Iteracion: 6419 Gradiente: [0.013257331934213804,-0.22872475459812616] Loss: 22.892104436073744\n",
      "Iteracion: 6420 Gradiente: [0.013250283929622242,-0.22860315749793092] Loss: 22.89205195925639\n",
      "Iteracion: 6421 Gradiente: [0.01324323967194232,-0.22848162504248723] Loss: 22.89199953822076\n",
      "Iteracion: 6422 Gradiente: [0.01323619915921957,-0.22836015719742672] Loss: 22.89194717290758\n",
      "Iteracion: 6423 Gradiente: [0.013229162389548795,-0.2282387539283943] Loss: 22.891894863257637\n",
      "Iteracion: 6424 Gradiente: [0.013222129360703624,-0.22811741520107442] Loss: 22.89184260921171\n",
      "Iteracion: 6425 Gradiente: [0.013215100070902963,-0.22799614098114265] Loss: 22.891790410710747\n",
      "Iteracion: 6426 Gradiente: [0.013208074518070134,-0.22787493123430913] Loss: 22.89173826769567\n",
      "Iteracion: 6427 Gradiente: [0.013201052700197616,-0.2277537859263005] Loss: 22.891686180107513\n",
      "Iteracion: 6428 Gradiente: [0.013194034615333786,-0.22763270502285604] Loss: 22.89163414788737\n",
      "Iteracion: 6429 Gradiente: [0.013187020261640706,-0.22751168848972808] Loss: 22.89158217097636\n",
      "Iteracion: 6430 Gradiente: [0.013180009636912852,-0.22739073629270892] Loss: 22.891530249315686\n",
      "Iteracion: 6431 Gradiente: [0.013173002739155967,-0.22726984839759581] Loss: 22.891478382846653\n",
      "Iteracion: 6432 Gradiente: [0.013165999566595588,-0.2271490247701922] Loss: 22.89142657151057\n",
      "Iteracion: 6433 Gradiente: [0.01315900011707735,-0.2270282653763394] Loss: 22.891374815248845\n",
      "Iteracion: 6434 Gradiente: [0.013152004388710262,-0.22690757018188623] Loss: 22.891323114002926\n",
      "Iteracion: 6435 Gradiente: [0.013145012379524701,-0.22678693915270073] Loss: 22.891271467714315\n",
      "Iteracion: 6436 Gradiente: [0.013138024087458197,-0.2266663722546749] Loss: 22.891219876324627\n",
      "Iteracion: 6437 Gradiente: [0.013131039510529756,-0.22654586945371663] Loss: 22.89116833977548\n",
      "Iteracion: 6438 Gradiente: [0.013124058646884389,-0.2264254307157432] Loss: 22.89111685800861\n",
      "Iteracion: 6439 Gradiente: [0.013117081494442573,-0.22630505600670064] Loss: 22.891065430965746\n",
      "Iteracion: 6440 Gradiente: [0.013110108051265949,-0.2261847452925494] Loss: 22.89101405858874\n",
      "Iteracion: 6441 Gradiente: [0.013103138315508052,-0.22606449853925928] Loss: 22.890962740819482\n",
      "Iteracion: 6442 Gradiente: [0.013096172284968096,-0.22594431571283985] Loss: 22.890911477599925\n",
      "Iteracion: 6443 Gradiente: [0.013089209957701086,-0.22582419677930604] Loss: 22.89086026887207\n",
      "Iteracion: 6444 Gradiente: [0.013082251331978038,-0.2257041417046749] Loss: 22.89080911457802\n",
      "Iteracion: 6445 Gradiente: [0.013075296405621847,-0.2255841504550085] Loss: 22.8907580146599\n",
      "Iteracion: 6446 Gradiente: [0.013068345176725416,-0.22546422299637356] Loss: 22.890706969059885\n",
      "Iteracion: 6447 Gradiente: [0.013061397643291646,-0.22534435929485816] Loss: 22.89065597772027\n",
      "Iteracion: 6448 Gradiente: [0.013054453803449443,-0.2252245593165617] Loss: 22.890605040583374\n",
      "Iteracion: 6449 Gradiente: [0.013047513655081389,-0.22510482302761545] Loss: 22.89055415759157\n",
      "Iteracion: 6450 Gradiente: [0.01304057719630786,-0.22498515039415695] Loss: 22.890503328687306\n",
      "Iteracion: 6451 Gradiente: [0.013033644425191445,-0.22486554138234105] Loss: 22.890452553813088\n",
      "Iteracion: 6452 Gradiente: [0.013026715339707569,-0.2247459959583504] Loss: 22.890401832911476\n",
      "Iteracion: 6453 Gradiente: [0.013019789937924504,-0.2246265140883776] Loss: 22.890351165925097\n",
      "Iteracion: 6454 Gradiente: [0.013012868217992945,-0.22450709573862965] Loss: 22.89030055279666\n",
      "Iteracion: 6455 Gradiente: [0.013005950177805896,-0.22438774087534247] Loss: 22.890249993468906\n",
      "Iteracion: 6456 Gradiente: [0.012999035815528259,-0.22426844946476182] Loss: 22.890199487884654\n",
      "Iteracion: 6457 Gradiente: [0.012992125129094726,-0.2241492214731591] Loss: 22.89014903598673\n",
      "Iteracion: 6458 Gradiente: [0.012985218116630412,-0.2240300568668142] Loss: 22.89009863771812\n",
      "Iteracion: 6459 Gradiente: [0.012978314776109794,-0.22391095561203353] Loss: 22.890048293021813\n",
      "Iteracion: 6460 Gradiente: [0.012971415105539563,-0.22379191767514056] Loss: 22.889998001840834\n",
      "Iteracion: 6461 Gradiente: [0.012964519103069469,-0.22367294302246812] Loss: 22.88994776411831\n",
      "Iteracion: 6462 Gradiente: [0.012957626766771568,-0.22355403162036916] Loss: 22.889897579797417\n",
      "Iteracion: 6463 Gradiente: [0.012950738094743505,-0.22343518343521646] Loss: 22.889847448821406\n",
      "Iteracion: 6464 Gradiente: [0.0129438530848347,-0.2233163984334131] Loss: 22.889797371133557\n",
      "Iteracion: 6465 Gradiente: [0.012936971735162691,-0.22319767658136508] Loss: 22.889747346677208\n",
      "Iteracion: 6466 Gradiente: [0.012930094043904697,-0.22307901784549433] Loss: 22.889697375395812\n",
      "Iteracion: 6467 Gradiente: [0.012923220008984989,-0.22296042219225295] Loss: 22.889647457232815\n",
      "Iteracion: 6468 Gradiente: [0.012916349628631944,-0.2228418895880934] Loss: 22.889597592131768\n",
      "Iteracion: 6469 Gradiente: [0.012909482900721515,-0.22272341999950823] Loss: 22.88954778003627\n",
      "Iteracion: 6470 Gradiente: [0.012902619823410077,-0.2226050133929905] Loss: 22.889498020889967\n",
      "Iteracion: 6471 Gradiente: [0.012895760394642745,-0.2224866697350638] Loss: 22.889448314636574\n",
      "Iteracion: 6472 Gradiente: [0.012888904612534209,-0.22236838899226033] Loss: 22.889398661219882\n",
      "Iteracion: 6473 Gradiente: [0.012882052475322325,-0.22225017113112172] Loss: 22.88934906058371\n",
      "Iteracion: 6474 Gradiente: [0.012875203980670828,-0.22213201611823966] Loss: 22.88929951267196\n",
      "Iteracion: 6475 Gradiente: [0.012868359127033576,-0.2220139239201817] Loss: 22.889250017428598\n",
      "Iteracion: 6476 Gradiente: [0.012861517912390734,-0.22189589450355693] Loss: 22.88920057479762\n",
      "Iteracion: 6477 Gradiente: [0.012854680334676042,-0.22177792783499856] Loss: 22.88915118472311\n",
      "Iteracion: 6478 Gradiente: [0.012847846392042091,-0.22166002388113953] Loss: 22.889101847149202\n",
      "Iteracion: 6479 Gradiente: [0.012841016082533466,-0.22154218260864328] Loss: 22.889052562020087\n",
      "Iteracion: 6480 Gradiente: [0.012834189404266757,-0.2214244039841816] Loss: 22.889003329280015\n",
      "Iteracion: 6481 Gradiente: [0.012827366355153913,-0.2213066879744589] Loss: 22.888954148873303\n",
      "Iteracion: 6482 Gradiente: [0.012820546933503844,-0.22118903454617347] Loss: 22.88890502074431\n",
      "Iteracion: 6483 Gradiente: [0.012813731137267343,-0.22107144366606163] Loss: 22.88885594483749\n",
      "Iteracion: 6484 Gradiente: [0.012806918964507948,-0.22095391530087152] Loss: 22.888806921097306\n",
      "Iteracion: 6485 Gradiente: [0.012800110413311926,-0.2208364494173671] Loss: 22.8887579494683\n",
      "Iteracion: 6486 Gradiente: [0.012793305481654709,-0.22071904598233835] Loss: 22.88870902989512\n",
      "Iteracion: 6487 Gradiente: [0.012786504167741934,-0.22060170496257772] Loss: 22.888660162322402\n",
      "Iteracion: 6488 Gradiente: [0.012779706469649455,-0.22048442632490245] Loss: 22.888611346694876\n",
      "Iteracion: 6489 Gradiente: [0.012772912385434173,-0.22036721003614967] Loss: 22.888562582957338\n",
      "Iteracion: 6490 Gradiente: [0.012766121913167202,-0.2202500560631742] Loss: 22.88851387105458\n",
      "Iteracion: 6491 Gradiente: [0.012759335050913024,-0.2201329643728472] Loss: 22.888465210931567\n",
      "Iteracion: 6492 Gradiente: [0.012752551796721908,-0.22001593493206] Loss: 22.88841660253322\n",
      "Iteracion: 6493 Gradiente: [0.012745772148745498,-0.2198989677077132] Loss: 22.88836804580459\n",
      "Iteracion: 6494 Gradiente: [0.012738996104982903,-0.21978206266673675] Loss: 22.88831954069071\n",
      "Iteracion: 6495 Gradiente: [0.012732223663727874,-0.21966521977606016] Loss: 22.888271087136737\n",
      "Iteracion: 6496 Gradiente: [0.012725454822744571,-0.21954843900265927] Loss: 22.88822268508787\n",
      "Iteracion: 6497 Gradiente: [0.01271868958032864,-0.21943172031350003] Loss: 22.888174334489353\n",
      "Iteracion: 6498 Gradiente: [0.012711927934588137,-0.21931506367557413] Loss: 22.888126035286486\n",
      "Iteracion: 6499 Gradiente: [0.012705169883596075,-0.21919846905589419] Loss: 22.888077787424645\n",
      "Iteracion: 6500 Gradiente: [0.01269841542529188,-0.21908193642150012] Loss: 22.888029590849282\n",
      "Iteracion: 6501 Gradiente: [0.012691664557806348,-0.21896546573943426] Loss: 22.887981445505822\n",
      "Iteracion: 6502 Gradiente: [0.012684917279425651,-0.21884905697674975] Loss: 22.88793335133985\n",
      "Iteracion: 6503 Gradiente: [0.012678173587984058,-0.2187327101005452] Loss: 22.88788530829695\n",
      "Iteracion: 6504 Gradiente: [0.012671433481735524,-0.21861642507790802] Loss: 22.887837316322816\n",
      "Iteracion: 6505 Gradiente: [0.012664696958730322,-0.21850020187595712] Loss: 22.887789375363106\n",
      "Iteracion: 6506 Gradiente: [0.012657964017159884,-0.21838404046182447] Loss: 22.887741485363616\n",
      "Iteracion: 6507 Gradiente: [0.012651234654913423,-0.21826794080267078] Loss: 22.88769364627018\n",
      "Iteracion: 6508 Gradiente: [0.01264450887024869,-0.21815190286565714] Loss: 22.887645858028687\n",
      "Iteracion: 6509 Gradiente: [0.012637786661241535,-0.21803592661797117] Loss: 22.88759812058506\n",
      "Iteracion: 6510 Gradiente: [0.012631068025998123,-0.21792001202681585] Loss: 22.887550433885345\n",
      "Iteracion: 6511 Gradiente: [0.012624352962503357,-0.21780415905942085] Loss: 22.887502797875573\n",
      "Iteracion: 6512 Gradiente: [0.012617641468911719,-0.217688367683022] Loss: 22.88745521250188\n",
      "Iteracion: 6513 Gradiente: [0.012610933543370114,-0.21757263786487044] Loss: 22.887407677710403\n",
      "Iteracion: 6514 Gradiente: [0.012604229184040605,-0.2174569695722384] Loss: 22.887360193447414\n",
      "Iteracion: 6515 Gradiente: [0.012597528388910935,-0.21734136277242452] Loss: 22.887312759659192\n",
      "Iteracion: 6516 Gradiente: [0.01259083115613464,-0.21722581743273348] Loss: 22.887265376292063\n",
      "Iteracion: 6517 Gradiente: [0.012584137483879468,-0.21711033352048784] Loss: 22.88721804329246\n",
      "Iteracion: 6518 Gradiente: [0.012577447370150215,-0.21699491100303614] Loss: 22.88717076060683\n",
      "Iteracion: 6519 Gradiente: [0.012570760813099468,-0.21687954984773736] Loss: 22.887123528181693\n",
      "Iteracion: 6520 Gradiente: [0.012564077810736762,-0.21676425002197514] Loss: 22.887076345963617\n",
      "Iteracion: 6521 Gradiente: [0.01255739836123837,-0.21664901149314086] Loss: 22.88702921389922\n",
      "Iteracion: 6522 Gradiente: [0.012550722462979518,-0.21653383422863173] Loss: 22.886982131935227\n",
      "Iteracion: 6523 Gradiente: [0.01254405011353299,-0.2164187181959098] Loss: 22.88693510001837\n",
      "Iteracion: 6524 Gradiente: [0.012537381311466334,-0.2163036633623952] Loss: 22.886888118095417\n",
      "Iteracion: 6525 Gradiente: [0.012530716054774871,-0.21618866969555933] Loss: 22.88684118611327\n",
      "Iteracion: 6526 Gradiente: [0.012524054341460556,-0.2160737371628912] Loss: 22.886794304018817\n",
      "Iteracion: 6527 Gradiente: [0.012517396169767873,-0.21595886573188244] Loss: 22.886747471759016\n",
      "Iteracion: 6528 Gradiente: [0.012510741537738568,-0.21584405537005377] Loss: 22.88670068928094\n",
      "Iteracion: 6529 Gradiente: [0.012504090443595335,-0.21572930604493218] Loss: 22.886653956531628\n",
      "Iteracion: 6530 Gradiente: [0.012497442885330657,-0.21561461772407617] Loss: 22.88660727345823\n",
      "Iteracion: 6531 Gradiente: [0.012490798861109435,-0.21549999037505335] Loss: 22.88656064000796\n",
      "Iteracion: 6532 Gradiente: [0.012484158369059629,-0.2153854239654476] Loss: 22.886514056128057\n",
      "Iteracion: 6533 Gradiente: [0.012477521407199295,-0.2152709184628672] Loss: 22.886467521765827\n",
      "Iteracion: 6534 Gradiente: [0.01247088797380324,-0.21515647383492353] Loss: 22.886421036868647\n",
      "Iteracion: 6535 Gradiente: [0.01246425806693973,-0.21504209004925662] Loss: 22.886374601383913\n",
      "Iteracion: 6536 Gradiente: [0.012457631684741462,-0.2149277670735221] Loss: 22.88632821525909\n",
      "Iteracion: 6537 Gradiente: [0.012451008825379025,-0.21481350487538695] Loss: 22.886281878441768\n",
      "Iteracion: 6538 Gradiente: [0.012444389486980375,-0.21469930342254126] Loss: 22.886235590879494\n",
      "Iteracion: 6539 Gradiente: [0.012437773667529465,-0.21458516268269925] Loss: 22.886189352519914\n",
      "Iteracion: 6540 Gradiente: [0.012431161365259413,-0.2144710826235772] Loss: 22.886143163310702\n",
      "Iteracion: 6541 Gradiente: [0.01242455257834744,-0.21435706321291345] Loss: 22.886097023199667\n",
      "Iteracion: 6542 Gradiente: [0.012417947304808764,-0.21424310441847158] Loss: 22.88605093213459\n",
      "Iteracion: 6543 Gradiente: [0.012411345542776075,-0.21412920620802645] Loss: 22.88600489006332\n",
      "Iteracion: 6544 Gradiente: [0.012404747290457863,-0.21401536854936484] Loss: 22.8859588969338\n",
      "Iteracion: 6545 Gradiente: [0.012398152546014292,-0.21390159141029474] Loss: 22.885912952694003\n",
      "Iteracion: 6546 Gradiente: [0.012391561307658587,-0.21378787475863584] Loss: 22.885867057291964\n",
      "Iteracion: 6547 Gradiente: [0.012384973573307433,-0.2136742185622474] Loss: 22.88582121067575\n",
      "Iteracion: 6548 Gradiente: [0.012378389341146582,-0.21356062278898308] Loss: 22.885775412793496\n",
      "Iteracion: 6549 Gradiente: [0.012371808609512414,-0.21344708740671028] Loss: 22.885729663593448\n",
      "Iteracion: 6550 Gradiente: [0.01236523137628467,-0.2133336123833382] Loss: 22.88568396302383\n",
      "Iteracion: 6551 Gradiente: [0.01235865763965857,-0.21322019768677478] Loss: 22.885638311032935\n",
      "Iteracion: 6552 Gradiente: [0.012352087397887127,-0.21310684328494284] Loss: 22.885592707569135\n",
      "Iteracion: 6553 Gradiente: [0.012345520649184512,-0.21299354914578275] Loss: 22.885547152580845\n",
      "Iteracion: 6554 Gradiente: [0.012338957391487307,-0.21288031523726816] Loss: 22.88550164601655\n",
      "Iteracion: 6555 Gradiente: [0.012332397623034315,-0.2127671415273755] Loss: 22.885456187824772\n",
      "Iteracion: 6556 Gradiente: [0.012325841341838858,-0.21265402798410757] Loss: 22.88541077795407\n",
      "Iteracion: 6557 Gradiente: [0.012319288546302687,-0.21254097457546284] Loss: 22.885365416353114\n",
      "Iteracion: 6558 Gradiente: [0.012312739234279018,-0.21242798126948917] Loss: 22.88532010297055\n",
      "Iteracion: 6559 Gradiente: [0.012306193404237812,-0.21231504803421591] Loss: 22.885274837755166\n",
      "Iteracion: 6560 Gradiente: [0.012299651054050287,-0.21220217483772263] Loss: 22.885229620655753\n",
      "Iteracion: 6561 Gradiente: [0.012293112182006401,-0.2120893616480829] Loss: 22.88518445162114\n",
      "Iteracion: 6562 Gradiente: [0.012286576786258744,-0.21197660843339772] Loss: 22.885139330600264\n",
      "Iteracion: 6563 Gradiente: [0.01228004486495422,-0.2118639151617798] Loss: 22.88509425754206\n",
      "Iteracion: 6564 Gradiente: [0.0122735164161071,-0.21175128180136962] Loss: 22.88504923239557\n",
      "Iteracion: 6565 Gradiente: [0.012266991438077449,-0.21163870832030748] Loss: 22.885004255109845\n",
      "Iteracion: 6566 Gradiente: [0.012260469928943963,-0.21152619468676218] Loss: 22.884959325634036\n",
      "Iteracion: 6567 Gradiente: [0.012253951886809016,-0.21141374086891923] Loss: 22.8849144439173\n",
      "Iteracion: 6568 Gradiente: [0.012247437309822355,-0.2113013468349808] Loss: 22.884869609908865\n",
      "Iteracion: 6569 Gradiente: [0.012240926196222782,-0.2111890125531578] Loss: 22.884824823558052\n",
      "Iteracion: 6570 Gradiente: [0.012234418544066254,-0.2110767379916903] Loss: 22.884780084814146\n",
      "Iteracion: 6571 Gradiente: [0.01222791435168252,-0.2109645231188208] Loss: 22.884735393626595\n",
      "Iteracion: 6572 Gradiente: [0.012221413617032795,-0.21085236790282808] Loss: 22.88469074994482\n",
      "Iteracion: 6573 Gradiente: [0.012214916338371988,-0.2107402723119898] Loss: 22.884646153718332\n",
      "Iteracion: 6574 Gradiente: [0.012208422513917109,-0.21062823631460575] Loss: 22.884601604896673\n",
      "Iteracion: 6575 Gradiente: [0.012201932141879486,-0.21051625987899064] Loss: 22.884557103429486\n",
      "Iteracion: 6576 Gradiente: [0.012195445220217493,-0.2104043429734928] Loss: 22.884512649266405\n",
      "Iteracion: 6577 Gradiente: [0.012188961747249512,-0.21029248556645327] Loss: 22.884468242357155\n",
      "Iteracion: 6578 Gradiente: [0.012182481720971812,-0.21018068762625244] Loss: 22.8844238826515\n",
      "Iteracion: 6579 Gradiente: [0.012176005139728356,-0.21006894912126614] Loss: 22.884379570099277\n",
      "Iteracion: 6580 Gradiente: [0.012169532001663205,-0.2099572700198967] Loss: 22.88433530465036\n",
      "Iteracion: 6581 Gradiente: [0.012163062304892,-0.20984565029056615] Loss: 22.884291086254663\n",
      "Iteracion: 6582 Gradiente: [0.012156596047645962,-0.20973408990170792] Loss: 22.884246914862192\n",
      "Iteracion: 6583 Gradiente: [0.012150133228061577,-0.2096225888217769] Loss: 22.88420279042297\n",
      "Iteracion: 6584 Gradiente: [0.012143673844323643,-0.20951114701924164] Loss: 22.8841587128871\n",
      "Iteracion: 6585 Gradiente: [0.012137217894549697,-0.20939976446259165] Loss: 22.884114682204704\n",
      "Iteracion: 6586 Gradiente: [0.012130765376999383,-0.20928844112032507] Loss: 22.88407069832597\n",
      "Iteracion: 6587 Gradiente: [0.012124316289687916,-0.2091771769609719] Loss: 22.88402676120119\n",
      "Iteracion: 6588 Gradiente: [0.012117870631026524,-0.209065971953054] Loss: 22.88398287078063\n",
      "Iteracion: 6589 Gradiente: [0.012111428399077796,-0.2089548260651308] Loss: 22.88393902701466\n",
      "Iteracion: 6590 Gradiente: [0.012104989591972527,-0.20884373926577665] Loss: 22.88389522985368\n",
      "Iteracion: 6591 Gradiente: [0.012098554207895518,-0.208732711523576] Loss: 22.88385147924815\n",
      "Iteracion: 6592 Gradiente: [0.012092122245174626,-0.20862174280712498] Loss: 22.883807775148586\n",
      "Iteracion: 6593 Gradiente: [0.012085693701845913,-0.20851083308505308] Loss: 22.88376411750554\n",
      "Iteracion: 6594 Gradiente: [0.012079268576042069,-0.2083999823259988] Loss: 22.883720506269665\n",
      "Iteracion: 6595 Gradiente: [0.012072846866108005,-0.2082891904986049] Loss: 22.88367694139159\n",
      "Iteracion: 6596 Gradiente: [0.012066428570075042,-0.20817845757155265] Loss: 22.883633422822058\n",
      "Iteracion: 6597 Gradiente: [0.01206001368624167,-0.20806778351352134] Loss: 22.883589950511844\n",
      "Iteracion: 6598 Gradiente: [0.012053602212859952,-0.20795716829321062] Loss: 22.883546524411774\n",
      "Iteracion: 6599 Gradiente: [0.012047194147940369,-0.20784661187934989] Loss: 22.883503144472733\n",
      "Iteracion: 6600 Gradiente: [0.012040789489707512,-0.20773611424067523] Loss: 22.883459810645622\n",
      "Iteracion: 6601 Gradiente: [0.012034388236457024,-0.20762567534593213] Loss: 22.883416522881472\n",
      "Iteracion: 6602 Gradiente: [0.012027990386320653,-0.20751529516389577] Loss: 22.883373281131277\n",
      "Iteracion: 6603 Gradiente: [0.01202159593741025,-0.20740497366335392] Loss: 22.883330085346156\n",
      "Iteracion: 6604 Gradiente: [0.012015204888001563,-0.20729471081310832] Loss: 22.883286935477223\n",
      "Iteracion: 6605 Gradiente: [0.012008817236338131,-0.20718450658197488] Loss: 22.883243831475713\n",
      "Iteracion: 6606 Gradiente: [0.012002432980446542,-0.2070743609387987] Loss: 22.883200773292813\n",
      "Iteracion: 6607 Gradiente: [0.011996052118643281,-0.20696427385242658] Loss: 22.88315776087986\n",
      "Iteracion: 6608 Gradiente: [0.011989674649187994,-0.20685424529172328] Loss: 22.88311479418818\n",
      "Iteracion: 6609 Gradiente: [0.011983300570118633,-0.20674427522558445] Loss: 22.88307187316919\n",
      "Iteracion: 6610 Gradiente: [0.011976929879668318,-0.20663436362291077] Loss: 22.883028997774325\n",
      "Iteracion: 6611 Gradiente: [0.01197056257615543,-0.2065245104526137] Loss: 22.882986167955092\n",
      "Iteracion: 6612 Gradiente: [0.011964198657679503,-0.20641471568363426] Loss: 22.88294338366305\n",
      "Iteracion: 6613 Gradiente: [0.011957838122412075,-0.20630497928492797] Loss: 22.882900644849816\n",
      "Iteracion: 6614 Gradiente: [0.011951480968575841,-0.2061953012254597] Loss: 22.88285795146701\n",
      "Iteracion: 6615 Gradiente: [0.011945127194433287,-0.20608568147421213] Loss: 22.882815303466362\n",
      "Iteracion: 6616 Gradiente: [0.011938776798148372,-0.20597612000019025] Loss: 22.88277270079966\n",
      "Iteracion: 6617 Gradiente: [0.01193242977799874,-0.20586661677240697] Loss: 22.882730143418666\n",
      "Iteracion: 6618 Gradiente: [0.011926086132028028,-0.20575717175990557] Loss: 22.882687631275267\n",
      "Iteracion: 6619 Gradiente: [0.011919745858546094,-0.20564778493173239] Loss: 22.88264516432138\n",
      "Iteracion: 6620 Gradiente: [0.011913408955801211,-0.20553845625695075] Loss: 22.882602742508954\n",
      "Iteracion: 6621 Gradiente: [0.011907075421919443,-0.20542918570465052] Loss: 22.882560365790017\n",
      "Iteracion: 6622 Gradiente: [0.011900745255155698,-0.20531997324392867] Loss: 22.88251803411663\n",
      "Iteracion: 6623 Gradiente: [0.011894418453647405,-0.20521081884390663] Loss: 22.882475747440925\n",
      "Iteracion: 6624 Gradiente: [0.0118880950156741,-0.20510172247371278] Loss: 22.882433505715042\n",
      "Iteracion: 6625 Gradiente: [0.011881774939484065,-0.20499268410249674] Loss: 22.88239130889123\n",
      "Iteracion: 6626 Gradiente: [0.011875458223200516,-0.20488370369942643] Loss: 22.882349156921727\n",
      "Iteracion: 6627 Gradiente: [0.011869144865077412,-0.20477478123368467] Loss: 22.882307049758893\n",
      "Iteracion: 6628 Gradiente: [0.011862834863296711,-0.20466591667447104] Loss: 22.882264987355075\n",
      "Iteracion: 6629 Gradiente: [0.011856528216203326,-0.20455710999099128] Loss: 22.88222296966269\n",
      "Iteracion: 6630 Gradiente: [0.011850224921948893,-0.20444836115248474] Loss: 22.882180996634222\n",
      "Iteracion: 6631 Gradiente: [0.011843924978609265,-0.20433967012820348] Loss: 22.88213906822219\n",
      "Iteracion: 6632 Gradiente: [0.011837628384553985,-0.2042310368874046] Loss: 22.882097184379173\n",
      "Iteracion: 6633 Gradiente: [0.011831335137937533,-0.2041224613993728] Loss: 22.882055345057793\n",
      "Iteracion: 6634 Gradiente: [0.011825045236995872,-0.2040139436334036] Loss: 22.882013550210704\n",
      "Iteracion: 6635 Gradiente: [0.0118187586800038,-0.20390548355880705] Loss: 22.881971799790676\n",
      "Iteracion: 6636 Gradiente: [0.01181247546516128,-0.20379708114491363] Loss: 22.881930093750448\n",
      "Iteracion: 6637 Gradiente: [0.011806195590604791,-0.20368873636107498] Loss: 22.881888432042853\n",
      "Iteracion: 6638 Gradiente: [0.01179991905461198,-0.20358044917664925] Loss: 22.881846814620765\n",
      "Iteracion: 6639 Gradiente: [0.01179364585546144,-0.20347221956101388] Loss: 22.881805241437114\n",
      "Iteracion: 6640 Gradiente: [0.011787375991325651,-0.20336404748356324] Loss: 22.881763712444883\n",
      "Iteracion: 6641 Gradiente: [0.011781109460412154,-0.2032559329137115] Loss: 22.881722227597084\n",
      "Iteracion: 6642 Gradiente: [0.011774846261012802,-0.20314787582088153] Loss: 22.881680786846797\n",
      "Iteracion: 6643 Gradiente: [0.011768586391328502,-0.2030398761745186] Loss: 22.881639390147136\n",
      "Iteracion: 6644 Gradiente: [0.011762329849562055,-0.20293193394408426] Loss: 22.881598037451308\n",
      "Iteracion: 6645 Gradiente: [0.011756076634041317,-0.20282404909904747] Loss: 22.881556728712507\n",
      "Iteracion: 6646 Gradiente: [0.011749826742892348,-0.20271622160890837] Loss: 22.88151546388401\n",
      "Iteracion: 6647 Gradiente: [0.011743580174225107,-0.20260845144318082] Loss: 22.881474242919158\n",
      "Iteracion: 6648 Gradiente: [0.011737336926534189,-0.20250073857137554] Loss: 22.881433065771315\n",
      "Iteracion: 6649 Gradiente: [0.011731096997978814,-0.20239308296303615] Loss: 22.88139193239391\n",
      "Iteracion: 6650 Gradiente: [0.011724860386690732,-0.20228548458772763] Loss: 22.881350842740407\n",
      "Iteracion: 6651 Gradiente: [0.011718627091054638,-0.2021779434150138] Loss: 22.88130979676432\n",
      "Iteracion: 6652 Gradiente: [0.011712397109114174,-0.20207045941449497] Loss: 22.88126879441926\n",
      "Iteracion: 6653 Gradiente: [0.01170617043936204,-0.2019630325557628] Loss: 22.881227835658795\n",
      "Iteracion: 6654 Gradiente: [0.011699947079803982,-0.20185566280845002] Loss: 22.88118692043664\n",
      "Iteracion: 6655 Gradiente: [0.011693727028815222,-0.20174835014218964] Loss: 22.8811460487065\n",
      "Iteracion: 6656 Gradiente: [0.011687510284563512,-0.2016410945266377] Loss: 22.881105220422132\n",
      "Iteracion: 6657 Gradiente: [0.011681296845440177,-0.20153389593145596] Loss: 22.881064435537358\n",
      "Iteracion: 6658 Gradiente: [0.01167508670947844,-0.20142675432634433] Loss: 22.88102369400605\n",
      "Iteracion: 6659 Gradiente: [0.011668879875041208,-0.2013196696809968] Loss: 22.88098299578213\n",
      "Iteracion: 6660 Gradiente: [0.01166267634033223,-0.2012126419651338] Loss: 22.880942340819562\n",
      "Iteracion: 6661 Gradiente: [0.011656476103649045,-0.20110567114848976] Loss: 22.880901729072367\n",
      "Iteracion: 6662 Gradiente: [0.011650279163142348,-0.2009987572008171] Loss: 22.880861160494575\n",
      "Iteracion: 6663 Gradiente: [0.011644085517055676,-0.2008919000918837] Loss: 22.88082063504033\n",
      "Iteracion: 6664 Gradiente: [0.011637895163732991,-0.20078509979146897] Loss: 22.880780152663764\n",
      "Iteracion: 6665 Gradiente: [0.011631708101444361,-0.20067835626936842] Loss: 22.880739713319137\n",
      "Iteracion: 6666 Gradiente: [0.01162552432845132,-0.20057166949539712] Loss: 22.880699316960655\n",
      "Iteracion: 6667 Gradiente: [0.011619343842824984,-0.20046503943939578] Loss: 22.88065896354265\n",
      "Iteracion: 6668 Gradiente: [0.011613166642969,-0.2003584660712034] Loss: 22.88061865301945\n",
      "Iteracion: 6669 Gradiente: [0.011606992727138277,-0.2002519493606846] Loss: 22.88057838534551\n",
      "Iteracion: 6670 Gradiente: [0.011600822093569718,-0.20014548927771555] Loss: 22.880538160475222\n",
      "Iteracion: 6671 Gradiente: [0.011594654740390335,-0.2000390857922021] Loss: 22.88049797836313\n",
      "Iteracion: 6672 Gradiente: [0.011588490666080512,-0.19993273887403984] Loss: 22.88045783896378\n",
      "Iteracion: 6673 Gradiente: [0.011582329868760628,-0.19982644849316422] Loss: 22.880417742231753\n",
      "Iteracion: 6674 Gradiente: [0.011576172346640117,-0.1997202146195227] Loss: 22.880377688121698\n",
      "Iteracion: 6675 Gradiente: [0.011570018098079041,-0.19961403722306745] Loss: 22.880337676588308\n",
      "Iteracion: 6676 Gradiente: [0.01156386712136926,-0.199507916273771] Loss: 22.880297707586344\n",
      "Iteracion: 6677 Gradiente: [0.011557719414662415,-0.19940185174163064] Loss: 22.880257781070554\n",
      "Iteracion: 6678 Gradiente: [0.011551574976283518,-0.19929584359665153] Loss: 22.880217896995816\n",
      "Iteracion: 6679 Gradiente: [0.011545433804431581,-0.19918989180885732] Loss: 22.880178055317007\n",
      "Iteracion: 6680 Gradiente: [0.011539295897470462,-0.1990839963482833] Loss: 22.880138255989046\n",
      "Iteracion: 6681 Gradiente: [0.01153316125353759,-0.1989781571849908] Loss: 22.880098498966937\n",
      "Iteracion: 6682 Gradiente: [0.01152702987101198,-0.19887237428904345] Loss: 22.880058784205684\n",
      "Iteracion: 6683 Gradiente: [0.011520901748087908,-0.1987666476305332] Loss: 22.880019111660385\n",
      "Iteracion: 6684 Gradiente: [0.011514776883052491,-0.19866097717955913] Loss: 22.87997948128617\n",
      "Iteracion: 6685 Gradiente: [0.011508655274216533,-0.19855536290623862] Loss: 22.87993989303819\n",
      "Iteracion: 6686 Gradiente: [0.011502536919901255,-0.19844980478070334] Loss: 22.879900346871683\n",
      "Iteracion: 6687 Gradiente: [0.011496421818258303,-0.19834430277310844] Loss: 22.87986084274189\n",
      "Iteracion: 6688 Gradiente: [0.011490309967493317,-0.19823885685362513] Loss: 22.879821380604174\n",
      "Iteracion: 6689 Gradiente: [0.011484201366000472,-0.19813346699242848] Loss: 22.879781960413858\n",
      "Iteracion: 6690 Gradiente: [0.011478096012029936,-0.19802813315971618] Loss: 22.87974258212637\n",
      "Iteracion: 6691 Gradiente: [0.01147199390385083,-0.19792285532570386] Loss: 22.87970324569716\n",
      "Iteracion: 6692 Gradiente: [0.01146589503975027,-0.19781763346061798] Loss: 22.879663951081756\n",
      "Iteracion: 6693 Gradiente: [0.011459799418101586,-0.19771246753469865] Loss: 22.879624698235673\n",
      "Iteracion: 6694 Gradiente: [0.011453707036885892,-0.1976073575182261] Loss: 22.879585487114547\n",
      "Iteracion: 6695 Gradiente: [0.011447617894787262,-0.19750230338144933] Loss: 22.879546317674006\n",
      "Iteracion: 6696 Gradiente: [0.011441531989666487,-0.19739730509468956] Loss: 22.879507189869734\n",
      "Iteracion: 6697 Gradiente: [0.011435449320085429,-0.19729236262823616] Loss: 22.879468103657498\n",
      "Iteracion: 6698 Gradiente: [0.011429369884215628,-0.19718747595242098] Loss: 22.879429058993075\n",
      "Iteracion: 6699 Gradiente: [0.011423293680308196,-0.19708264503758421] Loss: 22.879390055832314\n",
      "Iteracion: 6700 Gradiente: [0.011417220706740257,-0.19697786985407836] Loss: 22.879351094131074\n",
      "Iteracion: 6701 Gradiente: [0.011411150961833982,-0.19687315037227127] Loss: 22.87931217384529\n",
      "Iteracion: 6702 Gradiente: [0.011405084443626378,-0.19676848656256365] Loss: 22.879273294930947\n",
      "Iteracion: 6703 Gradiente: [0.011399021150740889,-0.19666387839534044] Loss: 22.879234457344058\n",
      "Iteracion: 6704 Gradiente: [0.011392961081162412,-0.196559325841037] Loss: 22.8791956610407\n",
      "Iteracion: 6705 Gradiente: [0.011386904233334386,-0.19645482887007865] Loss: 22.87915690597699\n",
      "Iteracion: 6706 Gradiente: [0.01138085060558088,-0.19635038745291322] Loss: 22.87911819210911\n",
      "Iteracion: 6707 Gradiente: [0.011374800196093323,-0.1962460015600134] Loss: 22.879079519393215\n",
      "Iteracion: 6708 Gradiente: [0.011368753003075464,-0.1961416711618656] Loss: 22.879040887785624\n",
      "Iteracion: 6709 Gradiente: [0.011362709025082534,-0.19603739622894903] Loss: 22.87900229724259\n",
      "Iteracion: 6710 Gradiente: [0.011356668260195118,-0.19593317673179164] Loss: 22.878963747720498\n",
      "Iteracion: 6711 Gradiente: [0.01135063070676381,-0.19582901264091912] Loss: 22.87892523917573\n",
      "Iteracion: 6712 Gradiente: [0.011344596363068149,-0.19572490392687314] Loss: 22.878886771564712\n",
      "Iteracion: 6713 Gradiente: [0.011338565227423677,-0.19562085056021652] Loss: 22.878848344843963\n",
      "Iteracion: 6714 Gradiente: [0.011332537298132668,-0.19551685251152182] Loss: 22.878809958969985\n",
      "Iteracion: 6715 Gradiente: [0.011326512573385608,-0.1954129097513879] Loss: 22.878771613899392\n",
      "Iteracion: 6716 Gradiente: [0.0113204910516032,-0.1953090222504129] Loss: 22.878733309588785\n",
      "Iteracion: 6717 Gradiente: [0.011314472731136978,-0.19520518997921693] Loss: 22.87869504599485\n",
      "Iteracion: 6718 Gradiente: [0.011308457610074167,-0.19510141290845115] Loss: 22.878656823074305\n",
      "Iteracion: 6719 Gradiente: [0.01130244568687715,-0.1949976910087591] Loss: 22.878618640783923\n",
      "Iteracion: 6720 Gradiente: [0.011296436959879468,-0.19489402425080762] Loss: 22.87858049908048\n",
      "Iteracion: 6721 Gradiente: [0.011290431427257393,-0.19479041260528962] Loss: 22.878542397920864\n",
      "Iteracion: 6722 Gradiente: [0.01128442908730752,-0.19468685604290528] Loss: 22.878504337261983\n",
      "Iteracion: 6723 Gradiente: [0.01127842993842781,-0.19458335453436662] Loss: 22.878466317060766\n",
      "Iteracion: 6724 Gradiente: [0.01127243397891012,-0.19447990805040416] Loss: 22.878428337274205\n",
      "Iteracion: 6725 Gradiente: [0.011266441207032093,-0.19437651656176888] Loss: 22.87839039785938\n",
      "Iteracion: 6726 Gradiente: [0.011260451621049583,-0.19427318003922356] Loss: 22.87835249877333\n",
      "Iteracion: 6727 Gradiente: [0.011254465219265816,-0.19416989845354968] Loss: 22.878314639973198\n",
      "Iteracion: 6728 Gradiente: [0.011248482000124228,-0.19406667177553144] Loss: 22.878276821416176\n",
      "Iteracion: 6729 Gradiente: [0.011242501961864567,-0.19396349997598297] Loss: 22.878239043059466\n",
      "Iteracion: 6730 Gradiente: [0.011236525102783427,-0.19386038302572892] Loss: 22.87820130486035\n",
      "Iteracion: 6731 Gradiente: [0.011230551421120557,-0.1937573208956137] Loss: 22.87816360677614\n",
      "Iteracion: 6732 Gradiente: [0.011224580915282445,-0.19365431355649024] Loss: 22.8781259487642\n",
      "Iteracion: 6733 Gradiente: [0.011218613583543894,-0.19355136097923034] Loss: 22.878088330781917\n",
      "Iteracion: 6734 Gradiente: [0.011212649424257393,-0.19344846313471736] Loss: 22.87805075278674\n",
      "Iteracion: 6735 Gradiente: [0.011206688435602056,-0.19334561999386407] Loss: 22.8780132147362\n",
      "Iteracion: 6736 Gradiente: [0.01120073061597111,-0.19324283152757998] Loss: 22.877975716587795\n",
      "Iteracion: 6737 Gradiente: [0.011194775963823152,-0.19314009770679555] Loss: 22.877938258299125\n",
      "Iteracion: 6738 Gradiente: [0.011188824477305086,-0.1930374185024643] Loss: 22.877900839827824\n",
      "Iteracion: 6739 Gradiente: [0.011182876154767504,-0.19293479388554977] Loss: 22.877863461131557\n",
      "Iteracion: 6740 Gradiente: [0.011176930994486156,-0.19283222382703707] Loss: 22.877826122168063\n",
      "Iteracion: 6741 Gradiente: [0.01117098899491964,-0.19272970829790925] Loss: 22.877788822895077\n",
      "Iteracion: 6742 Gradiente: [0.011165050154273597,-0.19262724726918634] Loss: 22.877751563270444\n",
      "Iteracion: 6743 Gradiente: [0.011159114470953569,-0.19252484071188672] Loss: 22.87771434325198\n",
      "Iteracion: 6744 Gradiente: [0.011153181943170883,-0.1924224885970606] Loss: 22.87767716279762\n",
      "Iteracion: 6745 Gradiente: [0.011147252569311187,-0.19232019089575883] Loss: 22.877640021865282\n",
      "Iteracion: 6746 Gradiente: [0.011141326347732653,-0.19221794757905297] Loss: 22.877602920412983\n",
      "Iteracion: 6747 Gradiente: [0.01113540327663903,-0.19211575861803767] Loss: 22.87756585839873\n",
      "Iteracion: 6748 Gradiente: [0.01112948335448228,-0.19201362398380664] Loss: 22.877528835780602\n",
      "Iteracion: 6749 Gradiente: [0.011123566579491732,-0.191911543647486] Loss: 22.877491852516755\n",
      "Iteracion: 6750 Gradiente: [0.011117652950048296,-0.1918095175802049] Loss: 22.877454908565305\n",
      "Iteracion: 6751 Gradiente: [0.011111742464529091,-0.19170754575310767] Loss: 22.877418003884504\n",
      "Iteracion: 6752 Gradiente: [0.011105835121099025,-0.1916056281373726] Loss: 22.877381138432597\n",
      "Iteracion: 6753 Gradiente: [0.011099930918229006,-0.1915037647041667] Loss: 22.87734431216787\n",
      "Iteracion: 6754 Gradiente: [0.011094029854274368,-0.19140195542468585] Loss: 22.8773075250487\n",
      "Iteracion: 6755 Gradiente: [0.011088131927436962,-0.1913002002701468] Loss: 22.877270777033445\n",
      "Iteracion: 6756 Gradiente: [0.011082237136136541,-0.19119849921176904] Loss: 22.877234068080547\n",
      "Iteracion: 6757 Gradiente: [0.01107634547868012,-0.19109685222079817] Loss: 22.877197398148493\n",
      "Iteracion: 6758 Gradiente: [0.011070456953550927,-0.1909952592684783] Loss: 22.87716076719579\n",
      "Iteracion: 6759 Gradiente: [0.011064571558785019,-0.19089372032609914] Loss: 22.87712417518102\n",
      "Iteracion: 6760 Gradiente: [0.011058689292953734,-0.19079223536493425] Loss: 22.877087622062774\n",
      "Iteracion: 6761 Gradiente: [0.011052810154239031,-0.1906908043562937] Loss: 22.877051107799723\n",
      "Iteracion: 6762 Gradiente: [0.011046934141107082,-0.1905894272714889] Loss: 22.87701463235055\n",
      "Iteracion: 6763 Gradiente: [0.011041061251893325,-0.19048810408185055] Loss: 22.876978195674024\n",
      "Iteracion: 6764 Gradiente: [0.011035191484753189,-0.1903868347587385] Loss: 22.87694179772889\n",
      "Iteracion: 6765 Gradiente: [0.011029324838204957,-0.19028561927350437] Loss: 22.87690543847401\n",
      "Iteracion: 6766 Gradiente: [0.011023461310640907,-0.19018445759752195] Loss: 22.876869117868246\n",
      "Iteracion: 6767 Gradiente: [0.01101760090023447,-0.19008334970219706] Loss: 22.876832835870506\n",
      "Iteracion: 6768 Gradiente: [0.011011743605417715,-0.18998229555893253] Loss: 22.876796592439767\n",
      "Iteracion: 6769 Gradiente: [0.011005889424503342,-0.1898812951391516] Loss: 22.876760387535015\n",
      "Iteracion: 6770 Gradiente: [0.011000038355866574,-0.18978034841429406] Loss: 22.87672422111532\n",
      "Iteracion: 6771 Gradiente: [0.01099419039779453,-0.1896794553558171] Loss: 22.87668809313975\n",
      "Iteracion: 6772 Gradiente: [0.010988345548742966,-0.18957861593518147] Loss: 22.876652003567457\n",
      "Iteracion: 6773 Gradiente: [0.01098250380693268,-0.18947783012388192] Loss: 22.876615952357614\n",
      "Iteracion: 6774 Gradiente: [0.010976665170892375,-0.18937709789340573] Loss: 22.876579939469444\n",
      "Iteracion: 6775 Gradiente: [0.010970829638762326,-0.18927641921528046] Loss: 22.87654396486221\n",
      "Iteracion: 6776 Gradiente: [0.010964997208993547,-0.18917579406102997] Loss: 22.876508028495216\n",
      "Iteracion: 6777 Gradiente: [0.010959167879820104,-0.18907522240220598] Loss: 22.87647213032779\n",
      "Iteracion: 6778 Gradiente: [0.010953341649756492,-0.18897470421035992] Loss: 22.876436270319388\n",
      "Iteracion: 6779 Gradiente: [0.010947518517186457,-0.188874239457065] Loss: 22.876400448429408\n",
      "Iteracion: 6780 Gradiente: [0.010941698480276802,-0.18877382811392115] Loss: 22.876364664617338\n",
      "Iteracion: 6781 Gradiente: [0.010935881537496547,-0.18867347015252847] Loss: 22.876328918842695\n",
      "Iteracion: 6782 Gradiente: [0.01093006768722565,-0.18857316554450657] Loss: 22.876293211065036\n",
      "Iteracion: 6783 Gradiente: [0.010924256927790072,-0.18847291426149143] Loss: 22.876257541244005\n",
      "Iteracion: 6784 Gradiente: [0.010918449257512937,-0.18837271627513713] Loss: 22.876221909339225\n",
      "Iteracion: 6785 Gradiente: [0.010912644674686096,-0.18827257155711277] Loss: 22.87618631531042\n",
      "Iteracion: 6786 Gradiente: [0.010906843177838254,-0.18817248007908927] Loss: 22.876150759117287\n",
      "Iteracion: 6787 Gradiente: [0.010901044765174107,-0.18807244181277222] Loss: 22.876115240719653\n",
      "Iteracion: 6788 Gradiente: [0.010895249435228039,-0.1879724567298627] Loss: 22.87607976007732\n",
      "Iteracion: 6789 Gradiente: [0.010889457186162113,-0.187872524802097] Loss: 22.876044317150146\n",
      "Iteracion: 6790 Gradiente: [0.010883668016443455,-0.18777264600121177] Loss: 22.876008911898047\n",
      "Iteracion: 6791 Gradiente: [0.010877881924492764,-0.18767282029895885] Loss: 22.875973544280995\n",
      "Iteracion: 6792 Gradiente: [0.010872098908564946,-0.18757304766711455] Loss: 22.87593821425895\n",
      "Iteracion: 6793 Gradiente: [0.010866318967096807,-0.18747332807746322] Loss: 22.87590292179198\n",
      "Iteracion: 6794 Gradiente: [0.010860542098427572,-0.18737366150180676] Loss: 22.875867666840154\n",
      "Iteracion: 6795 Gradiente: [0.010854768300805517,-0.18727404791196597] Loss: 22.87583244936359\n",
      "Iteracion: 6796 Gradiente: [0.010848997572755556,-0.1871744872797656] Loss: 22.875797269322447\n",
      "Iteracion: 6797 Gradiente: [0.010843229912700281,-0.18707497957704694] Loss: 22.875762126676957\n",
      "Iteracion: 6798 Gradiente: [0.010837465318898391,-0.18697552477567983] Loss: 22.875727021387338\n",
      "Iteracion: 6799 Gradiente: [0.010831703789602899,-0.18687612284754435] Loss: 22.875691953413913\n",
      "Iteracion: 6800 Gradiente: [0.01082594532340977,-0.18677677376451834] Loss: 22.875656922716985\n",
      "Iteracion: 6801 Gradiente: [0.01082018991862128,-0.18667747749851363] Loss: 22.875621929256933\n",
      "Iteracion: 6802 Gradiente: [0.010814437573522658,-0.18657823402145557] Loss: 22.87558697299422\n",
      "Iteracion: 6803 Gradiente: [0.01080868828659239,-0.1864790433052728] Loss: 22.87555205388925\n",
      "Iteracion: 6804 Gradiente: [0.010802942056122334,-0.1863799053219219] Loss: 22.875517171902533\n",
      "Iteracion: 6805 Gradiente: [0.010797198880559715,-0.18628082004336405] Loss: 22.875482326994657\n",
      "Iteracion: 6806 Gradiente: [0.010791458758217232,-0.1861817874415827] Loss: 22.875447519126162\n",
      "Iteracion: 6807 Gradiente: [0.010785721687553481,-0.1860828074885692] Loss: 22.875412748257688\n",
      "Iteracion: 6808 Gradiente: [0.010779987666734315,-0.18598388015634756] Loss: 22.875378014349906\n",
      "Iteracion: 6809 Gradiente: [0.010774256694404964,-0.1858850054169255] Loss: 22.87534331736356\n",
      "Iteracion: 6810 Gradiente: [0.010768528768818442,-0.18578618324235094] Loss: 22.87530865725934\n",
      "Iteracion: 6811 Gradiente: [0.010762803888358501,-0.18568741360467744] Loss: 22.875274033998085\n",
      "Iteracion: 6812 Gradiente: [0.01075708205149984,-0.18558869647597015] Loss: 22.87523944754061\n",
      "Iteracion: 6813 Gradiente: [0.010751363256476527,-0.18549003182832327] Loss: 22.875204897847816\n",
      "Iteracion: 6814 Gradiente: [0.010745647501723472,-0.18539141963383313] Loss: 22.875170384880626\n",
      "Iteracion: 6815 Gradiente: [0.010739934785672745,-0.1852928598646096] Loss: 22.875135908599976\n",
      "Iteracion: 6816 Gradiente: [0.010734225106658831,-0.18519435249278626] Loss: 22.875101468966868\n",
      "Iteracion: 6817 Gradiente: [0.010728518463202856,-0.1850958974904991] Loss: 22.87506706594238\n",
      "Iteracion: 6818 Gradiente: [0.010722814853499093,-0.1849974948299168] Loss: 22.87503269948755\n",
      "Iteracion: 6819 Gradiente: [0.010717114275942663,-0.18489914448321337] Loss: 22.87499836956355\n",
      "Iteracion: 6820 Gradiente: [0.010711416728948582,-0.18480084642257577] Loss: 22.874964076131523\n",
      "Iteracion: 6821 Gradiente: [0.010705722211099555,-0.18470260062019553] Loss: 22.874929819152683\n",
      "Iteracion: 6822 Gradiente: [0.010700030720550065,-0.1846044070483046] Loss: 22.8748955985883\n",
      "Iteracion: 6823 Gradiente: [0.010694342255748286,-0.18450626567913073] Loss: 22.874861414399643\n",
      "Iteracion: 6824 Gradiente: [0.010688656815169869,-0.18440817648491833] Loss: 22.874827266548046\n",
      "Iteracion: 6825 Gradiente: [0.010682974397176774,-0.18431013943792973] Loss: 22.874793154994897\n",
      "Iteracion: 6826 Gradiente: [0.010677295000102541,-0.1842121545104464] Loss: 22.874759079701587\n",
      "Iteracion: 6827 Gradiente: [0.010671618622262713,-0.18411422167476513] Loss: 22.8747250406296\n",
      "Iteracion: 6828 Gradiente: [0.010665945262219149,-0.18401634090318217] Loss: 22.87469103774043\n",
      "Iteracion: 6829 Gradiente: [0.010660274918363181,-0.18391851216801894] Loss: 22.87465707099561\n",
      "Iteracion: 6830 Gradiente: [0.010654607589034033,-0.1838207354416132] Loss: 22.874623140356718\n",
      "Iteracion: 6831 Gradiente: [0.010648943272582301,-0.1837230106963189] Loss: 22.874589245785366\n",
      "Iteracion: 6832 Gradiente: [0.010643281967512052,-0.18362533790449617] Loss: 22.87455538724322\n",
      "Iteracion: 6833 Gradiente: [0.010637623672061143,-0.18352771703853415] Loss: 22.874521564691996\n",
      "Iteracion: 6834 Gradiente: [0.010631968384759223,-0.18343014807081856] Loss: 22.874487778093414\n",
      "Iteracion: 6835 Gradiente: [0.010626316104061099,-0.1833326309737566] Loss: 22.87445402740928\n",
      "Iteracion: 6836 Gradiente: [0.010620666828196098,-0.1832351657197826] Loss: 22.874420312601394\n",
      "Iteracion: 6837 Gradiente: [0.010615020555767766,-0.1831377522813233] Loss: 22.87438663363165\n",
      "Iteracion: 6838 Gradiente: [0.010609377285035748,-0.18304039063083952] Loss: 22.87435299046193\n",
      "Iteracion: 6839 Gradiente: [0.01060373701437527,-0.18294308074080082] Loss: 22.87431938305418\n",
      "Iteracion: 6840 Gradiente: [0.010598099742295138,-0.1828458225836851] Loss: 22.87428581137039\n",
      "Iteracion: 6841 Gradiente: [0.010592465467234054,-0.1827486161319861] Loss: 22.87425227537258\n",
      "Iteracion: 6842 Gradiente: [0.01058683418744503,-0.18265146135822583] Loss: 22.874218775022836\n",
      "Iteracion: 6843 Gradiente: [0.010581205901389504,-0.18255435823492655] Loss: 22.874185310283245\n",
      "Iteracion: 6844 Gradiente: [0.0105755806074986,-0.18245730673462907] Loss: 22.87415188111598\n",
      "Iteracion: 6845 Gradiente: [0.010569958304311436,-0.18236030682988086] Loss: 22.874118487483187\n",
      "Iteracion: 6846 Gradiente: [0.010564338990010923,-0.18226335849326425] Loss: 22.874085129347115\n",
      "Iteracion: 6847 Gradiente: [0.01055872266310871,-0.1821664616973597] Loss: 22.874051806670046\n",
      "Iteracion: 6848 Gradiente: [0.010553109322052024,-0.18206961641476577] Loss: 22.87401851941427\n",
      "Iteracion: 6849 Gradiente: [0.01054749896528051,-0.1819728226180922] Loss: 22.873985267542146\n",
      "Iteracion: 6850 Gradiente: [0.010541891591028236,-0.18187608027997773] Loss: 22.873952051016037\n",
      "Iteracion: 6851 Gradiente: [0.010536287197894012,-0.18177938937305763] Loss: 22.87391886979842\n",
      "Iteracion: 6852 Gradiente: [0.010530685784280536,-0.181682749869987] Loss: 22.873885723851703\n",
      "Iteracion: 6853 Gradiente: [0.010525087348454084,-0.18158616174344822] Loss: 22.873852613138446\n",
      "Iteracion: 6854 Gradiente: [0.010519491888897885,-0.18148962496612456] Loss: 22.873819537621163\n",
      "Iteracion: 6855 Gradiente: [0.010513899404117902,-0.18139313951071165] Loss: 22.87378649726244\n",
      "Iteracion: 6856 Gradiente: [0.010508309892438205,-0.18129670534992984] Loss: 22.87375349202495\n",
      "Iteracion: 6857 Gradiente: [0.010502723352395075,-0.1812003224565044] Loss: 22.873720521871302\n",
      "Iteracion: 6858 Gradiente: [0.010497139782301209,-0.1811039908031842] Loss: 22.873687586764227\n",
      "Iteracion: 6859 Gradiente: [0.010491559180521411,-0.1810077103627337] Loss: 22.873654686666473\n",
      "Iteracion: 6860 Gradiente: [0.010485981545737861,-0.18091148110790733] Loss: 22.87362182154082\n",
      "Iteracion: 6861 Gradiente: [0.010480406875999885,-0.18081530301152107] Loss: 22.8735889913501\n",
      "Iteracion: 6862 Gradiente: [0.010474835170087241,-0.1807191760463533] Loss: 22.873556196057187\n",
      "Iteracion: 6863 Gradiente: [0.010469266426154415,-0.18062310018523628] Loss: 22.873523435624964\n",
      "Iteracion: 6864 Gradiente: [0.010463700642714003,-0.18052707540099816] Loss: 22.87349071001638\n",
      "Iteracion: 6865 Gradiente: [0.010458137818312707,-0.18043110166647633] Loss: 22.873458019194434\n",
      "Iteracion: 6866 Gradiente: [0.010452577951206384,-0.18033517895454285] Loss: 22.87342536312213\n",
      "Iteracion: 6867 Gradiente: [0.010447021039955947,-0.1802393072380628] Loss: 22.87339274176254\n",
      "Iteracion: 6868 Gradiente: [0.010441467082879777,-0.18014348648993408] Loss: 22.873360155078768\n",
      "Iteracion: 6869 Gradiente: [0.010435916078467737,-0.18004771668305466] Loss: 22.87332760303394\n",
      "Iteracion: 6870 Gradiente: [0.010430368025157576,-0.17995199779034413] Loss: 22.873295085591234\n",
      "Iteracion: 6871 Gradiente: [0.010424822921403157,-0.1798563297847321] Loss: 22.873262602713904\n",
      "Iteracion: 6872 Gradiente: [0.010419280765596757,-0.17976071263916835] Loss: 22.87323015436516\n",
      "Iteracion: 6873 Gradiente: [0.01041374155610318,-0.1796651463266187] Loss: 22.873197740508335\n",
      "Iteracion: 6874 Gradiente: [0.010408205291391444,-0.17956963082005567] Loss: 22.87316536110675\n",
      "Iteracion: 6875 Gradiente: [0.010402671969974147,-0.17947416609246383] Loss: 22.873133016123788\n",
      "Iteracion: 6876 Gradiente: [0.010397141590217037,-0.17937875211685453] Loss: 22.87310070552284\n",
      "Iteracion: 6877 Gradiente: [0.010391614150702822,-0.1792833888662365] Loss: 22.873068429267374\n",
      "Iteracion: 6878 Gradiente: [0.010386089649570825,-0.1791880763136581] Loss: 22.873036187320903\n",
      "Iteracion: 6879 Gradiente: [0.010380568085479543,-0.1790928144321569] Loss: 22.873003979646914\n",
      "Iteracion: 6880 Gradiente: [0.010375049456841149,-0.17899760319479358] Loss: 22.872971806209005\n",
      "Iteracion: 6881 Gradiente: [0.01036953376215024,-0.17890244257464286] Loss: 22.872939666970765\n",
      "Iteracion: 6882 Gradiente: [0.010364020999709093,-0.17880733254480236] Loss: 22.872907561895875\n",
      "Iteracion: 6883 Gradiente: [0.010358511168048305,-0.17871227307837229] Loss: 22.872875490948\n",
      "Iteracion: 6884 Gradiente: [0.010353004265524154,-0.17861726414847526] Loss: 22.872843454090837\n",
      "Iteracion: 6885 Gradiente: [0.01034750029072787,-0.17852230572823505] Loss: 22.8728114512882\n",
      "Iteracion: 6886 Gradiente: [0.010341999241927625,-0.1784273977908131] Loss: 22.872779482503844\n",
      "Iteracion: 6887 Gradiente: [0.010336501117641698,-0.17833254030936346] Loss: 22.872747547701618\n",
      "Iteracion: 6888 Gradiente: [0.0103310059164329,-0.1782377332570557] Loss: 22.872715646845432\n",
      "Iteracion: 6889 Gradiente: [0.01032551363653719,-0.17814297660709394] Loss: 22.872683779899162\n",
      "Iteracion: 6890 Gradiente: [0.010320024276541061,-0.178048270332674] Loss: 22.872651946826796\n",
      "Iteracion: 6891 Gradiente: [0.010314537834940058,-0.17795361440701107] Loss: 22.87262014759229\n",
      "Iteracion: 6892 Gradiente: [0.010309054309976773,-0.17785900880335118] Loss: 22.87258838215971\n",
      "Iteracion: 6893 Gradiente: [0.010303573700324857,-0.1777644534949277] Loss: 22.8725566504931\n",
      "Iteracion: 6894 Gradiente: [0.01029809600428564,-0.17766994845501158] Loss: 22.87252495255659\n",
      "Iteracion: 6895 Gradiente: [0.010292621220325296,-0.17757549365687833] Loss: 22.872493288314292\n",
      "Iteracion: 6896 Gradiente: [0.010287149346874002,-0.1774810890738171] Loss: 22.872461657730423\n",
      "Iteracion: 6897 Gradiente: [0.010281680382575095,-0.17738673467912183] Loss: 22.87243006076919\n",
      "Iteracion: 6898 Gradiente: [0.010276214325647478,-0.17729243044612558] Loss: 22.872398497394858\n",
      "Iteracion: 6899 Gradiente: [0.010270751174719333,-0.1771981763481487] Loss: 22.872366967571736\n",
      "Iteracion: 6900 Gradiente: [0.01026529092818104,-0.1771039723585428] Loss: 22.872335471264122\n",
      "Iteracion: 6901 Gradiente: [0.010259833584458988,-0.1770098184506716] Loss: 22.872304008436437\n",
      "Iteracion: 6902 Gradiente: [0.010254379141935032,-0.17691571459791267] Loss: 22.87227257905305\n",
      "Iteracion: 6903 Gradiente: [0.010248927599178614,-0.17682166077364864] Loss: 22.87224118307844\n",
      "Iteracion: 6904 Gradiente: [0.010243478954655908,-0.1767276569512846] Loss: 22.872209820477078\n",
      "Iteracion: 6905 Gradiente: [0.010238033206865301,-0.17663370310423307] Loss: 22.872178491213486\n",
      "Iteracion: 6906 Gradiente: [0.01023259035424265,-0.17653979920592822] Loss: 22.87214719525226\n",
      "Iteracion: 6907 Gradiente: [0.010227150395152762,-0.17644594522982202] Loss: 22.872115932557968\n",
      "Iteracion: 6908 Gradiente: [0.010221713328053283,-0.1763521411493728] Loss: 22.872084703095254\n",
      "Iteracion: 6909 Gradiente: [0.010216279151469129,-0.17625838693805124] Loss: 22.872053506828788\n",
      "Iteracion: 6910 Gradiente: [0.010210847863896788,-0.17616468256934467] Loss: 22.872022343723298\n",
      "Iteracion: 6911 Gradiente: [0.010205419463756964,-0.17607102801675806] Loss: 22.871991213743524\n",
      "Iteracion: 6912 Gradiente: [0.010199993949518671,-0.17597742325380505] Loss: 22.87196011685426\n",
      "Iteracion: 6913 Gradiente: [0.010194571319656613,-0.17588386825401728] Loss: 22.871929053020335\n",
      "Iteracion: 6914 Gradiente: [0.010189151572654964,-0.17579036299093917] Loss: 22.871898022206604\n",
      "Iteracion: 6915 Gradiente: [0.010183734706890846,-0.17569690743813193] Loss: 22.87186702437794\n",
      "Iteracion: 6916 Gradiente: [0.010178320720947908,-0.17560350156916063] Loss: 22.871836059499344\n",
      "Iteracion: 6917 Gradiente: [0.010172909613244959,-0.17551014535761786] Loss: 22.871805127535755\n",
      "Iteracion: 6918 Gradiente: [0.010167501382240592,-0.1754168387771023] Loss: 22.871774228452157\n",
      "Iteracion: 6919 Gradiente: [0.010162096026448353,-0.1753235818012295] Loss: 22.871743362213667\n",
      "Iteracion: 6920 Gradiente: [0.010156693544235889,-0.17523037440363018] Loss: 22.871712528785302\n",
      "Iteracion: 6921 Gradiente: [0.010151293934241797,-0.17513721655793854] Loss: 22.871681728132227\n",
      "Iteracion: 6922 Gradiente: [0.010145897194700144,-0.17504410823782662] Loss: 22.871650960219597\n",
      "Iteracion: 6923 Gradiente: [0.01014050332435564,-0.17495104941694753] Loss: 22.871620225012595\n",
      "Iteracion: 6924 Gradiente: [0.010135112321477398,-0.17485804006900002] Loss: 22.871589522476466\n",
      "Iteracion: 6925 Gradiente: [0.010129724184727707,-0.17476508016766998] Loss: 22.871558852576484\n",
      "Iteracion: 6926 Gradiente: [0.010124338912385155,-0.17467216968668248] Loss: 22.871528215277955\n",
      "Iteracion: 6927 Gradiente: [0.010118956503052345,-0.17457930859975654] Loss: 22.87149761054623\n",
      "Iteracion: 6928 Gradiente: [0.01011357695516608,-0.17448649688063445] Loss: 22.87146703834669\n",
      "Iteracion: 6929 Gradiente: [0.010108200267221907,-0.17439373450307089] Loss: 22.871436498644737\n",
      "Iteracion: 6930 Gradiente: [0.010102826437634842,-0.17430102144083826] Loss: 22.87140599140583\n",
      "Iteracion: 6931 Gradiente: [0.010097455465016007,-0.17420835766771128] Loss: 22.871375516595485\n",
      "Iteracion: 6932 Gradiente: [0.010092087347754842,-0.17411574315749037] Loss: 22.871345074179196\n",
      "Iteracion: 6933 Gradiente: [0.010086722084308993,-0.17402317788398894] Loss: 22.871314664122576\n",
      "Iteracion: 6934 Gradiente: [0.010081359673290536,-0.17393066182102276] Loss: 22.87128428639118\n",
      "Iteracion: 6935 Gradiente: [0.0100760001130728,-0.17383819494243571] Loss: 22.871253940950663\n",
      "Iteracion: 6936 Gradiente: [0.010070643402149434,-0.17374577722208132] Loss: 22.87122362776673\n",
      "Iteracion: 6937 Gradiente: [0.01006528953898472,-0.17365340863382564] Loss: 22.871193346805054\n",
      "Iteracion: 6938 Gradiente: [0.010059938522104517,-0.17356108915154542] Loss: 22.87116309803139\n",
      "Iteracion: 6939 Gradiente: [0.010054590350013845,-0.17346881874913567] Loss: 22.871132881411548\n",
      "Iteracion: 6940 Gradiente: [0.010049245021088874,-0.1733765974005087] Loss: 22.871102696911308\n",
      "Iteracion: 6941 Gradiente: [0.010043902534090422,-0.17328442507956984] Loss: 22.871072544496563\n",
      "Iteracion: 6942 Gradiente: [0.010038562887152125,-0.1731923017602766] Loss: 22.87104242413319\n",
      "Iteracion: 6943 Gradiente: [0.010033226078997851,-0.17310022741656325] Loss: 22.87101233578713\n",
      "Iteracion: 6944 Gradiente: [0.010027892108027459,-0.17300820202239972] Loss: 22.870982279424332\n",
      "Iteracion: 6945 Gradiente: [0.01002256097277249,-0.1729162255517598] Loss: 22.870952255010813\n",
      "Iteracion: 6946 Gradiente: [0.010017232671732284,-0.17282429797863313] Loss: 22.87092226251262\n",
      "Iteracion: 6947 Gradiente: [0.010011907203362588,-0.17273241927702757] Loss: 22.870892301895815\n",
      "Iteracion: 6948 Gradiente: [0.010006584566177897,-0.17264058942095892] Loss: 22.870862373126492\n",
      "Iteracion: 6949 Gradiente: [0.010001264758752388,-0.17254880838445671] Loss: 22.870832476170836\n",
      "Iteracion: 6950 Gradiente: [0.009995947779364656,-0.17245707614157776] Loss: 22.870802610994996\n",
      "Iteracion: 6951 Gradiente: [0.009990633626730035,-0.17236539266637022] Loss: 22.870772777565193\n",
      "Iteracion: 6952 Gradiente: [0.009985322299186805,-0.17227375793291663] Loss: 22.87074297584771\n",
      "Iteracion: 6953 Gradiente: [0.009980013795356513,-0.17218217191529822] Loss: 22.87071320580882\n",
      "Iteracion: 6954 Gradiente: [0.009974708113656068,-0.17209063458761792] Loss: 22.870683467414832\n",
      "Iteracion: 6955 Gradiente: [0.009969405252657755,-0.17199914592398974] Loss: 22.87065376063214\n",
      "Iteracion: 6956 Gradiente: [0.009964105210795538,-0.17190770589854493] Loss: 22.870624085427117\n",
      "Iteracion: 6957 Gradiente: [0.009958807986735489,-0.17181631448541582] Loss: 22.87059444176622\n",
      "Iteracion: 6958 Gradiente: [0.009953513578656726,-0.17172497165877823] Loss: 22.8705648296159\n",
      "Iteracion: 6959 Gradiente: [0.00994822198535606,-0.17163367739278462] Loss: 22.87053524894266\n",
      "Iteracion: 6960 Gradiente: [0.00994293320522009,-0.17154243166162644] Loss: 22.870505699713046\n",
      "Iteracion: 6961 Gradiente: [0.00993764723672257,-0.17145123443950053] Loss: 22.870476181893654\n",
      "Iteracion: 6962 Gradiente: [0.009932364078429146,-0.1713600857006168] Loss: 22.87044669545106\n",
      "Iteracion: 6963 Gradiente: [0.009927083728746311,-0.17126898541920543] Loss: 22.87041724035193\n",
      "Iteracion: 6964 Gradiente: [0.009921806186283296,-0.1711779335694986] Loss: 22.870387816562946\n",
      "Iteracion: 6965 Gradiente: [0.009916531449639858,-0.17108693012574214] Loss: 22.870358424050828\n",
      "Iteracion: 6966 Gradiente: [0.00991125951715901,-0.17099597506221376] Loss: 22.87032906278232\n",
      "Iteracion: 6967 Gradiente: [0.009905990387413037,-0.1709050683531873] Loss: 22.870299732724217\n",
      "Iteracion: 6968 Gradiente: [0.009900724058863376,-0.1708142099729592] Loss: 22.87027043384334\n",
      "Iteracion: 6969 Gradiente: [0.009895460530068097,-0.17072339989583513] Loss: 22.870241166106567\n",
      "Iteracion: 6970 Gradiente: [0.009890199799413798,-0.17063263809614285] Loss: 22.870211929480764\n",
      "Iteracion: 6971 Gradiente: [0.009884941865644236,-0.1705419245482015] Loss: 22.870182723932874\n",
      "Iteracion: 6972 Gradiente: [0.00987968672710527,-0.17045125922636875] Loss: 22.870153549429855\n",
      "Iteracion: 6973 Gradiente: [0.009874434382354972,-0.17036064210500482] Loss: 22.870124405938732\n",
      "Iteracion: 6974 Gradiente: [0.009869184829981729,-0.17027007315848028] Loss: 22.870095293426505\n",
      "Iteracion: 6975 Gradiente: [0.00986393806836645,-0.17017955236119053] Loss: 22.870066211860262\n",
      "Iteracion: 6976 Gradiente: [0.009858694096153423,-0.1700890796875323] Loss: 22.870037161207115\n",
      "Iteracion: 6977 Gradiente: [0.009853452911752925,-0.1699986551119262] Loss: 22.87000814143419\n",
      "Iteracion: 6978 Gradiente: [0.009848214513723027,-0.1699082786088012] Loss: 22.869979152508662\n",
      "Iteracion: 6979 Gradiente: [0.009842978900626539,-0.16981795015259707] Loss: 22.86995019439775\n",
      "Iteracion: 6980 Gradiente: [0.009837746070974162,-0.16972766971776992] Loss: 22.869921267068687\n",
      "Iteracion: 6981 Gradiente: [0.009832516023190388,-0.16963743727879776] Loss: 22.86989237048878\n",
      "Iteracion: 6982 Gradiente: [0.009827288755854131,-0.1695472528101593] Loss: 22.869863504625297\n",
      "Iteracion: 6983 Gradiente: [0.00982206426747704,-0.1694571162863544] Loss: 22.869834669445638\n",
      "Iteracion: 6984 Gradiente: [0.00981684255657361,-0.1693670276818935] Loss: 22.869805864917158\n",
      "Iteracion: 6985 Gradiente: [0.00981162362180612,-0.16927698697129365] Loss: 22.869777091007276\n",
      "Iteracion: 6986 Gradiente: [0.009806407461512852,-0.1691869941291042] Loss: 22.869748347683444\n",
      "Iteracion: 6987 Gradiente: [0.009801194074220613,-0.16909704912987458] Loss: 22.869719634913164\n",
      "Iteracion: 6988 Gradiente: [0.00979598345865232,-0.16900715194816093] Loss: 22.869690952663927\n",
      "Iteracion: 6989 Gradiente: [0.009790775613189832,-0.16891730255854873] Loss: 22.869662300903332\n",
      "Iteracion: 6990 Gradiente: [0.009785570536329639,-0.16882750093563387] Loss: 22.869633679598948\n",
      "Iteracion: 6991 Gradiente: [0.009780368226681919,-0.16873774705401512] Loss: 22.869605088718398\n",
      "Iteracion: 6992 Gradiente: [0.009775168682705272,-0.16864804088831822] Loss: 22.86957652822936\n",
      "Iteracion: 6993 Gradiente: [0.009769971903021238,-0.16855838241316773] Loss: 22.86954799809949\n",
      "Iteracion: 6994 Gradiente: [0.009764777886040103,-0.1684687716032195] Loss: 22.86951949829656\n",
      "Iteracion: 6995 Gradiente: [0.009759586630372989,-0.1683792084331266] Loss: 22.869491028788318\n",
      "Iteracion: 6996 Gradiente: [0.009754398134559021,-0.1682896928775629] Loss: 22.869462589542557\n",
      "Iteracion: 6997 Gradiente: [0.009749212397135427,-0.16820022491121508] Loss: 22.869434180527104\n",
      "Iteracion: 6998 Gradiente: [0.0097440294165087,-0.16811080450878996] Loss: 22.869405801709828\n",
      "Iteracion: 6999 Gradiente: [0.009738849191472809,-0.1680214316449869] Loss: 22.86937745305864\n",
      "Iteracion: 7000 Gradiente: [0.009733671720222029,-0.16793210629455166] Loss: 22.869349134541455\n",
      "Iteracion: 7001 Gradiente: [0.00972849700159107,-0.16784282843220796] Loss: 22.869320846126257\n",
      "Iteracion: 7002 Gradiente: [0.009723325034006318,-0.16775359803271558] Loss: 22.86929258778105\n",
      "Iteracion: 7003 Gradiente: [0.009718155815968998,-0.16766441507084348] Loss: 22.86926435947386\n",
      "Iteracion: 7004 Gradiente: [0.00971298934601066,-0.16757527952137513] Loss: 22.869236161172758\n",
      "Iteracion: 7005 Gradiente: [0.009707825622756634,-0.16748619135909604] Loss: 22.86920799284586\n",
      "Iteracion: 7006 Gradiente: [0.009702664644647523,-0.16739715055882212] Loss: 22.869179854461287\n",
      "Iteracion: 7007 Gradiente: [0.009697506410326658,-0.1673081570953678] Loss: 22.869151745987224\n",
      "Iteracion: 7008 Gradiente: [0.009692350918247902,-0.1672192109435714] Loss: 22.86912366739186\n",
      "Iteracion: 7009 Gradiente: [0.009687198166984483,-0.16713031207828083] Loss: 22.869095618643463\n",
      "Iteracion: 7010 Gradiente: [0.00968204815514279,-0.16704146047435178] Loss: 22.869067599710277\n",
      "Iteracion: 7011 Gradiente: [0.009676900881080997,-0.16695265610667037] Loss: 22.86903961056063\n",
      "Iteracion: 7012 Gradiente: [0.00967175634355518,-0.16686389895011117] Loss: 22.869011651162854\n",
      "Iteracion: 7013 Gradiente: [0.009666614541018248,-0.16677518897958035] Loss: 22.86898372148532\n",
      "Iteracion: 7014 Gradiente: [0.009661475472003644,-0.16668652616999444] Loss: 22.868955821496435\n",
      "Iteracion: 7015 Gradiente: [0.009656339135057123,-0.16659791049628106] Loss: 22.868927951164665\n",
      "Iteracion: 7016 Gradiente: [0.009651205528822022,-0.16650934193337516] Loss: 22.86890011045845\n",
      "Iteracion: 7017 Gradiente: [0.009646074651754096,-0.1664208204562359] Loss: 22.868872299346304\n",
      "Iteracion: 7018 Gradiente: [0.00964094650239152,-0.16633234603983357] Loss: 22.8688445177968\n",
      "Iteracion: 7019 Gradiente: [0.009635821079287628,-0.1662439186591475] Loss: 22.868816765778483\n",
      "Iteracion: 7020 Gradiente: [0.009630698381028917,-0.16615553828917104] Loss: 22.86878904325998\n",
      "Iteracion: 7021 Gradiente: [0.009625578406182929,-0.16606720490490925] Loss: 22.868761350209915\n",
      "Iteracion: 7022 Gradiente: [0.009620461153285001,-0.16597891848138613] Loss: 22.868733686596972\n",
      "Iteracion: 7023 Gradiente: [0.009615346620798467,-0.16589067899364038] Loss: 22.86870605238989\n",
      "Iteracion: 7024 Gradiente: [0.009610234807383714,-0.1658024864167132] Loss: 22.86867844755734\n",
      "Iteracion: 7025 Gradiente: [0.009605125711583657,-0.1657143407256657] Loss: 22.868650872068176\n",
      "Iteracion: 7026 Gradiente: [0.009600019331881527,-0.16562624189557612] Loss: 22.868623325891146\n",
      "Iteracion: 7027 Gradiente: [0.009594915666916866,-0.16553818990152797] Loss: 22.868595808995135\n",
      "Iteracion: 7028 Gradiente: [0.00958981471526575,-0.1654501847186181] Loss: 22.868568321348995\n",
      "Iteracion: 7029 Gradiente: [0.00958471647544646,-0.16536222632196562] Loss: 22.86854086292163\n",
      "Iteracion: 7030 Gradiente: [0.00957962094599812,-0.16527431468669593] Loss: 22.868513433682004\n",
      "Iteracion: 7031 Gradiente: [0.00957452812538501,-0.16518644978795685] Loss: 22.868486033599048\n",
      "Iteracion: 7032 Gradiente: [0.009569438012346155,-0.16509863160089028] Loss: 22.868458662641846\n",
      "Iteracion: 7033 Gradiente: [0.0095643506053212,-0.16501086010067034] Loss: 22.868431320779354\n",
      "Iteracion: 7034 Gradiente: [0.00955926590303496,-0.16492313526246724] Loss: 22.8684040079807\n",
      "Iteracion: 7035 Gradiente: [0.009554183903824764,-0.16483545706148692] Loss: 22.868376724214958\n",
      "Iteracion: 7036 Gradiente: [0.009549104606409741,-0.16474782547292696] Loss: 22.868349469451296\n",
      "Iteracion: 7037 Gradiente: [0.009544028009305332,-0.16466024047201025] Loss: 22.868322243658845\n",
      "Iteracion: 7038 Gradiente: [0.009538954111083816,-0.16457270203396715] Loss: 22.868295046806846\n",
      "Iteracion: 7039 Gradiente: [0.009533882910249265,-0.16448521013404818] Loss: 22.868267878864515\n",
      "Iteracion: 7040 Gradiente: [0.009528814405435545,-0.16439776474750734] Loss: 22.868240739801134\n",
      "Iteracion: 7041 Gradiente: [0.00952374859516567,-0.16431036584962075] Loss: 22.868213629586016\n",
      "Iteracion: 7042 Gradiente: [0.009518685478184352,-0.1642230134156602] Loss: 22.868186548188458\n",
      "Iteracion: 7043 Gradiente: [0.009513625052729442,-0.16413570742094746] Loss: 22.868159495577864\n",
      "Iteracion: 7044 Gradiente: [0.009508567317704811,-0.16404844784077188] Loss: 22.868132471723627\n",
      "Iteracion: 7045 Gradiente: [0.009503512271403261,-0.1639612346504741] Loss: 22.868105476595154\n",
      "Iteracion: 7046 Gradiente: [0.009498459912588448,-0.16387406782538214] Loss: 22.868078510161933\n",
      "Iteracion: 7047 Gradiente: [0.009493410239742654,-0.1637869473408512] Loss: 22.868051572393473\n",
      "Iteracion: 7048 Gradiente: [0.009488363251490266,-0.16369987317224108] Loss: 22.868024663259273\n",
      "Iteracion: 7049 Gradiente: [0.009483318946306933,-0.16361284529493575] Loss: 22.867997782728914\n",
      "Iteracion: 7050 Gradiente: [0.009478277322885257,-0.1635258636843189] Loss: 22.867970930772003\n",
      "Iteracion: 7051 Gradiente: [0.009473238379717941,-0.16343892831579712] Loss: 22.867944107358134\n",
      "Iteracion: 7052 Gradiente: [0.0094682021154559,-0.16335203916478255] Loss: 22.86791731245698\n",
      "Iteracion: 7053 Gradiente: [0.00946316852857289,-0.16326519620671137] Loss: 22.867890546038254\n",
      "Iteracion: 7054 Gradiente: [0.009458137617699928,-0.16317839941702156] Loss: 22.867863808071647\n",
      "Iteracion: 7055 Gradiente: [0.009453109381383721,-0.1630916487711725] Loss: 22.86783709852694\n",
      "Iteracion: 7056 Gradiente: [0.009448083818355713,-0.16300494424462228] Loss: 22.867810417373914\n",
      "Iteracion: 7057 Gradiente: [0.009443060926951337,-0.1629182858128651] Loss: 22.867783764582374\n",
      "Iteracion: 7058 Gradiente: [0.00943804070589162,-0.1628316734513896] Loss: 22.867757140122194\n",
      "Iteracion: 7059 Gradiente: [0.00943302315377726,-0.1627451071357015] Loss: 22.86773054396326\n",
      "Iteracion: 7060 Gradiente: [0.009428008269162546,-0.16265858684132403] Loss: 22.867703976075465\n",
      "Iteracion: 7061 Gradiente: [0.009422996050563863,-0.16257211254379259] Loss: 22.86767743642877\n",
      "Iteracion: 7062 Gradiente: [0.009417986496615072,-0.16248568421865256] Loss: 22.86765092499316\n",
      "Iteracion: 7063 Gradiente: [0.009412979605854351,-0.16239930184146492] Loss: 22.867624441738634\n",
      "Iteracion: 7064 Gradiente: [0.009407975377049146,-0.16231296538779355] Loss: 22.867597986635253\n",
      "Iteracion: 7065 Gradiente: [0.009402973808592681,-0.16222667483323433] Loss: 22.86757155965309\n",
      "Iteracion: 7066 Gradiente: [0.009397974899051557,-0.16214043015338517] Loss: 22.867545160762244\n",
      "Iteracion: 7067 Gradiente: [0.009392978647110795,-0.16205423132385377] Loss: 22.867518789932856\n",
      "Iteracion: 7068 Gradiente: [0.009387985051315202,-0.1619680783202687] Loss: 22.8674924471351\n",
      "Iteracion: 7069 Gradiente: [0.009382994110313802,-0.16188197111826147] Loss: 22.86746613233918\n",
      "Iteracion: 7070 Gradiente: [0.00937800582261256,-0.1617959096934868] Loss: 22.86743984551534\n",
      "Iteracion: 7071 Gradiente: [0.00937302018692397,-0.16170989402160305] Loss: 22.867413586633816\n",
      "Iteracion: 7072 Gradiente: [0.009368037201666842,-0.16162392407829648] Loss: 22.867387355664928\n",
      "Iteracion: 7073 Gradiente: [0.009363056865531879,-0.1615379998392482] Loss: 22.867361152579\n",
      "Iteracion: 7074 Gradiente: [0.009358079177061995,-0.161452121280166] Loss: 22.867334977346406\n",
      "Iteracion: 7075 Gradiente: [0.009353104134877792,-0.1613662883767617] Loss: 22.867308829937528\n",
      "Iteracion: 7076 Gradiente: [0.00934813173761313,-0.16128050110476255] Loss: 22.867282710322794\n",
      "Iteracion: 7077 Gradiente: [0.009343161983932192,-0.16119475943990266] Loss: 22.867256618472652\n",
      "Iteracion: 7078 Gradiente: [0.009338194872223463,-0.16110906335795078] Loss: 22.867230554357572\n",
      "Iteracion: 7079 Gradiente: [0.00933323040120134,-0.16102341283466412] Loss: 22.867204517948114\n",
      "Iteracion: 7080 Gradiente: [0.009328268569417257,-0.1609378078458268] Loss: 22.8671785092148\n",
      "Iteracion: 7081 Gradiente: [0.009323309375576135,-0.1608522483672252] Loss: 22.86715252812822\n",
      "Iteracion: 7082 Gradiente: [0.009318352818108147,-0.16076673437467312] Loss: 22.86712657465899\n",
      "Iteracion: 7083 Gradiente: [0.009313398895787372,-0.16068126584397896] Loss: 22.86710064877774\n",
      "Iteracion: 7084 Gradiente: [0.009308447606992823,-0.16059584275098615] Loss: 22.86707475045513\n",
      "Iteracion: 7085 Gradiente: [0.009303498950560159,-0.16051046507152775] Loss: 22.867048879661922\n",
      "Iteracion: 7086 Gradiente: [0.009298552924952712,-0.16042513278146373] Loss: 22.867023036368796\n",
      "Iteracion: 7087 Gradiente: [0.009293609528775922,-0.16033984585666694] Loss: 22.86699722054656\n",
      "Iteracion: 7088 Gradiente: [0.009288668760672181,-0.16025460427301622] Loss: 22.866971432165972\n",
      "Iteracion: 7089 Gradiente: [0.009283730619290508,-0.16016940800640522] Loss: 22.866945671197893\n",
      "Iteracion: 7090 Gradiente: [0.009278795103109397,-0.16008425703274806] Loss: 22.866919937613186\n",
      "Iteracion: 7091 Gradiente: [0.00927386221078829,-0.15999915132796333] Loss: 22.86689423138273\n",
      "Iteracion: 7092 Gradiente: [0.00926893194098189,-0.15991409086798] Loss: 22.86686855247744\n",
      "Iteracion: 7093 Gradiente: [0.009264004292270063,-0.1598290756287485] Loss: 22.86684290086829\n",
      "Iteracion: 7094 Gradiente: [0.009259079263191932,-0.15974410558623026] Loss: 22.866817276526252\n",
      "Iteracion: 7095 Gradiente: [0.009254156852424937,-0.15965918071639476] Loss: 22.86679167942234\n",
      "Iteracion: 7096 Gradiente: [0.00924923705862284,-0.15957430099522274] Loss: 22.866766109527614\n",
      "Iteracion: 7097 Gradiente: [0.009244319880279288,-0.15948946639872] Loss: 22.866740566813135\n",
      "Iteracion: 7098 Gradiente: [0.009239405316100147,-0.15940467690289045] Loss: 22.866715051250036\n",
      "Iteracion: 7099 Gradiente: [0.009234493364642542,-0.1593199324837587] Loss: 22.86668956280942\n",
      "Iteracion: 7100 Gradiente: [0.009229584024528966,-0.1592352331173623] Loss: 22.866664101462487\n",
      "Iteracion: 7101 Gradiente: [0.009224677294324125,-0.1591505787797517] Loss: 22.866638667180418\n",
      "Iteracion: 7102 Gradiente: [0.009219773172766092,-0.15906596944698112] Loss: 22.86661325993445\n",
      "Iteracion: 7103 Gradiente: [0.009214871658382624,-0.15898140509512887] Loss: 22.866587879695846\n",
      "Iteracion: 7104 Gradiente: [0.009209972749761163,-0.15889688570028399] Loss: 22.866562526435896\n",
      "Iteracion: 7105 Gradiente: [0.009205076445517572,-0.15881241123854498] Loss: 22.86653720012593\n",
      "Iteracion: 7106 Gradiente: [0.00920018274433308,-0.1587279816860215] Loss: 22.866511900737283\n",
      "Iteracion: 7107 Gradiente: [0.009195291644779975,-0.15864359701883934] Loss: 22.866486628241354\n",
      "Iteracion: 7108 Gradiente: [0.009190403145504433,-0.15855925721313704] Loss: 22.86646138260955\n",
      "Iteracion: 7109 Gradiente: [0.009185517245169687,-0.1584749622450591] Loss: 22.866436163813322\n",
      "Iteracion: 7110 Gradiente: [0.009180633942143383,-0.1583907120907849] Loss: 22.866410971824127\n",
      "Iteracion: 7111 Gradiente: [0.00917575323535876,-0.15830650672647065] Loss: 22.866385806613497\n",
      "Iteracion: 7112 Gradiente: [0.00917087512328294,-0.15822234612831482] Loss: 22.86636066815294\n",
      "Iteracion: 7113 Gradiente: [0.009165999604538419,-0.15813823027251767] Loss: 22.866335556414047\n",
      "Iteracion: 7114 Gradiente: [0.009161126677913483,-0.15805415913528217] Loss: 22.866310471368376\n",
      "Iteracion: 7115 Gradiente: [0.009156256341716093,-0.15797013269285476] Loss: 22.866285412987587\n",
      "Iteracion: 7116 Gradiente: [0.009151388594725062,-0.15788615092146355] Loss: 22.866260381243322\n",
      "Iteracion: 7117 Gradiente: [0.009146523435708787,-0.15780221379735243] Loss: 22.86623537610728\n",
      "Iteracion: 7118 Gradiente: [0.009141660863106912,-0.15771832129679605] Loss: 22.866210397551153\n",
      "Iteracion: 7119 Gradiente: [0.009136800875569406,-0.15763447339606884] Loss: 22.866185445546716\n",
      "Iteracion: 7120 Gradiente: [0.00913194347178224,-0.15755067007145887] Loss: 22.866160520065712\n",
      "Iteracion: 7121 Gradiente: [0.009127088650290223,-0.1574669112992704] Loss: 22.866135621079984\n",
      "Iteracion: 7122 Gradiente: [0.009122236409833324,-0.15738319705581283] Loss: 22.86611074856133\n",
      "Iteracion: 7123 Gradiente: [0.009117386748980986,-0.15729952731741612] Loss: 22.86608590248164\n",
      "Iteracion: 7124 Gradiente: [0.009112539666391702,-0.1572159020604185] Loss: 22.866061082812802\n",
      "Iteracion: 7125 Gradiente: [0.009107695160561964,-0.1571323212611805] Loss: 22.866036289526747\n",
      "Iteracion: 7126 Gradiente: [0.00910285323028764,-0.15704878489605675] Loss: 22.86601152259544\n",
      "Iteracion: 7127 Gradiente: [0.009098013874077538,-0.15696529294143222] Loss: 22.86598678199084\n",
      "Iteracion: 7128 Gradiente: [0.009093177090625204,-0.15688184537369299] Loss: 22.865962067684993\n",
      "Iteracion: 7129 Gradiente: [0.009088342878599557,-0.15679844216923963] Loss: 22.865937379649917\n",
      "Iteracion: 7130 Gradiente: [0.009083511236601302,-0.15671508330448916] Loss: 22.865912717857704\n",
      "Iteracion: 7131 Gradiente: [0.009078682163195139,-0.15663176875587237] Loss: 22.865888082280442\n",
      "Iteracion: 7132 Gradiente: [0.009073855657096412,-0.15654849849982674] Loss: 22.86586347289029\n",
      "Iteracion: 7133 Gradiente: [0.009069031716896348,-0.15646527251280548] Loss: 22.86583888965939\n",
      "Iteracion: 7134 Gradiente: [0.00906421034125439,-0.15638209077127327] Loss: 22.86581433255993\n",
      "Iteracion: 7135 Gradiente: [0.009059391528814823,-0.1562989532517084] Loss: 22.86578980156415\n",
      "Iteracion: 7136 Gradiente: [0.009054575278209616,-0.15621585993059975] Loss: 22.86576529664432\n",
      "Iteracion: 7137 Gradiente: [0.00904976158804042,-0.15613281078445335] Loss: 22.865740817772682\n",
      "Iteracion: 7138 Gradiente: [0.009044950456960995,-0.1560498057897823] Loss: 22.86571636492156\n",
      "Iteracion: 7139 Gradiente: [0.009040141883656361,-0.15596684492311266] Loss: 22.865691938063303\n",
      "Iteracion: 7140 Gradiente: [0.009035335866737644,-0.15588392816098634] Loss: 22.865667537170278\n",
      "Iteracion: 7141 Gradiente: [0.009030532404815972,-0.1558010554799587] Loss: 22.86564316221488\n",
      "Iteracion: 7142 Gradiente: [0.009025731496607629,-0.15571822685658862] Loss: 22.865618813169544\n",
      "Iteracion: 7143 Gradiente: [0.009020933140741741,-0.1556354422674552] Loss: 22.865594490006735\n",
      "Iteracion: 7144 Gradiente: [0.009016137335775436,-0.1555527016891534] Loss: 22.865570192698918\n",
      "Iteracion: 7145 Gradiente: [0.009011344080431628,-0.15547000509828202] Loss: 22.865545921218636\n",
      "Iteracion: 7146 Gradiente: [0.009006553373262704,-0.15538735247146082] Loss: 22.865521675538425\n",
      "Iteracion: 7147 Gradiente: [0.009001765213029481,-0.15530474378530978] Loss: 22.86549745563085\n",
      "Iteracion: 7148 Gradiente: [0.008996979598383822,-0.15522217901646787] Loss: 22.865473261468516\n",
      "Iteracion: 7149 Gradiente: [0.008992196527829795,-0.1551396581415972] Loss: 22.86544909302406\n",
      "Iteracion: 7150 Gradiente: [0.008987416000180323,-0.15505718113735145] Loss: 22.865424950270153\n",
      "Iteracion: 7151 Gradiente: [0.008982638014077792,-0.15497474798040814] Loss: 22.86540083317949\n",
      "Iteracion: 7152 Gradiente: [0.008977862567889853,-0.154892358647475] Loss: 22.865376741724766\n",
      "Iteracion: 7153 Gradiente: [0.008973089660616058,-0.15481001311523127] Loss: 22.865352675878754\n",
      "Iteracion: 7154 Gradiente: [0.008968319290715007,-0.1547277113604037] Loss: 22.86532863561423\n",
      "Iteracion: 7155 Gradiente: [0.008963551456848034,-0.15464545335971755] Loss: 22.865304620903995\n",
      "Iteracion: 7156 Gradiente: [0.008958786157811953,-0.15456323908990274] Loss: 22.865280631720875\n",
      "Iteracion: 7157 Gradiente: [0.008954023392079572,-0.15448106852772223] Loss: 22.865256668037766\n",
      "Iteracion: 7158 Gradiente: [0.008949263158411706,-0.15439894164993256] Loss: 22.86523272982753\n",
      "Iteracion: 7159 Gradiente: [0.008944505455486744,-0.15431685843330906] Loss: 22.865208817063113\n",
      "Iteracion: 7160 Gradiente: [0.008939750281751913,-0.1542348188546505] Loss: 22.865184929717444\n",
      "Iteracion: 7161 Gradiente: [0.008934997636125293,-0.15415282289074586] Loss: 22.865161067763513\n",
      "Iteracion: 7162 Gradiente: [0.00893024751707685,-0.15407087051841492] Loss: 22.865137231174348\n",
      "Iteracion: 7163 Gradiente: [0.008925499923419504,-0.15398896171447732] Loss: 22.865113419922974\n",
      "Iteracion: 7164 Gradiente: [0.0089207548537388,-0.1539070964557715] Loss: 22.865089633982443\n",
      "Iteracion: 7165 Gradiente: [0.008916012306644915,-0.1538252747191539] Loss: 22.865065873325857\n",
      "Iteracion: 7166 Gradiente: [0.008911272280784033,-0.15374349648148614] Loss: 22.86504213792635\n",
      "Iteracion: 7167 Gradiente: [0.008906534774942543,-0.15366176171963347] Loss: 22.86501842775706\n",
      "Iteracion: 7168 Gradiente: [0.008901799787563884,-0.1535800704104974] Loss: 22.864994742791186\n",
      "Iteracion: 7169 Gradiente: [0.00889706731763719,-0.15349842253095952] Loss: 22.864971083001933\n",
      "Iteracion: 7170 Gradiente: [0.008892337363579372,-0.1534168180579428] Loss: 22.864947448362518\n",
      "Iteracion: 7171 Gradiente: [0.008887609924154086,-0.15333525696836692] Loss: 22.864923838846245\n",
      "Iteracion: 7172 Gradiente: [0.008882884997829403,-0.1532537392391782] Loss: 22.864900254426367\n",
      "Iteracion: 7173 Gradiente: [0.008878162583468453,-0.1531722648473136] Loss: 22.86487669507623\n",
      "Iteracion: 7174 Gradiente: [0.008873442679730677,-0.1530908337697344] Loss: 22.864853160769187\n",
      "Iteracion: 7175 Gradiente: [0.008868725285238573,-0.15300944598341507] Loss: 22.864829651478605\n",
      "Iteracion: 7176 Gradiente: [0.008864010398633581,-0.15292810146534314] Loss: 22.864806167177907\n",
      "Iteracion: 7177 Gradiente: [0.008859298018685043,-0.1528468001925088] Loss: 22.86478270784052\n",
      "Iteracion: 7178 Gradiente: [0.008854588143894187,-0.15276554214193172] Loss: 22.864759273439898\n",
      "Iteracion: 7179 Gradiente: [0.008849880773105194,-0.15268432729062412] Loss: 22.864735863949573\n",
      "Iteracion: 7180 Gradiente: [0.008845175904772873,-0.15260315561563143] Loss: 22.864712479343016\n",
      "Iteracion: 7181 Gradiente: [0.008840473537766987,-0.15252202709398782] Loss: 22.86468911959381\n",
      "Iteracion: 7182 Gradiente: [0.00883577367071761,-0.15244094170275535] Loss: 22.864665784675523\n",
      "Iteracion: 7183 Gradiente: [0.008831076302215024,-0.15235989941900835] Loss: 22.86464247456174\n",
      "Iteracion: 7184 Gradiente: [0.00882638143094899,-0.15227890021982848] Loss: 22.864619189226122\n",
      "Iteracion: 7185 Gradiente: [0.008821689055740005,-0.15219794408230136] Loss: 22.864595928642338\n",
      "Iteracion: 7186 Gradiente: [0.008816999175047613,-0.15211703098354737] Loss: 22.864572692784044\n",
      "Iteracion: 7187 Gradiente: [0.008812311787674313,-0.1520361609006796] Loss: 22.864549481624987\n",
      "Iteracion: 7188 Gradiente: [0.0088076268922066,-0.15195533381083295] Loss: 22.864526295138873\n",
      "Iteracion: 7189 Gradiente: [0.008802944487353177,-0.15187454969114864] Loss: 22.86450313329952\n",
      "Iteracion: 7190 Gradiente: [0.008798264571920338,-0.15179380851877594] Loss: 22.864479996080714\n",
      "Iteracion: 7191 Gradiente: [0.008793587144335408,-0.1517131102708967] Loss: 22.864456883456267\n",
      "Iteracion: 7192 Gradiente: [0.008788912203516474,-0.151632454924678] Loss: 22.864433795400064\n",
      "Iteracion: 7193 Gradiente: [0.00878423974799792,-0.1515518424573184] Loss: 22.86441073188595\n",
      "Iteracion: 7194 Gradiente: [0.008779569776478032,-0.15147127284602296] Loss: 22.864387692887885\n",
      "Iteracion: 7195 Gradiente: [0.008774902287689202,-0.151390746068004] Loss: 22.864364678379776\n",
      "Iteracion: 7196 Gradiente: [0.008770237280268134,-0.1513102621004934] Loss: 22.86434168833561\n",
      "Iteracion: 7197 Gradiente: [0.008765574753061856,-0.15122982092071963] Loss: 22.864318722729358\n",
      "Iteracion: 7198 Gradiente: [0.008760914704407697,-0.15114942250595848] Loss: 22.864295781535073\n",
      "Iteracion: 7199 Gradiente: [0.008756257133253105,-0.15106906683345767] Loss: 22.86427286472677\n",
      "Iteracion: 7200 Gradiente: [0.008751602038208262,-0.1509887538804988] Loss: 22.864249972278554\n",
      "Iteracion: 7201 Gradiente: [0.008746949417936396,-0.15090848362437193] Loss: 22.864227104164545\n",
      "Iteracion: 7202 Gradiente: [0.008742299271144323,-0.15082825604237726] Loss: 22.864204260358846\n",
      "Iteracion: 7203 Gradiente: [0.008737651596546433,-0.15074807111182467] Loss: 22.864181440835615\n",
      "Iteracion: 7204 Gradiente: [0.00873300639273964,-0.15066792881004681] Loss: 22.86415864556906\n",
      "Iteracion: 7205 Gradiente: [0.008728363658506547,-0.15058782911437293] Loss: 22.864135874533385\n",
      "Iteracion: 7206 Gradiente: [0.008723723392430808,-0.1505077720021603] Loss: 22.86411312770284\n",
      "Iteracion: 7207 Gradiente: [0.008719085593328183,-0.1504277574507621] Loss: 22.86409040505169\n",
      "Iteracion: 7208 Gradiente: [0.00871445025977664,-0.1503477854375582] Loss: 22.864067706554245\n",
      "Iteracion: 7209 Gradiente: [0.008709817390485835,-0.15026785593993294] Loss: 22.8640450321848\n",
      "Iteracion: 7210 Gradiente: [0.008705186984242157,-0.15018796893527775] Loss: 22.864022381917753\n",
      "Iteracion: 7211 Gradiente: [0.008700559039667155,-0.150108124401005] Loss: 22.863999755727445\n",
      "Iteracion: 7212 Gradiente: [0.008695933555408904,-0.15002832231454047] Loss: 22.863977153588312\n",
      "Iteracion: 7213 Gradiente: [0.008691310530216849,-0.14994856265331447] Loss: 22.863954575474747\n",
      "Iteracion: 7214 Gradiente: [0.008686689962760851,-0.14986884539477263] Loss: 22.86393202136126\n",
      "Iteracion: 7215 Gradiente: [0.0086820718516852,-0.14978917051637478] Loss: 22.86390949122229\n",
      "Iteracion: 7216 Gradiente: [0.008677456195858706,-0.1497095379955822] Loss: 22.86388698503241\n",
      "Iteracion: 7217 Gradiente: [0.00867284299371344,-0.14962994780988884] Loss: 22.863864502766095\n",
      "Iteracion: 7218 Gradiente: [0.008668232244192116,-0.1495503999367764] Loss: 22.863842044397988\n",
      "Iteracion: 7219 Gradiente: [0.00866362394587649,-0.14947089435375507] Loss: 22.863819609902624\n",
      "Iteracion: 7220 Gradiente: [0.008659018097488532,-0.14939143103834135] Loss: 22.863797199254673\n",
      "Iteracion: 7221 Gradiente: [0.008654414697691475,-0.14931200996806523] Loss: 22.863774812428755\n",
      "Iteracion: 7222 Gradiente: [0.008649813745167496,-0.14923263112047114] Loss: 22.863752449399556\n",
      "Iteracion: 7223 Gradiente: [0.008645215238608254,-0.14915329447311182] Loss: 22.863730110141784\n",
      "Iteracion: 7224 Gradiente: [0.008640619176839929,-0.14907400000354384] Loss: 22.863707794630177\n",
      "Iteracion: 7225 Gradiente: [0.008636025558483121,-0.14899474768935003] Loss: 22.863685502839466\n",
      "Iteracion: 7226 Gradiente: [0.008631434382252223,-0.14891553750811753] Loss: 22.863663234744475\n",
      "Iteracion: 7227 Gradiente: [0.008626845646715727,-0.14883636943745582] Loss: 22.86364099031998\n",
      "Iteracion: 7228 Gradiente: [0.008622259350791714,-0.1487572434549649] Loss: 22.863618769540842\n",
      "Iteracion: 7229 Gradiente: [0.008617675493104571,-0.1486781595382734] Loss: 22.863596572381923\n",
      "Iteracion: 7230 Gradiente: [0.00861309407225311,-0.1485991176650239] Loss: 22.8635743988181\n",
      "Iteracion: 7231 Gradiente: [0.008608515087132673,-0.14852011781285532] Loss: 22.863552248824327\n",
      "Iteracion: 7232 Gradiente: [0.00860393853623028,-0.1484411599594381] Loss: 22.863530122375508\n",
      "Iteracion: 7233 Gradiente: [0.008599364418366425,-0.14836224408244003] Loss: 22.863508019446623\n",
      "Iteracion: 7234 Gradiente: [0.008594792732340768,-0.14828337015953796] Loss: 22.86348594001271\n",
      "Iteracion: 7235 Gradiente: [0.008590223476666855,-0.14820453816844162] Loss: 22.86346388404876\n",
      "Iteracion: 7236 Gradiente: [0.008585656650168971,-0.14812574808684928] Loss: 22.863441851529824\n",
      "Iteracion: 7237 Gradiente: [0.008581092251593722,-0.14804699989248] Loss: 22.863419842430982\n",
      "Iteracion: 7238 Gradiente: [0.008576530279615706,-0.1479682935630656] Loss: 22.863397856727357\n",
      "Iteracion: 7239 Gradiente: [0.008571970732879209,-0.14788962907635497] Loss: 22.863375894394057\n",
      "Iteracion: 7240 Gradiente: [0.008567413610127044,-0.14781100641010028] Loss: 22.86335395540626\n",
      "Iteracion: 7241 Gradiente: [0.008562858910085918,-0.14773242554206809] Loss: 22.86333203973915\n",
      "Iteracion: 7242 Gradiente: [0.00855830663145222,-0.14765388645003827] Loss: 22.86331014736792\n",
      "Iteracion: 7243 Gradiente: [0.00855375677296403,-0.14757538911179988] Loss: 22.863288278267806\n",
      "Iteracion: 7244 Gradiente: [0.008549209333342371,-0.1474969335051533] Loss: 22.863266432414083\n",
      "Iteracion: 7245 Gradiente: [0.008544664311198364,-0.1474185196079206] Loss: 22.863244609782033\n",
      "Iteracion: 7246 Gradiente: [0.00854012170536483,-0.14734014739792087] Loss: 22.86322281034699\n",
      "Iteracion: 7247 Gradiente: [0.00853558151451163,-0.14726181685299433] Loss: 22.863201034084273\n",
      "Iteracion: 7248 Gradiente: [0.008531043737354102,-0.14718352795098966] Loss: 22.863179280969266\n",
      "Iteracion: 7249 Gradiente: [0.008526508372608533,-0.14710528066977038] Loss: 22.863157550977366\n",
      "Iteracion: 7250 Gradiente: [0.008521975419046156,-0.1470270749872043] Loss: 22.86313584408397\n",
      "Iteracion: 7251 Gradiente: [0.008517444875317892,-0.1469489108811807] Loss: 22.863114160264562\n",
      "Iteracion: 7252 Gradiente: [0.008512916740196866,-0.1468707883295932] Loss: 22.863092499494584\n",
      "Iteracion: 7253 Gradiente: [0.008508391012313155,-0.14679270731035518] Loss: 22.863070861749534\n",
      "Iteracion: 7254 Gradiente: [0.008503867690491045,-0.1467146678013819] Loss: 22.863049247004955\n",
      "Iteracion: 7255 Gradiente: [0.008499346773462927,-0.14663666978060316] Loss: 22.8630276552364\n",
      "Iteracion: 7256 Gradiente: [0.008494828259822876,-0.14655871322597064] Loss: 22.86300608641942\n",
      "Iteracion: 7257 Gradiente: [0.008490312148320337,-0.1464807981154375] Loss: 22.86298454052965\n",
      "Iteracion: 7258 Gradiente: [0.008485798437853494,-0.14640292442696107] Loss: 22.862963017542686\n",
      "Iteracion: 7259 Gradiente: [0.008481287126904628,-0.14632509213853453] Loss: 22.8629415174342\n",
      "Iteracion: 7260 Gradiente: [0.008476778214387082,-0.1462473012281367] Loss: 22.86292004017988\n",
      "Iteracion: 7261 Gradiente: [0.00847227169884377,-0.14616955167378098] Loss: 22.86289858575544\n",
      "Iteracion: 7262 Gradiente: [0.008467767579118875,-0.14609184345347412] Loss: 22.862877154136584\n",
      "Iteracion: 7263 Gradiente: [0.008463265853971317,-0.14601417654523866] Loss: 22.86285574529909\n",
      "Iteracion: 7264 Gradiente: [0.00845876652213633,-0.1459365509271129] Loss: 22.862834359218724\n",
      "Iteracion: 7265 Gradiente: [0.008454269582187143,-0.1458589665771536] Loss: 22.862812995871312\n",
      "Iteracion: 7266 Gradiente: [0.008449775033014362,-0.14578142347341203] Loss: 22.86279165523268\n",
      "Iteracion: 7267 Gradiente: [0.008445282873195955,-0.14570392159396992] Loss: 22.862770337278693\n",
      "Iteracion: 7268 Gradiente: [0.008440793101598842,-0.14562646091690315] Loss: 22.862749041985243\n",
      "Iteracion: 7269 Gradiente: [0.008436305716922258,-0.14554904142030836] Loss: 22.862727769328252\n",
      "Iteracion: 7270 Gradiente: [0.00843182071781617,-0.14547166308229792] Loss: 22.862706519283613\n",
      "Iteracion: 7271 Gradiente: [0.008427338103114341,-0.14539432588098566] Loss: 22.862685291827344\n",
      "Iteracion: 7272 Gradiente: [0.008422857871516953,-0.1453170297945024] Loss: 22.862664086935414\n",
      "Iteracion: 7273 Gradiente: [0.008418380021662604,-0.14523977480099623] Loss: 22.862642904583822\n",
      "Iteracion: 7274 Gradiente: [0.008413904552425796,-0.14516256087861237] Loss: 22.862621744748605\n",
      "Iteracion: 7275 Gradiente: [0.008409431462546498,-0.14508538800551668] Loss: 22.862600607405856\n",
      "Iteracion: 7276 Gradiente: [0.008404960750708786,-0.1450082561598882] Loss: 22.862579492531655\n",
      "Iteracion: 7277 Gradiente: [0.008400492415559787,-0.14493116531992065] Loss: 22.862558400102124\n",
      "Iteracion: 7278 Gradiente: [0.008396026455998632,-0.14485411546380564] Loss: 22.86253733009339\n",
      "Iteracion: 7279 Gradiente: [0.00839156287054171,-0.14477710656976736] Loss: 22.86251628248163\n",
      "Iteracion: 7280 Gradiente: [0.008387101658146889,-0.144700138616016] Loss: 22.86249525724304\n",
      "Iteracion: 7281 Gradiente: [0.008382642817520984,-0.14462321158078784] Loss: 22.86247425435382\n",
      "Iteracion: 7282 Gradiente: [0.00837818634730733,-0.14454632544233473] Loss: 22.86245327379023\n",
      "Iteracion: 7283 Gradiente: [0.008373732246244952,-0.14446948017891745] Loss: 22.862432315528533\n",
      "Iteracion: 7284 Gradiente: [0.008369280513169505,-0.14439267576879627] Loss: 22.862411379545016\n",
      "Iteracion: 7285 Gradiente: [0.008364831146790645,-0.1443159121902563] Loss: 22.862390465816013\n",
      "Iteracion: 7286 Gradiente: [0.008360384145730867,-0.14423918942159777] Loss: 22.862369574317878\n",
      "Iteracion: 7287 Gradiente: [0.008355939508935725,-0.14416250744111198] Loss: 22.86234870502693\n",
      "Iteracion: 7288 Gradiente: [0.008351497234949079,-0.1440858662271257] Loss: 22.8623278579196\n",
      "Iteracion: 7289 Gradiente: [0.008347057322707012,-0.144009265757956] Loss: 22.86230703297233\n",
      "Iteracion: 7290 Gradiente: [0.008342619770857596,-0.14393270601194696] Loss: 22.862286230161512\n",
      "Iteracion: 7291 Gradiente: [0.00833818457814175,-0.14385618696744848] Loss: 22.86226544946364\n",
      "Iteracion: 7292 Gradiente: [0.008333751743263444,-0.1437797086028258] Loss: 22.862244690855206\n",
      "Iteracion: 7293 Gradiente: [0.008329321264966438,-0.14370327089645255] Loss: 22.862223954312746\n",
      "Iteracion: 7294 Gradiente: [0.008324893142156498,-0.14362687382670386] Loss: 22.862203239812786\n",
      "Iteracion: 7295 Gradiente: [0.00832046737346938,-0.14355051737198204] Loss: 22.8621825473319\n",
      "Iteracion: 7296 Gradiente: [0.008316043957537052,-0.14347420151070292] Loss: 22.862161876846667\n",
      "Iteracion: 7297 Gradiente: [0.008311622893359072,-0.1433979262212695] Loss: 22.862141228333726\n",
      "Iteracion: 7298 Gradiente: [0.00830720417954751,-0.143321691482123] Loss: 22.862120601769725\n",
      "Iteracion: 7299 Gradiente: [0.008302787814731498,-0.14324549727171057] Loss: 22.862099997131324\n",
      "Iteracion: 7300 Gradiente: [0.008298373797849005,-0.1431693435684758] Loss: 22.86207941439521\n",
      "Iteracion: 7301 Gradiente: [0.008293962127608741,-0.1430932303508867] Loss: 22.862058853538105\n",
      "Iteracion: 7302 Gradiente: [0.0082895528028331,-0.14301715759741582] Loss: 22.86203831453677\n",
      "Iteracion: 7303 Gradiente: [0.008285145822113312,-0.14294112528655903] Loss: 22.862017797367933\n",
      "Iteracion: 7304 Gradiente: [0.00828074118428977,-0.1428651333968124] Loss: 22.86199730200843\n",
      "Iteracion: 7305 Gradiente: [0.008276338888072133,-0.1427891819066885] Loss: 22.861976828435047\n",
      "Iteracion: 7306 Gradiente: [0.008271938932304579,-0.14271327079470655] Loss: 22.86195637662464\n",
      "Iteracion: 7307 Gradiente: [0.008267541315652239,-0.14263740003940248] Loss: 22.86193594655409\n",
      "Iteracion: 7308 Gradiente: [0.008263146036930872,-0.14256156961932] Loss: 22.86191553820025\n",
      "Iteracion: 7309 Gradiente: [0.008258753094842556,-0.14248577951301797] Loss: 22.861895151540057\n",
      "Iteracion: 7310 Gradiente: [0.008254362488248528,-0.1424100296990583] Loss: 22.861874786550445\n",
      "Iteracion: 7311 Gradiente: [0.008249974215827176,-0.1423343201560255] Loss: 22.861854443208408\n",
      "Iteracion: 7312 Gradiente: [0.008245588276283418,-0.1422586508625141] Loss: 22.861834121490908\n",
      "Iteracion: 7313 Gradiente: [0.008241204668382806,-0.14218302179712433] Loss: 22.86181382137495\n",
      "Iteracion: 7314 Gradiente: [0.008236823391009314,-0.14210743293846306] Loss: 22.861793542837585\n",
      "Iteracion: 7315 Gradiente: [0.008232444442942704,-0.14203188426515442] Loss: 22.861773285855886\n",
      "Iteracion: 7316 Gradiente: [0.008228067822703149,-0.1419563757558489] Loss: 22.861753050406907\n",
      "Iteracion: 7317 Gradiente: [0.008223693529329997,-0.14188090738917733] Loss: 22.861732836467795\n",
      "Iteracion: 7318 Gradiente: [0.008219321561455217,-0.14180547914380667] Loss: 22.861712644015675\n",
      "Iteracion: 7319 Gradiente: [0.008214951917857623,-0.14173009099840606] Loss: 22.86169247302769\n",
      "Iteracion: 7320 Gradiente: [0.00821058459724308,-0.1416547429316612] Loss: 22.861672323481056\n",
      "Iteracion: 7321 Gradiente: [0.008206219598362926,-0.14157943492226438] Loss: 22.86165219535294\n",
      "Iteracion: 7322 Gradiente: [0.00820185692019398,-0.14150416694890947] Loss: 22.861632088620613\n",
      "Iteracion: 7323 Gradiente: [0.008197496561263999,-0.14142893899032435] Loss: 22.861612003261307\n",
      "Iteracion: 7324 Gradiente: [0.008193138520402954,-0.14135375102523373] Loss: 22.8615919392523\n",
      "Iteracion: 7325 Gradiente: [0.00818878279659619,-0.1412786030323615] Loss: 22.86157189657093\n",
      "Iteracion: 7326 Gradiente: [0.008184429388334517,-0.14120349499047566] Loss: 22.861551875194497\n",
      "Iteracion: 7327 Gradiente: [0.008180078294357903,-0.1411284268783362] Loss: 22.861531875100358\n",
      "Iteracion: 7328 Gradiente: [0.008175729513741696,-0.14105339867469838] Loss: 22.861511896265906\n",
      "Iteracion: 7329 Gradiente: [0.008171383044889543,-0.14097841035836656] Loss: 22.86149193866851\n",
      "Iteracion: 7330 Gradiente: [0.008167038886916582,-0.14090346190811723] Loss: 22.861472002285645\n",
      "Iteracion: 7331 Gradiente: [0.008162697038303197,-0.1408285533027718] Loss: 22.861452087094708\n",
      "Iteracion: 7332 Gradiente: [0.008158357498025263,-0.14075368452113654] Loss: 22.861432193073203\n",
      "Iteracion: 7333 Gradiente: [0.008154020264745062,-0.1406788555420448] Loss: 22.861412320198603\n",
      "Iteracion: 7334 Gradiente: [0.008149685337294462,-0.14060406634433315] Loss: 22.861392468448468\n",
      "Iteracion: 7335 Gradiente: [0.008145352714435224,-0.14052931690685558] Loss: 22.86137263780033\n",
      "Iteracion: 7336 Gradiente: [0.008141022394822055,-0.1404546072084784] Loss: 22.861352828231727\n",
      "Iteracion: 7337 Gradiente: [0.008136694377480087,-0.14037993722806377] Loss: 22.86133303972029\n",
      "Iteracion: 7338 Gradiente: [0.008132368661032766,-0.1403053069445015] Loss: 22.86131327224361\n",
      "Iteracion: 7339 Gradiente: [0.008128045244273114,-0.14023071633668896] Loss: 22.86129352577933\n",
      "Iteracion: 7340 Gradiente: [0.00812372412592121,-0.140156165383537] Loss: 22.861273800305142\n",
      "Iteracion: 7341 Gradiente: [0.008119405304839233,-0.14008165406395817] Loss: 22.861254095798703\n",
      "Iteracion: 7342 Gradiente: [0.00811508877973021,-0.14000718235688606] Loss: 22.861234412237753\n",
      "Iteracion: 7343 Gradiente: [0.00811077454945727,-0.13993275024125787] Loss: 22.861214749599984\n",
      "Iteracion: 7344 Gradiente: [0.008106462612662806,-0.139858357696032] Loss: 22.861195107863193\n",
      "Iteracion: 7345 Gradiente: [0.008102152968325527,-0.13978400470016047] Loss: 22.861175487005152\n",
      "Iteracion: 7346 Gradiente: [0.008097845615134247,-0.1397096912326243] Loss: 22.86115588700367\n",
      "Iteracion: 7347 Gradiente: [0.008093540551817569,-0.13963541727241022] Loss: 22.861136307836578\n",
      "Iteracion: 7348 Gradiente: [0.008089237777249044,-0.1395611827985115] Loss: 22.86111674948171\n",
      "Iteracion: 7349 Gradiente: [0.008084937290211277,-0.1394869877899347] Loss: 22.861097211916963\n",
      "Iteracion: 7350 Gradiente: [0.008080639089303076,-0.13941283222571066] Loss: 22.86107769512023\n",
      "Iteracion: 7351 Gradiente: [0.008076343173506946,-0.13933871608485618] Loss: 22.861058199069443\n",
      "Iteracion: 7352 Gradiente: [0.008072049541551489,-0.13926463934641806] Loss: 22.861038723742542\n",
      "Iteracion: 7353 Gradiente: [0.008067758192326361,-0.13919060198944186] Loss: 22.861019269117502\n",
      "Iteracion: 7354 Gradiente: [0.008063469124394373,-0.13911660399300368] Loss: 22.8609998351723\n",
      "Iteracion: 7355 Gradiente: [0.008059182336730449,-0.13904264533616814] Loss: 22.860980421884985\n",
      "Iteracion: 7356 Gradiente: [0.008054897827980767,-0.13896872599802837] Loss: 22.860961029233575\n",
      "Iteracion: 7357 Gradiente: [0.00805061559708046,-0.13889484595767432] Loss: 22.860941657196143\n",
      "Iteracion: 7358 Gradiente: [0.008046335642739185,-0.13882100519421753] Loss: 22.860922305750783\n",
      "Iteracion: 7359 Gradiente: [0.008042057963731964,-0.13874720368677865] Loss: 22.860902974875593\n",
      "Iteracion: 7360 Gradiente: [0.00803778255889256,-0.1386734414144847] Loss: 22.860883664548716\n",
      "Iteracion: 7361 Gradiente: [0.008033509426926837,-0.13859971835648252] Loss: 22.860864374748303\n",
      "Iteracion: 7362 Gradiente: [0.008029238566714033,-0.13852603449191936] Loss: 22.860845105452533\n",
      "Iteracion: 7363 Gradiente: [0.0080249699770557,-0.1384523897999582] Loss: 22.86082585663962\n",
      "Iteracion: 7364 Gradiente: [0.008020703656732546,-0.13837878425977454] Loss: 22.860806628287783\n",
      "Iteracion: 7365 Gradiente: [0.008016439604469383,-0.13830521785055866] Loss: 22.86078742037528\n",
      "Iteracion: 7366 Gradiente: [0.008012177819037448,-0.13823169055150744] Loss: 22.86076823288037\n",
      "Iteracion: 7367 Gradiente: [0.008007918299431557,-0.13815820234181875] Loss: 22.860749065781377\n",
      "Iteracion: 7368 Gradiente: [0.008003661044268522,-0.13808475320071983] Loss: 22.86072991905659\n",
      "Iteracion: 7369 Gradiente: [0.007999406052378314,-0.13801134310743943] Loss: 22.860710792684362\n",
      "Iteracion: 7370 Gradiente: [0.007995153322508487,-0.13793797204122207] Loss: 22.86069168664307\n",
      "Iteracion: 7371 Gradiente: [0.007990902853667119,-0.1378646399813075] Loss: 22.86067260091109\n",
      "Iteracion: 7372 Gradiente: [0.007986654644462495,-0.1377913469069694] Loss: 22.860653535466838\n",
      "Iteracion: 7373 Gradiente: [0.007982408693619429,-0.13771809279748576] Loss: 22.86063449028876\n",
      "Iteracion: 7374 Gradiente: [0.007978165000209477,-0.13764487763212638] Loss: 22.86061546535529\n",
      "Iteracion: 7375 Gradiente: [0.007973923562793554,-0.13757170139020058] Loss: 22.860596460644924\n",
      "Iteracion: 7376 Gradiente: [0.00796968438031153,-0.13749856405100794] Loss: 22.860577476136154\n",
      "Iteracion: 7377 Gradiente: [0.007965447451481585,-0.13742546559387137] Loss: 22.86055851180753\n",
      "Iteracion: 7378 Gradiente: [0.00796121277512043,-0.13735240599811943] Loss: 22.860539567637552\n",
      "Iteracion: 7379 Gradiente: [0.007956980350047615,-0.13727938524309036] Loss: 22.860520643604854\n",
      "Iteracion: 7380 Gradiente: [0.007952750175069431,-0.13720640330813721] Loss: 22.860501739687983\n",
      "Iteracion: 7381 Gradiente: [0.007948522248905002,-0.13713346017262443] Loss: 22.8604828558656\n",
      "Iteracion: 7382 Gradiente: [0.007944296570563362,-0.13706055581591367] Loss: 22.8604639921163\n",
      "Iteracion: 7383 Gradiente: [0.007940073138577001,-0.1369876902174062] Loss: 22.860445148418766\n",
      "Iteracion: 7384 Gradiente: [0.007935851951994739,-0.13691486335648062] Loss: 22.86042632475169\n",
      "Iteracion: 7385 Gradiente: [0.007931633009512022,-0.13684207521255054] Loss: 22.86040752109376\n",
      "Iteracion: 7386 Gradiente: [0.007927416309913345,-0.13676932576503506] Loss: 22.860388737423722\n",
      "Iteracion: 7387 Gradiente: [0.00792320185208458,-0.13669661499335498] Loss: 22.860369973720328\n",
      "Iteracion: 7388 Gradiente: [0.007918989634742957,-0.13662394287695712] Loss: 22.860351229962355\n",
      "Iteracion: 7389 Gradiente: [0.007914779656817927,-0.13655130939528135] Loss: 22.86033250612859\n",
      "Iteracion: 7390 Gradiente: [0.007910571917048515,-0.13647871452779523] Loss: 22.860313802197872\n",
      "Iteracion: 7391 Gradiente: [0.007906366414151005,-0.13640615825397356] Loss: 22.860295118149036\n",
      "Iteracion: 7392 Gradiente: [0.007902163147026424,-0.13633364055329444] Loss: 22.860276453960935\n",
      "Iteracion: 7393 Gradiente: [0.007897962114581485,-0.1362611614052459] Loss: 22.860257809612484\n",
      "Iteracion: 7394 Gradiente: [0.007893763315556157,-0.13618872078933536] Loss: 22.860239185082584\n",
      "Iteracion: 7395 Gradiente: [0.007889566748693256,-0.13611631868508217] Loss: 22.86022058035015\n",
      "Iteracion: 7396 Gradiente: [0.007885372412785804,-0.13604395507201397] Loss: 22.860201995394156\n",
      "Iteracion: 7397 Gradiente: [0.007881180306758514,-0.13597162992966158] Loss: 22.860183430193565\n",
      "Iteracion: 7398 Gradiente: [0.007876990429423358,-0.13589934323757216] Loss: 22.860164884727393\n",
      "Iteracion: 7399 Gradiente: [0.007872802779516519,-0.135827094975309] Loss: 22.86014635897467\n",
      "Iteracion: 7400 Gradiente: [0.007868617355919127,-0.1357548851224383] Loss: 22.860127852914413\n",
      "Iteracion: 7401 Gradiente: [0.00786443415741284,-0.1356827136585436] Loss: 22.860109366525716\n",
      "Iteracion: 7402 Gradiente: [0.007860253182780259,-0.13561058056321637] Loss: 22.860090899787657\n",
      "Iteracion: 7403 Gradiente: [0.007856074430901572,-0.13553848581605527] Loss: 22.86007245267934\n",
      "Iteracion: 7404 Gradiente: [0.007851897900588748,-0.13546642939667397] Loss: 22.860054025179913\n",
      "Iteracion: 7405 Gradiente: [0.007847723590695447,-0.1353944112846944] Loss: 22.86003561726853\n",
      "Iteracion: 7406 Gradiente: [0.007843551499922797,-0.13532243145975803] Loss: 22.860017228924356\n",
      "Iteracion: 7407 Gradiente: [0.007839381627206876,-0.13525048990150274] Loss: 22.859998860126602\n",
      "Iteracion: 7408 Gradiente: [0.007835213971313237,-0.1351785865895895] Loss: 22.859980510854495\n",
      "Iteracion: 7409 Gradiente: [0.00783104853101501,-0.13510672150368716] Loss: 22.859962181087283\n",
      "Iteracion: 7410 Gradiente: [0.0078268853052928,-0.13503489462346419] Loss: 22.85994387080421\n",
      "Iteracion: 7411 Gradiente: [0.00782272429287237,-0.1349631059286152] Loss: 22.859925579984584\n",
      "Iteracion: 7412 Gradiente: [0.007818565492524005,-0.1348913553988428] Loss: 22.85990730860771\n",
      "Iteracion: 7413 Gradiente: [0.007814408903084314,-0.13481964301385668] Loss: 22.85988905665291\n",
      "Iteracion: 7414 Gradiente: [0.00781025452348653,-0.13474796875337122] Loss: 22.85987082409957\n",
      "Iteracion: 7415 Gradiente: [0.007806102352371151,-0.13467633259712977] Loss: 22.859852610927042\n",
      "Iteracion: 7416 Gradiente: [0.007801952388813523,-0.13460473452485847] Loss: 22.859834417114726\n",
      "Iteracion: 7417 Gradiente: [0.007797804631419088,-0.13453317451632643] Loss: 22.859816242642047\n",
      "Iteracion: 7418 Gradiente: [0.007793659079182665,-0.1344616525512862] Loss: 22.85979808748844\n",
      "Iteracion: 7419 Gradiente: [0.007789515730770328,-0.13439016860952174] Loss: 22.85977995163338\n",
      "Iteracion: 7420 Gradiente: [0.00778537458512479,-0.13431872267081377] Loss: 22.859761835056357\n",
      "Iteracion: 7421 Gradiente: [0.007781235640952862,-0.13424731471496518] Loss: 22.85974373773686\n",
      "Iteracion: 7422 Gradiente: [0.007777098897273049,-0.1341759447217716] Loss: 22.85972565965444\n",
      "Iteracion: 7423 Gradiente: [0.0077729643527656355,-0.1341046126710598] Loss: 22.859707600788628\n",
      "Iteracion: 7424 Gradiente: [0.007768832006298491,-0.13403331854265588] Loss: 22.85968956111902\n",
      "Iteracion: 7425 Gradiente: [0.007764701856703482,-0.13396206231639998] Loss: 22.859671540625172\n",
      "Iteracion: 7426 Gradiente: [0.007760573902930901,-0.13389084397213555] Loss: 22.859653539286754\n",
      "Iteracion: 7427 Gradiente: [0.007756448143667664,-0.13381966348972998] Loss: 22.859635557083358\n",
      "Iteracion: 7428 Gradiente: [0.007752324577688796,-0.1337485208490598] Loss: 22.85961759399466\n",
      "Iteracion: 7429 Gradiente: [0.0077482032040317485,-0.133677416029996] Loss: 22.85959965000036\n",
      "Iteracion: 7430 Gradiente: [0.007744084021443124,-0.13360634901243257] Loss: 22.859581725080133\n",
      "Iteracion: 7431 Gradiente: [0.007739967028659104,-0.13353531977628338] Loss: 22.859563819213722\n",
      "Iteracion: 7432 Gradiente: [0.0077358522245864,-0.13346432830145705] Loss: 22.859545932380858\n",
      "Iteracion: 7433 Gradiente: [0.00773173960809288,-0.1333933745678761] Loss: 22.859528064561328\n",
      "Iteracion: 7434 Gradiente: [0.007727629177981044,-0.13332245855547833] Loss: 22.859510215734904\n",
      "Iteracion: 7435 Gradiente: [0.0077235209331329695,-0.13325158024420697] Loss: 22.859492385881406\n",
      "Iteracion: 7436 Gradiente: [0.007719414872365367,-0.13318073961402146] Loss: 22.859474574980652\n",
      "Iteracion: 7437 Gradiente: [0.007715310994404945,-0.13310993664489576] Loss: 22.859456783012526\n",
      "Iteracion: 7438 Gradiente: [0.0077112092983005216,-0.13303917131679566] Loss: 22.859439009956883\n",
      "Iteracion: 7439 Gradiente: [0.007707109782769331,-0.1329684436097163] Loss: 22.8594212557936\n",
      "Iteracion: 7440 Gradiente: [0.0077030124466034525,-0.13289775350365954] Loss: 22.859403520502628\n",
      "Iteracion: 7441 Gradiente: [0.007698917288771175,-0.1328271009786307] Loss: 22.859385804063905\n",
      "Iteracion: 7442 Gradiente: [0.0076948243079641545,-0.1327564860146572] Loss: 22.859368106457364\n",
      "Iteracion: 7443 Gradiente: [0.007690733503181946,-0.13268590859176294] Loss: 22.859350427663003\n",
      "Iteracion: 7444 Gradiente: [0.007686644873165467,-0.132615368689995] Loss: 22.859332767660835\n",
      "Iteracion: 7445 Gradiente: [0.007682558416757956,-0.1325448662894047] Loss: 22.85931512643088\n",
      "Iteracion: 7446 Gradiente: [0.007678474132938125,-0.13247440137004923] Loss: 22.859297503953158\n",
      "Iteracion: 7447 Gradiente: [0.007674392020390049,-0.13240397391201064] Loss: 22.85927990020776\n",
      "Iteracion: 7448 Gradiente: [0.007670312078088651,-0.13233358389536665] Loss: 22.859262315174785\n",
      "Iteracion: 7449 Gradiente: [0.007666234304700955,-0.13226323130022308] Loss: 22.859244748834314\n",
      "Iteracion: 7450 Gradiente: [0.007662158699257778,-0.1321929161066734] Loss: 22.85922720116649\n",
      "Iteracion: 7451 Gradiente: [0.007658085260484881,-0.13212263829484125] Loss: 22.859209672151458\n",
      "Iteracion: 7452 Gradiente: [0.007654013987313609,-0.13205239784484976] Loss: 22.859192161769407\n",
      "Iteracion: 7453 Gradiente: [0.007649944878571091,-0.1319821947368369] Loss: 22.85917467000052\n",
      "Iteracion: 7454 Gradiente: [0.007645877933022878,-0.13191202895095464] Loss: 22.859157196825\n",
      "Iteracion: 7455 Gradiente: [0.007641813149642947,-0.13184190046735392] Loss: 22.859139742223118\n",
      "Iteracion: 7456 Gradiente: [0.007637750527214849,-0.1317718092662093] Loss: 22.859122306175085\n",
      "Iteracion: 7457 Gradiente: [0.007633690064639609,-0.1317017553276957] Loss: 22.859104888661207\n",
      "Iteracion: 7458 Gradiente: [0.007629631760610778,-0.1316317386320146] Loss: 22.859087489661782\n",
      "Iteracion: 7459 Gradiente: [0.007625575614180965,-0.1315617591593527] Loss: 22.85907010915712\n",
      "Iteracion: 7460 Gradiente: [0.007621521624069298,-0.13149181688992873] Loss: 22.859052747127564\n",
      "Iteracion: 7461 Gradiente: [0.007617469789264911,-0.13142191180395885] Loss: 22.85903540355347\n",
      "Iteracion: 7462 Gradiente: [0.007613420108597779,-0.1313520438816745] Loss: 22.85901807841523\n",
      "Iteracion: 7463 Gradiente: [0.007609372580733975,-0.13128221310332813] Loss: 22.859000771693243\n",
      "Iteracion: 7464 Gradiente: [0.007605327204723266,-0.13121241944916448] Loss: 22.858983483367947\n",
      "Iteracion: 7465 Gradiente: [0.007601283979321731,-0.1311426628994531] Loss: 22.858966213419773\n",
      "Iteracion: 7466 Gradiente: [0.007597242903418078,-0.1310729434344632] Loss: 22.85894896182918\n",
      "Iteracion: 7467 Gradiente: [0.007593203975846071,-0.13100326103448315] Loss: 22.858931728576657\n",
      "Iteracion: 7468 Gradiente: [0.007589167195548422,-0.1309336156798036] Loss: 22.85891451364273\n",
      "Iteracion: 7469 Gradiente: [0.007585132561345631,-0.1308640073507308] Loss: 22.858897317007898\n",
      "Iteracion: 7470 Gradiente: [0.007581100072029775,-0.13079443602758425] Loss: 22.858880138652726\n",
      "Iteracion: 7471 Gradiente: [0.00757706972658904,-0.13072490169068457] Loss: 22.858862978557777\n",
      "Iteracion: 7472 Gradiente: [0.007573041523734029,-0.13065540432037656] Loss: 22.858845836703644\n",
      "Iteracion: 7473 Gradiente: [0.007569015462431139,-0.13058594389700198] Loss: 22.858828713070952\n",
      "Iteracion: 7474 Gradiente: [0.007564991541490447,-0.1305165204009216] Loss: 22.85881160764031\n",
      "Iteracion: 7475 Gradiente: [0.007560969759739086,-0.13044713381250558] Loss: 22.858794520392355\n",
      "Iteracion: 7476 Gradiente: [0.007556950116120713,-0.1303777841121279] Loss: 22.858777451307812\n",
      "Iteracion: 7477 Gradiente: [0.007552932609464354,-0.13030847128017933] Loss: 22.858760400367334\n",
      "Iteracion: 7478 Gradiente: [0.0075489172386416685,-0.13023919529705938] Loss: 22.858743367551646\n",
      "Iteracion: 7479 Gradiente: [0.007544904002499683,-0.13016995614317953] Loss: 22.858726352841472\n",
      "Iteracion: 7480 Gradiente: [0.007540892899990581,-0.13010075379895505] Loss: 22.858709356217574\n",
      "Iteracion: 7481 Gradiente: [0.007536883929941496,-0.13003158824481933] Loss: 22.858692377660734\n",
      "Iteracion: 7482 Gradiente: [0.007532877091099029,-0.12996245946121937] Loss: 22.85867541715174\n",
      "Iteracion: 7483 Gradiente: [0.007528872382329155,-0.1298933674286083] Loss: 22.8586584746714\n",
      "Iteracion: 7484 Gradiente: [0.00752486980265985,-0.12982431212743759] Loss: 22.858641550200566\n",
      "Iteracion: 7485 Gradiente: [0.007520869350941931,-0.12975529353818172] Loss: 22.858624643720077\n",
      "Iteracion: 7486 Gradiente: [0.0075168710259390535,-0.12968631164132974] Loss: 22.858607755210823\n",
      "Iteracion: 7487 Gradiente: [0.007512874826628982,-0.12961736641736638] Loss: 22.858590884653676\n",
      "Iteracion: 7488 Gradiente: [0.007508880751674951,-0.12954845784681007] Loss: 22.858574032029587\n",
      "Iteracion: 7489 Gradiente: [0.007504888800238518,-0.1294795859101569] Loss: 22.85855719731948\n",
      "Iteracion: 7490 Gradiente: [0.007500898970994286,-0.12941075058794194] Loss: 22.858540380504316\n",
      "Iteracion: 7491 Gradiente: [0.007496911262820542,-0.12934195186069933] Loss: 22.858523581565073\n",
      "Iteracion: 7492 Gradiente: [0.007492925674621158,-0.12927318970897292] Loss: 22.858506800482736\n",
      "Iteracion: 7493 Gradiente: [0.007488942205382424,-0.12920446411331146] Loss: 22.85849003723833\n",
      "Iteracion: 7494 Gradiente: [0.007484960853795049,-0.12913577505429039] Loss: 22.858473291812906\n",
      "Iteracion: 7495 Gradiente: [0.00748098161888322,-0.12906712251247765] Loss: 22.8584565641875\n",
      "Iteracion: 7496 Gradiente: [0.007477004499482594,-0.12899850646846195] Loss: 22.858439854343228\n",
      "Iteracion: 7497 Gradiente: [0.007473029494356827,-0.1289299269028465] Loss: 22.85842316226115\n",
      "Iteracion: 7498 Gradiente: [0.007469056602516844,-0.12886138379623066] Loss: 22.858406487922405\n",
      "Iteracion: 7499 Gradiente: [0.007465085822753774,-0.12879287712923404] Loss: 22.858389831308116\n",
      "Iteracion: 7500 Gradiente: [0.0074611171539572755,-0.1287244068824876] Loss: 22.85837319239947\n",
      "Iteracion: 7501 Gradiente: [0.007457150595138273,-0.1286559730366194] Loss: 22.85835657117762\n",
      "Iteracion: 7502 Gradiente: [0.007453186144924947,-0.1285875755722928] Loss: 22.85833996762379\n",
      "Iteracion: 7503 Gradiente: [0.007449223802392642,-0.12851921447015316] Loss: 22.858323381719167\n",
      "Iteracion: 7504 Gradiente: [0.007445263566331543,-0.12845088971087576] Loss: 22.858306813445022\n",
      "Iteracion: 7505 Gradiente: [0.007441305435744046,-0.1283826012751329] Loss: 22.858290262782592\n",
      "Iteracion: 7506 Gradiente: [0.007437349409331281,-0.12831434914362252] Loss: 22.858273729713165\n",
      "Iteracion: 7507 Gradiente: [0.0074333954861638555,-0.12824613329703544] Loss: 22.858257214218042\n",
      "Iteracion: 7508 Gradiente: [0.007429443664916372,-0.1281779537160927] Loss: 22.858240716278548\n",
      "Iteracion: 7509 Gradiente: [0.007425493944600703,-0.12810981038150615] Loss: 22.858224235876005\n",
      "Iteracion: 7510 Gradiente: [0.0074215463240742945,-0.12804170327400813] Loss: 22.85820777299177\n",
      "Iteracion: 7511 Gradiente: [0.007417600802247648,-0.12797363237433784] Loss: 22.85819132760725\n",
      "Iteracion: 7512 Gradiente: [0.00741365737798579,-0.1279055976632473] Loss: 22.858174899703805\n",
      "Iteracion: 7513 Gradiente: [0.007409716050190696,-0.12783759912149611] Loss: 22.85815848926288\n",
      "Iteracion: 7514 Gradiente: [0.007405776817671494,-0.12776963672985991] Loss: 22.85814209626591\n",
      "Iteracion: 7515 Gradiente: [0.007401839679456164,-0.12770171046911163] Loss: 22.858125720694353\n",
      "Iteracion: 7516 Gradiente: [0.0073979046342780444,-0.12763382032005322] Loss: 22.858109362529664\n",
      "Iteracion: 7517 Gradiente: [0.007393971681141428,-0.12756596626347944] Loss: 22.858093021753383\n",
      "Iteracion: 7518 Gradiente: [0.00739004081875881,-0.12749814828021055] Loss: 22.858076698346974\n",
      "Iteracion: 7519 Gradiente: [0.007386112046220698,-0.12743036635105914] Loss: 22.858060392292007\n",
      "Iteracion: 7520 Gradiente: [0.007382185362379801,-0.12736262045686028] Loss: 22.85804410357003\n",
      "Iteracion: 7521 Gradiente: [0.007378260766100197,-0.12729491057845693] Loss: 22.858027832162623\n",
      "Iteracion: 7522 Gradiente: [0.007374338256152176,-0.1272272366967087] Loss: 22.858011578051375\n",
      "Iteracion: 7523 Gradiente: [0.007370417831507817,-0.12715959879247596] Loss: 22.857995341217904\n",
      "Iteracion: 7524 Gradiente: [0.007366499491135414,-0.12709199684662498] Loss: 22.857979121643837\n",
      "Iteracion: 7525 Gradiente: [0.0073625832339577835,-0.12702443084003884] Loss: 22.857962919310822\n",
      "Iteracion: 7526 Gradiente: [0.007358669058706369,-0.12695690075362076] Loss: 22.857946734200553\n",
      "Iteracion: 7527 Gradiente: [0.0073547569643627264,-0.12688940656826755] Loss: 22.85793056629472\n",
      "Iteracion: 7528 Gradiente: [0.007350846949816514,-0.12682194826489532] Loss: 22.857914415575017\n",
      "Iteracion: 7529 Gradiente: [0.007346939014028445,-0.1267545258244231] Loss: 22.8578982820232\n",
      "Iteracion: 7530 Gradiente: [0.007343033155624804,-0.12668713922780117] Loss: 22.85788216562099\n",
      "Iteracion: 7531 Gradiente: [0.007339129373924417,-0.12661978845594923] Loss: 22.857866066350184\n",
      "Iteracion: 7532 Gradiente: [0.007335227667370722,-0.12655247348984738] Loss: 22.857849984192544\n",
      "Iteracion: 7533 Gradiente: [0.007331328035208647,-0.12648519431044142] Loss: 22.85783391912992\n",
      "Iteracion: 7534 Gradiente: [0.007327430476273851,-0.12641795089870805] Loss: 22.8578178711441\n",
      "Iteracion: 7535 Gradiente: [0.007323534989252304,-0.12635074323564482] Loss: 22.857801840216954\n",
      "Iteracion: 7536 Gradiente: [0.007319641573185247,-0.12628357130223858] Loss: 22.857785826330336\n",
      "Iteracion: 7537 Gradiente: [0.007315750227108234,-0.12621643507948796] Loss: 22.857769829466147\n",
      "Iteracion: 7538 Gradiente: [0.007311860949727134,-0.12614933454841631] Loss: 22.85775384960628\n",
      "Iteracion: 7539 Gradiente: [0.007307973739981813,-0.12608226969004807] Loss: 22.857737886732664\n",
      "Iteracion: 7540 Gradiente: [0.007304088596851935,-0.1260152404854144] Loss: 22.857721940827258\n",
      "Iteracion: 7541 Gradiente: [0.007300205519197789,-0.1259482469155613] Loss: 22.85770601187199\n",
      "Iteracion: 7542 Gradiente: [0.007296324505869241,-0.12588128896154924] Loss: 22.857690099848877\n",
      "Iteracion: 7543 Gradiente: [0.007292445555793847,-0.12581436660444087] Loss: 22.857674204739897\n",
      "Iteracion: 7544 Gradiente: [0.007288568667878318,-0.1257474798253123] Loss: 22.857658326527087\n",
      "Iteracion: 7545 Gradiente: [0.007284693841072946,-0.12568062860524723] Loss: 22.857642465192484\n",
      "Iteracion: 7546 Gradiente: [0.00728082107420865,-0.12561381292534446] Loss: 22.857626620718136\n",
      "Iteracion: 7547 Gradiente: [0.007276950366195934,-0.12554703276671] Loss: 22.857610793086135\n",
      "Iteracion: 7548 Gradiente: [0.0072730817159794015,-0.12548028811045692] Loss: 22.857594982278563\n",
      "Iteracion: 7549 Gradiente: [0.007269215122533031,-0.12541357893770808] Loss: 22.85757918827754\n",
      "Iteracion: 7550 Gradiente: [0.007265350584652689,-0.1253469052296045] Loss: 22.85756341106522\n",
      "Iteracion: 7551 Gradiente: [0.007261488101295299,-0.12528026696728886] Loss: 22.857547650623715\n",
      "Iteracion: 7552 Gradiente: [0.007257627671349572,-0.12521366413192006] Loss: 22.857531906935247\n",
      "Iteracion: 7553 Gradiente: [0.007253769293750641,-0.12514709670466134] Loss: 22.85751617998199\n",
      "Iteracion: 7554 Gradiente: [0.007249912967373954,-0.12508056466668951] Loss: 22.857500469746125\n",
      "Iteracion: 7555 Gradiente: [0.007246058691108222,-0.1250140679991946] Loss: 22.85748477620992\n",
      "Iteracion: 7556 Gradiente: [0.007242206463875315,-0.12494760668337032] Loss: 22.857469099355622\n",
      "Iteracion: 7557 Gradiente: [0.007238356284616051,-0.1248811807004202] Loss: 22.857453439165486\n",
      "Iteracion: 7558 Gradiente: [0.007234508152182192,-0.12481479003156531] Loss: 22.85743779562178\n",
      "Iteracion: 7559 Gradiente: [0.007230662065671823,-0.12474843465801845] Loss: 22.857422168706837\n",
      "Iteracion: 7560 Gradiente: [0.007226818023698911,-0.1246821145610351] Loss: 22.857406558402975\n",
      "Iteracion: 7561 Gradiente: [0.007222976025404174,-0.12461582972184738] Loss: 22.857390964692545\n",
      "Iteracion: 7562 Gradiente: [0.0072191360695436895,-0.12454958012171934] Loss: 22.857375387557873\n",
      "Iteracion: 7563 Gradiente: [0.007215298155244909,-0.12448336574190554] Loss: 22.85735982698139\n",
      "Iteracion: 7564 Gradiente: [0.007211462281314122,-0.12441718656368757] Loss: 22.857344282945455\n",
      "Iteracion: 7565 Gradiente: [0.007207628446627723,-0.12435104256835353] Loss: 22.85732875543251\n",
      "Iteracion: 7566 Gradiente: [0.0072037966501909525,-0.12428493373719303] Loss: 22.857313244424958\n",
      "Iteracion: 7567 Gradiente: [0.0071999668907769395,-0.12421886005152202] Loss: 22.857297749905307\n",
      "Iteracion: 7568 Gradiente: [0.00719613916734545,-0.1241528214926518] Loss: 22.857282271855986\n",
      "Iteracion: 7569 Gradiente: [0.007192313478806985,-0.12408681804190978] Loss: 22.857266810259503\n",
      "Iteracion: 7570 Gradiente: [0.007188489824217944,-0.12402084968062184] Loss: 22.85725136509837\n",
      "Iteracion: 7571 Gradiente: [0.007184668202338192,-0.1239549163901432] Loss: 22.857235936355117\n",
      "Iteracion: 7572 Gradiente: [0.00718084861219855,-0.1238890181518227] Loss: 22.857220524012295\n",
      "Iteracion: 7573 Gradiente: [0.007177031052705729,-0.12382315494702709] Loss: 22.857205128052446\n",
      "Iteracion: 7574 Gradiente: [0.007173215522654649,-0.12375732675713742] Loss: 22.857189748458197\n",
      "Iteracion: 7575 Gradiente: [0.007169402021115919,-0.12369153356353095] Loss: 22.857174385212126\n",
      "Iteracion: 7576 Gradiente: [0.007165590546886354,-0.12362577534760912] Loss: 22.857159038296853\n",
      "Iteracion: 7577 Gradiente: [0.00716178109905646,-0.12356005209076777] Loss: 22.857143707695016\n",
      "Iteracion: 7578 Gradiente: [0.007157973676404102,-0.12349436377442966] Loss: 22.857128393389303\n",
      "Iteracion: 7579 Gradiente: [0.0071541682778805205,-0.12342871038001621] Loss: 22.857113095362365\n",
      "Iteracion: 7580 Gradiente: [0.007150364902454953,-0.1233630918889619] Loss: 22.857097813596898\n",
      "Iteracion: 7581 Gradiente: [0.007146563549048324,-0.12329750828270912] Loss: 22.857082548075628\n",
      "Iteracion: 7582 Gradiente: [0.007142764216537026,-0.1232319595427164] Loss: 22.857067298781292\n",
      "Iteracion: 7583 Gradiente: [0.007138966903812616,-0.12316644565044896] Loss: 22.857052065696617\n",
      "Iteracion: 7584 Gradiente: [0.007135171609831067,-0.12310096658737849] Loss: 22.857036848804388\n",
      "Iteracion: 7585 Gradiente: [0.007131378333589093,-0.12303552233498631] Loss: 22.85702164808741\n",
      "Iteracion: 7586 Gradiente: [0.007127587073965932,-0.1229701128747666] Loss: 22.857006463528453\n",
      "Iteracion: 7587 Gradiente: [0.007123797829916611,-0.12290473818822296] Loss: 22.856991295110372\n",
      "Iteracion: 7588 Gradiente: [0.00712001060032037,-0.12283939825687101] Loss: 22.85697614281599\n",
      "Iteracion: 7589 Gradiente: [0.007116225384158762,-0.12277409306223058] Loss: 22.856961006628165\n",
      "Iteracion: 7590 Gradiente: [0.007112442180357448,-0.1227088225858342] Loss: 22.8569458865298\n",
      "Iteracion: 7591 Gradiente: [0.007108660987762505,-0.12264358680923024] Loss: 22.856930782503774\n",
      "Iteracion: 7592 Gradiente: [0.007104881805320437,-0.12257838571396935] Loss: 22.856915694533008\n",
      "Iteracion: 7593 Gradiente: [0.007101104632031744,-0.12251321928161213] Loss: 22.856900622600435\n",
      "Iteracion: 7594 Gradiente: [0.007097329466835352,-0.12244808749372851] Loss: 22.85688556668901\n",
      "Iteracion: 7595 Gradiente: [0.007093556308678709,-0.12238299033189967] Loss: 22.856870526781687\n",
      "Iteracion: 7596 Gradiente: [0.00708978515646379,-0.12231792777771915] Loss: 22.856855502861478\n",
      "Iteracion: 7597 Gradiente: [0.0070860160090262525,-0.12225289981279465] Loss: 22.856840494911374\n",
      "Iteracion: 7598 Gradiente: [0.007082248865350494,-0.12218790641873459] Loss: 22.8568255029144\n",
      "Iteracion: 7599 Gradiente: [0.007078483724501439,-0.12212294757715109] Loss: 22.856810526853607\n",
      "Iteracion: 7600 Gradiente: [0.007074720585239902,-0.1220580232696862] Loss: 22.85679556671204\n",
      "Iteracion: 7601 Gradiente: [0.0070709594466838626,-0.12199313347797087] Loss: 22.856780622472787\n",
      "Iteracion: 7602 Gradiente: [0.007067200307583714,-0.12192827818366456] Loss: 22.85676569411895\n",
      "Iteracion: 7603 Gradiente: [0.007063443166938062,-0.12186345736842472] Loss: 22.85675078163363\n",
      "Iteracion: 7604 Gradiente: [0.0070596880237000425,-0.12179867101392053] Loss: 22.856735884999953\n",
      "Iteracion: 7605 Gradiente: [0.007055934876905212,-0.12173391910182568] Loss: 22.856721004201102\n",
      "Iteracion: 7606 Gradiente: [0.007052183725374069,-0.12166920161383578] Loss: 22.85670613922023\n",
      "Iteracion: 7607 Gradiente: [0.007048434567961218,-0.12160451853165467] Loss: 22.856691290040487\n",
      "Iteracion: 7608 Gradiente: [0.007044687403818747,-0.12153986983698045] Loss: 22.85667645664512\n",
      "Iteracion: 7609 Gradiente: [0.007040942231766204,-0.12147525551153689] Loss: 22.856661639017336\n",
      "Iteracion: 7610 Gradiente: [0.007037199050802201,-0.12141067553705073] Loss: 22.856646837140392\n",
      "Iteracion: 7611 Gradiente: [0.007033457859743446,-0.12134612989526516] Loss: 22.856632050997515\n",
      "Iteracion: 7612 Gradiente: [0.007029718657657706,-0.1212816185679222] Loss: 22.856617280571992\n",
      "Iteracion: 7613 Gradiente: [0.0070259814434981156,-0.1212171415367789] Loss: 22.85660252584712\n",
      "Iteracion: 7614 Gradiente: [0.00702224621608328,-0.1211526987836064] Loss: 22.85658778680621\n",
      "Iteracion: 7615 Gradiente: [0.007018512974465807,-0.12108829029017892] Loss: 22.856573063432588\n",
      "Iteracion: 7616 Gradiente: [0.0070147817175391465,-0.12102391603828337] Loss: 22.85655835570958\n",
      "Iteracion: 7617 Gradiente: [0.007011052444290537,-0.12095957600971528] Loss: 22.856543663620595\n",
      "Iteracion: 7618 Gradiente: [0.007007325153554689,-0.12089527018628524] Loss: 22.856528987148977\n",
      "Iteracion: 7619 Gradiente: [0.007003599844436318,-0.12083099854980155] Loss: 22.85651432627814\n",
      "Iteracion: 7620 Gradiente: [0.006999876515745503,-0.12076676108209708] Loss: 22.856499680991497\n",
      "Iteracion: 7621 Gradiente: [0.006996155166511168,-0.12070255776500018] Loss: 22.85648505127248\n",
      "Iteracion: 7622 Gradiente: [0.006992435795737606,-0.12063838858035184] Loss: 22.85647043710455\n",
      "Iteracion: 7623 Gradiente: [0.006988718402200789,-0.1205742535100183] Loss: 22.85645583847116\n",
      "Iteracion: 7624 Gradiente: [0.006985002984983642,-0.12051015253585456] Loss: 22.856441255355815\n",
      "Iteracion: 7625 Gradiente: [0.006981289543015616,-0.12044608563973422] Loss: 22.856426687742005\n",
      "Iteracion: 7626 Gradiente: [0.006977578075278264,-0.12038205280353911] Loss: 22.856412135613255\n",
      "Iteracion: 7627 Gradiente: [0.006973868580552297,-0.1203180540091721] Loss: 22.856397598953134\n",
      "Iteracion: 7628 Gradiente: [0.006970161057902639,-0.12025408923853102] Loss: 22.856383077745154\n",
      "Iteracion: 7629 Gradiente: [0.006966455506360111,-0.12019015847351835] Loss: 22.856368571972908\n",
      "Iteracion: 7630 Gradiente: [0.006962751924712051,-0.12012626169606998] Loss: 22.85635408162\n",
      "Iteracion: 7631 Gradiente: [0.006959050312045178,-0.12006239888810685] Loss: 22.856339606670016\n",
      "Iteracion: 7632 Gradiente: [0.006955350667360942,-0.11999857003156648] Loss: 22.856325147106595\n",
      "Iteracion: 7633 Gradiente: [0.00695165298943247,-0.11993477510841152] Loss: 22.8563107029134\n",
      "Iteracion: 7634 Gradiente: [0.006947957277329427,-0.11987101410059395] Loss: 22.856296274074058\n",
      "Iteracion: 7635 Gradiente: [0.006944263529994525,-0.11980728699008504] Loss: 22.856281860572288\n",
      "Iteracion: 7636 Gradiente: [0.006940571746324053,-0.11974359375886555] Loss: 22.85626746239175\n",
      "Iteracion: 7637 Gradiente: [0.006936881925434098,-0.11967993438891682] Loss: 22.856253079516176\n",
      "Iteracion: 7638 Gradiente: [0.006933194066114841,-0.11961630886224638] Loss: 22.856238711929283\n",
      "Iteracion: 7639 Gradiente: [0.006929508167225625,-0.11955271716086753] Loss: 22.856224359614835\n",
      "Iteracion: 7640 Gradiente: [0.006925824227975378,-0.11948915926678297] Loss: 22.856210022556603\n",
      "Iteracion: 7641 Gradiente: [0.006922142247254707,-0.11942563516202362] Loss: 22.85619570073834\n",
      "Iteracion: 7642 Gradiente: [0.006918462224032851,-0.11936214482862581] Loss: 22.856181394143885\n",
      "Iteracion: 7643 Gradiente: [0.00691478415716252,-0.11929868824864147] Loss: 22.856167102757016\n",
      "Iteracion: 7644 Gradiente: [0.006911108045586426,-0.11923526540412759] Loss: 22.856152826561598\n",
      "Iteracion: 7645 Gradiente: [0.006907433888352443,-0.11917187627714405] Loss: 22.856138565541485\n",
      "Iteracion: 7646 Gradiente: [0.006903761684441179,-0.11910852084976492] Loss: 22.85612431968052\n",
      "Iteracion: 7647 Gradiente: [0.006900091432794397,-0.11904519910407482] Loss: 22.856110088962602\n",
      "Iteracion: 7648 Gradiente: [0.006896423132406918,-0.11898191102216629] Loss: 22.856095873371636\n",
      "Iteracion: 7649 Gradiente: [0.006892756782283034,-0.11891865658613969] Loss: 22.85608167289155\n",
      "Iteracion: 7650 Gradiente: [0.006889092381217665,-0.11885543577811776] Loss: 22.856067487506266\n",
      "Iteracion: 7651 Gradiente: [0.006885429928242578,-0.11879224858021958] Loss: 22.856053317199734\n",
      "Iteracion: 7652 Gradiente: [0.006881769422362064,-0.1187290949745736] Loss: 22.85603916195595\n",
      "Iteracion: 7653 Gradiente: [0.006878110862487574,-0.11866597494332408] Loss: 22.8560250217589\n",
      "Iteracion: 7654 Gradiente: [0.006874454247693507,-0.11860288846861736] Loss: 22.85601089659256\n",
      "Iteracion: 7655 Gradiente: [0.006870799576727412,-0.11853983553262389] Loss: 22.85599678644097\n",
      "Iteracion: 7656 Gradiente: [0.006867146848896747,-0.11847681611749519] Loss: 22.85598269128819\n",
      "Iteracion: 7657 Gradiente: [0.0068634960627936914,-0.11841383020543196] Loss: 22.85596861111824\n",
      "Iteracion: 7658 Gradiente: [0.006859847217590224,-0.11835087777861114] Loss: 22.855954545915225\n",
      "Iteracion: 7659 Gradiente: [0.006856200312240427,-0.11828795881923249] Loss: 22.85594049566322\n",
      "Iteracion: 7660 Gradiente: [0.006852555345742909,-0.1182250733095002] Loss: 22.85592646034631\n",
      "Iteracion: 7661 Gradiente: [0.006848912316861326,-0.11816222123164609] Loss: 22.85591243994867\n",
      "Iteracion: 7662 Gradiente: [0.006845271224956188,-0.1180994025678712] Loss: 22.855898434454414\n",
      "Iteracion: 7663 Gradiente: [0.006841632068639569,-0.1180366173004335] Loss: 22.85588444384769\n",
      "Iteracion: 7664 Gradiente: [0.006837994846997238,-0.11797386541157179] Loss: 22.85587046811269\n",
      "Iteracion: 7665 Gradiente: [0.006834359559012644,-0.11791114688354204] Loss: 22.85585650723361\n",
      "Iteracion: 7666 Gradiente: [0.006830726203753556,-0.11784846169860058] Loss: 22.855842561194645\n",
      "Iteracion: 7667 Gradiente: [0.006827094779975103,-0.11778580983903382] Loss: 22.855828629980003\n",
      "Iteracion: 7668 Gradiente: [0.006823465286853055,-0.1177231912871156] Loss: 22.855814713573956\n",
      "Iteracion: 7669 Gradiente: [0.006819837723334861,-0.11766060602513614] Loss: 22.855800811960748\n",
      "Iteracion: 7670 Gradiente: [0.006816212088184178,-0.11759805403541464] Loss: 22.85578692512467\n",
      "Iteracion: 7671 Gradiente: [0.006812588380688567,-0.11753553530024093] Loss: 22.85577305305002\n",
      "Iteracion: 7672 Gradiente: [0.006808966599577578,-0.11747304980194974] Loss: 22.85575919572106\n",
      "Iteracion: 7673 Gradiente: [0.006805346743847925,-0.11741059752287294] Loss: 22.85574535312215\n",
      "Iteracion: 7674 Gradiente: [0.006801728812644114,-0.11734817844533842] Loss: 22.855731525237637\n",
      "Iteracion: 7675 Gradiente: [0.006798112804844436,-0.11728579255170123] Loss: 22.855717712051863\n",
      "Iteracion: 7676 Gradiente: [0.006794498719435183,-0.11722343982432039] Loss: 22.855703913549235\n",
      "Iteracion: 7677 Gradiente: [0.006790886555333486,-0.11716112024556485] Loss: 22.855690129714098\n",
      "Iteracion: 7678 Gradiente: [0.006787276311619432,-0.11709883379780889] Loss: 22.855676360530904\n",
      "Iteracion: 7679 Gradiente: [0.006783667987160887,-0.1170365804634424] Loss: 22.855662605984033\n",
      "Iteracion: 7680 Gradiente: [0.006780061581077727,-0.1169743602248566] Loss: 22.855648866057983\n",
      "Iteracion: 7681 Gradiente: [0.00677645709221224,-0.11691217306446132] Loss: 22.855635140737157\n",
      "Iteracion: 7682 Gradiente: [0.0067728545196824065,-0.1168500189646655] Loss: 22.855621430006067\n",
      "Iteracion: 7683 Gradiente: [0.006769253862275567,-0.11678789790790348] Loss: 22.855607733849197\n",
      "Iteracion: 7684 Gradiente: [0.006765655119103068,-0.11672580987660137] Loss: 22.85559405225106\n",
      "Iteracion: 7685 Gradiente: [0.0067620582892052045,-0.11666375485319733] Loss: 22.855580385196152\n",
      "Iteracion: 7686 Gradiente: [0.006758463371458371,-0.11660173282015146] Loss: 22.85556673266904\n",
      "Iteracion: 7687 Gradiente: [0.006754870364919914,-0.11653974375991917] Loss: 22.855553094654265\n",
      "Iteracion: 7688 Gradiente: [0.0067512792685079145,-0.11647778765497657] Loss: 22.855539471136442\n",
      "Iteracion: 7689 Gradiente: [0.006747690081189717,-0.11641586448780276] Loss: 22.855525862100098\n",
      "Iteracion: 7690 Gradiente: [0.006744102802028351,-0.11635397424088355] Loss: 22.855512267529864\n",
      "Iteracion: 7691 Gradiente: [0.006740517429993057,-0.11629211689671783] Loss: 22.855498687410385\n",
      "Iteracion: 7692 Gradiente: [0.006736933963966863,-0.11623029243782051] Loss: 22.855485121726293\n",
      "Iteracion: 7693 Gradiente: [0.006733352403067746,-0.11616850084670022] Loss: 22.85547157046223\n",
      "Iteracion: 7694 Gradiente: [0.006729772746328422,-0.11610674210588078] Loss: 22.85545803360286\n",
      "Iteracion: 7695 Gradiente: [0.006726194992498336,-0.11604501619791302] Loss: 22.855444511132884\n",
      "Iteracion: 7696 Gradiente: [0.006722619140793995,-0.11598332310532745] Loss: 22.855431003037022\n",
      "Iteracion: 7697 Gradiente: [0.006719045190161903,-0.11592166281068093] Loss: 22.85541750929997\n",
      "Iteracion: 7698 Gradiente: [0.006715473139450978,-0.11586003529654516] Loss: 22.85540402990647\n",
      "Iteracion: 7699 Gradiente: [0.006711902987866362,-0.11579844054548095] Loss: 22.85539056484129\n",
      "Iteracion: 7700 Gradiente: [0.0067083347341660255,-0.11573687854008483] Loss: 22.855377114089176\n",
      "Iteracion: 7701 Gradiente: [0.00670476837755037,-0.11567534926293478] Loss: 22.855363677634944\n",
      "Iteracion: 7702 Gradiente: [0.006701203916820001,-0.11561385269664216] Loss: 22.85535025546336\n",
      "Iteracion: 7703 Gradiente: [0.006697641351101425,-0.11555238882381053] Loss: 22.855336847559265\n",
      "Iteracion: 7704 Gradiente: [0.00669408067943588,-0.11549095762705572] Loss: 22.85532345390749\n",
      "Iteracion: 7705 Gradiente: [0.006690521900676079,-0.11542955908901315] Loss: 22.85531007449288\n",
      "Iteracion: 7706 Gradiente: [0.006686965013880316,-0.1153681931923168] Loss: 22.855296709300312\n",
      "Iteracion: 7707 Gradiente: [0.006683410017983723,-0.11530685991961866] Loss: 22.85528335831464\n",
      "Iteracion: 7708 Gradiente: [0.006679856912047436,-0.11524555925357068] Loss: 22.855270021520784\n",
      "Iteracion: 7709 Gradiente: [0.006676305695083329,-0.11518429117683494] Loss: 22.855256698903677\n",
      "Iteracion: 7710 Gradiente: [0.006672756366041691,-0.11512305567209159] Loss: 22.855243390448212\n",
      "Iteracion: 7711 Gradiente: [0.006669208923903132,-0.11506185272202411] Loss: 22.855230096139362\n",
      "Iteracion: 7712 Gradiente: [0.00666566336774963,-0.11500068230931919] Loss: 22.855216815962077\n",
      "Iteracion: 7713 Gradiente: [0.00666211969646137,-0.11493954441668801] Loss: 22.855203549901322\n",
      "Iteracion: 7714 Gradiente: [0.006658577909273807,-0.11487843902682625] Loss: 22.85519029794214\n",
      "Iteracion: 7715 Gradiente: [0.006655038004825542,-0.11481736612247602] Loss: 22.855177060069487\n",
      "Iteracion: 7716 Gradiente: [0.006651499982340662,-0.11475632568635348] Loss: 22.85516383626842\n",
      "Iteracion: 7717 Gradiente: [0.006647963840769459,-0.11469531770120073] Loss: 22.855150626523972\n",
      "Iteracion: 7718 Gradiente: [0.006644429579200543,-0.11463434214976141] Loss: 22.855137430821205\n",
      "Iteracion: 7719 Gradiente: [0.006640897196451571,-0.11457339901480262] Loss: 22.85512424914517\n",
      "Iteracion: 7720 Gradiente: [0.006637366691716314,-0.11451248827907866] Loss: 22.85511108148099\n",
      "Iteracion: 7721 Gradiente: [0.006633838063795376,-0.11445160992538028] Loss: 22.855097927813762\n",
      "Iteracion: 7722 Gradiente: [0.006630311311824736,-0.11439076393647957] Loss: 22.855084788128604\n",
      "Iteracion: 7723 Gradiente: [0.006626786434882585,-0.11432995029516987] Loss: 22.855071662410644\n",
      "Iteracion: 7724 Gradiente: [0.006623263431752472,-0.11426916898426545] Loss: 22.85505855064504\n",
      "Iteracion: 7725 Gradiente: [0.006619742301626275,-0.1142084199865683] Loss: 22.85504545281697\n",
      "Iteracion: 7726 Gradiente: [0.006616223043411651,-0.11414770328490415] Loss: 22.855032368911623\n",
      "Iteracion: 7727 Gradiente: [0.006612705656201949,-0.11408701886209786] Loss: 22.85501929891418\n",
      "Iteracion: 7728 Gradiente: [0.006609190138832825,-0.11402636670100132] Loss: 22.855006242809857\n",
      "Iteracion: 7729 Gradiente: [0.006605676490546368,-0.1139657467844484] Loss: 22.85499320058391\n",
      "Iteracion: 7730 Gradiente: [0.006602164710202866,-0.11390515909530308] Loss: 22.854980172221577\n",
      "Iteracion: 7731 Gradiente: [0.006598654796742191,-0.11384460361643983] Loss: 22.854967157708096\n",
      "Iteracion: 7732 Gradiente: [0.0065951467492586366,-0.11378408033072643] Loss: 22.854954157028775\n",
      "Iteracion: 7733 Gradiente: [0.006591640566880604,-0.11372358922104388] Loss: 22.85494117016891\n",
      "Iteracion: 7734 Gradiente: [0.006588136248405855,-0.11366313027029638] Loss: 22.8549281971138\n",
      "Iteracion: 7735 Gradiente: [0.006584633792922053,-0.11360270346138357] Loss: 22.85491523784876\n",
      "Iteracion: 7736 Gradiente: [0.006581133199507387,-0.11354230877721463] Loss: 22.854902292359142\n",
      "Iteracion: 7737 Gradiente: [0.006577634467096042,-0.11348194620071451] Loss: 22.854889360630317\n",
      "Iteracion: 7738 Gradiente: [0.006574137594700839,-0.11342161571481336] Loss: 22.854876442647633\n",
      "Iteracion: 7739 Gradiente: [0.006570642581369649,-0.11336131730244929] Loss: 22.8548635383965\n",
      "Iteracion: 7740 Gradiente: [0.006567149426103924,-0.11330105094657199] Loss: 22.854850647862303\n",
      "Iteracion: 7741 Gradiente: [0.006563658127860587,-0.11324081663014228] Loss: 22.854837771030482\n",
      "Iteracion: 7742 Gradiente: [0.0065601686857443536,-0.11318061433612019] Loss: 22.85482490788645\n",
      "Iteracion: 7743 Gradiente: [0.006556681098657199,-0.1131204440474896] Loss: 22.854812058415668\n",
      "Iteracion: 7744 Gradiente: [0.006553195365769208,-0.11306030574722593] Loss: 22.854799222603603\n",
      "Iteracion: 7745 Gradiente: [0.006549711486042042,-0.11300019941832767] Loss: 22.85478640043575\n",
      "Iteracion: 7746 Gradiente: [0.006546229458282937,-0.11294012504380717] Loss: 22.85477359189755\n",
      "Iteracion: 7747 Gradiente: [0.006542749281806929,-0.11288008260666137] Loss: 22.854760796974574\n",
      "Iteracion: 7748 Gradiente: [0.00653927095541178,-0.11282007208992298] Loss: 22.854748015652344\n",
      "Iteracion: 7749 Gradiente: [0.006535794478297893,-0.11276009347661203] Loss: 22.854735247916373\n",
      "Iteracion: 7750 Gradiente: [0.006532319849311344,-0.11270014674977749] Loss: 22.854722493752238\n",
      "Iteracion: 7751 Gradiente: [0.006528847067592854,-0.11264023189245952] Loss: 22.854709753145503\n",
      "Iteracion: 7752 Gradiente: [0.006525376132051975,-0.11258034888772338] Loss: 22.854697026081762\n",
      "Iteracion: 7753 Gradiente: [0.006521907041768789,-0.11252049771862976] Loss: 22.854684312546627\n",
      "Iteracion: 7754 Gradiente: [0.006518439795854647,-0.11246067836824913] Loss: 22.85467161252571\n",
      "Iteracion: 7755 Gradiente: [0.006514974393138573,-0.11240089081967662] Loss: 22.854658926004635\n",
      "Iteracion: 7756 Gradiente: [0.006511510832809601,-0.11234113505599717] Loss: 22.854646252969086\n",
      "Iteracion: 7757 Gradiente: [0.006508049113796233,-0.11228141106031526] Loss: 22.85463359340468\n",
      "Iteracion: 7758 Gradiente: [0.006504589235101814,-0.11222171881574615] Loss: 22.85462094729713\n",
      "Iteracion: 7759 Gradiente: [0.006501131195835797,-0.11216205830540306] Loss: 22.854608314632124\n",
      "Iteracion: 7760 Gradiente: [0.006497674994988264,-0.1121024295124182] Loss: 22.854595695395368\n",
      "Iteracion: 7761 Gradiente: [0.006494220631485821,-0.1120428324199338] Loss: 22.854583089572607\n",
      "Iteracion: 7762 Gradiente: [0.00649076810446445,-0.11198326701109072] Loss: 22.854570497149552\n",
      "Iteracion: 7763 Gradiente: [0.006487317412856441,-0.11192373326905075] Loss: 22.85455791811199\n",
      "Iteracion: 7764 Gradiente: [0.006483868555832828,-0.11186423117696952] Loss: 22.854545352445655\n",
      "Iteracion: 7765 Gradiente: [0.006480421532321164,-0.11180476071802765] Loss: 22.854532800136383\n",
      "Iteracion: 7766 Gradiente: [0.0064769763412944785,-0.11174532187541028] Loss: 22.854520261169924\n",
      "Iteracion: 7767 Gradiente: [0.006473532981853699,-0.11168591463230595] Loss: 22.85450773553213\n",
      "Iteracion: 7768 Gradiente: [0.0064700914530097485,-0.11162653897191509] Loss: 22.854495223208843\n",
      "Iteracion: 7769 Gradiente: [0.00646665175377071,-0.1115671948774492] Loss: 22.85448272418586\n",
      "Iteracion: 7770 Gradiente: [0.006463213883221405,-0.11150788233212244] Loss: 22.854470238449093\n",
      "Iteracion: 7771 Gradiente: [0.0064597778403566505,-0.11144860131916587] Loss: 22.85445776598438\n",
      "Iteracion: 7772 Gradiente: [0.006456343624120109,-0.11138935182181899] Loss: 22.85444530677766\n",
      "Iteracion: 7773 Gradiente: [0.006452911233644916,-0.11133013382332232] Loss: 22.85443286081478\n",
      "Iteracion: 7774 Gradiente: [0.006449480668034842,-0.11127094730692542] Loss: 22.85442042808171\n",
      "Iteracion: 7775 Gradiente: [0.006446051926178598,-0.11121179225590012] Loss: 22.854408008564388\n",
      "Iteracion: 7776 Gradiente: [0.006442625007075738,-0.11115266865351986] Loss: 22.854395602248715\n",
      "Iteracion: 7777 Gradiente: [0.006439199909891613,-0.11109357648305741] Loss: 22.854383209120716\n",
      "Iteracion: 7778 Gradiente: [0.006435776633533881,-0.11103451572780898] Loss: 22.85437082916633\n",
      "Iteracion: 7779 Gradiente: [0.006432355177066521,-0.11097548637107361] Loss: 22.854358462371586\n",
      "Iteracion: 7780 Gradiente: [0.006428935539634987,-0.1109164883961513] Loss: 22.85434610872249\n",
      "Iteracion: 7781 Gradiente: [0.006425517720137464,-0.1108575217863662] Loss: 22.854333768205038\n",
      "Iteracion: 7782 Gradiente: [0.006422101717746879,-0.1107985865250362] Loss: 22.854321440805318\n",
      "Iteracion: 7783 Gradiente: [0.006418687531351944,-0.1107396825955032] Loss: 22.854309126509353\n",
      "Iteracion: 7784 Gradiente: [0.006415275160103799,-0.11068080998110404] Loss: 22.854296825303212\n",
      "Iteracion: 7785 Gradiente: [0.0064118646028210454,-0.11062196866520206] Loss: 22.85428453717302\n",
      "Iteracion: 7786 Gradiente: [0.006408455858801668,-0.11056315863114463] Loss: 22.854272262104818\n",
      "Iteracion: 7787 Gradiente: [0.00640504892694859,-0.11050437986230671] Loss: 22.854260000084764\n",
      "Iteracion: 7788 Gradiente: [0.006401643806313473,-0.11044563234206789] Loss: 22.85424775109898\n",
      "Iteracion: 7789 Gradiente: [0.006398240495985874,-0.1103869160538108] Loss: 22.85423551513359\n",
      "Iteracion: 7790 Gradiente: [0.006394838994973877,-0.11032823098093457] Loss: 22.854223292174797\n",
      "Iteracion: 7791 Gradiente: [0.006391439302276088,-0.11026957710684648] Loss: 22.85421108220873\n",
      "Iteracion: 7792 Gradiente: [0.006388041416963119,-0.11021095441495668] Loss: 22.854198885221617\n",
      "Iteracion: 7793 Gradiente: [0.006384645338073369,-0.11015236288869126] Loss: 22.854186701199637\n",
      "Iteracion: 7794 Gradiente: [0.006381251064673658,-0.11009380251147706] Loss: 22.854174530129004\n",
      "Iteracion: 7795 Gradiente: [0.006377858595737962,-0.11003527326675844] Loss: 22.854162371995972\n",
      "Iteracion: 7796 Gradiente: [0.006374467930361523,-0.109976775137982] Loss: 22.85415022678676\n",
      "Iteracion: 7797 Gradiente: [0.006371079067457685,-0.10991830810861317] Loss: 22.854138094487666\n",
      "Iteracion: 7798 Gradiente: [0.006367692006322537,-0.10985987216210256] Loss: 22.85412597508493\n",
      "Iteracion: 7799 Gradiente: [0.006364306745840054,-0.10980146728193603] Loss: 22.854113868564877\n",
      "Iteracion: 7800 Gradiente: [0.0063609232849965265,-0.10974309345160052] Loss: 22.85410177491379\n",
      "Iteracion: 7801 Gradiente: [0.006357541622914672,-0.1096847506545842] Loss: 22.854089694118\n",
      "Iteracion: 7802 Gradiente: [0.006354161758664153,-0.10962643887438807] Loss: 22.854077626163846\n",
      "Iteracion: 7803 Gradiente: [0.006350783691121365,-0.10956815809453009] Loss: 22.85406557103766\n",
      "Iteracion: 7804 Gradiente: [0.006347407419672398,-0.10950990829851352] Loss: 22.854053528725817\n",
      "Iteracion: 7805 Gradiente: [0.006344032943052487,-0.10945168946988144] Loss: 22.854041499214706\n",
      "Iteracion: 7806 Gradiente: [0.006340660260444982,-0.10939350159216363] Loss: 22.854029482490695\n",
      "Iteracion: 7807 Gradiente: [0.006337289370829543,-0.1093353446489092] Loss: 22.854017478540218\n",
      "Iteracion: 7808 Gradiente: [0.00633392027320383,-0.10927721862367588] Loss: 22.85400548734968\n",
      "Iteracion: 7809 Gradiente: [0.006330552966823196,-0.10921912350001411] Loss: 22.853993508905514\n",
      "Iteracion: 7810 Gradiente: [0.0063271874505943515,-0.10916105926150407] Loss: 22.853981543194198\n",
      "Iteracion: 7811 Gradiente: [0.006323823723512116,-0.10910302589172977] Loss: 22.85396959020217\n",
      "Iteracion: 7812 Gradiente: [0.006320461784666994,-0.10904502337427573] Loss: 22.85395764991593\n",
      "Iteracion: 7813 Gradiente: [0.006317101633163702,-0.10898705169273969] Loss: 22.853945722321946\n",
      "Iteracion: 7814 Gradiente: [0.0063137432680567445,-0.1089291108307273] Loss: 22.853933807406758\n",
      "Iteracion: 7815 Gradiente: [0.006310386688396837,-0.10887120077185344] Loss: 22.85392190515686\n",
      "Iteracion: 7816 Gradiente: [0.0063070318930783745,-0.1088133214997511] Loss: 22.85391001555879\n",
      "Iteracion: 7817 Gradiente: [0.006303678881363339,-0.10875547299804028] Loss: 22.853898138599103\n",
      "Iteracion: 7818 Gradiente: [0.006300327652227603,-0.10869765525036819] Loss: 22.853886274264394\n",
      "Iteracion: 7819 Gradiente: [0.006296978204582615,-0.10863986824039304] Loss: 22.85387442254123\n",
      "Iteracion: 7820 Gradiente: [0.0062936305376932,-0.1085821119517604] Loss: 22.85386258341618\n",
      "Iteracion: 7821 Gradiente: [0.006290284650565543,-0.10852438636814128] Loss: 22.853850756875865\n",
      "Iteracion: 7822 Gradiente: [0.006286940542151834,-0.10846669147321913] Loss: 22.853838942906908\n",
      "Iteracion: 7823 Gradiente: [0.006283598211601316,-0.10840902725067139] Loss: 22.853827141495973\n",
      "Iteracion: 7824 Gradiente: [0.006280257657890805,-0.10835139368419675] Loss: 22.85381535262968\n",
      "Iteracion: 7825 Gradiente: [0.006276918880131651,-0.10829379075749443] Loss: 22.85380357629469\n",
      "Iteracion: 7826 Gradiente: [0.006273581877367936,-0.10823621845427643] Loss: 22.853791812477695\n",
      "Iteracion: 7827 Gradiente: [0.006270246648661744,-0.10817867675826172] Loss: 22.853780061165384\n",
      "Iteracion: 7828 Gradiente: [0.00626691319309316,-0.10812116565317827] Loss: 22.85376832234447\n",
      "Iteracion: 7829 Gradiente: [0.006263581509685423,-0.10806368512276379] Loss: 22.853756596001684\n",
      "Iteracion: 7830 Gradiente: [0.006260251597516723,-0.10800623515076199] Loss: 22.853744882123756\n",
      "Iteracion: 7831 Gradiente: [0.006256923455519352,-0.10794881572093604] Loss: 22.85373318069741\n",
      "Iteracion: 7832 Gradiente: [0.006253597082934448,-0.1078914268170371] Loss: 22.85372149170944\n",
      "Iteracion: 7833 Gradiente: [0.006250272478862939,-0.10783406842283298] Loss: 22.853709815146615\n",
      "Iteracion: 7834 Gradiente: [0.0062469496421745895,-0.10777674052211819] Loss: 22.85369815099574\n",
      "Iteracion: 7835 Gradiente: [0.006243628571956113,-0.10771944309867697] Loss: 22.853686499243597\n",
      "Iteracion: 7836 Gradiente: [0.006240309267386124,-0.1076621761363016] Loss: 22.853674859877017\n",
      "Iteracion: 7837 Gradiente: [0.006236991727439545,-0.10760493961880234] Loss: 22.853663232882845\n",
      "Iteracion: 7838 Gradiente: [0.0062336759511273,-0.10754773352999697] Loss: 22.85365161824792\n",
      "Iteracion: 7839 Gradiente: [0.00623036193763653,-0.10749055785370155] Loss: 22.853640015959098\n",
      "Iteracion: 7840 Gradiente: [0.006227049686003738,-0.10743341257374854] Loss: 22.853628426003272\n",
      "Iteracion: 7841 Gradiente: [0.0062237391952029006,-0.10737629767398431] Loss: 22.853616848367317\n",
      "Iteracion: 7842 Gradiente: [0.006220430464484631,-0.107319213138247] Loss: 22.853605283038146\n",
      "Iteracion: 7843 Gradiente: [0.006217123492704483,-0.10726215895040561] Loss: 22.853593730002668\n",
      "Iteracion: 7844 Gradiente: [0.006213818279024016,-0.1072051350943204] Loss: 22.85358218924783\n",
      "Iteracion: 7845 Gradiente: [0.006210514822568788,-0.10714814155386397] Loss: 22.853570660760575\n",
      "Iteracion: 7846 Gradiente: [0.006207213122208562,-0.1070911783129303] Loss: 22.853559144527836\n",
      "Iteracion: 7847 Gradiente: [0.006203913177192059,-0.10703424535540028] Loss: 22.853547640536622\n",
      "Iteracion: 7848 Gradiente: [0.006200614986507465,-0.10697734266517939] Loss: 22.8535361487739\n",
      "Iteracion: 7849 Gradiente: [0.006197318549203601,-0.1069204702261782] Loss: 22.85352466922669\n",
      "Iteracion: 7850 Gradiente: [0.006194023864498869,-0.10686362802230308] Loss: 22.85351320188198\n",
      "Iteracion: 7851 Gradiente: [0.006190730931302823,-0.10680681603749125] Loss: 22.85350174672682\n",
      "Iteracion: 7852 Gradiente: [0.006187439748737233,-0.1067500342556753] Loss: 22.85349030374825\n",
      "Iteracion: 7853 Gradiente: [0.0061841503157969175,-0.10669328266080053] Loss: 22.853478872933312\n",
      "Iteracion: 7854 Gradiente: [0.006180862631681331,-0.10663656123681259] Loss: 22.853467454269108\n",
      "Iteracion: 7855 Gradiente: [0.006177576695332239,-0.10657986996767879] Loss: 22.853456047742686\n",
      "Iteracion: 7856 Gradiente: [0.006174292505979413,-0.10652320883735757] Loss: 22.853444653341153\n",
      "Iteracion: 7857 Gradiente: [0.006171010062612936,-0.10646657782983268] Loss: 22.853433271051646\n",
      "Iteracion: 7858 Gradiente: [0.006167729364246573,-0.10640997692909196] Loss: 22.853421900861257\n",
      "Iteracion: 7859 Gradiente: [0.0061644504099670405,-0.10635340611912862] Loss: 22.853410542757146\n",
      "Iteracion: 7860 Gradiente: [0.006161173198945372,-0.10629686538394197] Loss: 22.85339919672645\n",
      "Iteracion: 7861 Gradiente: [0.006157897730167861,-0.10624035470754573] Loss: 22.853387862756346\n",
      "Iteracion: 7862 Gradiente: [0.0061546240026795354,-0.106183874073963] Loss: 22.85337654083402\n",
      "Iteracion: 7863 Gradiente: [0.006151352015631536,-0.10612742346721712] Loss: 22.853365230946636\n",
      "Iteracion: 7864 Gradiente: [0.006148081768164578,-0.10607100287134123] Loss: 22.85335393308145\n",
      "Iteracion: 7865 Gradiente: [0.006144813259188216,-0.10601461227038934] Loss: 22.853342647225645\n",
      "Iteracion: 7866 Gradiente: [0.006141546487869695,-0.10595825164841083] Loss: 22.85333137336646\n",
      "Iteracion: 7867 Gradiente: [0.00613828145322941,-0.10590192098947272] Loss: 22.853320111491144\n",
      "Iteracion: 7868 Gradiente: [0.006135018154425135,-0.10584562027763861] Loss: 22.853308861586978\n",
      "Iteracion: 7869 Gradiente: [0.006131756590379686,-0.10578934949699755] Loss: 22.853297623641197\n",
      "Iteracion: 7870 Gradiente: [0.00612849676034936,-0.10573310863162746] Loss: 22.853286397641135\n",
      "Iteracion: 7871 Gradiente: [0.006125238663359293,-0.10567689766562603] Loss: 22.853275183574056\n",
      "Iteracion: 7872 Gradiente: [0.006121982298475359,-0.10562071658309928] Loss: 22.853263981427304\n",
      "Iteracion: 7873 Gradiente: [0.006118727664799432,-0.10556456536815849] Loss: 22.853252791188204\n",
      "Iteracion: 7874 Gradiente: [0.006115474761381278,-0.1055084440049286] Loss: 22.853241612844066\n",
      "Iteracion: 7875 Gradiente: [0.0061122235871674015,-0.10545235247754547] Loss: 22.853230446382284\n",
      "Iteracion: 7876 Gradiente: [0.006108974141482312,-0.10539629077013425] Loss: 22.853219291790218\n",
      "Iteracion: 7877 Gradiente: [0.006105726423373881,-0.10534025886684345] Loss: 22.85320814905525\n",
      "Iteracion: 7878 Gradiente: [0.006102480431801875,-0.10528425675183542] Loss: 22.853197018164767\n",
      "Iteracion: 7879 Gradiente: [0.006099236165840694,-0.10522828440927405] Loss: 22.853185899106172\n",
      "Iteracion: 7880 Gradiente: [0.006095993624673686,-0.10517234182332587] Loss: 22.853174791866923\n",
      "Iteracion: 7881 Gradiente: [0.006092752807334515,-0.10511642897817262] Loss: 22.853163696434418\n",
      "Iteracion: 7882 Gradiente: [0.006089513712930738,-0.10506054585800394] Loss: 22.853152612796126\n",
      "Iteracion: 7883 Gradiente: [0.006086276340597389,-0.10500469244701284] Loss: 22.85314154093951\n",
      "Iteracion: 7884 Gradiente: [0.006083040689244967,-0.10494886872941601] Loss: 22.853130480852027\n",
      "Iteracion: 7885 Gradiente: [0.006079806758195142,-0.10489307468941363] Loss: 22.853119432521186\n",
      "Iteracion: 7886 Gradiente: [0.006076574546214412,-0.1048373103112444] Loss: 22.85310839593449\n",
      "Iteracion: 7887 Gradiente: [0.006073344052695499,-0.10478157557912422] Loss: 22.853097371079457\n",
      "Iteracion: 7888 Gradiente: [0.0060701152765327985,-0.10472587047730322] Loss: 22.853086357943607\n",
      "Iteracion: 7889 Gradiente: [0.006066888217006294,-0.10467019499001821] Loss: 22.8530753565145\n",
      "Iteracion: 7890 Gradiente: [0.0060636628730605935,-0.10461454910153083] Loss: 22.85306436677966\n",
      "Iteracion: 7891 Gradiente: [0.006060439243680093,-0.10455893279611377] Loss: 22.853053388726675\n",
      "Iteracion: 7892 Gradiente: [0.006057217328014038,-0.10450334605803632] Loss: 22.853042422343126\n",
      "Iteracion: 7893 Gradiente: [0.006053997125488308,-0.10444778887155964] Loss: 22.85303146761663\n",
      "Iteracion: 7894 Gradiente: [0.006050778634840981,-0.10439226122099209] Loss: 22.85302052453475\n",
      "Iteracion: 7895 Gradiente: [0.006047561855142666,-0.10433676309063375] Loss: 22.853009593085144\n",
      "Iteracion: 7896 Gradiente: [0.006044346785683766,-0.10428129446477925] Loss: 22.85299867325544\n",
      "Iteracion: 7897 Gradiente: [0.006041133425350153,-0.10422585532775522] Loss: 22.852987765033276\n",
      "Iteracion: 7898 Gradiente: [0.00603792177342181,-0.10417044566387355] Loss: 22.852976868406326\n",
      "Iteracion: 7899 Gradiente: [0.006034711828827237,-0.10411506545747441] Loss: 22.852965983362257\n",
      "Iteracion: 7900 Gradiente: [0.0060315035907545205,-0.10405971469289194] Loss: 22.85295510988877\n",
      "Iteracion: 7901 Gradiente: [0.006028297058302694,-0.10400439335447278] Loss: 22.852944247973532\n",
      "Iteracion: 7902 Gradiente: [0.006025092230637104,-0.10394910142656985] Loss: 22.852933397604286\n",
      "Iteracion: 7903 Gradiente: [0.006021889106585831,-0.10389383889356504] Loss: 22.852922558768782\n",
      "Iteracion: 7904 Gradiente: [0.006018687685577599,-0.1038386057398057] Loss: 22.852911731454693\n",
      "Iteracion: 7905 Gradiente: [0.006015487966462274,-0.10378340194969375] Loss: 22.85290091564982\n",
      "Iteracion: 7906 Gradiente: [0.006012289948401417,-0.103728227507611] Loss: 22.852890111341914\n",
      "Iteracion: 7907 Gradiente: [0.006009093630574587,-0.10367308239795214] Loss: 22.852879318518767\n",
      "Iteracion: 7908 Gradiente: [0.006005899011933024,-0.10361796660512906] Loss: 22.852868537168153\n",
      "Iteracion: 7909 Gradiente: [0.00600270609166292,-0.10356288011355327] Loss: 22.85285776727789\n",
      "Iteracion: 7910 Gradiente: [0.005999514868809304,-0.10350782290764957] Loss: 22.852847008835784\n",
      "Iteracion: 7911 Gradiente: [0.005996325342517632,-0.10345279497184616] Loss: 22.85283626182969\n",
      "Iteracion: 7912 Gradiente: [0.0059931375119380926,-0.10339779629057719] Loss: 22.852825526247432\n",
      "Iteracion: 7913 Gradiente: [0.0059899513760361366,-0.1033428268483006] Loss: 22.852814802076853\n",
      "Iteracion: 7914 Gradiente: [0.00598676693406522,-0.10328788662946191] Loss: 22.852804089305867\n",
      "Iteracion: 7915 Gradiente: [0.005983584185033427,-0.10323297561853018] Loss: 22.852793387922297\n",
      "Iteracion: 7916 Gradiente: [0.005980403128003786,-0.10317809379998076] Loss: 22.852782697914098\n",
      "Iteracion: 7917 Gradiente: [0.005977223762140701,-0.10312324115828903] Loss: 22.852772019269132\n",
      "Iteracion: 7918 Gradiente: [0.0059740460864579365,-0.10306841767794903] Loss: 22.852761351975342\n",
      "Iteracion: 7919 Gradiente: [0.0059708701002051615,-0.1030136233434502] Loss: 22.852750696020678\n",
      "Iteracion: 7920 Gradiente: [0.005967695802434984,-0.10295885813929892] Loss: 22.852740051393038\n",
      "Iteracion: 7921 Gradiente: [0.005964523192178225,-0.10290412205001367] Loss: 22.85272941808042\n",
      "Iteracion: 7922 Gradiente: [0.005961352268499809,-0.10284941506011785] Loss: 22.852718796070793\n",
      "Iteracion: 7923 Gradiente: [0.0059581830307015105,-0.10279473715412996] Loss: 22.85270818535213\n",
      "Iteracion: 7924 Gradiente: [0.0059550154776976194,-0.10274008831659907] Loss: 22.852697585912445\n",
      "Iteracion: 7925 Gradiente: [0.005951849608701802,-0.10268546853206714] Loss: 22.852686997739745\n",
      "Iteracion: 7926 Gradiente: [0.005948685422730667,-0.10263087778509157] Loss: 22.852676420822036\n",
      "Iteracion: 7927 Gradiente: [0.0059455229190073546,-0.10257631606022764] Loss: 22.852665855147368\n",
      "Iteracion: 7928 Gradiente: [0.005942362096442366,-0.10252178334205932] Loss: 22.85265530070379\n",
      "Iteracion: 7929 Gradiente: [0.005939202954387686,-0.10246727961515063] Loss: 22.85264475747935\n",
      "Iteracion: 7930 Gradiente: [0.005936045491830555,-0.1024128048640963] Loss: 22.852634225462158\n",
      "Iteracion: 7931 Gradiente: [0.005932889707824529,-0.10235835907349428] Loss: 22.852623704640266\n",
      "Iteracion: 7932 Gradiente: [0.005929735601620223,-0.10230394222794104] Loss: 22.85261319500177\n",
      "Iteracion: 7933 Gradiente: [0.00592658317212719,-0.10224955431205972] Loss: 22.8526026965348\n",
      "Iteracion: 7934 Gradiente: [0.005923432418513622,-0.10219519531046697] Loss: 22.852592209227474\n",
      "Iteracion: 7935 Gradiente: [0.005920283339996028,-0.10214086520778393] Loss: 22.852581733067954\n",
      "Iteracion: 7936 Gradiente: [0.005917135935662069,-0.10208656398864886] Loss: 22.852571268044365\n",
      "Iteracion: 7937 Gradiente: [0.005913990204593726,-0.102032291637709] Loss: 22.852560814144862\n",
      "Iteracion: 7938 Gradiente: [0.005910846145854976,-0.10197804813961812] Loss: 22.85255037135763\n",
      "Iteracion: 7939 Gradiente: [0.005907703758593167,-0.10192383347903658] Loss: 22.85253993967086\n",
      "Iteracion: 7940 Gradiente: [0.0059045630420077565,-0.1018696476406267] Loss: 22.852529519072768\n",
      "Iteracion: 7941 Gradiente: [0.005901423994998822,-0.10181549060907938] Loss: 22.852519109551526\n",
      "Iteracion: 7942 Gradiente: [0.005898286616868139,-0.10176136236906927] Loss: 22.852508711095407\n",
      "Iteracion: 7943 Gradiente: [0.005895150906690105,-0.10170726290529046] Loss: 22.852498323692615\n",
      "Iteracion: 7944 Gradiente: [0.005892016863466173,-0.10165319220245336] Loss: 22.852487947331404\n",
      "Iteracion: 7945 Gradiente: [0.005888884486428954,-0.10159915024525906] Loss: 22.852477582000056\n",
      "Iteracion: 7946 Gradiente: [0.00588575377472201,-0.10154513701842365] Loss: 22.852467227686837\n",
      "Iteracion: 7947 Gradiente: [0.005882624727316473,-0.101491152506683] Loss: 22.85245688438003\n",
      "Iteracion: 7948 Gradiente: [0.005879497343429799,-0.10143719669476543] Loss: 22.852446552067935\n",
      "Iteracion: 7949 Gradiente: [0.00587637162226334,-0.10138326956740566] Loss: 22.852436230738867\n",
      "Iteracion: 7950 Gradiente: [0.005873247562784438,-0.10132937110936477] Loss: 22.852425920381158\n",
      "Iteracion: 7951 Gradiente: [0.005870125164092125,-0.10127550130540304] Loss: 22.852415620983145\n",
      "Iteracion: 7952 Gradiente: [0.005867004425337541,-0.1012216601402835] Loss: 22.852405332533163\n",
      "Iteracion: 7953 Gradiente: [0.005863885345650033,-0.10116784759878143] Loss: 22.852395055019585\n",
      "Iteracion: 7954 Gradiente: [0.005860767924186424,-0.10111406366567634] Loss: 22.85238478843078\n",
      "Iteracion: 7955 Gradiente: [0.005857652160130063,-0.10106030832575547] Loss: 22.852374532755157\n",
      "Iteracion: 7956 Gradiente: [0.0058545380523905045,-0.10100658156382968] Loss: 22.852364287981086\n",
      "Iteracion: 7957 Gradiente: [0.00585142560031405,-0.10095288336469335] Loss: 22.852354054097013\n",
      "Iteracion: 7958 Gradiente: [0.0058483148029068845,-0.10089921371316966] Loss: 22.852343831091318\n",
      "Iteracion: 7959 Gradiente: [0.00584520565921783,-0.10084557259408365] Loss: 22.85233361895246\n",
      "Iteracion: 7960 Gradiente: [0.0058420981684776056,-0.10079195999226064] Loss: 22.852323417668906\n",
      "Iteracion: 7961 Gradiente: [0.005838992329733135,-0.10073837589254554] Loss: 22.852313227229086\n",
      "Iteracion: 7962 Gradiente: [0.0058358881421819815,-0.10068482027978061] Loss: 22.85230304762149\n",
      "Iteracion: 7963 Gradiente: [0.005832785604890016,-0.10063129313882466] Loss: 22.852292878834614\n",
      "Iteracion: 7964 Gradiente: [0.005829684716982797,-0.10057779445454121] Loss: 22.85228272085693\n",
      "Iteracion: 7965 Gradiente: [0.005826585477665466,-0.10052432421179702] Loss: 22.852272573676963\n",
      "Iteracion: 7966 Gradiente: [0.005823487886021894,-0.10047088239547328] Loss: 22.852262437283223\n",
      "Iteracion: 7967 Gradiente: [0.005820391941199432,-0.10041746899045687] Loss: 22.852252311664262\n",
      "Iteracion: 7968 Gradiente: [0.005817297642107633,-0.10036408398165442] Loss: 22.85224219680862\n",
      "Iteracion: 7969 Gradiente: [0.005814204988166693,-0.10031072735395566] Loss: 22.85223209270486\n",
      "Iteracion: 7970 Gradiente: [0.0058111139783515375,-0.10025739909227838] Loss: 22.852221999341552\n",
      "Iteracion: 7971 Gradiente: [0.00580802461170246,-0.10020409918154888] Loss: 22.852211916707258\n",
      "Iteracion: 7972 Gradiente: [0.005804936887564812,-0.10015082760668198] Loss: 22.852201844790596\n",
      "Iteracion: 7973 Gradiente: [0.005801850804904044,-0.10009758435262212] Loss: 22.852191783580164\n",
      "Iteracion: 7974 Gradiente: [0.0057987663629982455,-0.10004436940430542] Loss: 22.852181733064594\n",
      "Iteracion: 7975 Gradiente: [0.00579568356066981,-0.09999118274669906] Loss: 22.8521716932325\n",
      "Iteracion: 7976 Gradiente: [0.005792602397502833,-0.09993802436473895] Loss: 22.852161664072543\n",
      "Iteracion: 7977 Gradiente: [0.005789522872241075,-0.0998848942434145] Loss: 22.852151645573365\n",
      "Iteracion: 7978 Gradiente: [0.005786444984186308,-0.0998317923676904] Loss: 22.85214163772364\n",
      "Iteracion: 7979 Gradiente: [0.0057833687323655646,-0.09977871872255761] Loss: 22.852131640512045\n",
      "Iteracion: 7980 Gradiente: [0.0057802941160720895,-0.09972567329299858] Loss: 22.852121653927288\n",
      "Iteracion: 7981 Gradiente: [0.005777221134310177,-0.0996726560640198] Loss: 22.85211167795804\n",
      "Iteracion: 7982 Gradiente: [0.005774149786183595,-0.09961966702062892] Loss: 22.85210171259303\n",
      "Iteracion: 7983 Gradiente: [0.005771080070968537,-0.09956670614783623] Loss: 22.852091757820997\n",
      "Iteracion: 7984 Gradiente: [0.005768011987681613,-0.09951377343067082] Loss: 22.852081813630683\n",
      "Iteracion: 7985 Gradiente: [0.00576494553548154,-0.09946086885416155] Loss: 22.852071880010833\n",
      "Iteracion: 7986 Gradiente: [0.005761880713515665,-0.09940799240334855] Loss: 22.8520619569502\n",
      "Iteracion: 7987 Gradiente: [0.005758817520914287,-0.09935514406328044] Loss: 22.852052044437585\n",
      "Iteracion: 7988 Gradiente: [0.00575575595671296,-0.09930232381901546] Loss: 22.852042142461755\n",
      "Iteracion: 7989 Gradiente: [0.005752696020161352,-0.0992495316556127] Loss: 22.852032251011504\n",
      "Iteracion: 7990 Gradiente: [0.005749637710300704,-0.09919676755814741] Loss: 22.85202237007565\n",
      "Iteracion: 7991 Gradiente: [0.005746581026374997,-0.09914403151169514] Loss: 22.852012499643042\n",
      "Iteracion: 7992 Gradiente: [0.005743525967541056,-0.09909132350133815] Loss: 22.85200263970248\n",
      "Iteracion: 7993 Gradiente: [0.005740472532900753,-0.09903864351217646] Loss: 22.85199279024284\n",
      "Iteracion: 7994 Gradiente: [0.005737420721492488,-0.09898599152931856] Loss: 22.851982951252936\n",
      "Iteracion: 7995 Gradiente: [0.005734370532553612,-0.09893336753786729] Loss: 22.8519731227217\n",
      "Iteracion: 7996 Gradiente: [0.005731321965084627,-0.09888077152295315] Loss: 22.851963304637966\n",
      "Iteracion: 7997 Gradiente: [0.00572827501845173,-0.09882820346968775] Loss: 22.851953496990667\n",
      "Iteracion: 7998 Gradiente: [0.005725229691569212,-0.09877566336321747] Loss: 22.85194369976868\n",
      "Iteracion: 7999 Gradiente: [0.005722185983656421,-0.09872315118868305] Loss: 22.85193391296092\n",
      "Iteracion: 8000 Gradiente: [0.0057191438939573425,-0.09867066693122671] Loss: 22.85192413655633\n",
      "Iteracion: 8001 Gradiente: [0.005716103421537847,-0.0986182105760122] Loss: 22.851914370543867\n",
      "Iteracion: 8002 Gradiente: [0.005713064565538654,-0.09856578210820537] Loss: 22.851904614912442\n",
      "Iteracion: 8003 Gradiente: [0.00571002732504553,-0.0985133815129841] Loss: 22.851894869651066\n",
      "Iteracion: 8004 Gradiente: [0.005706991699270247,-0.09846100877552526] Loss: 22.851885134748695\n",
      "Iteracion: 8005 Gradiente: [0.005703957687343101,-0.0984086638810215] Loss: 22.851875410194317\n",
      "Iteracion: 8006 Gradiente: [0.005700925288360281,-0.09835634681467104] Loss: 22.85186569597693\n",
      "Iteracion: 8007 Gradiente: [0.005697894501529769,-0.09830405756167776] Loss: 22.85185599208556\n",
      "Iteracion: 8008 Gradiente: [0.005694865325844489,-0.09825179610726215] Loss: 22.851846298509198\n",
      "Iteracion: 8009 Gradiente: [0.005691837760674427,-0.09819956243663389] Loss: 22.851836615236916\n",
      "Iteracion: 8010 Gradiente: [0.00568881180501819,-0.09814735653503016] Loss: 22.851826942257755\n",
      "Iteracion: 8011 Gradiente: [0.00568578745800797,-0.09809517838768862] Loss: 22.85181727956076\n",
      "Iteracion: 8012 Gradiente: [0.005682764718918066,-0.09804302797984618] Loss: 22.851807627134992\n",
      "Iteracion: 8013 Gradiente: [0.005679743586730031,-0.09799090529676609] Loss: 22.851797984969572\n",
      "Iteracion: 8014 Gradiente: [0.005676724060675534,-0.09793881032370386] Loss: 22.851788353053543\n",
      "Iteracion: 8015 Gradiente: [0.005673706139972978,-0.09788674304592403] Loss: 22.851778731376058\n",
      "Iteracion: 8016 Gradiente: [0.0056706898235972854,-0.09783470344871385] Loss: 22.851769119926203\n",
      "Iteracion: 8017 Gradiente: [0.005667675110852126,-0.0977826915173458] Loss: 22.85175951869311\n",
      "Iteracion: 8018 Gradiente: [0.00566466200078537,-0.09773070723711849] Loss: 22.851749927665928\n",
      "Iteracion: 8019 Gradiente: [0.005661650492593632,-0.0976787505933307] Loss: 22.85174034683381\n",
      "Iteracion: 8020 Gradiente: [0.005658640585374996,-0.09762682157129063] Loss: 22.8517307761859\n",
      "Iteracion: 8021 Gradiente: [0.005655632278320392,-0.0975749201563128] Loss: 22.851721215711397\n",
      "Iteracion: 8022 Gradiente: [0.005652625570619799,-0.09752304633371824] Loss: 22.851711665399478\n",
      "Iteracion: 8023 Gradiente: [0.0056496204613428825,-0.09747120008884143] Loss: 22.851702125239328\n",
      "Iteracion: 8024 Gradiente: [0.005646616949794255,-0.09741938140701183] Loss: 22.851692595220165\n",
      "Iteracion: 8025 Gradiente: [0.005643615034799154,-0.09736759027359802] Loss: 22.85168307533122\n",
      "Iteracion: 8026 Gradiente: [0.005640614715811883,-0.09731582667393456] Loss: 22.851673565561708\n",
      "Iteracion: 8027 Gradiente: [0.005637615991938105,-0.09726409059338567] Loss: 22.851664065900867\n",
      "Iteracion: 8028 Gradiente: [0.005634618862189692,-0.09721238201733087] Loss: 22.85165457633799\n",
      "Iteracion: 8029 Gradiente: [0.005631623325878839,-0.097160700931137] Loss: 22.851645096862296\n",
      "Iteracion: 8030 Gradiente: [0.005628629382086577,-0.09710904732019543] Loss: 22.851635627463093\n",
      "Iteracion: 8031 Gradiente: [0.0056256370298958325,-0.09705742116990272] Loss: 22.851626168129656\n",
      "Iteracion: 8032 Gradiente: [0.005622646268571429,-0.09700582246565403] Loss: 22.851616718851297\n",
      "Iteracion: 8033 Gradiente: [0.005619657097255034,-0.09695425119285896] Loss: 22.851607279617316\n",
      "Iteracion: 8034 Gradiente: [0.005616669515056098,-0.09690270733693657] Loss: 22.851597850417026\n",
      "Iteracion: 8035 Gradiente: [0.00561368352116555,-0.09685119088330936] Loss: 22.851588431239783\n",
      "Iteracion: 8036 Gradiente: [0.00561069911470895,-0.09679970181741027] Loss: 22.851579022074944\n",
      "Iteracion: 8037 Gradiente: [0.005607716294835541,-0.09674824012468027] Loss: 22.85156962291184\n",
      "Iteracion: 8038 Gradiente: [0.00560473506066046,-0.09669680579057041] Loss: 22.85156023373983\n",
      "Iteracion: 8039 Gradiente: [0.005601755411486427,-0.09664539880052393] Loss: 22.851550854548314\n",
      "Iteracion: 8040 Gradiente: [0.005598777346357527,-0.09659401914001495] Loss: 22.851541485326695\n",
      "Iteracion: 8041 Gradiente: [0.005595800864457109,-0.09654266679451003] Loss: 22.851532126064345\n",
      "Iteracion: 8042 Gradiente: [0.005592825964969469,-0.09649134174948663] Loss: 22.851522776750702\n",
      "Iteracion: 8043 Gradiente: [0.005589852646971849,-0.09644004399043667] Loss: 22.851513437375168\n",
      "Iteracion: 8044 Gradiente: [0.00558688090970918,-0.09638877350284787] Loss: 22.851504107927205\n",
      "Iteracion: 8045 Gradiente: [0.005583910752392285,-0.0963375302722195] Loss: 22.851494788396238\n",
      "Iteracion: 8046 Gradiente: [0.005580942173999877,-0.09628631428407246] Loss: 22.851485478771746\n",
      "Iteracion: 8047 Gradiente: [0.005577975173853626,-0.09623512552391252] Loss: 22.85147617904318\n",
      "Iteracion: 8048 Gradiente: [0.005575009751080036,-0.09618396397726722] Loss: 22.851466889200022\n",
      "Iteracion: 8049 Gradiente: [0.00557204590472035,-0.09613282962967536] Loss: 22.851457609231776\n",
      "Iteracion: 8050 Gradiente: [0.005569083634115183,-0.09608172246666899] Loss: 22.85144833912795\n",
      "Iteracion: 8051 Gradiente: [0.005566122938354094,-0.09603064247379743] Loss: 22.851439078878034\n",
      "Iteracion: 8052 Gradiente: [0.005563163816546535,-0.09597958963662083] Loss: 22.851429828471566\n",
      "Iteracion: 8053 Gradiente: [0.0055602062678957506,-0.09592856394069939] Loss: 22.851420587898076\n",
      "Iteracion: 8054 Gradiente: [0.005557250291554776,-0.09587756537160542] Loss: 22.85141135714714\n",
      "Iteracion: 8055 Gradiente: [0.005554295886762854,-0.09582659391491291] Loss: 22.85140213620829\n",
      "Iteracion: 8056 Gradiente: [0.005551343052560279,-0.09577564955621429] Loss: 22.85139292507109\n",
      "Iteracion: 8057 Gradiente: [0.005548391788194825,-0.09572473228109869] Loss: 22.851383723725153\n",
      "Iteracion: 8058 Gradiente: [0.005545442092728573,-0.09567384207517536] Loss: 22.851374532160037\n",
      "Iteracion: 8059 Gradiente: [0.005542493965630039,-0.09562297892403405] Loss: 22.851365350365363\n",
      "Iteracion: 8060 Gradiente: [0.005539547405678983,-0.09557214281331548] Loss: 22.851356178330732\n",
      "Iteracion: 8061 Gradiente: [0.00553660241219518,-0.09552133372863451] Loss: 22.851347016045786\n",
      "Iteracion: 8062 Gradiente: [0.005533658984455769,-0.09547055165561673] Loss: 22.851337863500145\n",
      "Iteracion: 8063 Gradiente: [0.005530717121534205,-0.09541979657990764] Loss: 22.85132872068349\n",
      "Iteracion: 8064 Gradiente: [0.0055277768224537265,-0.09536906848716373] Loss: 22.85131958758545\n",
      "Iteracion: 8065 Gradiente: [0.005524838086622215,-0.09531836736302411] Loss: 22.851310464195695\n",
      "Iteracion: 8066 Gradiente: [0.005521900913157652,-0.09526769319315763] Loss: 22.8513013505039\n",
      "Iteracion: 8067 Gradiente: [0.005518965301104117,-0.09521704596323831] Loss: 22.85129224649977\n",
      "Iteracion: 8068 Gradiente: [0.005516031249691385,-0.09516642565894291] Loss: 22.851283152173007\n",
      "Iteracion: 8069 Gradiente: [0.0055130987581549105,-0.09511583226595187] Loss: 22.85127406751331\n",
      "Iteracion: 8070 Gradiente: [0.0055101678256581485,-0.09506526576995936] Loss: 22.85126499251043\n",
      "Iteracion: 8071 Gradiente: [0.005507238451359816,-0.09501472615666746] Loss: 22.851255927154064\n",
      "Iteracion: 8072 Gradiente: [0.005504310634372208,-0.09496421341178755] Loss: 22.85124687143401\n",
      "Iteracion: 8073 Gradiente: [0.0055013843738644635,-0.09491372752103473] Loss: 22.85123782533996\n",
      "Iteracion: 8074 Gradiente: [0.005498459669085302,-0.09486326847012923] Loss: 22.851228788861736\n",
      "Iteracion: 8075 Gradiente: [0.0054955365191100706,-0.09481283624480762] Loss: 22.851219761989096\n",
      "Iteracion: 8076 Gradiente: [0.005492614923232964,-0.0947624308308009] Loss: 22.851210744711818\n",
      "Iteracion: 8077 Gradiente: [0.005489694880453536,-0.09471205221386624] Loss: 22.851201737019725\n",
      "Iteracion: 8078 Gradiente: [0.005486776390198619,-0.0946617003797435] Loss: 22.851192738902615\n",
      "Iteracion: 8079 Gradiente: [0.005483859451476292,-0.09461137531420066] Loss: 22.851183750350298\n",
      "Iteracion: 8080 Gradiente: [0.005480944063491696,-0.09456107700300767] Loss: 22.85117477135265\n",
      "Iteracion: 8081 Gradiente: [0.005478030225375127,-0.09451080543194539] Loss: 22.85116580189947\n",
      "Iteracion: 8082 Gradiente: [0.005475117936345934,-0.09446056058679145] Loss: 22.851156841980625\n",
      "Iteracion: 8083 Gradiente: [0.005472207195570415,-0.09441034245334083] Loss: 22.851147891586\n",
      "Iteracion: 8084 Gradiente: [0.005469298002274551,-0.09436015101738775] Loss: 22.851138950705423\n",
      "Iteracion: 8085 Gradiente: [0.005466390355588639,-0.09430998626474446] Loss: 22.851130019328835\n",
      "Iteracion: 8086 Gradiente: [0.005463484254752871,-0.09425984818121905] Loss: 22.851121097446114\n",
      "Iteracion: 8087 Gradiente: [0.005460579698777224,-0.0942097367526453] Loss: 22.85111218504715\n",
      "Iteracion: 8088 Gradiente: [0.005457676686972945,-0.09415965196484327] Loss: 22.851103282121887\n",
      "Iteracion: 8089 Gradiente: [0.005454775218515806,-0.09410959380365078] Loss: 22.85109438866024\n",
      "Iteracion: 8090 Gradiente: [0.005451875292609051,-0.09405956225491181] Loss: 22.851085504652154\n",
      "Iteracion: 8091 Gradiente: [0.005448976908364027,-0.09400955730448005] Loss: 22.85107663008758\n",
      "Iteracion: 8092 Gradiente: [0.005446080065009558,-0.09395957893821448] Loss: 22.851067764956483\n",
      "Iteracion: 8093 Gradiente: [0.005443184761722364,-0.0939096271419814] Loss: 22.85105890924883\n",
      "Iteracion: 8094 Gradiente: [0.005440290997536105,-0.09385970190166451] Loss: 22.851050062954616\n",
      "Iteracion: 8095 Gradiente: [0.005437398771916454,-0.09380980320312879] Loss: 22.8510412260638\n",
      "Iteracion: 8096 Gradiente: [0.00543450808378149,-0.09375993103227825] Loss: 22.85103239856642\n",
      "Iteracion: 8097 Gradiente: [0.005431618932476567,-0.09371008537500244] Loss: 22.851023580452487\n",
      "Iteracion: 8098 Gradiente: [0.005428731317169877,-0.09366026621720588] Loss: 22.851014771712034\n",
      "Iteracion: 8099 Gradiente: [0.005425845236927292,-0.09361047354480784] Loss: 22.85100597233507\n",
      "Iteracion: 8100 Gradiente: [0.005422960691085639,-0.09356070734371945] Loss: 22.850997182311666\n",
      "Iteracion: 8101 Gradiente: [0.005420077678653949,-0.09351096759987539] Loss: 22.850988401631867\n",
      "Iteracion: 8102 Gradiente: [0.005417196199018311,-0.09346125429920077] Loss: 22.850979630285742\n",
      "Iteracion: 8103 Gradiente: [0.005414316251215231,-0.09341156742764613] Loss: 22.850970868263367\n",
      "Iteracion: 8104 Gradiente: [0.005411437834575849,-0.09336190697115114] Loss: 22.850962115554843\n",
      "Iteracion: 8105 Gradiente: [0.005408560948137619,-0.09331227291568259] Loss: 22.850953372150254\n",
      "Iteracion: 8106 Gradiente: [0.005405685591073469,-0.09326266524720524] Loss: 22.850944638039717\n",
      "Iteracion: 8107 Gradiente: [0.005402811762621695,-0.09321308395168787] Loss: 22.850935913213366\n",
      "Iteracion: 8108 Gradiente: [0.0053999394621058626,-0.09316352901510262] Loss: 22.850927197661324\n",
      "Iteracion: 8109 Gradiente: [0.005397068688562475,-0.09311400042344384] Loss: 22.850918491373704\n",
      "Iteracion: 8110 Gradiente: [0.005394199441172987,-0.09306449816270804] Loss: 22.8509097943407\n",
      "Iteracion: 8111 Gradiente: [0.0053913317191908545,-0.09301502221889042] Loss: 22.850901106552474\n",
      "Iteracion: 8112 Gradiente: [0.005388465521709425,-0.09296557257800787] Loss: 22.85089242799915\n",
      "Iteracion: 8113 Gradiente: [0.00538560084808258,-0.09291614922606636] Loss: 22.850883758670964\n",
      "Iteracion: 8114 Gradiente: [0.005382737697302294,-0.09286675214910171] Loss: 22.850875098558088\n",
      "Iteracion: 8115 Gradiente: [0.00537987606873287,-0.09281738133313494] Loss: 22.85086644765072\n",
      "Iteracion: 8116 Gradiente: [0.005377015961535865,-0.0927680367642079] Loss: 22.85085780593907\n",
      "Iteracion: 8117 Gradiente: [0.005374157374740207,-0.09271871842837515] Loss: 22.8508491734134\n",
      "Iteracion: 8118 Gradiente: [0.005371300307788829,-0.09266942631167534] Loss: 22.85084055006392\n",
      "Iteracion: 8119 Gradiente: [0.005368444759637706,-0.09262016040018371] Loss: 22.850831935880866\n",
      "Iteracion: 8120 Gradiente: [0.005365590729617035,-0.09257092067996169] Loss: 22.850823330854507\n",
      "Iteracion: 8121 Gradiente: [0.005362738216874163,-0.09252170713708653] Loss: 22.850814734975128\n",
      "Iteracion: 8122 Gradiente: [0.005359887220651179,-0.0924725197576399] Loss: 22.850806148232962\n",
      "Iteracion: 8123 Gradiente: [0.0053570377400414294,-0.09242335852771741] Loss: 22.850797570618333\n",
      "Iteracion: 8124 Gradiente: [0.005354189774323004,-0.09237422343341267] Loss: 22.850789002121523\n",
      "Iteracion: 8125 Gradiente: [0.005351343322671672,-0.09232511446083198] Loss: 22.850780442732844\n",
      "Iteracion: 8126 Gradiente: [0.005348498384318153,-0.09227603159608565] Loss: 22.850771892442598\n",
      "Iteracion: 8127 Gradiente: [0.005345654958422112,-0.09222697482529772] Loss: 22.850763351241145\n",
      "Iteracion: 8128 Gradiente: [0.005342813044162161,-0.0921779441345952] Loss: 22.8507548191188\n",
      "Iteracion: 8129 Gradiente: [0.005339972640748177,-0.09212893951011386] Loss: 22.850746296065928\n",
      "Iteracion: 8130 Gradiente: [0.005337133747448775,-0.0920799609379909] Loss: 22.85073778207288\n",
      "Iteracion: 8131 Gradiente: [0.005334296363332669,-0.09203100840438326] Loss: 22.85072927713002\n",
      "Iteracion: 8132 Gradiente: [0.005331460487711107,-0.09198208189544145] Loss: 22.850720781227754\n",
      "Iteracion: 8133 Gradiente: [0.005328626119585541,-0.09193318139734205] Loss: 22.850712294356416\n",
      "Iteracion: 8134 Gradiente: [0.005325793258392272,-0.09188430689624252] Loss: 22.85070381650646\n",
      "Iteracion: 8135 Gradiente: [0.005322961903173488,-0.09183545837833276] Loss: 22.85069534766828\n",
      "Iteracion: 8136 Gradiente: [0.005320132053289702,-0.0917866358297875] Loss: 22.850686887832293\n",
      "Iteracion: 8137 Gradiente: [0.005317303707748048,-0.09173783923681356] Loss: 22.850678436988932\n",
      "Iteracion: 8138 Gradiente: [0.00531447686589767,-0.09168906858560391] Loss: 22.850669995128648\n",
      "Iteracion: 8139 Gradiente: [0.005311651526861283,-0.09164032386237012] Loss: 22.850661562241864\n",
      "Iteracion: 8140 Gradiente: [0.005308827689920766,-0.09159160505332349] Loss: 22.850653138319064\n",
      "Iteracion: 8141 Gradiente: [0.005306005354181783,-0.0915429121446928] Loss: 22.850644723350715\n",
      "Iteracion: 8142 Gradiente: [0.0053031845188314716,-0.09149424512271077] Loss: 22.850636317327304\n",
      "Iteracion: 8143 Gradiente: [0.005300365183199081,-0.09144560397360876] Loss: 22.850627920239322\n",
      "Iteracion: 8144 Gradiente: [0.005297547346359958,-0.09139698868363423] Loss: 22.850619532077253\n",
      "Iteracion: 8145 Gradiente: [0.005294731007544821,-0.09134839923904406] Loss: 22.85061115283164\n",
      "Iteracion: 8146 Gradiente: [0.005291916166022285,-0.09129983562609094] Loss: 22.850602782492977\n",
      "Iteracion: 8147 Gradiente: [0.005289102820975699,-0.09125129783104337] Loss: 22.850594421051788\n",
      "Iteracion: 8148 Gradiente: [0.005286290971561887,-0.09120278584017984] Loss: 22.850586068498657\n",
      "Iteracion: 8149 Gradiente: [0.005283480617021041,-0.09115429963977893] Loss: 22.850577724824106\n",
      "Iteracion: 8150 Gradiente: [0.005280671756537458,-0.09110583921613168] Loss: 22.850569390018713\n",
      "Iteracion: 8151 Gradiente: [0.0052778643893570155,-0.09105740455553146] Loss: 22.850561064073034\n",
      "Iteracion: 8152 Gradiente: [0.0052750585146441155,-0.09100899564428472] Loss: 22.850552746977645\n",
      "Iteracion: 8153 Gradiente: [0.005272254131644634,-0.09096061246869892] Loss: 22.850544438723166\n",
      "Iteracion: 8154 Gradiente: [0.005269451239443394,-0.09091225501510097] Loss: 22.85053613930019\n",
      "Iteracion: 8155 Gradiente: [0.005266649837496592,-0.09086392326980075] Loss: 22.85052784869931\n",
      "Iteracion: 8156 Gradiente: [0.00526384992474315,-0.09081561721914717] Loss: 22.850519566911167\n",
      "Iteracion: 8157 Gradiente: [0.005261051500492424,-0.09076733684947248] Loss: 22.8505112939264\n",
      "Iteracion: 8158 Gradiente: [0.005258254564081236,-0.09071908214711838] Loss: 22.850503029735624\n",
      "Iteracion: 8159 Gradiente: [0.0052554591146000956,-0.09067085309844529] Loss: 22.850494774329523\n",
      "Iteracion: 8160 Gradiente: [0.005252665151212454,-0.09062264968981898] Loss: 22.850486527698717\n",
      "Iteracion: 8161 Gradiente: [0.005249872673178402,-0.09057447190760305] Loss: 22.850478289833934\n",
      "Iteracion: 8162 Gradiente: [0.005247081679671813,-0.09052631973817818] Loss: 22.850470060725797\n",
      "Iteracion: 8163 Gradiente: [0.005244292170036147,-0.09047819316791816] Loss: 22.85046184036506\n",
      "Iteracion: 8164 Gradiente: [0.0052415041434007515,-0.09043009218322015] Loss: 22.850453628742383\n",
      "Iteracion: 8165 Gradiente: [0.00523871759887129,-0.09038201677048836] Loss: 22.850445425848477\n",
      "Iteracion: 8166 Gradiente: [0.005235932535788379,-0.09033396691611913] Loss: 22.850437231674075\n",
      "Iteracion: 8167 Gradiente: [0.005233148953349579,-0.09028594260652566] Loss: 22.85042904620991\n",
      "Iteracion: 8168 Gradiente: [0.005230366850755294,-0.09023794382812879] Loss: 22.850420869446715\n",
      "Iteracion: 8169 Gradiente: [0.0052275862272059236,-0.09018997056735678] Loss: 22.850412701375244\n",
      "Iteracion: 8170 Gradiente: [0.005224807081850713,-0.09014202281064669] Loss: 22.850404541986258\n",
      "Iteracion: 8171 Gradiente: [0.005222029414061542,-0.09009410054443036] Loss: 22.850396391270532\n",
      "Iteracion: 8172 Gradiente: [0.005219253222994287,-0.0900462037551609] Loss: 22.850388249218852\n",
      "Iteracion: 8173 Gradiente: [0.005216478507754611,-0.08999833242929896] Loss: 22.850380115821984\n",
      "Iteracion: 8174 Gradiente: [0.005213705267603549,-0.08995048655330512] Loss: 22.85037199107076\n",
      "Iteracion: 8175 Gradiente: [0.005210933501888349,-0.0899026661136432] Loss: 22.85036387495597\n",
      "Iteracion: 8176 Gradiente: [0.005208163209689095,-0.08985487109679582] Loss: 22.850355767468443\n",
      "Iteracion: 8177 Gradiente: [0.005205394390316087,-0.08980710148924315] Loss: 22.850347668598992\n",
      "Iteracion: 8178 Gradiente: [0.005202627042771723,-0.08975935727749065] Loss: 22.850339578338456\n",
      "Iteracion: 8179 Gradiente: [0.005199861166545361,-0.0897116384480204] Loss: 22.850331496677722\n",
      "Iteracion: 8180 Gradiente: [0.005197096760771084,-0.08966394498734177] Loss: 22.8503234236076\n",
      "Iteracion: 8181 Gradiente: [0.005194333824629401,-0.08961627688197175] Loss: 22.850315359118998\n",
      "Iteracion: 8182 Gradiente: [0.00519157235728945,-0.0895686341184336] Loss: 22.850307303202765\n",
      "Iteracion: 8183 Gradiente: [0.00518881235806532,-0.08952101668324831] Loss: 22.850299255849798\n",
      "Iteracion: 8184 Gradiente: [0.0051860538261649936,-0.08947342456295242] Loss: 22.850291217050998\n",
      "Iteracion: 8185 Gradiente: [0.005183296760786031,-0.08942585774408846] Loss: 22.850283186797288\n",
      "Iteracion: 8186 Gradiente: [0.005180541161155361,-0.0893783162132036] Loss: 22.85027516507955\n",
      "Iteracion: 8187 Gradiente: [0.005177787026404227,-0.08933079995686163] Loss: 22.85026715188873\n",
      "Iteracion: 8188 Gradiente: [0.005175034355885563,-0.08928330896161718] Loss: 22.850259147215766\n",
      "Iteracion: 8189 Gradiente: [0.005172283148718293,-0.0892358432140469] Loss: 22.85025115105159\n",
      "Iteracion: 8190 Gradiente: [0.005169533404321669,-0.08918840270071561] Loss: 22.850243163387166\n",
      "Iteracion: 8191 Gradiente: [0.00516678512170851,-0.08914098740822271] Loss: 22.850235184213467\n",
      "Iteracion: 8192 Gradiente: [0.005164038300075428,-0.08909359732315873] Loss: 22.850227213521467\n",
      "Iteracion: 8193 Gradiente: [0.00516129293878104,-0.08904623243211632] Loss: 22.85021925130212\n",
      "Iteracion: 8194 Gradiente: [0.005158549037068383,-0.08899889272170054] Loss: 22.850211297546466\n",
      "Iteracion: 8195 Gradiente: [0.005155806594028907,-0.08895157817853215] Loss: 22.850203352245465\n",
      "Iteracion: 8196 Gradiente: [0.0051530656089785985,-0.08890428878922556] Loss: 22.850195415390164\n",
      "Iteracion: 8197 Gradiente: [0.005150326081198387,-0.08885702454040457] Loss: 22.85018748697158\n",
      "Iteracion: 8198 Gradiente: [0.005147588009754145,-0.08880978541871419] Loss: 22.850179566980714\n",
      "Iteracion: 8199 Gradiente: [0.005144851393913541,-0.08876257141079312] Loss: 22.850171655408648\n",
      "Iteracion: 8200 Gradiente: [0.005142116232952769,-0.08871538250328789] Loss: 22.85016375224641\n",
      "Iteracion: 8201 Gradiente: [0.005139382526101599,-0.08866821868285382] Loss: 22.85015585748506\n",
      "Iteracion: 8202 Gradiente: [0.005136650272708228,-0.08862107993614549] Loss: 22.85014797111568\n",
      "Iteracion: 8203 Gradiente: [0.005133919471746632,-0.08857396624984884] Loss: 22.850140093129347\n",
      "Iteracion: 8204 Gradiente: [0.005131190122588692,-0.08852687761063247] Loss: 22.85013222351713\n",
      "Iteracion: 8205 Gradiente: [0.005128462224418703,-0.08847981400518362] Loss: 22.85012436227016\n",
      "Iteracion: 8206 Gradiente: [0.005125735776425699,-0.08843277542019443] Loss: 22.850116509379518\n",
      "Iteracion: 8207 Gradiente: [0.005123010777973036,-0.08838576184235655] Loss: 22.85010866483634\n",
      "Iteracion: 8208 Gradiente: [0.005120287228190061,-0.08833877325838155] Loss: 22.850100828631746\n",
      "Iteracion: 8209 Gradiente: [0.005117565126296123,-0.08829180965498105] Loss: 22.85009300075686\n",
      "Iteracion: 8210 Gradiente: [0.0051148444716460515,-0.08824487101886973] Loss: 22.850085181202854\n",
      "Iteracion: 8211 Gradiente: [0.005112125263275402,-0.08819795733678339] Loss: 22.850077369960847\n",
      "Iteracion: 8212 Gradiente: [0.005109407500595845,-0.08815106859544611] Loss: 22.85006956702204\n",
      "Iteracion: 8213 Gradiente: [0.005106691182739572,-0.088104204781605] Loss: 22.850061772377597\n",
      "Iteracion: 8214 Gradiente: [0.005103976308942037,-0.0880573658820051] Loss: 22.85005398601867\n",
      "Iteracion: 8215 Gradiente: [0.005101262878500279,-0.08801055188340061] Loss: 22.850046207936497\n",
      "Iteracion: 8216 Gradiente: [0.005098550890558802,-0.08796376277255812] Loss: 22.850038438122272\n",
      "Iteracion: 8217 Gradiente: [0.005095840344425066,-0.08791699853623954] Loss: 22.85003067656716\n",
      "Iteracion: 8218 Gradiente: [0.005093131239311788,-0.08787025916122412] Loss: 22.850022923262433\n",
      "Iteracion: 8219 Gradiente: [0.005090423574370106,-0.08782354463429982] Loss: 22.850015178199307\n",
      "Iteracion: 8220 Gradiente: [0.00508771734892548,-0.08777685494225042] Loss: 22.85000744136903\n",
      "Iteracion: 8221 Gradiente: [0.005085012562208628,-0.08773019007187391] Loss: 22.849999712762827\n",
      "Iteracion: 8222 Gradiente: [0.005082309213533639,-0.08768355000996901] Loss: 22.84999199237196\n",
      "Iteracion: 8223 Gradiente: [0.005079607301953123,-0.08763693474335846] Loss: 22.849984280187712\n",
      "Iteracion: 8224 Gradiente: [0.005076906826834223,-0.08759034425885126] Loss: 22.849976576201364\n",
      "Iteracion: 8225 Gradiente: [0.00507420778726555,-0.08754377854328178] Loss: 22.849968880404177\n",
      "Iteracion: 8226 Gradiente: [0.005071510182608563,-0.08749723758347562] Loss: 22.849961192787454\n",
      "Iteracion: 8227 Gradiente: [0.005068814012132824,-0.08745072136626889] Loss: 22.849953513342516\n",
      "Iteracion: 8228 Gradiente: [0.005066119275071893,-0.08740422987850825] Loss: 22.849945842060656\n",
      "Iteracion: 8229 Gradiente: [0.005063425970588279,-0.08735776310705058] Loss: 22.84993817893321\n",
      "Iteracion: 8230 Gradiente: [0.005060734097931648,-0.0873113210387571] Loss: 22.84993052395151\n",
      "Iteracion: 8231 Gradiente: [0.005058043656283455,-0.0872649036604957] Loss: 22.84992287710688\n",
      "Iteracion: 8232 Gradiente: [0.005055354645041158,-0.08721851095913363] Loss: 22.849915238390704\n",
      "Iteracion: 8233 Gradiente: [0.005052667063284844,-0.08717214292155913] Loss: 22.849907607794297\n",
      "Iteracion: 8234 Gradiente: [0.005049980910410075,-0.0871257995346518] Loss: 22.849899985309058\n",
      "Iteracion: 8235 Gradiente: [0.005047296185603993,-0.08707948078531089] Loss: 22.849892370926373\n",
      "Iteracion: 8236 Gradiente: [0.005044612887975101,-0.08703318666044417] Loss: 22.84988476463761\n",
      "Iteracion: 8237 Gradiente: [0.005041931016920861,-0.08698691714695196] Loss: 22.84987716643416\n",
      "Iteracion: 8238 Gradiente: [0.005039250571664411,-0.08694067223175234] Loss: 22.849869576307455\n",
      "Iteracion: 8239 Gradiente: [0.005036571551309521,-0.08689445190177487] Loss: 22.84986199424888\n",
      "Iteracion: 8240 Gradiente: [0.005033893955281125,-0.08684825614393858] Loss: 22.84985442024987\n",
      "Iteracion: 8241 Gradiente: [0.005031217782811836,-0.08680208494518205] Loss: 22.84984685430187\n",
      "Iteracion: 8242 Gradiente: [0.005028543032947633,-0.0867559382924585] Loss: 22.849839296396297\n",
      "Iteracion: 8243 Gradiente: [0.005025869705208189,-0.08670981617270404] Loss: 22.849831746524647\n",
      "Iteracion: 8244 Gradiente: [0.005023197798572217,-0.08666371857289144] Loss: 22.849824204678324\n",
      "Iteracion: 8245 Gradiente: [0.005020527312444756,-0.08661764547997457] Loss: 22.849816670848835\n",
      "Iteracion: 8246 Gradiente: [0.0050178582459201,-0.08657159688093614] Loss: 22.84980914502763\n",
      "Iteracion: 8247 Gradiente: [0.005015190598445921,-0.08652557276273948] Loss: 22.84980162720623\n",
      "Iteracion: 8248 Gradiente: [0.005012524369292729,-0.0864795731123697] Loss: 22.84979411737611\n",
      "Iteracion: 8249 Gradiente: [0.0050098595574487115,-0.08643359791683498] Loss: 22.849786615528778\n",
      "Iteracion: 8250 Gradiente: [0.0050071961623605905,-0.08638764716312271] Loss: 22.849779121655768\n",
      "Iteracion: 8251 Gradiente: [0.005004534183189927,-0.0863417208382419] Loss: 22.84977163574856\n",
      "Iteracion: 8252 Gradiente: [0.0050018736191494405,-0.0862958189292088] Loss: 22.849764157798727\n",
      "Iteracion: 8253 Gradiente: [0.004999214469664063,-0.0862499414230322] Loss: 22.84975668779779\n",
      "Iteracion: 8254 Gradiente: [0.004996556733776932,-0.08620408830675169] Loss: 22.849749225737305\n",
      "Iteracion: 8255 Gradiente: [0.004993900410872243,-0.08615825956739229] Loss: 22.84974177160883\n",
      "Iteracion: 8256 Gradiente: [0.004991245500044291,-0.086112455192004] Loss: 22.849734325403947\n",
      "Iteracion: 8257 Gradiente: [0.004988592000738853,-0.08606667516762201] Loss: 22.84972688711421\n",
      "Iteracion: 8258 Gradiente: [0.0049859399121487515,-0.0860209194813045] Loss: 22.84971945673122\n",
      "Iteracion: 8259 Gradiente: [0.004983289233463969,-0.08597518812011534] Loss: 22.84971203424657\n",
      "Iteracion: 8260 Gradiente: [0.004980639963944592,-0.08592948107112287] Loss: 22.849704619651853\n",
      "Iteracion: 8261 Gradiente: [0.004977992102852605,-0.08588379832140008] Loss: 22.849697212938725\n",
      "Iteracion: 8262 Gradiente: [0.004975345649512519,-0.08583813985802541] Loss: 22.84968981409873\n",
      "Iteracion: 8263 Gradiente: [0.0049727006030366285,-0.08579250566809585] Loss: 22.849682423123575\n",
      "Iteracion: 8264 Gradiente: [0.004970056962751338,-0.08574689573870296] Loss: 22.84967504000487\n",
      "Iteracion: 8265 Gradiente: [0.004967414727929054,-0.08570131005694688] Loss: 22.84966766473425\n",
      "Iteracion: 8266 Gradiente: [0.004964773897750282,-0.08565574860994095] Loss: 22.849660297303398\n",
      "Iteracion: 8267 Gradiente: [0.004962134471513006,-0.08561021138479923] Loss: 22.849652937703965\n",
      "Iteracion: 8268 Gradiente: [0.004959496448530369,-0.0855646983686423] Loss: 22.84964558592763\n",
      "Iteracion: 8269 Gradiente: [0.00495685982791656,-0.0855192095486057] Loss: 22.84963824196608\n",
      "Iteracion: 8270 Gradiente: [0.004954224609139146,-0.08547374491181449] Loss: 22.849630905810997\n",
      "Iteracion: 8271 Gradiente: [0.004951590791222316,-0.08542830444542489] Loss: 22.849623577454086\n",
      "Iteracion: 8272 Gradiente: [0.0049489583736052134,-0.08538288813657904] Loss: 22.849616256887067\n",
      "Iteracion: 8273 Gradiente: [0.004946327355451293,-0.08533749597243698] Loss: 22.849608944101657\n",
      "Iteracion: 8274 Gradiente: [0.004943697735996011,-0.0852921279401644] Loss: 22.849601639089588\n",
      "Iteracion: 8275 Gradiente: [0.004941069514463455,-0.08524678402693422] Loss: 22.84959434184259\n",
      "Iteracion: 8276 Gradiente: [0.0049384426903079275,-0.08520146421991145] Loss: 22.849587052352412\n",
      "Iteracion: 8277 Gradiente: [0.004935817262607619,-0.0851561685062928] Loss: 22.8495797706108\n",
      "Iteracion: 8278 Gradiente: [0.004933193230605563,-0.08511089687326935] Loss: 22.84957249660953\n",
      "Iteracion: 8279 Gradiente: [0.004930570593662272,-0.08506564930803234] Loss: 22.849565230340364\n",
      "Iteracion: 8280 Gradiente: [0.004927949351056782,-0.08502042579778633] Loss: 22.849557971795086\n",
      "Iteracion: 8281 Gradiente: [0.00492532950194402,-0.08497522632974892] Loss: 22.84955072096551\n",
      "Iteracion: 8282 Gradiente: [0.004922711045530074,-0.08493005089114132] Loss: 22.849543477843383\n",
      "Iteracion: 8283 Gradiente: [0.004920093981307143,-0.08488489946917568] Loss: 22.849536242420545\n",
      "Iteracion: 8284 Gradiente: [0.004917478308241622,-0.08483977205110235] Loss: 22.84952901468882\n",
      "Iteracion: 8285 Gradiente: [0.004914864025786869,-0.08479466862414521] Loss: 22.849521794640015\n",
      "Iteracion: 8286 Gradiente: [0.004912251133238025,-0.08474958917555178] Loss: 22.84951458226597\n",
      "Iteracion: 8287 Gradiente: [0.004909639629816335,-0.08470453369257314] Loss: 22.84950737755852\n",
      "Iteracion: 8288 Gradiente: [0.004907029514729781,-0.08465950216247412] Loss: 22.84950018050953\n",
      "Iteracion: 8289 Gradiente: [0.004904420787199607,-0.08461449457252239] Loss: 22.849492991110864\n",
      "Iteracion: 8290 Gradiente: [0.0049018134465010615,-0.08456951090999046] Loss: 22.849485809354345\n",
      "Iteracion: 8291 Gradiente: [0.004899207492083709,-0.0845245511621453] Loss: 22.84947863523189\n",
      "Iteracion: 8292 Gradiente: [0.0048966029229603695,-0.08447961531628832] Loss: 22.849471468735373\n",
      "Iteracion: 8293 Gradiente: [0.004893999738560713,-0.08443470335970353] Loss: 22.849464309856693\n",
      "Iteracion: 8294 Gradiente: [0.004891397938073775,-0.08438981527969519] Loss: 22.849457158587757\n",
      "Iteracion: 8295 Gradiente: [0.004888797520802276,-0.08434495106356614] Loss: 22.84945001492046\n",
      "Iteracion: 8296 Gradiente: [0.004886198486000619,-0.08430011069863105] Loss: 22.84944287884674\n",
      "Iteracion: 8297 Gradiente: [0.004883600832846469,-0.0842552941722154] Loss: 22.84943575035849\n",
      "Iteracion: 8298 Gradiente: [0.004881004560787498,-0.08421050147163468] Loss: 22.849428629447694\n",
      "Iteracion: 8299 Gradiente: [0.004878409668917053,-0.08416573258423199] Loss: 22.849421516106272\n",
      "Iteracion: 8300 Gradiente: [0.004875816156642069,-0.08412098749734084] Loss: 22.849414410326176\n",
      "Iteracion: 8301 Gradiente: [0.004873224023079577,-0.0840762661983149] Loss: 22.84940731209937\n",
      "Iteracion: 8302 Gradiente: [0.004870633267632722,-0.08403156867450197] Loss: 22.84940022141782\n",
      "Iteracion: 8303 Gradiente: [0.004868043889500958,-0.08398689491326508] Loss: 22.84939313827355\n",
      "Iteracion: 8304 Gradiente: [0.0048654558878733194,-0.08394224490197502] Loss: 22.849386062658482\n",
      "Iteracion: 8305 Gradiente: [0.004862869262186109,-0.08389761862799858] Loss: 22.849378994564645\n",
      "Iteracion: 8306 Gradiente: [0.004860284011638782,-0.08385301607871583] Loss: 22.849371933984052\n",
      "Iteracion: 8307 Gradiente: [0.004857700135473427,-0.08380843724151982] Loss: 22.84936488090871\n",
      "Iteracion: 8308 Gradiente: [0.0048551176329340254,-0.08376388210380555] Loss: 22.84935783533061\n",
      "Iteracion: 8309 Gradiente: [0.004852536503355509,-0.08371935065296725] Loss: 22.849350797241836\n",
      "Iteracion: 8310 Gradiente: [0.004849956746101233,-0.08367484287640832] Loss: 22.849343766634384\n",
      "Iteracion: 8311 Gradiente: [0.004847378360158435,-0.08363035876155986] Loss: 22.84933674350032\n",
      "Iteracion: 8312 Gradiente: [0.004844801344960578,-0.08358589829583221] Loss: 22.849329727831716\n",
      "Iteracion: 8313 Gradiente: [0.004842225699880487,-0.08354146146664784] Loss: 22.84932271962061\n",
      "Iteracion: 8314 Gradiente: [0.0048396514240304596,-0.08349704826144944] Loss: 22.849315718859078\n",
      "Iteracion: 8315 Gradiente: [0.004837078516861955,-0.08345265866766631] Loss: 22.849308725539213\n",
      "Iteracion: 8316 Gradiente: [0.004834506977468322,-0.08340829267275787] Loss: 22.849301739653086\n",
      "Iteracion: 8317 Gradiente: [0.004831936805195862,-0.08336395026417355] Loss: 22.849294761192827\n",
      "Iteracion: 8318 Gradiente: [0.004829367999283818,-0.08331963142937618] Loss: 22.84928779015051\n",
      "Iteracion: 8319 Gradiente: [0.004826800559038702,-0.08327533615583108] Loss: 22.84928082651827\n",
      "Iteracion: 8320 Gradiente: [0.004824234483664706,-0.0832310644310161] Loss: 22.84927387028822\n",
      "Iteracion: 8321 Gradiente: [0.004821669772528026,-0.08318681624240654] Loss: 22.849266921452497\n",
      "Iteracion: 8322 Gradiente: [0.004819106424856538,-0.08314259157749397] Loss: 22.84925998000323\n",
      "Iteracion: 8323 Gradiente: [0.004816544440030649,-0.08309839042376505] Loss: 22.84925304593258\n",
      "Iteracion: 8324 Gradiente: [0.0048139838171503396,-0.08305421276873173] Loss: 22.8492461192327\n",
      "Iteracion: 8325 Gradiente: [0.004811424555614963,-0.08301005859989227] Loss: 22.849239199895756\n",
      "Iteracion: 8326 Gradiente: [0.0048088666545273405,-0.08296592790477296] Loss: 22.84923228791392\n",
      "Iteracion: 8327 Gradiente: [0.004806310113500937,-0.08292182067087393] Loss: 22.84922538327938\n",
      "Iteracion: 8328 Gradiente: [0.004803754931508782,-0.08287773688573985] Loss: 22.849218485984306\n",
      "Iteracion: 8329 Gradiente: [0.004801201107857385,-0.08283367653690389] Loss: 22.849211596020925\n",
      "Iteracion: 8330 Gradiente: [0.004798648642011472,-0.08278963961189482] Loss: 22.849204713381404\n",
      "Iteracion: 8331 Gradiente: [0.004796097533090915,-0.08274562609827048] Loss: 22.84919783805802\n",
      "Iteracion: 8332 Gradiente: [0.004793547780427805,-0.0827016359835787] Loss: 22.849190970042923\n",
      "Iteracion: 8333 Gradiente: [0.004790999383273705,-0.08265766925538334] Loss: 22.849184109328398\n",
      "Iteracion: 8334 Gradiente: [0.004788452340960703,-0.08261372590124835] Loss: 22.84917725590667\n",
      "Iteracion: 8335 Gradiente: [0.0047859066527792034,-0.08256980590874718] Loss: 22.849170409769975\n",
      "Iteracion: 8336 Gradiente: [0.004783362317861399,-0.08252590926546782] Loss: 22.849163570910587\n",
      "Iteracion: 8337 Gradiente: [0.004780819335654958,-0.08248203595898775] Loss: 22.849156739320755\n",
      "Iteracion: 8338 Gradiente: [0.00477827770529018,-0.08243818597690904] Loss: 22.849149914992765\n",
      "Iteracion: 8339 Gradiente: [0.004775737426189153,-0.08239435930682362] Loss: 22.849143097918887\n",
      "Iteracion: 8340 Gradiente: [0.004773198497598704,-0.08235055593633961] Loss: 22.84913628809141\n",
      "Iteracion: 8341 Gradiente: [0.004770660918797868,-0.08230677585307002] Loss: 22.849129485502626\n",
      "Iteracion: 8342 Gradiente: [0.004768124689045787,-0.08226301904463859] Loss: 22.84912269014486\n",
      "Iteracion: 8343 Gradiente: [0.004765589807531493,-0.08221928549867503] Loss: 22.849115902010418\n",
      "Iteracion: 8344 Gradiente: [0.004763056273815399,-0.08217557520279636] Loss: 22.849109121091637\n",
      "Iteracion: 8345 Gradiente: [0.004760524086860111,-0.08213188814466056] Loss: 22.84910234738079\n",
      "Iteracion: 8346 Gradiente: [0.0047579932460555105,-0.0820882243119101] Loss: 22.849095580870266\n",
      "Iteracion: 8347 Gradiente: [0.004755463750804741,-0.08204458369218628] Loss: 22.84908882155241\n",
      "Iteracion: 8348 Gradiente: [0.004752935600342312,-0.08200096627315373] Loss: 22.849082069419566\n",
      "Iteracion: 8349 Gradiente: [0.004750408793842098,-0.08195737204248559] Loss: 22.849075324464078\n",
      "Iteracion: 8350 Gradiente: [0.0047478833307176655,-0.0819138009878459] Loss: 22.849068586678342\n",
      "Iteracion: 8351 Gradiente: [0.004745359210237628,-0.08187025309691312] Loss: 22.849061856054742\n",
      "Iteracion: 8352 Gradiente: [0.00474283643162039,-0.08182672835737972] Loss: 22.849055132585637\n",
      "Iteracion: 8353 Gradiente: [0.004740314994186671,-0.08178322675693318] Loss: 22.849048416263432\n",
      "Iteracion: 8354 Gradiente: [0.004737794897267614,-0.08173974828327152] Loss: 22.849041707080556\n",
      "Iteracion: 8355 Gradiente: [0.004735276140038991,-0.08169629292410269] Loss: 22.84903500502939\n",
      "Iteracion: 8356 Gradiente: [0.004732758721883101,-0.08165286066713749] Loss: 22.849028310102362\n",
      "Iteracion: 8357 Gradiente: [0.004730242642102667,-0.08160945150009055] Loss: 22.84902162229189\n",
      "Iteracion: 8358 Gradiente: [0.0047277278999833545,-0.08156606541068617] Loss: 22.849014941590433\n",
      "Iteracion: 8359 Gradiente: [0.0047252144946374605,-0.08152270238666723] Loss: 22.849008267990403\n",
      "Iteracion: 8360 Gradiente: [0.004722702425621605,-0.08147936241575605] Loss: 22.84900160148429\n",
      "Iteracion: 8361 Gradiente: [0.004720191692063243,-0.08143604548570534] Loss: 22.84899494206451\n",
      "Iteracion: 8362 Gradiente: [0.004717682293342781,-0.08139275158426014] Loss: 22.84898828972356\n",
      "Iteracion: 8363 Gradiente: [0.004715174228581987,-0.08134948069919012] Loss: 22.848981644453897\n",
      "Iteracion: 8364 Gradiente: [0.004712667497152741,-0.08130623281825228] Loss: 22.848975006248036\n",
      "Iteracion: 8365 Gradiente: [0.0047101620984619785,-0.0812630079292108] Loss: 22.84896837509844\n",
      "Iteracion: 8366 Gradiente: [0.004707658031746101,-0.08121980601984381] Loss: 22.8489617509976\n",
      "Iteracion: 8367 Gradiente: [0.004705155296222566,-0.08117662707794106] Loss: 22.84895513393805\n",
      "Iteracion: 8368 Gradiente: [0.004702653891182725,-0.08113347109129367] Loss: 22.84894852391229\n",
      "Iteracion: 8369 Gradiente: [0.004700153816069512,-0.08109033804768587] Loss: 22.848941920912864\n",
      "Iteracion: 8370 Gradiente: [0.0046976550700170115,-0.08104722793493208] Loss: 22.848935324932267\n",
      "Iteracion: 8371 Gradiente: [0.004695157652392368,-0.08100414074083619] Loss: 22.848928735963067\n",
      "Iteracion: 8372 Gradiente: [0.004692661562424405,-0.08096107645321832] Loss: 22.848922153997805\n",
      "Iteracion: 8373 Gradiente: [0.004690166799503004,-0.08091803505989385] Loss: 22.848915579029025\n",
      "Iteracion: 8374 Gradiente: [0.004687673362856988,-0.0808750165486971] Loss: 22.84890901104929\n",
      "Iteracion: 8375 Gradiente: [0.004685181251855397,-0.0808320209074574] Loss: 22.848902450051185\n",
      "Iteracion: 8376 Gradiente: [0.004682690465718527,-0.0807890481240219] Loss: 22.84889589602728\n",
      "Iteracion: 8377 Gradiente: [0.004680201003665729,-0.0807460981862428] Loss: 22.848889348970157\n",
      "Iteracion: 8378 Gradiente: [0.004677712865195834,-0.08070317108196233] Loss: 22.84888280887242\n",
      "Iteracion: 8379 Gradiente: [0.004675226049437242,-0.08066026679905096] Loss: 22.84887627572668\n",
      "Iteracion: 8380 Gradiente: [0.004672740555763729,-0.08061738532537355] Loss: 22.848869749525527\n",
      "Iteracion: 8381 Gradiente: [0.004670256383478962,-0.08057452664880115] Loss: 22.84886323026159\n",
      "Iteracion: 8382 Gradiente: [0.004667773531870504,-0.0805316907572165] Loss: 22.848856717927493\n",
      "Iteracion: 8383 Gradiente: [0.004665292000155811,-0.08048887763851033] Loss: 22.848850212515856\n",
      "Iteracion: 8384 Gradiente: [0.004662811787801502,-0.08044608728056536] Loss: 22.848843714019342\n",
      "Iteracion: 8385 Gradiente: [0.004660332893907556,-0.0804033196712942] Loss: 22.848837222430596\n",
      "Iteracion: 8386 Gradiente: [0.004657855317935855,-0.08036057479859361] Loss: 22.848830737742258\n",
      "Iteracion: 8387 Gradiente: [0.004655379059036591,-0.08031785265038446] Loss: 22.848824259947015\n",
      "Iteracion: 8388 Gradiente: [0.004652904116579748,-0.08027515321458137] Loss: 22.848817789037533\n",
      "Iteracion: 8389 Gradiente: [0.004650430489894575,-0.0802324764791083] Loss: 22.84881132500649\n",
      "Iteracion: 8390 Gradiente: [0.004647958178358636,-0.08018982243189114] Loss: 22.84880486784657\n",
      "Iteracion: 8391 Gradiente: [0.004645487181134437,-0.08014719106087813] Loss: 22.848798417550483\n",
      "Iteracion: 8392 Gradiente: [0.00464301749755028,-0.08010458235401087] Loss: 22.848791974110917\n",
      "Iteracion: 8393 Gradiente: [0.00464054912690699,-0.0800619962992414] Loss: 22.848785537520584\n",
      "Iteracion: 8394 Gradiente: [0.004638082068534762,-0.08001943288452698] Loss: 22.84877910777222\n",
      "Iteracion: 8395 Gradiente: [0.004635616321680421,-0.07997689209783149] Loss: 22.84877268485855\n",
      "Iteracion: 8396 Gradiente: [0.004633151885793533,-0.07993437392711845] Loss: 22.84876626877228\n",
      "Iteracion: 8397 Gradiente: [0.004630688760066922,-0.07989187836037033] Loss: 22.848759859506192\n",
      "Iteracion: 8398 Gradiente: [0.004628226943844993,-0.07984940538556806] Loss: 22.848753457052997\n",
      "Iteracion: 8399 Gradiente: [0.004625766436296886,-0.0798069549907087] Loss: 22.848747061405493\n",
      "Iteracion: 8400 Gradiente: [0.004623307236880691,-0.07976452716377953] Loss: 22.84874067255641\n",
      "Iteracion: 8401 Gradiente: [0.004620849344890606,-0.07972212189278202] Loss: 22.848734290498545\n",
      "Iteracion: 8402 Gradiente: [0.0046183927594815564,-0.07967973916573629] Loss: 22.848727915224686\n",
      "Iteracion: 8403 Gradiente: [0.00461593748006616,-0.0796373789706486] Loss: 22.848721546727578\n",
      "Iteracion: 8404 Gradiente: [0.004613483506084511,-0.0795950412955334] Loss: 22.84871518500006\n",
      "Iteracion: 8405 Gradiente: [0.00461103083663564,-0.07955272612843013] Loss: 22.84870883003494\n",
      "Iteracion: 8406 Gradiente: [0.004608579471089532,-0.07951043345736934] Loss: 22.848702481825\n",
      "Iteracion: 8407 Gradiente: [0.004606129408776383,-0.07946816327039023] Loss: 22.848696140363074\n",
      "Iteracion: 8408 Gradiente: [0.004603680649012176,-0.07942591555553756] Loss: 22.84868980564198\n",
      "Iteracion: 8409 Gradiente: [0.004601233191121423,-0.07938369030086652] Loss: 22.848683477654568\n",
      "Iteracion: 8410 Gradiente: [0.004598787034277052,-0.07934148749444177] Loss: 22.848677156393666\n",
      "Iteracion: 8411 Gradiente: [0.0045963421779238916,-0.07929930712432286] Loss: 22.84867084185215\n",
      "Iteracion: 8412 Gradiente: [0.004593898621291714,-0.07925714917858416] Loss: 22.848664534022838\n",
      "Iteracion: 8413 Gradiente: [0.004591456363808296,-0.07921501364529959] Loss: 22.84865823289862\n",
      "Iteracion: 8414 Gradiente: [0.004589015404681618,-0.07917290051255914] Loss: 22.84865193847235\n",
      "Iteracion: 8415 Gradiente: [0.004586575743224823,-0.07913080976845327] Loss: 22.848645650736948\n",
      "Iteracion: 8416 Gradiente: [0.0045841373787965265,-0.07908874140107827] Loss: 22.848639369685255\n",
      "Iteracion: 8417 Gradiente: [0.0045817003106757665,-0.07904669539853823] Loss: 22.848633095310195\n",
      "Iteracion: 8418 Gradiente: [0.004579264538127366,-0.07900467174894578] Loss: 22.848626827604644\n",
      "Iteracion: 8419 Gradiente: [0.004576830060511838,-0.07896267044041672] Loss: 22.848620566561557\n",
      "Iteracion: 8420 Gradiente: [0.004574396877182115,-0.07892069146106924] Loss: 22.848614312173815\n",
      "Iteracion: 8421 Gradiente: [0.004571964987352809,-0.07887873479903919] Loss: 22.848608064434348\n",
      "Iteracion: 8422 Gradiente: [0.004569534390484856,-0.07883680044245231] Loss: 22.8486018233361\n",
      "Iteracion: 8423 Gradiente: [0.004567105085802344,-0.07879488837945677] Loss: 22.848595588872012\n",
      "Iteracion: 8424 Gradiente: [0.004564677072498095,-0.07875299859820745] Loss: 22.848589361035025\n",
      "Iteracion: 8425 Gradiente: [0.00456225035004536,-0.07871113108684812] Loss: 22.848583139818086\n",
      "Iteracion: 8426 Gradiente: [0.004559824917765809,-0.07866928583354044] Loss: 22.848576925214186\n",
      "Iteracion: 8427 Gradiente: [0.004557400774804894,-0.07862746282645988] Loss: 22.848570717216255\n",
      "Iteracion: 8428 Gradiente: [0.004554977920724923,-0.07858566205376612] Loss: 22.848564515817312\n",
      "Iteracion: 8429 Gradiente: [0.004552556354722507,-0.0785438835036454] Loss: 22.848558321010323\n",
      "Iteracion: 8430 Gradiente: [0.004550136075961101,-0.07850212716429278] Loss: 22.848552132788292\n",
      "Iteracion: 8431 Gradiente: [0.0045477170840086956,-0.07846039302388555] Loss: 22.848545951144196\n",
      "Iteracion: 8432 Gradiente: [0.004545299377930216,-0.0784186810706359] Loss: 22.848539776071053\n",
      "Iteracion: 8433 Gradiente: [0.004542882957260493,-0.07837699129273558] Loss: 22.84853360756189\n",
      "Iteracion: 8434 Gradiente: [0.00454046782122551,-0.07833532367840117] Loss: 22.848527445609715\n",
      "Iteracion: 8435 Gradiente: [0.004538053969132723,-0.07829367821585163] Loss: 22.848521290207565\n",
      "Iteracion: 8436 Gradiente: [0.004535641400421279,-0.07825205489330168] Loss: 22.848515141348482\n",
      "Iteracion: 8437 Gradiente: [0.004533230114206314,-0.07821045369899385] Loss: 22.8485089990255\n",
      "Iteracion: 8438 Gradiente: [0.004530820109946868,-0.07816887462115561] Loss: 22.848502863231673\n",
      "Iteracion: 8439 Gradiente: [0.004528411386890714,-0.0781273176480326] Loss: 22.84849673396008\n",
      "Iteracion: 8440 Gradiente: [0.004526003944373732,-0.07808578276787254] Loss: 22.848490611203754\n",
      "Iteracion: 8441 Gradiente: [0.00452359778178959,-0.0780442699689265] Loss: 22.848484494955805\n",
      "Iteracion: 8442 Gradiente: [0.004521192898438168,-0.07800277923945581] Loss: 22.848478385209294\n",
      "Iteracion: 8443 Gradiente: [0.004518789293455446,-0.07796131056773893] Loss: 22.848472281957303\n",
      "Iteracion: 8444 Gradiente: [0.00451638696638194,-0.07791986394203589] Loss: 22.84846618519294\n",
      "Iteracion: 8445 Gradiente: [0.00451398591640005,-0.07787843935063421] Loss: 22.84846009490932\n",
      "Iteracion: 8446 Gradiente: [0.004511586142965029,-0.07783703678181328] Loss: 22.848454011099534\n",
      "Iteracion: 8447 Gradiente: [0.004509187645293385,-0.07779565622386984] Loss: 22.84844793375671\n",
      "Iteracion: 8448 Gradiente: [0.004506790422737102,-0.07775429766510177] Loss: 22.848441862873962\n",
      "Iteracion: 8449 Gradiente: [0.004504394474676587,-0.07771296109380958] Loss: 22.848435798444445\n",
      "Iteracion: 8450 Gradiente: [0.0045019998002554,-0.07767164649831469] Loss: 22.84842974046128\n",
      "Iteracion: 8451 Gradiente: [0.0044996063989032106,-0.07763035386692761] Loss: 22.848423688917656\n",
      "Iteracion: 8452 Gradiente: [0.004497214270018427,-0.07758908318796713] Loss: 22.84841764380665\n",
      "Iteracion: 8453 Gradiente: [0.004494823412898086,-0.07754783444976461] Loss: 22.848411605121495\n",
      "Iteracion: 8454 Gradiente: [0.004492433826824064,-0.07750660764065778] Loss: 22.848405572855313\n",
      "Iteracion: 8455 Gradiente: [0.00449004551112561,-0.07746540274898805] Loss: 22.848399547001307\n",
      "Iteracion: 8456 Gradiente: [0.004487658465082708,-0.07742421976310633] Loss: 22.848393527552677\n",
      "Iteracion: 8457 Gradiente: [0.004485272688098499,-0.07738305867136255] Loss: 22.84838751450258\n",
      "Iteracion: 8458 Gradiente: [0.004482888179477603,-0.07734191946211683] Loss: 22.848381507844223\n",
      "Iteracion: 8459 Gradiente: [0.004480504938428945,-0.07730080212374352] Loss: 22.848375507570818\n",
      "Iteracion: 8460 Gradiente: [0.00447812296456694,-0.07725970664459976] Loss: 22.84836951367557\n",
      "Iteracion: 8461 Gradiente: [0.004475742256903459,-0.07721863301308171] Loss: 22.84836352615173\n",
      "Iteracion: 8462 Gradiente: [0.004473362814982807,-0.07717758121756214] Loss: 22.848357544992478\n",
      "Iteracion: 8463 Gradiente: [0.00447098463797128,-0.07713655124643945] Loss: 22.84835157019108\n",
      "Iteracion: 8464 Gradiente: [0.00446860772524739,-0.07709554308811294] Loss: 22.848345601740764\n",
      "Iteracion: 8465 Gradiente: [0.00446623207623702,-0.07705455673097662] Loss: 22.848339639634776\n",
      "Iteracion: 8466 Gradiente: [0.0044638576902722585,-0.07701359216344131] Loss: 22.848333683866393\n",
      "Iteracion: 8467 Gradiente: [0.00446148456643698,-0.07697264937393769] Loss: 22.848327734428874\n",
      "Iteracion: 8468 Gradiente: [0.00445911270427833,-0.07693172835087386] Loss: 22.84832179131546\n",
      "Iteracion: 8469 Gradiente: [0.004456742103170086,-0.07689082908267673] Loss: 22.848315854519456\n",
      "Iteracion: 8470 Gradiente: [0.004454372762292754,-0.07684995155778912] Loss: 22.848309924034147\n",
      "Iteracion: 8471 Gradiente: [0.004452004681005898,-0.07680909576464963] Loss: 22.848303999852813\n",
      "Iteracion: 8472 Gradiente: [0.004449637858674767,-0.07676826169170402] Loss: 22.848298081968757\n",
      "Iteracion: 8473 Gradiente: [0.004447272294593556,-0.07672744932740526] Loss: 22.848292170375284\n",
      "Iteracion: 8474 Gradiente: [0.0044449079880867735,-0.07668665866021472] Loss: 22.848286265065703\n",
      "Iteracion: 8475 Gradiente: [0.00444254493858125,-0.07664588967859048] Loss: 22.848280366033354\n",
      "Iteracion: 8476 Gradiente: [0.004440183145261282,-0.07660514237101487] Loss: 22.848274473271548\n",
      "Iteracion: 8477 Gradiente: [0.004437822607704334,-0.07656441672594809] Loss: 22.84826858677362\n",
      "Iteracion: 8478 Gradiente: [0.004435463325019858,-0.07652371273188739] Loss: 22.84826270653292\n",
      "Iteracion: 8479 Gradiente: [0.0044331052965115225,-0.07648303037732541] Loss: 22.84825683254279\n",
      "Iteracion: 8480 Gradiente: [0.004430748521673422,-0.07644236965074853] Loss: 22.84825096479657\n",
      "Iteracion: 8481 Gradiente: [0.004428392999735327,-0.07640173054066277] Loss: 22.848245103287663\n",
      "Iteracion: 8482 Gradiente: [0.004426038730131646,-0.076361113035572] Loss: 22.84823924800941\n",
      "Iteracion: 8483 Gradiente: [0.004423685712055203,-0.07632051712399743] Loss: 22.848233398955177\n",
      "Iteracion: 8484 Gradiente: [0.004421333944938511,-0.07627994279445455] Loss: 22.84822755611838\n",
      "Iteracion: 8485 Gradiente: [0.0044189834281032365,-0.07623939003547058] Loss: 22.84822171949239\n",
      "Iteracion: 8486 Gradiente: [0.004416634160857787,-0.07619885883557925] Loss: 22.848215889070612\n",
      "Iteracion: 8487 Gradiente: [0.004414286142587306,-0.07615834918331525] Loss: 22.848210064846445\n",
      "Iteracion: 8488 Gradiente: [0.004411939372543353,-0.07611786106722912] Loss: 22.848204246813314\n",
      "Iteracion: 8489 Gradiente: [0.0044095938501593915,-0.07607739447586705] Loss: 22.848198434964612\n",
      "Iteracion: 8490 Gradiente: [0.0044072495747570885,-0.07603694939778206] Loss: 22.8481926292938\n",
      "Iteracion: 8491 Gradiente: [0.004404906545577584,-0.07599652582154756] Loss: 22.848186829794276\n",
      "Iteracion: 8492 Gradiente: [0.004402564762013602,-0.0759561237357275] Loss: 22.848181036459504\n",
      "Iteracion: 8493 Gradiente: [0.004400224223451233,-0.07591574312889536] Loss: 22.848175249282928\n",
      "Iteracion: 8494 Gradiente: [0.004397884929212144,-0.07587538398963124] Loss: 22.848169468257996\n",
      "Iteracion: 8495 Gradiente: [0.0043955468786236905,-0.07583504630652284] Loss: 22.84816369337816\n",
      "Iteracion: 8496 Gradiente: [0.004393210070936486,-0.07579473006817092] Loss: 22.84815792463691\n",
      "Iteracion: 8497 Gradiente: [0.004390874505622833,-0.075754435263165] Loss: 22.84815216202771\n",
      "Iteracion: 8498 Gradiente: [0.004388540182011032,-0.07571416188011154] Loss: 22.848146405544036\n",
      "Iteracion: 8499 Gradiente: [0.004386207099387699,-0.07567390990762514] Loss: 22.84814065517939\n",
      "Iteracion: 8500 Gradiente: [0.004383875257082082,-0.07563367933432398] Loss: 22.84813491092724\n",
      "Iteracion: 8501 Gradiente: [0.004381544654380795,-0.07559347014883523] Loss: 22.84812917278113\n",
      "Iteracion: 8502 Gradiente: [0.004379215290700245,-0.07555328233978334] Loss: 22.848123440734533\n",
      "Iteracion: 8503 Gradiente: [0.004376887165455893,-0.07551311589579986] Loss: 22.848117714780997\n",
      "Iteracion: 8504 Gradiente: [0.00437456027790688,-0.07547297080553268] Loss: 22.84811199491401\n",
      "Iteracion: 8505 Gradiente: [0.0043722346274231915,-0.0754328470576245] Loss: 22.848106281127123\n",
      "Iteracion: 8506 Gradiente: [0.0043699102132876535,-0.07539274464073528] Loss: 22.848100573413873\n",
      "Iteracion: 8507 Gradiente: [0.004367587034868355,-0.07535266354352228] Loss: 22.8480948717678\n",
      "Iteracion: 8508 Gradiente: [0.004365265091476544,-0.07531260375465401] Loss: 22.848089176182455\n",
      "Iteracion: 8509 Gradiente: [0.004362944382612947,-0.07527256526279315] Loss: 22.848083486651408\n",
      "Iteracion: 8510 Gradiente: [0.004360624907421121,-0.07523254805662714] Loss: 22.848077803168184\n",
      "Iteracion: 8511 Gradiente: [0.004358306665403688,-0.07519255212483339] Loss: 22.84807212572642\n",
      "Iteracion: 8512 Gradiente: [0.004355989655783788,-0.07515257745610811] Loss: 22.848066454319632\n",
      "Iteracion: 8513 Gradiente: [0.004353673877938983,-0.07511262403914452] Loss: 22.848060788941417\n",
      "Iteracion: 8514 Gradiente: [0.004351359331303683,-0.07507269186263912] Loss: 22.848055129585415\n",
      "Iteracion: 8515 Gradiente: [0.004349046015083028,-0.07503278091530845] Loss: 22.848049476245155\n",
      "Iteracion: 8516 Gradiente: [0.004346733928721846,-0.07499289118586142] Loss: 22.848043828914296\n",
      "Iteracion: 8517 Gradiente: [0.004344423071507701,-0.07495302266302038] Loss: 22.84803818758642\n",
      "Iteracion: 8518 Gradiente: [0.00434211344290721,-0.07491317533550396] Loss: 22.848032552255155\n",
      "Iteracion: 8519 Gradiente: [0.004339805042166253,-0.07487334919205076] Loss: 22.848026922914126\n",
      "Iteracion: 8520 Gradiente: [0.004337497868572389,-0.07483354422140173] Loss: 22.848021299556976\n",
      "Iteracion: 8521 Gradiente: [0.0043351919215240285,-0.07479376041229836] Loss: 22.848015682177333\n",
      "Iteracion: 8522 Gradiente: [0.004332887200443262,-0.07475399775348528] Loss: 22.84801007076884\n",
      "Iteracion: 8523 Gradiente: [0.004330583704647021,-0.07471425623372158] Loss: 22.84800446532516\n",
      "Iteracion: 8524 Gradiente: [0.0043282814333688675,-0.07467453584177512] Loss: 22.84799886583994\n",
      "Iteracion: 8525 Gradiente: [0.0043259803861862645,-0.07463483656639862] Loss: 22.847993272306855\n",
      "Iteracion: 8526 Gradiente: [0.004323680562256982,-0.07459515839637922] Loss: 22.84798768471959\n",
      "Iteracion: 8527 Gradiente: [0.004321381960931111,-0.07455550132049661] Loss: 22.847982103071804\n",
      "Iteracion: 8528 Gradiente: [0.004319084581605163,-0.07451586532753254] Loss: 22.84797652735718\n",
      "Iteracion: 8529 Gradiente: [0.004316788423689862,-0.07447625040627687] Loss: 22.84797095756944\n",
      "Iteracion: 8530 Gradiente: [0.004314493486470876,-0.07443665654552992] Loss: 22.847965393702246\n",
      "Iteracion: 8531 Gradiente: [0.004312199769252819,-0.07439708373409697] Loss: 22.847959835749343\n",
      "Iteracion: 8532 Gradiente: [0.004309907271471995,-0.074357531960784] Loss: 22.847954283704407\n",
      "Iteracion: 8533 Gradiente: [0.004307615992532495,-0.07431800121440399] Loss: 22.847948737561204\n",
      "Iteracion: 8534 Gradiente: [0.004305325931595879,-0.07427849148378828] Loss: 22.847943197313416\n",
      "Iteracion: 8535 Gradiente: [0.004303037088203609,-0.07423900275775258] Loss: 22.847937662954802\n",
      "Iteracion: 8536 Gradiente: [0.004300749461625249,-0.07419953502513697] Loss: 22.847932134479077\n",
      "Iteracion: 8537 Gradiente: [0.004298463051157834,-0.07416008827478286] Loss: 22.847926611880027\n",
      "Iteracion: 8538 Gradiente: [0.004296177856285984,-0.07412066249552834] Loss: 22.84792109515136\n",
      "Iteracion: 8539 Gradiente: [0.0042938938762214695,-0.0740812576762321] Loss: 22.847915584286866\n",
      "Iteracion: 8540 Gradiente: [0.004291611110472596,-0.0740418738057438] Loss: 22.84791007928032\n",
      "Iteracion: 8541 Gradiente: [0.004289329558232188,-0.07400251087293329] Loss: 22.847904580125462\n",
      "Iteracion: 8542 Gradiente: [0.004287049219012336,-0.07396316886666186] Loss: 22.847899086816096\n",
      "Iteracion: 8543 Gradiente: [0.004284770092078815,-0.07392384777580942] Loss: 22.847893599345987\n",
      "Iteracion: 8544 Gradiente: [0.004282492176759926,-0.07388454758925818] Loss: 22.84788811770896\n",
      "Iteracion: 8545 Gradiente: [0.004280215472520391,-0.07384526829588746] Loss: 22.847882641898785\n",
      "Iteracion: 8546 Gradiente: [0.004277939978581458,-0.07380600988459882] Loss: 22.84787717190928\n",
      "Iteracion: 8547 Gradiente: [0.004275665694456166,-0.07376677234428174] Loss: 22.847871707734274\n",
      "Iteracion: 8548 Gradiente: [0.004273392619283337,-0.07372755566385318] Loss: 22.847866249367556\n",
      "Iteracion: 8549 Gradiente: [0.004271120752587384,-0.0736883598322122] Loss: 22.847860796802973\n",
      "Iteracion: 8550 Gradiente: [0.004268850093612287,-0.07364918483828392] Loss: 22.84785535003434\n",
      "Iteracion: 8551 Gradiente: [0.00426658064186635,-0.07361003067097964] Loss: 22.84784990905552\n",
      "Iteracion: 8552 Gradiente: [0.004264312396659875,-0.07357089731923097] Loss: 22.84784447386033\n",
      "Iteracion: 8553 Gradiente: [0.004262045357264318,-0.07353178477197748] Loss: 22.847839044442637\n",
      "Iteracion: 8554 Gradiente: [0.004259779523110296,-0.07349269301815285] Loss: 22.8478336207963\n",
      "Iteracion: 8555 Gradiente: [0.004257514893568745,-0.07345362204670304] Loss: 22.847828202915196\n",
      "Iteracion: 8556 Gradiente: [0.0042552514680267,-0.07341457184657862] Loss: 22.847822790793177\n",
      "Iteracion: 8557 Gradiente: [0.004252989245690249,-0.07337554240674535] Loss: 22.84781738442413\n",
      "Iteracion: 8558 Gradiente: [0.004250728226075277,-0.0733365337161563] Loss: 22.84781198380195\n",
      "Iteracion: 8559 Gradiente: [0.004248468408512925,-0.07329754576378242] Loss: 22.8478065889205\n",
      "Iteracion: 8560 Gradiente: [0.004246209792370337,-0.07325857853859942] Loss: 22.84780119977371\n",
      "Iteracion: 8561 Gradiente: [0.004243952376835599,-0.07321963202959789] Loss: 22.847795816355454\n",
      "Iteracion: 8562 Gradiente: [0.004241696161481438,-0.07318070622575164] Loss: 22.84779043865965\n",
      "Iteracion: 8563 Gradiente: [0.004239441145614364,-0.07314180111605709] Loss: 22.847785066680235\n",
      "Iteracion: 8564 Gradiente: [0.0042371873285228885,-0.07310291668951692] Loss: 22.847779700411113\n",
      "Iteracion: 8565 Gradiente: [0.004234934709649944,-0.0730640529351291] Loss: 22.847774339846247\n",
      "Iteracion: 8566 Gradiente: [0.0042326832884498344,-0.07302520984190022] Loss: 22.847768984979528\n",
      "Iteracion: 8567 Gradiente: [0.004230433064070856,-0.07298638739885893] Loss: 22.847763635804906\n",
      "Iteracion: 8568 Gradiente: [0.0042281840360108925,-0.07294758559501642] Loss: 22.847758292316357\n",
      "Iteracion: 8569 Gradiente: [0.0042259362035868735,-0.07290880441940455] Loss: 22.84775295450782\n",
      "Iteracion: 8570 Gradiente: [0.004223689566180156,-0.07287004386105818] Loss: 22.847747622373262\n",
      "Iteracion: 8571 Gradiente: [0.0042214441231758805,-0.07283130390901237] Loss: 22.84774229590665\n",
      "Iteracion: 8572 Gradiente: [0.0042191998739175085,-0.07279258455231456] Loss: 22.847736975101974\n",
      "Iteracion: 8573 Gradiente: [0.004216956817737127,-0.07275388578001601] Loss: 22.847731659953173\n",
      "Iteracion: 8574 Gradiente: [0.0042147149540644096,-0.0727152075811715] Loss: 22.84772635045427\n",
      "Iteracion: 8575 Gradiente: [0.0042124742821840755,-0.0726765499448477] Loss: 22.84772104659925\n",
      "Iteracion: 8576 Gradiente: [0.004210234801504006,-0.07263791286011051] Loss: 22.847715748382132\n",
      "Iteracion: 8577 Gradiente: [0.004207996511487977,-0.07259929631602825] Loss: 22.847710455796893\n",
      "Iteracion: 8578 Gradiente: [0.004205759411447237,-0.07256070030168509] Loss: 22.847705168837567\n",
      "Iteracion: 8579 Gradiente: [0.004203523500647558,-0.07252212480617075] Loss: 22.847699887498152\n",
      "Iteracion: 8580 Gradiente: [0.004201288778544191,-0.0724835698185732] Loss: 22.847694611772695\n",
      "Iteracion: 8581 Gradiente: [0.004199055244428488,-0.07244503532799437] Loss: 22.847689341655226\n",
      "Iteracion: 8582 Gradiente: [0.004196822897741489,-0.072406521323533] Loss: 22.847684077139785\n",
      "Iteracion: 8583 Gradiente: [0.0041945917378996,-0.07236802779429514] Loss: 22.847678818220402\n",
      "Iteracion: 8584 Gradiente: [0.004192361764177121,-0.07232955472939959] Loss: 22.847673564891146\n",
      "Iteracion: 8585 Gradiente: [0.00419013297599046,-0.07229110211796694] Loss: 22.847668317146056\n",
      "Iteracion: 8586 Gradiente: [0.004187905372690656,-0.07225266994912213] Loss: 22.847663074979213\n",
      "Iteracion: 8587 Gradiente: [0.004185678953615479,-0.07221425821200095] Loss: 22.84765783838469\n",
      "Iteracion: 8588 Gradiente: [0.004183453718211657,-0.07217586689573553] Loss: 22.847652607356547\n",
      "Iteracion: 8589 Gradiente: [0.004181229665862437,-0.07213749598946985] Loss: 22.847647381888883\n",
      "Iteracion: 8590 Gradiente: [0.004179006795816538,-0.07209914548235853] Loss: 22.847642161975774\n",
      "Iteracion: 8591 Gradiente: [0.0041767851075671086,-0.07206081536355195] Loss: 22.847636947611317\n",
      "Iteracion: 8592 Gradiente: [0.004174564600356234,-0.07202250562221858] Loss: 22.847631738789627\n",
      "Iteracion: 8593 Gradiente: [0.004172345273640114,-0.07198421624751863] Loss: 22.847626535504812\n",
      "Iteracion: 8594 Gradiente: [0.004170127126817154,-0.07194594722862296] Loss: 22.84762133775096\n",
      "Iteracion: 8595 Gradiente: [0.00416791015924313,-0.07190769855471153] Loss: 22.84761614552222\n",
      "Iteracion: 8596 Gradiente: [0.004165694370274764,-0.07186947021497012] Loss: 22.847610958812705\n",
      "Iteracion: 8597 Gradiente: [0.004163479759263093,-0.0718312621985883] Loss: 22.84760577761656\n",
      "Iteracion: 8598 Gradiente: [0.004161266325710736,-0.0717930744947554] Loss: 22.847600601927915\n",
      "Iteracion: 8599 Gradiente: [0.00415905406878873,-0.071754907092683] Loss: 22.847595431740906\n",
      "Iteracion: 8600 Gradiente: [0.004156842988034744,-0.07171675998156933] Loss: 22.847590267049714\n",
      "Iteracion: 8601 Gradiente: [0.00415463308263592,-0.07167863315063899] Loss: 22.847585107848463\n",
      "Iteracion: 8602 Gradiente: [0.004152424352120458,-0.07164052658909933] Loss: 22.847579954131326\n",
      "Iteracion: 8603 Gradiente: [0.004150216795852657,-0.07160244028617768] Loss: 22.847574805892506\n",
      "Iteracion: 8604 Gradiente: [0.004148010413275453,-0.07156437423109878] Loss: 22.84756966312614\n",
      "Iteracion: 8605 Gradiente: [0.004145805203554194,-0.07152632841310916] Loss: 22.847564525826417\n",
      "Iteracion: 8606 Gradiente: [0.0041436011662815036,-0.07148830282143924] Loss: 22.847559393987535\n",
      "Iteracion: 8607 Gradiente: [0.00414139830077147,-0.07145029744534016] Loss: 22.84755426760371\n",
      "Iteracion: 8608 Gradiente: [0.004139196606312605,-0.07141231227406664] Loss: 22.847549146669106\n",
      "Iteracion: 8609 Gradiente: [0.004136996082319418,-0.07137434729688093] Loss: 22.847544031177964\n",
      "Iteracion: 8610 Gradiente: [0.004134796728159055,-0.07133640250304284] Loss: 22.847538921124467\n",
      "Iteracion: 8611 Gradiente: [0.004132598543324662,-0.07129847788181655] Loss: 22.847533816502835\n",
      "Iteracion: 8612 Gradiente: [0.004130401527097168,-0.07126057342248479] Loss: 22.84752871730732\n",
      "Iteracion: 8613 Gradiente: [0.004128205678907193,-0.07122268911432397] Loss: 22.847523623532133\n",
      "Iteracion: 8614 Gradiente: [0.0041260109980603,-0.07118482494662727] Loss: 22.84751853517154\n",
      "Iteracion: 8615 Gradiente: [0.0041238174840013166,-0.07114698090868311] Loss: 22.84751345221977\n",
      "Iteracion: 8616 Gradiente: [0.004121625136074651,-0.07110915698978969] Loss: 22.847508374671044\n",
      "Iteracion: 8617 Gradiente: [0.004119433953575443,-0.07107135317925911] Loss: 22.84750330251966\n",
      "Iteracion: 8618 Gradiente: [0.004117243936101052,-0.0710335694663872] Loss: 22.847498235759858\n",
      "Iteracion: 8619 Gradiente: [0.004115055082819671,-0.07099580584050204] Loss: 22.847493174385924\n",
      "Iteracion: 8620 Gradiente: [0.00411286739317139,-0.07095806229092089] Loss: 22.84748811839211\n",
      "Iteracion: 8621 Gradiente: [0.004110680866663567,-0.07092033880696344] Loss: 22.84748306777272\n",
      "Iteracion: 8622 Gradiente: [0.004108495502555343,-0.07088263537796978] Loss: 22.84747802252203\n",
      "Iteracion: 8623 Gradiente: [0.004106311300220492,-0.07084495199327859] Loss: 22.84747298263433\n",
      "Iteracion: 8624 Gradiente: [0.004104128259030896,-0.07080728864223325] Loss: 22.847467948103926\n",
      "Iteracion: 8625 Gradiente: [0.00410194637850528,-0.07076964531417573] Loss: 22.84746291892511\n",
      "Iteracion: 8626 Gradiente: [0.004099765657844993,-0.07073202199847325] Loss: 22.84745789509223\n",
      "Iteracion: 8627 Gradiente: [0.004097586096582025,-0.07069441868447643] Loss: 22.847452876599554\n",
      "Iteracion: 8628 Gradiente: [0.004095407694046571,-0.07065683536155412] Loss: 22.847447863441445\n",
      "Iteracion: 8629 Gradiente: [0.004093230449611459,-0.07061927201908157] Loss: 22.8474428556122\n",
      "Iteracion: 8630 Gradiente: [0.0040910543626476205,-0.07058172864643489] Loss: 22.847437853106182\n",
      "Iteracion: 8631 Gradiente: [0.004088879432597991,-0.07054420523299498] Loss: 22.847432855917713\n",
      "Iteracion: 8632 Gradiente: [0.004086705658840136,-0.07050670176815241] Loss: 22.847427864041176\n",
      "Iteracion: 8633 Gradiente: [0.004084533040646458,-0.07046921824130786] Loss: 22.847422877470862\n",
      "Iteracion: 8634 Gradiente: [0.004082361577525262,-0.07043175464185367] Loss: 22.84741789620119\n",
      "Iteracion: 8635 Gradiente: [0.004080191268849375,-0.07039431095919764] Loss: 22.847412920226507\n",
      "Iteracion: 8636 Gradiente: [0.004078022113908257,-0.07035688718275637] Loss: 22.847407949541157\n",
      "Iteracion: 8637 Gradiente: [0.00407585411209747,-0.0703194833019463] Loss: 22.84740298413955\n",
      "Iteracion: 8638 Gradiente: [0.0040736872629745825,-0.07028209930618132] Loss: 22.84739802401606\n",
      "Iteracion: 8639 Gradiente: [0.004071521565765579,-0.07024473518489885] Loss: 22.847393069165076\n",
      "Iteracion: 8640 Gradiente: [0.004069357019929498,-0.07020739092752942] Loss: 22.84738811958097\n",
      "Iteracion: 8641 Gradiente: [0.004067193624795588,-0.07017006652351547] Loss: 22.847383175258198\n",
      "Iteracion: 8642 Gradiente: [0.004065031379849415,-0.07013276196229677] Loss: 22.84737823619114\n",
      "Iteracion: 8643 Gradiente: [0.004062870284371911,-0.07009547723333019] Loss: 22.847373302374184\n",
      "Iteracion: 8644 Gradiente: [0.004060710337881801,-0.07005821232606439] Loss: 22.847368373801782\n",
      "Iteracion: 8645 Gradiente: [0.004058551539544434,-0.07002096722997647] Loss: 22.847363450468347\n",
      "Iteracion: 8646 Gradiente: [0.004056393889025382,-0.06998374193451677] Loss: 22.8473585323683\n",
      "Iteracion: 8647 Gradiente: [0.004054237385527889,-0.06994653642916984] Loss: 22.847353619496108\n",
      "Iteracion: 8648 Gradiente: [0.004052082028476889,-0.06990935070341196] Loss: 22.847348711846177\n",
      "Iteracion: 8649 Gradiente: [0.0040499278172717364,-0.069872184746729] Loss: 22.847343809412987\n",
      "Iteracion: 8650 Gradiente: [0.004047774751339261,-0.06983503854860669] Loss: 22.847338912190967\n",
      "Iteracion: 8651 Gradiente: [0.004045622829998289,-0.06979791209854641] Loss: 22.847334020174596\n",
      "Iteracion: 8652 Gradiente: [0.004043472052741966,-0.06976080538604303] Loss: 22.847329133358343\n",
      "Iteracion: 8653 Gradiente: [0.004041322418912804,-0.06972371840060668] Loss: 22.84732425173666\n",
      "Iteracion: 8654 Gradiente: [0.004039173927855207,-0.06968665113175163] Loss: 22.847319375304043\n",
      "Iteracion: 8655 Gradiente: [0.004037026579090745,-0.06964960356899065] Loss: 22.84731450405496\n",
      "Iteracion: 8656 Gradiente: [0.0040348803718776105,-0.06961257570185365] Loss: 22.84730963798393\n",
      "Iteracion: 8657 Gradiente: [0.0040327353056094735,-0.06957556751986947] Loss: 22.847304777085412\n",
      "Iteracion: 8658 Gradiente: [0.004030591379690426,-0.06953857901257253] Loss: 22.84729992135393\n",
      "Iteracion: 8659 Gradiente: [0.004028448593546349,-0.06950161016950258] Loss: 22.847295070784\n",
      "Iteracion: 8660 Gradiente: [0.004026306946628703,-0.06946466098020009] Loss: 22.847290225370106\n",
      "Iteracion: 8661 Gradiente: [0.004024166438233579,-0.06942773143422395] Loss: 22.847285385106797\n",
      "Iteracion: 8662 Gradiente: [0.004022027067853174,-0.06939082152112472] Loss: 22.847280549988582\n",
      "Iteracion: 8663 Gradiente: [0.004019888834817683,-0.069353931230469] Loss: 22.847275720009986\n",
      "Iteracion: 8664 Gradiente: [0.00401775173847246,-0.06931706055182688] Loss: 22.847270895165554\n",
      "Iteracion: 8665 Gradiente: [0.0040156157783883374,-0.06928020947476282] Loss: 22.847266075449845\n",
      "Iteracion: 8666 Gradiente: [0.004013480953861404,-0.0692433779888623] Loss: 22.847261260857383\n",
      "Iteracion: 8667 Gradiente: [0.004011347264187748,-0.06920656608371409] Loss: 22.847256451382727\n",
      "Iteracion: 8668 Gradiente: [0.004009214708853885,-0.06916977374890555] Loss: 22.84725164702045\n",
      "Iteracion: 8669 Gradiente: [0.004007083287233589,-0.06913300097403159] Loss: 22.847246847765106\n",
      "Iteracion: 8670 Gradiente: [0.004004952998740426,-0.06909624774869379] Loss: 22.847242053611257\n",
      "Iteracion: 8671 Gradiente: [0.004002823842790803,-0.06905951406249795] Loss: 22.847237264553502\n",
      "Iteracion: 8672 Gradiente: [0.004000695818759444,-0.0690227999050568] Loss: 22.847232480586403\n",
      "Iteracion: 8673 Gradiente: [0.003998568926037175,-0.06898610526599036] Loss: 22.84722770170458\n",
      "Iteracion: 8674 Gradiente: [0.003996443164092511,-0.06894943013491689] Loss: 22.847222927902585\n",
      "Iteracion: 8675 Gradiente: [0.003994318532188383,-0.0689127745014737] Loss: 22.847218159175043\n",
      "Iteracion: 8676 Gradiente: [0.00399219502993257,-0.06887613835528288] Loss: 22.84721339551657\n",
      "Iteracion: 8677 Gradiente: [0.003990072656513159,-0.06883952168599615] Loss: 22.847208636921753\n",
      "Iteracion: 8678 Gradiente: [0.003987951411436559,-0.06880292448325326] Loss: 22.84720388338523\n",
      "Iteracion: 8679 Gradiente: [0.003985831293962861,-0.06876634673671447] Loss: 22.84719913490161\n",
      "Iteracion: 8680 Gradiente: [0.003983712303742474,-0.06872978843602044] Loss: 22.84719439146553\n",
      "Iteracion: 8681 Gradiente: [0.003981594439957803,-0.0686932495708452] Loss: 22.847189653071613\n",
      "Iteracion: 8682 Gradiente: [0.0039794777021882055,-0.06865673013084823] Loss: 22.84718491971453\n",
      "Iteracion: 8683 Gradiente: [0.003977362089640716,-0.06862023010571137] Loss: 22.847180191388883\n",
      "Iteracion: 8684 Gradiente: [0.003975247601897536,-0.0685837494851043] Loss: 22.847175468089354\n",
      "Iteracion: 8685 Gradiente: [0.003973134238229174,-0.06854728825871703] Loss: 22.84717074981059\n",
      "Iteracion: 8686 Gradiente: [0.003971021998084249,-0.06851084641623792] Loss: 22.847166036547268\n",
      "Iteracion: 8687 Gradiente: [0.003968910880930329,-0.06847442394735737] Loss: 22.847161328294032\n",
      "Iteracion: 8688 Gradiente: [0.003966800886149713,-0.06843802084177639] Loss: 22.847156625045567\n",
      "Iteracion: 8689 Gradiente: [0.003964692012979753,-0.06840163708921156] Loss: 22.84715192679656\n",
      "Iteracion: 8690 Gradiente: [0.003962584261076548,-0.06836527267936011] Loss: 22.847147233541698\n",
      "Iteracion: 8691 Gradiente: [0.003960477629541022,-0.06832892760195376] Loss: 22.84714254527565\n",
      "Iteracion: 8692 Gradiente: [0.003958372118107908,-0.06829260184669815] Loss: 22.847137861993144\n",
      "Iteracion: 8693 Gradiente: [0.003956267726020239,-0.06825629540332917] Loss: 22.847133183688864\n",
      "Iteracion: 8694 Gradiente: [0.0039541644526347374,-0.06822000826158406] Loss: 22.847128510357518\n",
      "Iteracion: 8695 Gradiente: [0.003952062297512763,-0.06818374041119052] Loss: 22.847123841993835\n",
      "Iteracion: 8696 Gradiente: [0.003949961259891666,-0.06814749184190336] Loss: 22.847119178592514\n",
      "Iteracion: 8697 Gradiente: [0.003947861339245643,-0.06811126254346954] Loss: 22.84711452014828\n",
      "Iteracion: 8698 Gradiente: [0.003945762534953209,-0.06807505250564354] Loss: 22.84710986665589\n",
      "Iteracion: 8699 Gradiente: [0.003943664846477191,-0.06803886171818417] Loss: 22.847105218110055\n",
      "Iteracion: 8700 Gradiente: [0.0039415682732197865,-0.06800269017085678] Loss: 22.847100574505546\n",
      "Iteracion: 8701 Gradiente: [0.003939472814454348,-0.06796653785344067] Loss: 22.847095935837054\n",
      "Iteracion: 8702 Gradiente: [0.003937378469831288,-0.06793040475569825] Loss: 22.84709130209939\n",
      "Iteracion: 8703 Gradiente: [0.003935285238533955,-0.0678942908674248] Loss: 22.84708667328728\n",
      "Iteracion: 8704 Gradiente: [0.003933193120086761,-0.06785819617840237] Loss: 22.847082049395503\n",
      "Iteracion: 8705 Gradiente: [0.003931102113922217,-0.06782212067842153] Loss: 22.847077430418842\n",
      "Iteracion: 8706 Gradiente: [0.003929012219441574,-0.06778606435728077] Loss: 22.847072816352036\n",
      "Iteracion: 8707 Gradiente: [0.003926923435991133,-0.06775002720478843] Loss: 22.8470682071899\n",
      "Iteracion: 8708 Gradiente: [0.003924835762885929,-0.06771400921075851] Loss: 22.847063602927182\n",
      "Iteracion: 8709 Gradiente: [0.003922749199777324,-0.06767801036499357] Loss: 22.847059003558716\n",
      "Iteracion: 8710 Gradiente: [0.003920663745932984,-0.06764203065732169] Loss: 22.847054409079274\n",
      "Iteracion: 8711 Gradiente: [0.003918579400705843,-0.06760607007756932] Loss: 22.84704981948366\n",
      "Iteracion: 8712 Gradiente: [0.00391649616356915,-0.0675701286155667] Loss: 22.847045234766703\n",
      "Iteracion: 8713 Gradiente: [0.0039144140339753145,-0.06753420626114633] Loss: 22.8470406549232\n",
      "Iteracion: 8714 Gradiente: [0.003912333011294322,-0.06749830300415323] Loss: 22.84703607994796\n",
      "Iteracion: 8715 Gradiente: [0.003910253094967213,-0.06746241883443295] Loss: 22.847031509835837\n",
      "Iteracion: 8716 Gradiente: [0.003908174284491869,-0.06742655374183168] Loss: 22.847026944581636\n",
      "Iteracion: 8717 Gradiente: [0.0039060965790374286,-0.06739070771622338] Loss: 22.84702238418021\n",
      "Iteracion: 8718 Gradiente: [0.003904019978228727,-0.06735488074745781] Loss: 22.84701782862639\n",
      "Iteracion: 8719 Gradiente: [0.0039019444813429044,-0.0673190728254126] Loss: 22.847013277915025\n",
      "Iteracion: 8720 Gradiente: [0.003899870087876896,-0.06728328393995658] Loss: 22.847008732040976\n",
      "Iteracion: 8721 Gradiente: [0.0038977967971760563,-0.0672475140809728] Loss: 22.84700419099908\n",
      "Iteracion: 8722 Gradiente: [0.0038957246088424805,-0.0672117632383357] Loss: 22.84699965478425\n",
      "Iteracion: 8723 Gradiente: [0.0038936535220386757,-0.06717603140195069] Loss: 22.846995123391295\n",
      "Iteracion: 8724 Gradiente: [0.0038915835363061055,-0.06714031856170534] Loss: 22.846990596815107\n",
      "Iteracion: 8725 Gradiente: [0.0038895146510270705,-0.06710462470750235] Loss: 22.846986075050577\n",
      "Iteracion: 8726 Gradiente: [0.0038874468656321898,-0.06706894982924881] Loss: 22.846981558092594\n",
      "Iteracion: 8727 Gradiente: [0.003885380179615557,-0.06703329391685138] Loss: 22.846977045936033\n",
      "Iteracion: 8728 Gradiente: [0.0038833145922486287,-0.06699765696023453] Loss: 22.846972538575812\n",
      "Iteracion: 8729 Gradiente: [0.003881250102969602,-0.06696203894932017] Loss: 22.846968036006817\n",
      "Iteracion: 8730 Gradiente: [0.0038791867113379414,-0.06692643987402629] Loss: 22.846963538223935\n",
      "Iteracion: 8731 Gradiente: [0.003877124416595734,-0.06689085972429905] Loss: 22.84695904522213\n",
      "Iteracion: 8732 Gradiente: [0.0038750632182238103,-0.06685529849007198] Loss: 22.846954556996277\n",
      "Iteracion: 8733 Gradiente: [0.003873003115644262,-0.066819756161289] Loss: 22.846950073541315\n",
      "Iteracion: 8734 Gradiente: [0.0038709441083985513,-0.06678423272789165] Loss: 22.846945594852166\n",
      "Iteracion: 8735 Gradiente: [0.0038688861956359235,-0.0667487281798504] Loss: 22.846941120923773\n",
      "Iteracion: 8736 Gradiente: [0.0038668293769717364,-0.06671324250711379] Loss: 22.846936651751058\n",
      "Iteracion: 8737 Gradiente: [0.003864773651844189,-0.06667777569964622] Loss: 22.846932187329\n",
      "Iteracion: 8738 Gradiente: [0.0038627190195332636,-0.06664232774742669] Loss: 22.846927727652517\n",
      "Iteracion: 8739 Gradiente: [0.003860665479522633,-0.06660689864042695] Loss: 22.846923272716584\n",
      "Iteracion: 8740 Gradiente: [0.0038586130312220726,-0.0665714883686288] Loss: 22.846918822516145\n",
      "Iteracion: 8741 Gradiente: [0.00385656167403378,-0.06653609692201977] Loss: 22.846914377046176\n",
      "Iteracion: 8742 Gradiente: [0.0038545114075191123,-0.06650072429058357] Loss: 22.846909936301646\n",
      "Iteracion: 8743 Gradiente: [0.003852462230886052,-0.06646537046432961] Loss: 22.846905500277536\n",
      "Iteracion: 8744 Gradiente: [0.0038504141438257494,-0.06643003543324551] Loss: 22.846901068968815\n",
      "Iteracion: 8745 Gradiente: [0.003848367145512081,-0.06639471918735336] Loss: 22.846896642370496\n",
      "Iteracion: 8746 Gradiente: [0.003846321235335874,-0.06635942171666707] Loss: 22.84689222047754\n",
      "Iteracion: 8747 Gradiente: [0.0038442764128755395,-0.06632414301119821] Loss: 22.84688780328497\n",
      "Iteracion: 8748 Gradiente: [0.0038422326775342224,-0.06628888306096965] Loss: 22.846883390787795\n",
      "Iteracion: 8749 Gradiente: [0.003840190028715066,-0.0662536418560118] Loss: 22.846878982980986\n",
      "Iteracion: 8750 Gradiente: [0.003838148465868585,-0.06621841938635917] Loss: 22.84687457985961\n",
      "Iteracion: 8751 Gradiente: [0.003836107988302236,-0.06618321564205634] Loss: 22.846870181418634\n",
      "Iteracion: 8752 Gradiente: [0.003834068595509166,-0.06614803061314495] Loss: 22.846865787653122\n",
      "Iteracion: 8753 Gradiente: [0.003832030287006205,-0.06611286428966909] Loss: 22.846861398558083\n",
      "Iteracion: 8754 Gradiente: [0.0038299930620562844,-0.0660777166616962] Loss: 22.846857014128563\n",
      "Iteracion: 8755 Gradiente: [0.0038279569201942346,-0.06604258771927896] Loss: 22.8468526343596\n",
      "Iteracion: 8756 Gradiente: [0.0038259218608402536,-0.06600747745248334] Loss: 22.846848259246237\n",
      "Iteracion: 8757 Gradiente: [0.0038238878832904297,-0.06597238585138866] Loss: 22.846843888783518\n",
      "Iteracion: 8758 Gradiente: [0.0038218549871601227,-0.06593731290606127] Loss: 22.84683952296651\n",
      "Iteracion: 8759 Gradiente: [0.0038198231717084734,-0.0659022586065922] Loss: 22.846835161790274\n",
      "Iteracion: 8760 Gradiente: [0.003817792436440944,-0.0658672229430632] Loss: 22.846830805249894\n",
      "Iteracion: 8761 Gradiente: [0.0038157627808014165,-0.06583220590556836] Loss: 22.8468264533404\n",
      "Iteracion: 8762 Gradiente: [0.0038137342042261933,-0.06579720748420227] Loss: 22.846822106056898\n",
      "Iteracion: 8763 Gradiente: [0.0038117067060312593,-0.06576222766907604] Loss: 22.846817763394473\n",
      "Iteracion: 8764 Gradiente: [0.0038096802857182865,-0.06572726645029275] Loss: 22.846813425348206\n",
      "Iteracion: 8765 Gradiente: [0.003807654942806001,-0.06569232381796143] Loss: 22.846809091913183\n",
      "Iteracion: 8766 Gradiente: [0.0038056306764644885,-0.06565739976221524] Loss: 22.846804763084503\n",
      "Iteracion: 8767 Gradiente: [0.00380360748651943,-0.06562249427315538] Loss: 22.846800438857297\n",
      "Iteracion: 8768 Gradiente: [0.003801585371964696,-0.06558760734093738] Loss: 22.846796119226653\n",
      "Iteracion: 8769 Gradiente: [0.003799564332460174,-0.06555273895568056] Loss: 22.84679180418767\n",
      "Iteracion: 8770 Gradiente: [0.0037975443674611144,-0.06551788910752343] Loss: 22.84678749373548\n",
      "Iteracion: 8771 Gradiente: [0.0037955254763251864,-0.06548305778661738] Loss: 22.84678318786521\n",
      "Iteracion: 8772 Gradiente: [0.003793507658465008,-0.06544824498311215] Loss: 22.846778886571993\n",
      "Iteracion: 8773 Gradiente: [0.0037914909133727787,-0.06541345068715904] Loss: 22.846774589850963\n",
      "Iteracion: 8774 Gradiente: [0.0037894752403569024,-0.0653786748889272] Loss: 22.84677029769725\n",
      "Iteracion: 8775 Gradiente: [0.003787460639091478,-0.06534391757856958] Loss: 22.846766010106016\n",
      "Iteracion: 8776 Gradiente: [0.0037854471086783783,-0.0653091787462729] Loss: 22.846761727072376\n",
      "Iteracion: 8777 Gradiente: [0.003783434648833387,-0.06527445838220096] Loss: 22.84675744859154\n",
      "Iteracion: 8778 Gradiente: [0.0037814232588213297,-0.0652397564765413] Loss: 22.846753174658616\n",
      "Iteracion: 8779 Gradiente: [0.0037794129382405115,-0.065205073019474] Loss: 22.846748905268807\n",
      "Iteracion: 8780 Gradiente: [0.0037774036863358635,-0.06517040800120218] Loss: 22.84674464041726\n",
      "Iteracion: 8781 Gradiente: [0.0037753955025588465,-0.06513576141191883] Loss: 22.846740380099163\n",
      "Iteracion: 8782 Gradiente: [0.0037733883863741843,-0.06510113324182877] Loss: 22.846736124309686\n",
      "Iteracion: 8783 Gradiente: [0.003771382337275971,-0.06506652348113444] Loss: 22.84673187304403\n",
      "Iteracion: 8784 Gradiente: [0.003769377354623771,-0.06503193212005225] Loss: 22.846727626297376\n",
      "Iteracion: 8785 Gradiente: [0.003767373437942941,-0.06499735914879648] Loss: 22.846723384064923\n",
      "Iteracion: 8786 Gradiente: [0.003765370586634731,-0.06496280455759199] Loss: 22.846719146341876\n",
      "Iteracion: 8787 Gradiente: [0.0037633688000122826,-0.06492826833667303] Loss: 22.846714913123442\n",
      "Iteracion: 8788 Gradiente: [0.0037613680777297985,-0.06489375047626211] Loss: 22.846710684404833\n",
      "Iteracion: 8789 Gradiente: [0.0037593684189895765,-0.0648592509666114] Loss: 22.84670646018124\n",
      "Iteracion: 8790 Gradiente: [0.003757369823351079,-0.06482476979795825] Loss: 22.846702240447932\n",
      "Iteracion: 8791 Gradiente: [0.003755372290214609,-0.0647903069605532] Loss: 22.846698025200105\n",
      "Iteracion: 8792 Gradiente: [0.0037533758190221533,-0.06475586244464987] Loss: 22.846693814432992\n",
      "Iteracion: 8793 Gradiente: [0.003751380409211909,-0.06472143624050967] Loss: 22.846689608141833\n",
      "Iteracion: 8794 Gradiente: [0.0037493860602391274,-0.06468702833839496] Loss: 22.846685406321868\n",
      "Iteracion: 8795 Gradiente: [0.003747392771468109,-0.06465263872858032] Loss: 22.84668120896836\n",
      "Iteracion: 8796 Gradiente: [0.003745400542507582,-0.06461826740133117] Loss: 22.846677016076548\n",
      "Iteracion: 8797 Gradiente: [0.0037434093725702646,-0.06458391434694045] Loss: 22.846672827641683\n",
      "Iteracion: 8798 Gradiente: [0.0037414192612828857,-0.06454957955568368] Loss: 22.846668643659036\n",
      "Iteracion: 8799 Gradiente: [0.003739430207967113,-0.06451526301785813] Loss: 22.846664464123872\n",
      "Iteracion: 8800 Gradiente: [0.003737442212068724,-0.06448096472375747] Loss: 22.846660289031465\n",
      "Iteracion: 8801 Gradiente: [0.003735455273054337,-0.06444668466368302] Loss: 22.84665611837711\n",
      "Iteracion: 8802 Gradiente: [0.0037334693903744664,-0.06441242282794057] Loss: 22.846651952156055\n",
      "Iteracion: 8803 Gradiente: [0.0037314845635137315,-0.06437817920683671] Loss: 22.846647790363615\n",
      "Iteracion: 8804 Gradiente: [0.0037295007917199047,-0.06434395379070056] Loss: 22.84664363299507\n",
      "Iteracion: 8805 Gradiente: [0.0037275180745837133,-0.06430974656984544] Loss: 22.846639480045706\n",
      "Iteracion: 8806 Gradiente: [0.003725536411589777,-0.06427555753459503] Loss: 22.846635331510853\n",
      "Iteracion: 8807 Gradiente: [0.0037235558020531317,-0.06424138667528859] Loss: 22.846631187385803\n",
      "Iteracion: 8808 Gradiente: [0.003721576245544611,-0.0642072339822537] Loss: 22.846627047665866\n",
      "Iteracion: 8809 Gradiente: [0.003719597741328092,-0.06417309944584619] Loss: 22.84662291234636\n",
      "Iteracion: 8810 Gradiente: [0.003717620289014197,-0.06413898305640184] Loss: 22.84661878142261\n",
      "Iteracion: 8811 Gradiente: [0.003715643887989965,-0.06410488480427609] Loss: 22.846614654889965\n",
      "Iteracion: 8812 Gradiente: [0.0037136685376661185,-0.06407080467982833] Loss: 22.846610532743703\n",
      "Iteracion: 8813 Gradiente: [0.003711694237532015,-0.06403674267342035] Loss: 22.846606414979217\n",
      "Iteracion: 8814 Gradiente: [0.0037097209869263754,-0.0640026987754234] Loss: 22.846602301591826\n",
      "Iteracion: 8815 Gradiente: [0.00370774878535561,-0.06396867297620948] Loss: 22.84659819257688\n",
      "Iteracion: 8816 Gradiente: [0.0037057776322778108,-0.06393466526615275] Loss: 22.846594087929716\n",
      "Iteracion: 8817 Gradiente: [0.0037038075271482286,-0.06390067563563674] Loss: 22.84658998764571\n",
      "Iteracion: 8818 Gradiente: [0.003701838469399377,-0.06386670407505134] Loss: 22.84658589172023\n",
      "Iteracion: 8819 Gradiente: [0.0036998704584343995,-0.06383275057479203] Loss: 22.846581800148634\n",
      "Iteracion: 8820 Gradiente: [0.0036979034937180207,-0.06379881512525619] Loss: 22.846577712926273\n",
      "Iteracion: 8821 Gradiente: [0.003695937574723492,-0.06376489771684545] Loss: 22.84657363004855\n",
      "Iteracion: 8822 Gradiente: [0.0036939727009250114,-0.06373099833996676] Loss: 22.846569551510846\n",
      "Iteracion: 8823 Gradiente: [0.0036920088716793013,-0.06369711698503917] Loss: 22.846565477308538\n",
      "Iteracion: 8824 Gradiente: [0.003690046086328872,-0.06366325364248897] Loss: 22.84656140743701\n",
      "Iteracion: 8825 Gradiente: [0.003688084344624561,-0.0636294083027206] Loss: 22.846557341891685\n",
      "Iteracion: 8826 Gradiente: [0.003686123645732664,-0.06359558095618034] Loss: 22.846553280667937\n",
      "Iteracion: 8827 Gradiente: [0.003684163989298857,-0.06356177159328998] Loss: 22.846549223761183\n",
      "Iteracion: 8828 Gradiente: [0.00368220537465902,-0.06352798020449626] Loss: 22.846545171166845\n",
      "Iteracion: 8829 Gradiente: [0.003680247801246613,-0.06349420678024416] Loss: 22.84654112288033\n",
      "Iteracion: 8830 Gradiente: [0.003678291268510255,-0.06346045131098291] Loss: 22.846537078897043\n",
      "Iteracion: 8831 Gradiente: [0.00367633577598383,-0.0634267137871614] Loss: 22.846533039212435\n",
      "Iteracion: 8832 Gradiente: [0.0036743813230126912,-0.06339299419924593] Loss: 22.846529003821917\n",
      "Iteracion: 8833 Gradiente: [0.003672427909136407,-0.06335929253769544] Loss: 22.84652497272094\n",
      "Iteracion: 8834 Gradiente: [0.003670475533722121,-0.0633256087929843] Loss: 22.84652094590493\n",
      "Iteracion: 8835 Gradiente: [0.0036685241962430837,-0.06329194295558693] Loss: 22.846516923369347\n",
      "Iteracion: 8836 Gradiente: [0.003666573896199073,-0.06325829501597866] Loss: 22.846512905109638\n",
      "Iteracion: 8837 Gradiente: [0.0036646246329648116,-0.06322466496465004] Loss: 22.846508891121246\n",
      "Iteracion: 8838 Gradiente: [0.0036626764060419723,-0.06319105279208787] Loss: 22.846504881399646\n",
      "Iteracion: 8839 Gradiente: [0.003660729214893384,-0.06315745848878637] Loss: 22.846500875940286\n",
      "Iteracion: 8840 Gradiente: [0.003658783058801873,-0.06312388204525518] Loss: 22.846496874738655\n",
      "Iteracion: 8841 Gradiente: [0.0036568379374273263,-0.06309032345198666] Loss: 22.846492877790205\n",
      "Iteracion: 8842 Gradiente: [0.0036548938502022565,-0.06305678269949304] Loss: 22.84648888509043\n",
      "Iteracion: 8843 Gradiente: [0.00365295079638391,-0.06302325977829971] Loss: 22.84648489663481\n",
      "Iteracion: 8844 Gradiente: [0.0036510087756615422,-0.06298975467891464] Loss: 22.846480912418837\n",
      "Iteracion: 8845 Gradiente: [0.003649067787297137,-0.06295626739187317] Loss: 22.846476932437994\n",
      "Iteracion: 8846 Gradiente: [0.0036471278308728944,-0.06292279790769903] Loss: 22.846472956687798\n",
      "Iteracion: 8847 Gradiente: [0.0036451889057199575,-0.06288934621693455] Loss: 22.84646898516373\n",
      "Iteracion: 8848 Gradiente: [0.0036432510114735805,-0.06285591231010758] Loss: 22.84646501786131\n",
      "Iteracion: 8849 Gradiente: [0.0036413141473417452,-0.06282249617778012] Loss: 22.846461054776043\n",
      "Iteracion: 8850 Gradiente: [0.003639378312956865,-0.06278909781049057] Loss: 22.84645709590347\n",
      "Iteracion: 8851 Gradiente: [0.003637443507771347,-0.06275571719879641] Loss: 22.84645314123907\n",
      "Iteracion: 8852 Gradiente: [0.003635509731177914,-0.0627223543332601] Loss: 22.846449190778408\n",
      "Iteracion: 8853 Gradiente: [0.0036335769825219205,-0.06268900920445487] Loss: 22.846445244517\n",
      "Iteracion: 8854 Gradiente: [0.0036316452614774636,-0.06265568180293618] Loss: 22.84644130245039\n",
      "Iteracion: 8855 Gradiente: [0.003629714567429687,-0.06262237211928531] Loss: 22.846437364574104\n",
      "Iteracion: 8856 Gradiente: [0.0036277848997883664,-0.06258908014408592] Loss: 22.846433430883692\n",
      "Iteracion: 8857 Gradiente: [0.0036258562579500144,-0.06255580586792545] Loss: 22.84642950137472\n",
      "Iteracion: 8858 Gradiente: [0.003623928641456094,-0.06252254928139038] Loss: 22.846425576042726\n",
      "Iteracion: 8859 Gradiente: [0.0036220020498414366,-0.06248931037507267] Loss: 22.84642165488329\n",
      "Iteracion: 8860 Gradiente: [0.0036200764823149712,-0.06245608913958603] Loss: 22.84641773789195\n",
      "Iteracion: 8861 Gradiente: [0.003618151938511005,-0.062422885565526796] Loss: 22.846413825064296\n",
      "Iteracion: 8862 Gradiente: [0.003616228417921737,-0.06238969964350159] Loss: 22.84640991639588\n",
      "Iteracion: 8863 Gradiente: [0.003614305919902942,-0.06235653136413362] Loss: 22.846406011882312\n",
      "Iteracion: 8864 Gradiente: [0.0036123844439070276,-0.062323380718043234] Loss: 22.846402111519158\n",
      "Iteracion: 8865 Gradiente: [0.0036104639894697734,-0.062290247695850925] Loss: 22.84639821530199\n",
      "Iteracion: 8866 Gradiente: [0.003608544555928006,-0.06225713228819541] Loss: 22.846394323226434\n",
      "Iteracion: 8867 Gradiente: [0.003606626142943507,-0.06222403448569859] Loss: 22.846390435288072\n",
      "Iteracion: 8868 Gradiente: [0.0036047087497659428,-0.062190954279016115] Loss: 22.846386551482496\n",
      "Iteracion: 8869 Gradiente: [0.0036027923759670935,-0.06215789165878268] Loss: 22.846382671805326\n",
      "Iteracion: 8870 Gradiente: [0.0036008770209482086,-0.062124846615655586] Loss: 22.846378796252175\n",
      "Iteracion: 8871 Gradiente: [0.0035989626841560116,-0.06209181914028943] Loss: 22.846374924818647\n",
      "Iteracion: 8872 Gradiente: [0.003597049365196388,-0.06205880922333679] Loss: 22.846371057500388\n",
      "Iteracion: 8873 Gradiente: [0.0035951370632839523,-0.062025816855476514] Loss: 22.84636719429299\n",
      "Iteracion: 8874 Gradiente: [0.0035932257780274313,-0.06199284202736972] Loss: 22.84636333519211\n",
      "Iteracion: 8875 Gradiente: [0.003591315508950288,-0.06195988472969039] Loss: 22.846359480193367\n",
      "Iteracion: 8876 Gradiente: [0.003589406255372296,-0.06192694495312499] Loss: 22.846355629292397\n",
      "Iteracion: 8877 Gradiente: [0.0035874980168974463,-0.06189402268835084] Loss: 22.84635178248486\n",
      "Iteracion: 8878 Gradiente: [0.003585590792746037,-0.06186111792607297] Loss: 22.8463479397664\n",
      "Iteracion: 8879 Gradiente: [0.003583684582666062,-0.06182823065696823] Loss: 22.84634410113268\n",
      "Iteracion: 8880 Gradiente: [0.0035817793859147665,-0.061795360871750304] Loss: 22.846340266579325\n",
      "Iteracion: 8881 Gradiente: [0.0035798752020421414,-0.06176250856111925] Loss: 22.846336436102025\n",
      "Iteracion: 8882 Gradiente: [0.0035779720304788044,-0.061729673715782] Loss: 22.846332609696436\n",
      "Iteracion: 8883 Gradiente: [0.0035760698707785346,-0.0616968563264539] Loss: 22.846328787358242\n",
      "Iteracion: 8884 Gradiente: [0.0035741687222942647,-0.061664056383859744] Loss: 22.84632496908311\n",
      "Iteracion: 8885 Gradiente: [0.0035722685844641927,-0.061631273878723886] Loss: 22.846321154866743\n",
      "Iteracion: 8886 Gradiente: [0.003570369456794727,-0.06159850880177444] Loss: 22.846317344704783\n",
      "Iteracion: 8887 Gradiente: [0.0035684713388169104,-0.061565761143742986] Loss: 22.84631353859294\n",
      "Iteracion: 8888 Gradiente: [0.003566574229902623,-0.06153303089537351] Loss: 22.846309736526926\n",
      "Iteracion: 8889 Gradiente: [0.003564678129579117,-0.06150031804740775] Loss: 22.846305938502425\n",
      "Iteracion: 8890 Gradiente: [0.003562783037254273,-0.0614676225905967] Loss: 22.84630214451515\n",
      "Iteracion: 8891 Gradiente: [0.0035608889524723964,-0.06143494451569076] Loss: 22.84629835456079\n",
      "Iteracion: 8892 Gradiente: [0.003558995874617684,-0.061402283813454875] Loss: 22.84629456863508\n",
      "Iteracion: 8893 Gradiente: [0.003557103803140649,-0.06136964047465294] Loss: 22.846290786733725\n",
      "Iteracion: 8894 Gradiente: [0.003555212737596018,-0.06133701449004872] Loss: 22.84628700885245\n",
      "Iteracion: 8895 Gradiente: [0.0035533226774248305,-0.06130440585041773] Loss: 22.84628323498698\n",
      "Iteracion: 8896 Gradiente: [0.0035514336219828616,-0.061271814546545754] Loss: 22.84627946513306\n",
      "Iteracion: 8897 Gradiente: [0.0035495455708267325,-0.06123924056921067] Loss: 22.846275699286416\n",
      "Iteracion: 8898 Gradiente: [0.0035476585234457997,-0.061206683909199575] Loss: 22.84627193744276\n",
      "Iteracion: 8899 Gradiente: [0.0035457724793360513,-0.06117414455730608] Loss: 22.846268179597875\n",
      "Iteracion: 8900 Gradiente: [0.0035438874377935765,-0.06114162250433613] Loss: 22.846264425747517\n",
      "Iteracion: 8901 Gradiente: [0.0035420033985019473,-0.06110911774108286] Loss: 22.846260675887414\n",
      "Iteracion: 8902 Gradiente: [0.003540120360698514,-0.06107663025836606] Loss: 22.846256930013325\n",
      "Iteracion: 8903 Gradiente: [0.0035382383241056915,-0.06104416004698538] Loss: 22.846253188121032\n",
      "Iteracion: 8904 Gradiente: [0.003536357287998726,-0.06101170709776855] Loss: 22.84624945020629\n",
      "Iteracion: 8905 Gradiente: [0.0035344772519759243,-0.06097927140153168] Loss: 22.846245716264864\n",
      "Iteracion: 8906 Gradiente: [0.003532598215338112,-0.06094685294911315] Loss: 22.84624198629255\n",
      "Iteracion: 8907 Gradiente: [0.003530720177706333,-0.060914451731335834] Loss: 22.84623826028509\n",
      "Iteracion: 8908 Gradiente: [0.003528843138440152,-0.06088206773904273] Loss: 22.846234538238328\n",
      "Iteracion: 8909 Gradiente: [0.003526967097170086,-0.06084970096306854] Loss: 22.846230820148005\n",
      "Iteracion: 8910 Gradiente: [0.003525092053112644,-0.060817351394274625] Loss: 22.84622710600993\n",
      "Iteracion: 8911 Gradiente: [0.003523218006000661,-0.06078501902349735] Loss: 22.846223395819926\n",
      "Iteracion: 8912 Gradiente: [0.0035213449551671754,-0.06075270384160187] Loss: 22.846219689573747\n",
      "Iteracion: 8913 Gradiente: [0.003519472900111964,-0.06072040583945011] Loss: 22.846215987267257\n",
      "Iteracion: 8914 Gradiente: [0.0035176018403442794,-0.06068812500790344] Loss: 22.84621228889622\n",
      "Iteracion: 8915 Gradiente: [0.0035157317752369482,-0.06065586133784204] Loss: 22.846208594456485\n",
      "Iteracion: 8916 Gradiente: [0.0035138627042717494,-0.06062361482013969] Loss: 22.84620490394385\n",
      "Iteracion: 8917 Gradiente: [0.0035119946269437227,-0.06059138544567766] Loss: 22.846201217354174\n",
      "Iteracion: 8918 Gradiente: [0.003510127542829385,-0.06055917320533493] Loss: 22.846197534683245\n",
      "Iteracion: 8919 Gradiente: [0.0035082614512418784,-0.06052697809001219] Loss: 22.84619385592691\n",
      "Iteracion: 8920 Gradiente: [0.0035063963518079315,-0.06049480009059564] Loss: 22.846190181081045\n",
      "Iteracion: 8921 Gradiente: [0.0035045322439060554,-0.06046263919799225] Loss: 22.846186510141425\n",
      "Iteracion: 8922 Gradiente: [0.0035026691269308685,-0.06043049540311169] Loss: 22.846182843103954\n",
      "Iteracion: 8923 Gradiente: [0.003500807000502467,-0.06039836869685757] Loss: 22.846179179964462\n",
      "Iteracion: 8924 Gradiente: [0.00349894586408747,-0.0603662590701416] Loss: 22.84617552071882\n",
      "Iteracion: 8925 Gradiente: [0.0034970857170577574,-0.0603341665138925] Loss: 22.846171865362873\n",
      "Iteracion: 8926 Gradiente: [0.0034952265590068995,-0.06030209101902718] Loss: 22.846168213892497\n",
      "Iteracion: 8927 Gradiente: [0.00349336838922909,-0.0602700325764868] Loss: 22.84616456630354\n",
      "Iteracion: 8928 Gradiente: [0.0034915112073482155,-0.06023799117719442] Loss: 22.8461609225919\n",
      "Iteracion: 8929 Gradiente: [0.003489655012883001,-0.06020596681209097] Loss: 22.846157282753456\n",
      "Iteracion: 8930 Gradiente: [0.003487799805166484,-0.06017395947212509] Loss: 22.846153646784078\n",
      "Iteracion: 8931 Gradiente: [0.0034859455836719158,-0.060141969148247536] Loss: 22.846150014679658\n",
      "Iteracion: 8932 Gradiente: [0.00348409234799855,-0.0601099958314047] Loss: 22.84614638643609\n",
      "Iteracion: 8933 Gradiente: [0.0034822400975874266,-0.06007803951255871] Loss: 22.846142762049265\n",
      "Iteracion: 8934 Gradiente: [0.0034803888318312676,-0.06004610018267395] Loss: 22.84613914151509\n",
      "Iteracion: 8935 Gradiente: [0.003478538550315117,-0.06001417783271492] Loss: 22.846135524829457\n",
      "Iteracion: 8936 Gradiente: [0.003476689252513173,-0.05998227245365418] Loss: 22.846131911988287\n",
      "Iteracion: 8937 Gradiente: [0.0034748409377717356,-0.05995038403647648] Loss: 22.846128302987495\n",
      "Iteracion: 8938 Gradiente: [0.003472993605619005,-0.059918512572162776] Loss: 22.846124697822987\n",
      "Iteracion: 8939 Gradiente: [0.0034711472556400243,-0.05988665805169345] Loss: 22.846121096490712\n",
      "Iteracion: 8940 Gradiente: [0.0034693018872218317,-0.05985482046606568] Loss: 22.846117498986544\n",
      "Iteracion: 8941 Gradiente: [0.0034674574998139938,-0.059822999806279024] Loss: 22.846113905306474\n",
      "Iteracion: 8942 Gradiente: [0.0034656140929664996,-0.059791196063330075] Loss: 22.846110315446406\n",
      "Iteracion: 8943 Gradiente: [0.0034637716661620743,-0.05975940922822455] Loss: 22.84610672940228\n",
      "Iteracion: 8944 Gradiente: [0.0034619302188247047,-0.05972763929197716] Loss: 22.846103147170027\n",
      "Iteracion: 8945 Gradiente: [0.0034600897504968014,-0.059695886245602264] Loss: 22.84609956874563\n",
      "Iteracion: 8946 Gradiente: [0.003458250260570139,-0.05966415008012369] Loss: 22.84609599412501\n",
      "Iteracion: 8947 Gradiente: [0.0034564117485520757,-0.05963243078656515] Loss: 22.846092423304142\n",
      "Iteracion: 8948 Gradiente: [0.003454574213999232,-0.05960072835595417] Loss: 22.846088856278982\n",
      "Iteracion: 8949 Gradiente: [0.0034527376562749623,-0.059569042779332074] Loss: 22.846085293045494\n",
      "Iteracion: 8950 Gradiente: [0.00345090207500031,-0.059537374047730414] Loss: 22.846081733599643\n",
      "Iteracion: 8951 Gradiente: [0.0034490674695395757,-0.059505722152200374] Loss: 22.846078177937425\n",
      "Iteracion: 8952 Gradiente: [0.003447233839345169,-0.059474087083793495] Loss: 22.846074626054776\n",
      "Iteracion: 8953 Gradiente: [0.003445401184089292,-0.0594424688335522] Loss: 22.84607107794771\n",
      "Iteracion: 8954 Gradiente: [0.0034435695030670863,-0.05941086739254745] Loss: 22.846067533612207\n",
      "Iteracion: 8955 Gradiente: [0.0034417387957375923,-0.05937928275184478] Loss: 22.846063993044258\n",
      "Iteracion: 8956 Gradiente: [0.003439909061729433,-0.05934771490250258] Loss: 22.84606045623986\n",
      "Iteracion: 8957 Gradiente: [0.0034380803005092274,-0.05931616383559503] Loss: 22.846056923195018\n",
      "Iteracion: 8958 Gradiente: [0.0034362525115009626,-0.05928462954220374] Loss: 22.846053393905706\n",
      "Iteracion: 8959 Gradiente: [0.0034344256942214694,-0.05925311201341069] Loss: 22.846049868367974\n",
      "Iteracion: 8960 Gradiente: [0.0034325998480615755,-0.059221611240306636] Loss: 22.846046346577804\n",
      "Iteracion: 8961 Gradiente: [0.0034307749725883243,-0.059190127213979454] Loss: 22.84604282853122\n",
      "Iteracion: 8962 Gradiente: [0.003428951067350757,-0.059158659925523695] Loss: 22.846039314224257\n",
      "Iteracion: 8963 Gradiente: [0.003427128131768124,-0.05912720936604181] Loss: 22.846035803652914\n",
      "Iteracion: 8964 Gradiente: [0.003425306165282412,-0.05909577552664539] Loss: 22.846032296813245\n",
      "Iteracion: 8965 Gradiente: [0.0034234851673280295,-0.05906435839844886] Loss: 22.84602879370126\n",
      "Iteracion: 8966 Gradiente: [0.003421665137519388,-0.05903295797255801] Loss: 22.84602529431302\n",
      "Iteracion: 8967 Gradiente: [0.003419846075348687,-0.05900157424009483] Loss: 22.846021798644554\n",
      "Iteracion: 8968 Gradiente: [0.003418027980141384,-0.05897020719219282] Loss: 22.84601830669191\n",
      "Iteracion: 8969 Gradiente: [0.003416210851528945,-0.058938856819974815] Loss: 22.84601481845112\n",
      "Iteracion: 8970 Gradiente: [0.003414394689010199,-0.05890752311457336] Loss: 22.846011333918273\n",
      "Iteracion: 8971 Gradiente: [0.0034125794920318716,-0.058876206067129525] Loss: 22.846007853089407\n",
      "Iteracion: 8972 Gradiente: [0.0034107652599686842,-0.05884490566879694] Loss: 22.846004375960586\n",
      "Iteracion: 8973 Gradiente: [0.0034089519924729453,-0.058813621910712044] Loss: 22.84600090252788\n",
      "Iteracion: 8974 Gradiente: [0.003407139688959167,-0.05878235478403451] Loss: 22.845997432787343\n",
      "Iteracion: 8975 Gradiente: [0.00340532834890818,-0.05875110427992117] Loss: 22.845993966735087\n",
      "Iteracion: 8976 Gradiente: [0.00340351797188229,-0.058719870389531435] Loss: 22.845990504367176\n",
      "Iteracion: 8977 Gradiente: [0.00340170855724485,-0.05868865310404014] Loss: 22.845987045679674\n",
      "Iteracion: 8978 Gradiente: [0.0033999001045079543,-0.0586574524146202] Loss: 22.84598359066869\n",
      "Iteracion: 8979 Gradiente: [0.0033980926133277004,-0.058626268312434844] Loss: 22.84598013933031\n",
      "Iteracion: 8980 Gradiente: [0.003396286082987861,-0.058595100788679605] Loss: 22.845976691660628\n",
      "Iteracion: 8981 Gradiente: [0.0033944805130460055,-0.05856394983453868] Loss: 22.845973247655753\n",
      "Iteracion: 8982 Gradiente: [0.003392675903017069,-0.05853281544119978] Loss: 22.845969807311768\n",
      "Iteracion: 8983 Gradiente: [0.0033908722523259866,-0.05850169759986272] Loss: 22.845966370624815\n",
      "Iteracion: 8984 Gradiente: [0.0033890695605540105,-0.05847059630172282] Loss: 22.845962937590958\n",
      "Iteracion: 8985 Gradiente: [0.0033872678272378683,-0.058439511537982484] Loss: 22.845959508206366\n",
      "Iteracion: 8986 Gradiente: [0.003385467051595962,-0.05840844329986747] Loss: 22.84595608246714\n",
      "Iteracion: 8987 Gradiente: [0.0033836672333914446,-0.05837739157857603] Loss: 22.84595266036938\n",
      "Iteracion: 8988 Gradiente: [0.003381868372087145,-0.05834635636532954] Loss: 22.84594924190925\n",
      "Iteracion: 8989 Gradiente: [0.0033800704670729448,-0.05831533765135741] Loss: 22.845945827082865\n",
      "Iteracion: 8990 Gradiente: [0.00337827351784199,-0.058284335427886566] Loss: 22.845942415886384\n",
      "Iteracion: 8991 Gradiente: [0.003376477523977428,-0.058253349686147496] Loss: 22.845939008315916\n",
      "Iteracion: 8992 Gradiente: [0.003374682484818929,-0.05822238041738632] Loss: 22.84593560436763\n",
      "Iteracion: 8993 Gradiente: [0.0033728884000405893,-0.05819142761283338] Loss: 22.84593220403766\n",
      "Iteracion: 8994 Gradiente: [0.003371095268991553,-0.05816049126374476] Loss: 22.84592880732217\n",
      "Iteracion: 8995 Gradiente: [0.0033693030913279167,-0.05812957136136312] Loss: 22.84592541421733\n",
      "Iteracion: 8996 Gradiente: [0.0033675118663344015,-0.058098667896957085] Loss: 22.84592202471927\n",
      "Iteracion: 8997 Gradiente: [0.003365721593725842,-0.058067780861775734] Loss: 22.845918638824187\n",
      "Iteracion: 8998 Gradiente: [0.0033639322728229596,-0.05803691024709216] Loss: 22.845915256528233\n",
      "Iteracion: 8999 Gradiente: [0.003362143903180481,-0.05800605604417572] Loss: 22.8459118778276\n",
      "Iteracion: 9000 Gradiente: [0.003360356484193024,-0.05797521824430601] Loss: 22.845908502718444\n",
      "Iteracion: 9001 Gradiente: [0.003358570015565003,-0.05794439683875024] Loss: 22.845905131196954\n",
      "Iteracion: 9002 Gradiente: [0.003356784496607664,-0.057913591818803664] Loss: 22.845901763259327\n",
      "Iteracion: 9003 Gradiente: [0.0033549999269922638,-0.05788280317574568] Loss: 22.845898398901753\n",
      "Iteracion: 9004 Gradiente: [0.0033532163060651025,-0.05785203090087757] Loss: 22.845895038120407\n",
      "Iteracion: 9005 Gradiente: [0.0033514336333922757,-0.05782127498549367] Loss: 22.845891680911485\n",
      "Iteracion: 9006 Gradiente: [0.0033496519083371368,-0.05779053542090438] Loss: 22.84588832727123\n",
      "Iteracion: 9007 Gradiente: [0.00334787113062589,-0.05775981219840543] Loss: 22.845884977195826\n",
      "Iteracion: 9008 Gradiente: [0.0033460912995707305,-0.057729105309316374] Loss: 22.84588163068147\n",
      "Iteracion: 9009 Gradiente: [0.003344312414723542,-0.05769841474495259] Loss: 22.845878287724382\n",
      "Iteracion: 9010 Gradiente: [0.003342534475538628,-0.05766774049663894] Loss: 22.845874948320784\n",
      "Iteracion: 9011 Gradiente: [0.0033407574815555565,-0.05763708255569687] Loss: 22.845871612466915\n",
      "Iteracion: 9012 Gradiente: [0.003338981432396319,-0.057606440913449575] Loss: 22.845868280158985\n",
      "Iteracion: 9013 Gradiente: [0.003337206327353215,-0.05757581556124561] Loss: 22.84586495139322\n",
      "Iteracion: 9014 Gradiente: [0.0033354321660652884,-0.05754520649041576] Loss: 22.84586162616588\n",
      "Iteracion: 9015 Gradiente: [0.0033336589479754744,-0.05751461369230799] Loss: 22.84585830447318\n",
      "Iteracion: 9016 Gradiente: [0.0033318866725124963,-0.05748403715827403] Loss: 22.84585498631136\n",
      "Iteracion: 9017 Gradiente: [0.003330115339275608,-0.057453476879661736] Loss: 22.845851671676687\n",
      "Iteracion: 9018 Gradiente: [0.0033283449477541655,-0.0574229328478322] Loss: 22.845848360565387\n",
      "Iteracion: 9019 Gradiente: [0.003326575497523739,-0.05739240505413967] Loss: 22.84584505297373\n",
      "Iteracion: 9020 Gradiente: [0.0033248069878235734,-0.057361893489970346] Loss: 22.845841748897968\n",
      "Iteracion: 9021 Gradiente: [0.0033230394183637675,-0.05733139814668237] Loss: 22.84583844833438\n",
      "Iteracion: 9022 Gradiente: [0.0033212727886298885,-0.057300919015653186] Loss: 22.845835151279218\n",
      "Iteracion: 9023 Gradiente: [0.0033195070981141346,-0.057270456088263914] Loss: 22.84583185772875\n",
      "Iteracion: 9024 Gradiente: [0.0033177423461789126,-0.05724000935590811] Loss: 22.84582856767926\n",
      "Iteracion: 9025 Gradiente: [0.0033159785325239,-0.05720957880996487] Loss: 22.845825281127023\n",
      "Iteracion: 9026 Gradiente: [0.0033142156565759253,-0.05717916444183449] Loss: 22.845821998068313\n",
      "Iteracion: 9027 Gradiente: [0.0033124537177750806,-0.05714876624291823] Loss: 22.845818718499437\n",
      "Iteracion: 9028 Gradiente: [0.003310692715747147,-0.05711838420461236] Loss: 22.84581544241667\n",
      "Iteracion: 9029 Gradiente: [0.0033089326498118984,-0.057088018318338145] Loss: 22.84581216981631\n",
      "Iteracion: 9030 Gradiente: [0.0033071735195837467,-0.05705766857550074] Loss: 22.845808900694653\n",
      "Iteracion: 9031 Gradiente: [0.003305415324730158,-0.05702733496750862] Loss: 22.845805635048006\n",
      "Iteracion: 9032 Gradiente: [0.0033036580644619563,-0.05699701748580033] Loss: 22.845802372872676\n",
      "Iteracion: 9033 Gradiente: [0.003301901738433344,-0.056966716121794526] Loss: 22.845799114164958\n",
      "Iteracion: 9034 Gradiente: [0.003300146346087255,-0.056936430866926575] Loss: 22.84579585892118\n",
      "Iteracion: 9035 Gradiente: [0.0032983918869651536,-0.05690616171262851] Loss: 22.845792607137657\n",
      "Iteracion: 9036 Gradiente: [0.0032966383606378713,-0.05687590865033757] Loss: 22.84578935881071\n",
      "Iteracion: 9037 Gradiente: [0.0032948857664943415,-0.056845671671506184] Loss: 22.845786113936654\n",
      "Iteracion: 9038 Gradiente: [0.00329313410411487,-0.056815450767579635] Loss: 22.845782872511855\n",
      "Iteracion: 9039 Gradiente: [0.0032913833729404968,-0.05678524593001484] Loss: 22.845779634532605\n",
      "Iteracion: 9040 Gradiente: [0.0032896335725392115,-0.05675505715026859] Loss: 22.84577639999527\n",
      "Iteracion: 9041 Gradiente: [0.0032878847023321595,-0.056724884419805596] Loss: 22.845773168896173\n",
      "Iteracion: 9042 Gradiente: [0.0032861367619138565,-0.05669472773009036] Loss: 22.845769941231662\n",
      "Iteracion: 9043 Gradiente: [0.0032843897507689236,-0.056664587072595536] Loss: 22.845766716998085\n",
      "Iteracion: 9044 Gradiente: [0.003282643668301451,-0.05663446243880609] Loss: 22.845763496191825\n",
      "Iteracion: 9045 Gradiente: [0.0032808985142101696,-0.05660435382019007] Loss: 22.845760278809202\n",
      "Iteracion: 9046 Gradiente: [0.0032791542878546427,-0.05657426120824098] Loss: 22.845757064846566\n",
      "Iteracion: 9047 Gradiente: [0.0032774109887678073,-0.05654418459445054] Loss: 22.84575385430033\n",
      "Iteracion: 9048 Gradiente: [0.0032756686164513364,-0.05651412397031071] Loss: 22.845750647166824\n",
      "Iteracion: 9049 Gradiente: [0.0032739271705357472,-0.056484079327316535] Loss: 22.845747443442445\n",
      "Iteracion: 9050 Gradiente: [0.0032721866503266027,-0.056454050656983175] Loss: 22.845744243123537\n",
      "Iteracion: 9051 Gradiente: [0.003270447055490422,-0.05642403795080876] Loss: 22.84574104620652\n",
      "Iteracion: 9052 Gradiente: [0.0032687083854426647,-0.056394041200312016] Loss: 22.84573785268775\n",
      "Iteracion: 9053 Gradiente: [0.003266970639698267,-0.05636406039701001] Loss: 22.845734662563622\n",
      "Iteracion: 9054 Gradiente: [0.0032652338177911132,-0.056334095532424064] Loss: 22.845731475830526\n",
      "Iteracion: 9055 Gradiente: [0.0032634979192730875,-0.05630414659807646] Loss: 22.845728292484853\n",
      "Iteracion: 9056 Gradiente: [0.0032617629436354416,-0.05627421358550144] Loss: 22.845725112523024\n",
      "Iteracion: 9057 Gradiente: [0.003260028890312583,-0.05624429648623644] Loss: 22.845721935941413\n",
      "Iteracion: 9058 Gradiente: [0.0032582957588497643,-0.0562143952918215] Loss: 22.845718762736453\n",
      "Iteracion: 9059 Gradiente: [0.0032565635488642406,-0.05618450999379394] Loss: 22.845715592904543\n",
      "Iteracion: 9060 Gradiente: [0.0032548322597383125,-0.056154640583709774] Loss: 22.845712426442088\n",
      "Iteracion: 9061 Gradiente: [0.0032531018909992327,-0.05612478705312244] Loss: 22.84570926334552\n",
      "Iteracion: 9062 Gradiente: [0.003251372442168569,-0.05609494939358915] Loss: 22.84570610361125\n",
      "Iteracion: 9063 Gradiente: [0.0032496439127906265,-0.056065127596669956] Loss: 22.84570294723573\n",
      "Iteracion: 9064 Gradiente: [0.003247916302314972,-0.056035321653934864] Loss: 22.845699794215346\n",
      "Iteracion: 9065 Gradiente: [0.0032461896102773833,-0.05600553155695529] Loss: 22.84569664454656\n",
      "Iteracion: 9066 Gradiente: [0.0032444638363282744,-0.05597575729729864] Loss: 22.84569349822582\n",
      "Iteracion: 9067 Gradiente: [0.0032427389796898373,-0.055945998866560605] Loss: 22.845690355249538\n",
      "Iteracion: 9068 Gradiente: [0.0032410150401138557,-0.055916256256313565] Loss: 22.845687215614184\n",
      "Iteracion: 9069 Gradiente: [0.0032392920170688437,-0.055886529458150015] Loss: 22.84568407931619\n",
      "Iteracion: 9070 Gradiente: [0.0032375699099778405,-0.05585681846366792] Loss: 22.845680946351997\n",
      "Iteracion: 9071 Gradiente: [0.003235848718454311,-0.05582712326446] Loss: 22.845677816718112\n",
      "Iteracion: 9072 Gradiente: [0.003234128441907084,-0.055797443852135586] Loss: 22.845674690410945\n",
      "Iteracion: 9073 Gradiente: [0.003232409079981835,-0.05576778021829203] Loss: 22.845671567426972\n",
      "Iteracion: 9074 Gradiente: [0.0032306906321281303,-0.055738132354548615] Loss: 22.845668447762666\n",
      "Iteracion: 9075 Gradiente: [0.0032289730977955364,-0.055708500252521394] Loss: 22.845665331414505\n",
      "Iteracion: 9076 Gradiente: [0.003227256476601307,-0.05567888390382668] Loss: 22.845662218378948\n",
      "Iteracion: 9077 Gradiente: [0.0032255407680423787,-0.05564928330009108] Loss: 22.845659108652494\n",
      "Iteracion: 9078 Gradiente: [0.003223825971570212,-0.055619698432946764] Loss: 22.845656002231596\n",
      "Iteracion: 9079 Gradiente: [0.003222112086746165,-0.0555901292940252] Loss: 22.845652899112753\n",
      "Iteracion: 9080 Gradiente: [0.003220399113023594,-0.055560575874968404] Loss: 22.845649799292477\n",
      "Iteracion: 9081 Gradiente: [0.003218687050061438,-0.055531038167411735] Loss: 22.845646702767237\n",
      "Iteracion: 9082 Gradiente: [0.003216975897235367,-0.0555015161630088] Loss: 22.845643609533543\n",
      "Iteracion: 9083 Gradiente: [0.0032152656540840024,-0.05547200985341192] Loss: 22.845640519587878\n",
      "Iteracion: 9084 Gradiente: [0.003213556320107121,-0.055442519230277654] Loss: 22.84563743292677\n",
      "Iteracion: 9085 Gradiente: [0.003211847894904925,-0.05541304428526151] Loss: 22.84563434954672\n",
      "Iteracion: 9086 Gradiente: [0.0032101403779307702,-0.055383585010032495] Loss: 22.84563126944423\n",
      "Iteracion: 9087 Gradiente: [0.0032084337687791733,-0.05535414139625393] Loss: 22.845628192615813\n",
      "Iteracion: 9088 Gradiente: [0.00320672806692149,-0.05532471343560417] Loss: 22.845625119058024\n",
      "Iteracion: 9089 Gradiente: [0.0032050232718972894,-0.055295301119759915] Loss: 22.845622048767346\n",
      "Iteracion: 9090 Gradiente: [0.0032033193831760326,-0.055265904440406285] Loss: 22.845618981740326\n",
      "Iteracion: 9091 Gradiente: [0.0032016164003209726,-0.0552365233892291] Loss: 22.845615917973493\n",
      "Iteracion: 9092 Gradiente: [0.0031999143227807282,-0.05520715795792223] Loss: 22.84561285746337\n",
      "Iteracion: 9093 Gradiente: [0.003198213150030445,-0.05517780813818547] Loss: 22.845609800206514\n",
      "Iteracion: 9094 Gradiente: [0.003196512881684536,-0.05514847392171411] Loss: 22.845606746199465\n",
      "Iteracion: 9095 Gradiente: [0.003194813517326149,-0.05511915530020796] Loss: 22.845603695438733\n",
      "Iteracion: 9096 Gradiente: [0.0031931150564237973,-0.05508985226537959] Loss: 22.84560064792091\n",
      "Iteracion: 9097 Gradiente: [0.00319141749851705,-0.055060564808941] Loss: 22.84559760364255\n",
      "Iteracion: 9098 Gradiente: [0.0031897208430062087,-0.05503129292261709] Loss: 22.845594562600162\n",
      "Iteracion: 9099 Gradiente: [0.003188025089498107,-0.055002036598126365] Loss: 22.84559152479036\n",
      "Iteracion: 9100 Gradiente: [0.003186330237480206,-0.054972795827197864] Loss: 22.845588490209686\n",
      "Iteracion: 9101 Gradiente: [0.0031846362865024956,-0.05494357060156029] Loss: 22.84558545885469\n",
      "Iteracion: 9102 Gradiente: [0.0031829432361443347,-0.05491436091294683] Loss: 22.845582430721983\n",
      "Iteracion: 9103 Gradiente: [0.0031812510858363415,-0.054885166753100616] Loss: 22.845579405808092\n",
      "Iteracion: 9104 Gradiente: [0.0031795598350508195,-0.054855988113772014] Loss: 22.845576384109627\n",
      "Iteracion: 9105 Gradiente: [0.003177869483494078,-0.05482682498669564] Loss: 22.845573365623178\n",
      "Iteracion: 9106 Gradiente: [0.003176180030472627,-0.0547976773636402] Loss: 22.845570350345312\n",
      "Iteracion: 9107 Gradiente: [0.0031744914756226686,-0.05476854523635595] Loss: 22.84556733827261\n",
      "Iteracion: 9108 Gradiente: [0.003172803818537773,-0.05473942859659881] Loss: 22.845564329401682\n",
      "Iteracion: 9109 Gradiente: [0.0031711170586769793,-0.05471032743614105] Loss: 22.845561323729125\n",
      "Iteracion: 9110 Gradiente: [0.003169431195485117,-0.054681241746756606] Loss: 22.845558321251517\n",
      "Iteracion: 9111 Gradiente: [0.0031677462285699676,-0.05465217152021692] Loss: 22.845555321965495\n",
      "Iteracion: 9112 Gradiente: [0.0031660621574739404,-0.054623116748299456] Loss: 22.84555232586765\n",
      "Iteracion: 9113 Gradiente: [0.0031643789815689162,-0.05459407742279699] Loss: 22.845549332954587\n",
      "Iteracion: 9114 Gradiente: [0.0031626967006189942,-0.054565053535483916] Loss: 22.84554634322293\n",
      "Iteracion: 9115 Gradiente: [0.003161015314012161,-0.05453604507816093] Loss: 22.845543356669303\n",
      "Iteracion: 9116 Gradiente: [0.0031593348213050376,-0.054507052042623155] Loss: 22.845540373290305\n",
      "Iteracion: 9117 Gradiente: [0.00315765522192919,-0.05447807442067673] Loss: 22.845537393082598\n",
      "Iteracion: 9118 Gradiente: [0.003155976515511346,-0.054449112204121165] Loss: 22.845534416042764\n",
      "Iteracion: 9119 Gradiente: [0.003154298701506756,-0.054420165384772075] Loss: 22.845531442167463\n",
      "Iteracion: 9120 Gradiente: [0.0031526217794644634,-0.05439123395444068] Loss: 22.84552847145334\n",
      "Iteracion: 9121 Gradiente: [0.003150945748926877,-0.05436231790494546] Loss: 22.84552550389702\n",
      "Iteracion: 9122 Gradiente: [0.0031492706094392513,-0.05433341722811041] Loss: 22.845522539495146\n",
      "Iteracion: 9123 Gradiente: [0.003147596360516521,-0.05430453191576052] Loss: 22.845519578244343\n",
      "Iteracion: 9124 Gradiente: [0.0031459230017674145,-0.05427566195972358] Loss: 22.845516620141332\n",
      "Iteracion: 9125 Gradiente: [0.0031442505325486535,-0.054246807351846844] Loss: 22.845513665182704\n",
      "Iteracion: 9126 Gradiente: [0.003142578952409281,-0.05421796808396818] Loss: 22.84551071336513\n",
      "Iteracion: 9127 Gradiente: [0.003140908260920128,-0.05418914414793043] Loss: 22.84550776468528\n",
      "Iteracion: 9128 Gradiente: [0.0031392384577448714,-0.0541603355355754] Loss: 22.845504819139798\n",
      "Iteracion: 9129 Gradiente: [0.003137569542155916,-0.05413154223876949] Loss: 22.845501876725386\n",
      "Iteracion: 9130 Gradiente: [0.0031359015138737807,-0.054102764249360766] Loss: 22.845498937438688\n",
      "Iteracion: 9131 Gradiente: [0.003134234372443719,-0.054074001559210313] Loss: 22.845496001276395\n",
      "Iteracion: 9132 Gradiente: [0.0031325681171883463,-0.05404525416019581] Loss: 22.84549306823519\n",
      "Iteracion: 9133 Gradiente: [0.003130902747831025,-0.054016522044178096] Loss: 22.845490138311725\n",
      "Iteracion: 9134 Gradiente: [0.0031292382639075337,-0.053987805203029995] Loss: 22.845487211502725\n",
      "Iteracion: 9135 Gradiente: [0.0031275746647840682,-0.05395910362864266] Loss: 22.845484287804847\n",
      "Iteracion: 9136 Gradiente: [0.0031259119500551455,-0.05393041731289401] Loss: 22.845481367214806\n",
      "Iteracion: 9137 Gradiente: [0.00312425011931623,-0.05390174624766881] Loss: 22.845478449729296\n",
      "Iteracion: 9138 Gradiente: [0.0031225891720784676,-0.05387309042486213] Loss: 22.845475535345\n",
      "Iteracion: 9139 Gradiente: [0.0031209291077393194,-0.05384444983637498] Loss: 22.84547262405863\n",
      "Iteracion: 9140 Gradiente: [0.0031192699259975144,-0.05381582447410231] Loss: 22.845469715866898\n",
      "Iteracion: 9141 Gradiente: [0.003117611626415358,-0.05378721432994477] Loss: 22.845466810766524\n",
      "Iteracion: 9142 Gradiente: [0.0031159542084424176,-0.05375861939581696] Loss: 22.84546390875421\n",
      "Iteracion: 9143 Gradiente: [0.003114297671484678,-0.053730039663640976] Loss: 22.845461009826643\n",
      "Iteracion: 9144 Gradiente: [0.0031126420152323437,-0.053701475125326206] Loss: 22.84545811398059\n",
      "Iteracion: 9145 Gradiente: [0.0031109872391994033,-0.05367292577279604] Loss: 22.845455221212784\n",
      "Iteracion: 9146 Gradiente: [0.0031093333429453194,-0.05364439159797397] Loss: 22.845452331519898\n",
      "Iteracion: 9147 Gradiente: [0.003107680325839131,-0.05361587259280244] Loss: 22.845449444898716\n",
      "Iteracion: 9148 Gradiente: [0.0031060281876771493,-0.05358736874920022] Loss: 22.84544656134595\n",
      "Iteracion: 9149 Gradiente: [0.003104376927644618,-0.05355888005912834] Loss: 22.845443680858317\n",
      "Iteracion: 9150 Gradiente: [0.003102726545575744,-0.05353040651451183] Loss: 22.8454408034326\n",
      "Iteracion: 9151 Gradiente: [0.0031010770409949373,-0.05350194810730038] Loss: 22.845437929065522\n",
      "Iteracion: 9152 Gradiente: [0.0030994284132503935,-0.05347350482945714] Loss: 22.84543505775383\n",
      "Iteracion: 9153 Gradiente: [0.003097780661969788,-0.053445076672933564] Loss: 22.845432189494293\n",
      "Iteracion: 9154 Gradiente: [0.003096133786691742,-0.053416663629689394] Loss: 22.845429324283646\n",
      "Iteracion: 9155 Gradiente: [0.0030944877869103493,-0.053388265691693464] Loss: 22.845426462118652\n",
      "Iteracion: 9156 Gradiente: [0.0030928426621888625,-0.053359882850913226] Loss: 22.845423602996085\n",
      "Iteracion: 9157 Gradiente: [0.0030911984121019032,-0.053331515099320964] Loss: 22.845420746912694\n",
      "Iteracion: 9158 Gradiente: [0.003089555036106617,-0.05330316242889784] Loss: 22.845417893865285\n",
      "Iteracion: 9159 Gradiente: [0.0030879125338979445,-0.053274824831618044] Loss: 22.845415043850586\n",
      "Iteracion: 9160 Gradiente: [0.0030862709047823956,-0.05324650229948122] Loss: 22.845412196865382\n",
      "Iteracion: 9161 Gradiente: [0.003084630148394278,-0.0532181948244722] Loss: 22.845409352906476\n",
      "Iteracion: 9162 Gradiente: [0.0030829902643366345,-0.05318990239858342] Loss: 22.84540651197064\n",
      "Iteracion: 9163 Gradiente: [0.0030813512521045065,-0.053161625013815224] Loss: 22.845403674054662\n",
      "Iteracion: 9164 Gradiente: [0.003079713111127565,-0.053133362662179336] Loss: 22.845400839155317\n",
      "Iteracion: 9165 Gradiente: [0.0030780758411395937,-0.053105115335669] Loss: 22.845398007269427\n",
      "Iteracion: 9166 Gradiente: [0.0030764394415901584,-0.05307688302630472] Loss: 22.845395178393762\n",
      "Iteracion: 9167 Gradiente: [0.0030748039119269303,-0.053048665726108266] Loss: 22.84539235252511\n",
      "Iteracion: 9168 Gradiente: [0.003073169251813586,-0.05302046342708948] Loss: 22.845389529660334\n",
      "Iteracion: 9169 Gradiente: [0.00307153546072243,-0.05299227612128021] Loss: 22.845386709796173\n",
      "Iteracion: 9170 Gradiente: [0.003069902538269768,-0.052964103800703675] Loss: 22.845383892929483\n",
      "Iteracion: 9171 Gradiente: [0.003068270483826533,-0.05293594645740427] Loss: 22.845381079057063\n",
      "Iteracion: 9172 Gradiente: [0.0030666392970014537,-0.05290780408341351] Loss: 22.84537826817572\n",
      "Iteracion: 9173 Gradiente: [0.003065008977417468,-0.05287967667077069] Loss: 22.84537546028229\n",
      "Iteracion: 9174 Gradiente: [0.003063379524546879,-0.052851564211524975] Loss: 22.845372655373595\n",
      "Iteracion: 9175 Gradiente: [0.0030617509380211536,-0.052823466697721005] Loss: 22.845369853446456\n",
      "Iteracion: 9176 Gradiente: [0.0030601232171666953,-0.05279538412142356] Loss: 22.845367054497704\n",
      "Iteracion: 9177 Gradiente: [0.0030584963616651826,-0.05276731647468793] Loss: 22.84536425852417\n",
      "Iteracion: 9178 Gradiente: [0.003056870371145237,-0.05273926374956801] Loss: 22.845361465522693\n",
      "Iteracion: 9179 Gradiente: [0.003055245245076321,-0.052711225938136624] Loss: 22.84535867549013\n",
      "Iteracion: 9180 Gradiente: [0.0030536209829800025,-0.05268320303246566] Loss: 22.845355888423324\n",
      "Iteracion: 9181 Gradiente: [0.0030519975843285845,-0.052655195024635996] Loss: 22.845353104319088\n",
      "Iteracion: 9182 Gradiente: [0.0030503750486720565,-0.05262720190672473] Loss: 22.845350323174312\n",
      "Iteracion: 9183 Gradiente: [0.0030487533756096735,-0.05259922367081546] Loss: 22.845347544985824\n",
      "Iteracion: 9184 Gradiente: [0.003047132564776689,-0.05257126030898919] Loss: 22.84534476975049\n",
      "Iteracion: 9185 Gradiente: [0.003045512615623617,-0.05254331181334327] Loss: 22.845341997465166\n",
      "Iteracion: 9186 Gradiente: [0.0030438935276303407,-0.05251537817597942] Loss: 22.84533922812674\n",
      "Iteracion: 9187 Gradiente: [0.003042275300354428,-0.05248745938899653] Loss: 22.84533646173204\n",
      "Iteracion: 9188 Gradiente: [0.0030406579334481877,-0.05245955544449477] Loss: 22.845333698277987\n",
      "Iteracion: 9189 Gradiente: [0.0030390414263526586,-0.05243166633458642] Loss: 22.8453309377614\n",
      "Iteracion: 9190 Gradiente: [0.0030374257787144643,-0.052403792051381494] Loss: 22.845328180179205\n",
      "Iteracion: 9191 Gradiente: [0.0030358109899026433,-0.052375932587007054] Loss: 22.84532542552825\n",
      "Iteracion: 9192 Gradiente: [0.0030341970595789765,-0.052348087933578334] Loss: 22.845322673805438\n",
      "Iteracion: 9193 Gradiente: [0.0030325839872944018,-0.05232025808322085] Loss: 22.84531992500763\n",
      "Iteracion: 9194 Gradiente: [0.0030309717725704862,-0.052292443028064996] Loss: 22.84531717913175\n",
      "Iteracion: 9195 Gradiente: [0.003029360414894692,-0.05226464276025003] Loss: 22.84531443617468\n",
      "Iteracion: 9196 Gradiente: [0.003027749913880484,-0.05223685727191001] Loss: 22.845311696133304\n",
      "Iteracion: 9197 Gradiente: [0.0030261402691328008,-0.05220908655518433] Loss: 22.84530895900456\n",
      "Iteracion: 9198 Gradiente: [0.0030245314800955232,-0.05218133060222504] Loss: 22.8453062247853\n",
      "Iteracion: 9199 Gradiente: [0.0030229235463006413,-0.052153589405184185] Loss: 22.845303493472457\n",
      "Iteracion: 9200 Gradiente: [0.0030213164673085656,-0.05212586295621786] Loss: 22.845300765062948\n",
      "Iteracion: 9201 Gradiente: [0.003019710242807605,-0.052098151247475855] Loss: 22.845298039553686\n",
      "Iteracion: 9202 Gradiente: [0.0030181048721677445,-0.05207045427113097] Loss: 22.845295316941574\n",
      "Iteracion: 9203 Gradiente: [0.0030165003550100284,-0.05204277201934862] Loss: 22.845292597223544\n",
      "Iteracion: 9204 Gradiente: [0.0030148966907717067,-0.052015104484305394] Loss: 22.845289880396514\n",
      "Iteracion: 9205 Gradiente: [0.003013293879158141,-0.051987451658169564] Loss: 22.84528716645741\n",
      "Iteracion: 9206 Gradiente: [0.0030116919196482663,-0.05195981353312516] Loss: 22.84528445540318\n",
      "Iteracion: 9207 Gradiente: [0.003010090811801547,-0.05193219010135645] Loss: 22.84528174723074\n",
      "Iteracion: 9208 Gradiente: [0.003008490555159445,-0.0519045813550503] Loss: 22.845279041937022\n",
      "Iteracion: 9209 Gradiente: [0.0030068911492397393,-0.05187698728640366] Loss: 22.84527633951898\n",
      "Iteracion: 9210 Gradiente: [0.003005292593615157,-0.05184940788760848] Loss: 22.845273639973534\n",
      "Iteracion: 9211 Gradiente: [0.0030036948877826338,-0.05182184315087308] Loss: 22.84527094329767\n",
      "Iteracion: 9212 Gradiente: [0.003002098031391635,-0.0517942930683945] Loss: 22.845268249488296\n",
      "Iteracion: 9213 Gradiente: [0.003000502023863305,-0.05176675763239018] Loss: 22.8452655585424\n",
      "Iteracion: 9214 Gradiente: [0.0029989068649020584,-0.051739236835063214] Loss: 22.845262870456914\n",
      "Iteracion: 9215 Gradiente: [0.002997312553978304,-0.05171173066863707] Loss: 22.845260185228806\n",
      "Iteracion: 9216 Gradiente: [0.002995719090629715,-0.051684239125335554] Loss: 22.84525750285504\n",
      "Iteracion: 9217 Gradiente: [0.0029941264744167027,-0.051656762197379656] Loss: 22.84525482333257\n",
      "Iteracion: 9218 Gradiente: [0.0029925347049489424,-0.051629299876999706] Loss: 22.845252146658403\n",
      "Iteracion: 9219 Gradiente: [0.002990943781636209,-0.05160185215643575] Loss: 22.84524947282946\n",
      "Iteracion: 9220 Gradiente: [0.002989353704130811,-0.051574419027922265] Loss: 22.845246801842748\n",
      "Iteracion: 9221 Gradiente: [0.0029877644719306317,-0.05154700048370309] Loss: 22.845244133695246\n",
      "Iteracion: 9222 Gradiente: [0.0029861760846320823,-0.05151959651602276] Loss: 22.845241468383918\n",
      "Iteracion: 9223 Gradiente: [0.00298458854172452,-0.05149220711713518] Loss: 22.845238805905765\n",
      "Iteracion: 9224 Gradiente: [0.002983001842855515,-0.0514648322792894] Loss: 22.845236146257765\n",
      "Iteracion: 9225 Gradiente: [0.002981415987491687,-0.05143747199475091] Loss: 22.845233489436904\n",
      "Iteracion: 9226 Gradiente: [0.0029798309752616585,-0.05141012625577659] Loss: 22.845230835440212\n",
      "Iteracion: 9227 Gradiente: [0.0029782468056311017,-0.051382795054638206] Loss: 22.845228184264645\n",
      "Iteracion: 9228 Gradiente: [0.0029766634782456927,-0.051355478383602694] Loss: 22.84522553590723\n",
      "Iteracion: 9229 Gradiente: [0.0029750809925853145,-0.05132817623494823] Loss: 22.84522289036494\n",
      "Iteracion: 9230 Gradiente: [0.0029734993481118485,-0.05130088860096128] Loss: 22.845220247634824\n",
      "Iteracion: 9231 Gradiente: [0.002971918544626343,-0.051273615473910183] Loss: 22.84521760771389\n",
      "Iteracion: 9232 Gradiente: [0.0029703385815546806,-0.05124635684609006] Loss: 22.845214970599102\n",
      "Iteracion: 9233 Gradiente: [0.0029687594583568473,-0.051219112709797204] Loss: 22.845212336287535\n",
      "Iteracion: 9234 Gradiente: [0.0029671811747495743,-0.05119188305732081] Loss: 22.845209704776174\n",
      "Iteracion: 9235 Gradiente: [0.0029656037301464266,-0.05116466788096439] Loss: 22.845207076062064\n",
      "Iteracion: 9236 Gradiente: [0.002964027124117289,-0.051137467173035016] Loss: 22.8452044501422\n",
      "Iteracion: 9237 Gradiente: [0.002962451356271837,-0.051110280925836096] Loss: 22.84520182701365\n",
      "Iteracion: 9238 Gradiente: [0.002960876426309748,-0.051083109131671973] Loss: 22.845199206673442\n",
      "Iteracion: 9239 Gradiente: [0.0029593023334361607,-0.051055951782877904] Loss: 22.845196589118565\n",
      "Iteracion: 9240 Gradiente: [0.002957729077522231,-0.05102880887175753] Loss: 22.84519397434612\n",
      "Iteracion: 9241 Gradiente: [0.0029561566580099453,-0.051001680390640895] Loss: 22.845191362353123\n",
      "Iteracion: 9242 Gradiente: [0.002954585074380134,-0.0509745663318616] Loss: 22.84518875313661\n",
      "Iteracion: 9243 Gradiente: [0.002953014326247209,-0.05094746668774898] Loss: 22.84518614669365\n",
      "Iteracion: 9244 Gradiente: [0.002951444413211372,-0.05092038145063545] Loss: 22.84518354302126\n",
      "Iteracion: 9245 Gradiente: [0.002949875334704188,-0.050893310612871424] Loss: 22.84518094211654\n",
      "Iteracion: 9246 Gradiente: [0.0029483070904433363,-0.05086625416678956] Loss: 22.84517834397652\n",
      "Iteracion: 9247 Gradiente: [0.0029467396798736447,-0.050839212104748795] Loss: 22.845175748598272\n",
      "Iteracion: 9248 Gradiente: [0.0029451731026171046,-0.05081218441909634] Loss: 22.845173155978845\n",
      "Iteracion: 9249 Gradiente: [0.002943607358254023,-0.050785171102187596] Loss: 22.845170566115325\n",
      "Iteracion: 9250 Gradiente: [0.0029420424461695425,-0.05075817214639441] Loss: 22.84516797900478\n",
      "Iteracion: 9251 Gradiente: [0.0029404783661334476,-0.050731187544069085] Loss: 22.845165394644262\n",
      "Iteracion: 9252 Gradiente: [0.0029389151175678307,-0.05070421728758892] Loss: 22.84516281303087\n",
      "Iteracion: 9253 Gradiente: [0.0029373527001060515,-0.05067726136932436] Loss: 22.845160234161686\n",
      "Iteracion: 9254 Gradiente: [0.002935791113175886,-0.0506503197816586] Loss: 22.845157658033774\n",
      "Iteracion: 9255 Gradiente: [0.002934230356570803,-0.05062339251695877] Loss: 22.845155084644247\n",
      "Iteracion: 9256 Gradiente: [0.002932670429652262,-0.050596479567624066] Loss: 22.845152513990172\n",
      "Iteracion: 9257 Gradiente: [0.002931111332049833,-0.05056958092603807] Loss: 22.845149946068652\n",
      "Iteracion: 9258 Gradiente: [0.0029295530632272933,-0.05054269658460022] Loss: 22.845147380876764\n",
      "Iteracion: 9259 Gradiente: [0.0029279956229051624,-0.05051582653570013] Loss: 22.845144818411637\n",
      "Iteracion: 9260 Gradiente: [0.002926439010573745,-0.05048897077174104] Loss: 22.845142258670347\n",
      "Iteracion: 9261 Gradiente: [0.0029248832257043962,-0.050462129285136] Loss: 22.84513970165001\n",
      "Iteracion: 9262 Gradiente: [0.0029233282680735328,-0.05043530206828019] Loss: 22.84513714734772\n",
      "Iteracion: 9263 Gradiente: [0.0029217741369990335,-0.05040848911360361] Loss: 22.845134595760612\n",
      "Iteracion: 9264 Gradiente: [0.0029202208322014183,-0.050381690413512696] Loss: 22.845132046885787\n",
      "Iteracion: 9265 Gradiente: [0.002918668353159622,-0.050354905960434945] Loss: 22.84512950072035\n",
      "Iteracion: 9266 Gradiente: [0.002917116699448267,-0.0503281357467961] Loss: 22.84512695726144\n",
      "Iteracion: 9267 Gradiente: [0.0029155658706410273,-0.05030137976502521] Loss: 22.84512441650618\n",
      "Iteracion: 9268 Gradiente: [0.0029140158663238935,-0.05027463800755273] Loss: 22.845121878451685\n",
      "Iteracion: 9269 Gradiente: [0.0029124666860553816,-0.05024791046681827] Loss: 22.845119343095075\n",
      "Iteracion: 9270 Gradiente: [0.002910918329428114,-0.05022119713526318] Loss: 22.845116810433506\n",
      "Iteracion: 9271 Gradiente: [0.0029093707958584977,-0.050194498005340434] Loss: 22.8451142804641\n",
      "Iteracion: 9272 Gradiente: [0.002907824085067053,-0.050167813069491744] Loss: 22.84511175318401\n",
      "Iteracion: 9273 Gradiente: [0.002906278196557347,-0.05014114232017353] Loss: 22.845109228590356\n",
      "Iteracion: 9274 Gradiente: [0.0029047331298651595,-0.0501144857498448] Loss: 22.84510670668028\n",
      "Iteracion: 9275 Gradiente: [0.002903188884610586,-0.05008784335096538] Loss: 22.845104187450964\n",
      "Iteracion: 9276 Gradiente: [0.0029016454602488768,-0.05006121511600756] Loss: 22.84510167089952\n",
      "Iteracion: 9277 Gradiente: [0.002900102856404866,-0.05003460103743767] Loss: 22.845099157023114\n",
      "Iteracion: 9278 Gradiente: [0.0028985610727573887,-0.05000800110772339] Loss: 22.84509664581892\n",
      "Iteracion: 9279 Gradiente: [0.002897020108698219,-0.04998141531935308] Loss: 22.84509413728406\n",
      "Iteracion: 9280 Gradiente: [0.0028954799638938765,-0.04995484366480044] Loss: 22.845091631415748\n",
      "Iteracion: 9281 Gradiente: [0.0028939406378820347,-0.04992828613655552] Loss: 22.845089128211093\n",
      "Iteracion: 9282 Gradiente: [0.0028924021302657366,-0.04990174272710514] Loss: 22.845086627667303\n",
      "Iteracion: 9283 Gradiente: [0.0028908644405059173,-0.04987521342894941] Loss: 22.84508412978154\n",
      "Iteracion: 9284 Gradiente: [0.0028893275682056203,-0.04984869823458477] Loss: 22.84508163455098\n",
      "Iteracion: 9285 Gradiente: [0.0028877915129148354,-0.04982219713651321] Loss: 22.845079141972786\n",
      "Iteracion: 9286 Gradiente: [0.002886256274349345,-0.049795710127232815] Loss: 22.845076652044153\n",
      "Iteracion: 9287 Gradiente: [0.00288472185190282,-0.04976923719926229] Loss: 22.845074164762263\n",
      "Iteracion: 9288 Gradiente: [0.0028831882452427255,-0.04974277834511097] Loss: 22.84507168012431\n",
      "Iteracion: 9289 Gradiente: [0.0028816554538934726,-0.04971633355729862] Loss: 22.845069198127465\n",
      "Iteracion: 9290 Gradiente: [0.0028801234773624174,-0.04968990282835198] Loss: 22.84506671876893\n",
      "Iteracion: 9291 Gradiente: [0.0028785923153805014,-0.049663486150784426] Loss: 22.8450642420459\n",
      "Iteracion: 9292 Gradiente: [0.0028770619672798146,-0.04963708351714236] Loss: 22.845061767955578\n",
      "Iteracion: 9293 Gradiente: [0.002875532432843405,-0.0496106949199446] Loss: 22.84505929649516\n",
      "Iteracion: 9294 Gradiente: [0.0028740037115132584,-0.04958432035173874] Loss: 22.845056827661857\n",
      "Iteracion: 9295 Gradiente: [0.002872475802952105,-0.049557959805059566] Loss: 22.84505436145286\n",
      "Iteracion: 9296 Gradiente: [0.0028709487066085632,-0.049531613272459175] Loss: 22.845051897865392\n",
      "Iteracion: 9297 Gradiente: [0.0028694224222041006,-0.04950528074647949] Loss: 22.845049436896662\n",
      "Iteracion: 9298 Gradiente: [0.0028678969491797565,-0.04947896221968039] Loss: 22.845046978543905\n",
      "Iteracion: 9299 Gradiente: [0.0028663722870949943,-0.04945265768462263] Loss: 22.845044522804326\n",
      "Iteracion: 9300 Gradiente: [0.0028648484356097017,-0.04942636713385949] Loss: 22.84504206967513\n",
      "Iteracion: 9301 Gradiente: [0.0028633253942681827,-0.049400090559958795] Loss: 22.845039619153564\n",
      "Iteracion: 9302 Gradiente: [0.002861803162555058,-0.04937382795549562] Loss: 22.845037171236857\n",
      "Iteracion: 9303 Gradiente: [0.0028602817402060055,-0.04934757931303307] Loss: 22.84503472592222\n",
      "Iteracion: 9304 Gradiente: [0.0028587611266686964,-0.049321344625155444] Loss: 22.845032283206912\n",
      "Iteracion: 9305 Gradiente: [0.0028572413214504877,-0.049295123884448816] Loss: 22.84502984308816\n",
      "Iteracion: 9306 Gradiente: [0.0028557223242652676,-0.04926891708349004] Loss: 22.845027405563204\n",
      "Iteracion: 9307 Gradiente: [0.0028542041346355518,-0.049242724214870984] Loss: 22.84502497062926\n",
      "Iteracion: 9308 Gradiente: [0.002852686752102803,-0.04921654527118401] Loss: 22.84502253828363\n",
      "Iteracion: 9309 Gradiente: [0.0028511701762056418,-0.04919038024503119] Loss: 22.845020108523514\n",
      "Iteracion: 9310 Gradiente: [0.0028496544066740626,-0.04916422912900297] Loss: 22.84501768134619\n",
      "Iteracion: 9311 Gradiente: [0.002848139442928262,-0.04913809191571292] Loss: 22.845015256748887\n",
      "Iteracion: 9312 Gradiente: [0.0028466252845504416,-0.049111968597769715] Loss: 22.845012834728905\n",
      "Iteracion: 9313 Gradiente: [0.002845111931158802,-0.04908585916778385] Loss: 22.84501041528346\n",
      "Iteracion: 9314 Gradiente: [0.0028435993823270186,-0.04905976361837148] Loss: 22.845007998409848\n",
      "Iteracion: 9315 Gradiente: [0.002842087637606975,-0.04903368194215408] Loss: 22.845005584105316\n",
      "Iteracion: 9316 Gradiente: [0.0028405766965306613,-0.049007614131760135] Loss: 22.845003172367132\n",
      "Iteracion: 9317 Gradiente: [0.0028390665587721743,-0.04898156017980971] Loss: 22.84500076319257\n",
      "Iteracion: 9318 Gradiente: [0.0028375572239535056,-0.04895552007893447] Loss: 22.844998356578923\n",
      "Iteracion: 9319 Gradiente: [0.0028360486914304297,-0.04892949382178268] Loss: 22.844995952523448\n",
      "Iteracion: 9320 Gradiente: [0.0028345409608884135,-0.04890348140098825] Loss: 22.84499355102344\n",
      "Iteracion: 9321 Gradiente: [0.002833034031894499,-0.04887748280919411] Loss: 22.84499115207617\n",
      "Iteracion: 9322 Gradiente: [0.0028315279041085735,-0.04885149803904755] Loss: 22.844988755678926\n",
      "Iteracion: 9323 Gradiente: [0.002830022576912938,-0.048825527083207136] Loss: 22.844986361829015\n",
      "Iteracion: 9324 Gradiente: [0.0028285180500139027,-0.04879956993432373] Loss: 22.844983970523696\n",
      "Iteracion: 9325 Gradiente: [0.0028270143230220888,-0.04877362658505291] Loss: 22.8449815817603\n",
      "Iteracion: 9326 Gradiente: [0.002825511395357694,-0.04874769702806982] Loss: 22.8449791955361\n",
      "Iteracion: 9327 Gradiente: [0.002824009266843556,-0.048721781256027795] Loss: 22.844976811848415\n",
      "Iteracion: 9328 Gradiente: [0.002822507936829766,-0.048695879261608135] Loss: 22.84497443069454\n",
      "Iteracion: 9329 Gradiente: [0.0028210074048994707,-0.04866999103748846] Loss: 22.844972052071775\n",
      "Iteracion: 9330 Gradiente: [0.0028195076707731915,-0.04864411657633857] Loss: 22.844969675977435\n",
      "Iteracion: 9331 Gradiente: [0.0028180087340312337,-0.048618255870842474] Loss: 22.844967302408822\n",
      "Iteracion: 9332 Gradiente: [0.002816510594077689,-0.048592408913695685] Loss: 22.84496493136328\n",
      "Iteracion: 9333 Gradiente: [0.002815013250550654,-0.04856657569758814] Loss: 22.844962562838095\n",
      "Iteracion: 9334 Gradiente: [0.0028135167031668592,-0.048540756215203255] Loss: 22.84496019683061\n",
      "Iteracion: 9335 Gradiente: [0.002812020951285869,-0.04851495045925513] Loss: 22.84495783333813\n",
      "Iteracion: 9336 Gradiente: [0.002810525994662309,-0.048489158422434286] Loss: 22.844955472358023\n",
      "Iteracion: 9337 Gradiente: [0.0028090318327627984,-0.04846338009745483] Loss: 22.84495311388755\n",
      "Iteracion: 9338 Gradiente: [0.002807538465272804,-0.048437615477019594] Loss: 22.8449507579241\n",
      "Iteracion: 9339 Gradiente: [0.002806045891662734,-0.04841186455384907] Loss: 22.844948404464994\n",
      "Iteracion: 9340 Gradiente: [0.0028045541115195266,-0.04838612732066174] Loss: 22.844946053507556\n",
      "Iteracion: 9341 Gradiente: [0.0028030631243884346,-0.04836040377018283] Loss: 22.844943705049136\n",
      "Iteracion: 9342 Gradiente: [0.0028015729300449265,-0.048334693895124306] Loss: 22.844941359087084\n",
      "Iteracion: 9343 Gradiente: [0.00280008352785804,-0.04830899768822891] Loss: 22.84493901561872\n",
      "Iteracion: 9344 Gradiente: [0.0027985949174222924,-0.0482833151422304] Loss: 22.844936674641435\n",
      "Iteracion: 9345 Gradiente: [0.0027971070985122045,-0.04825764624985342] Loss: 22.84493433615255\n",
      "Iteracion: 9346 Gradiente: [0.0027956200705138673,-0.04823199100384947] Loss: 22.844932000149445\n",
      "Iteracion: 9347 Gradiente: [0.0027941338330881157,-0.04820634939696191] Loss: 22.844929666629444\n",
      "Iteracion: 9348 Gradiente: [0.0027926483857432533,-0.04818072142194201] Loss: 22.844927335589933\n",
      "Iteracion: 9349 Gradiente: [0.002791163728205485,-0.04815510707153479] Loss: 22.844925007028248\n",
      "Iteracion: 9350 Gradiente: [0.0027896798599120606,-0.04812950633850477] Loss: 22.844922680941792\n",
      "Iteracion: 9351 Gradiente: [0.0027881967803807584,-0.04810391921561627] Loss: 22.844920357327922\n",
      "Iteracion: 9352 Gradiente: [0.002786714489440101,-0.04807834569562033] Loss: 22.84491803618399\n",
      "Iteracion: 9353 Gradiente: [0.0027852329865169167,-0.048052785771292385] Loss: 22.8449157175074\n",
      "Iteracion: 9354 Gradiente: [0.0027837522711867753,-0.04802723943540599] Loss: 22.84491340129551\n",
      "Iteracion: 9355 Gradiente: [0.0027822723430366144,-0.04800170668073752] Loss: 22.844911087545697\n",
      "Iteracion: 9356 Gradiente: [0.002780793201657161,-0.04797618750006502] Loss: 22.84490877625534\n",
      "Iteracion: 9357 Gradiente: [0.0027793148465642997,-0.04795068188617743] Loss: 22.844906467421858\n",
      "Iteracion: 9358 Gradiente: [0.0027778372775353927,-0.047925189831849124] Loss: 22.84490416104261\n",
      "Iteracion: 9359 Gradiente: [0.0027763604939451624,-0.047899711329883014] Loss: 22.844901857114976\n",
      "Iteracion: 9360 Gradiente: [0.002774884495508445,-0.04787424637306899] Loss: 22.84489955563638\n",
      "Iteracion: 9361 Gradiente: [0.002773409281716492,-0.047848794954208886] Loss: 22.84489725660423\n",
      "Iteracion: 9362 Gradiente: [0.002771934852212136,-0.04782335706610373] Loss: 22.844894960015882\n",
      "Iteracion: 9363 Gradiente: [0.0027704612065254726,-0.047797932701563066] Loss: 22.844892665868755\n",
      "Iteracion: 9364 Gradiente: [0.0027689883443334415,-0.04777252185339028] Loss: 22.844890374160247\n",
      "Iteracion: 9365 Gradiente: [0.0027675162650931877,-0.047747124514410426] Loss: 22.84488808488781\n",
      "Iteracion: 9366 Gradiente: [0.0027660449685479684,-0.047721740677429264] Loss: 22.844885798048793\n",
      "Iteracion: 9367 Gradiente: [0.002764574454099981,-0.047696370335279424] Loss: 22.844883513640657\n",
      "Iteracion: 9368 Gradiente: [0.0027631047215398515,-0.04767101348077567] Loss: 22.84488123166079\n",
      "Iteracion: 9369 Gradiente: [0.002761635770277356,-0.047645670106756674] Loss: 22.844878952106612\n",
      "Iteracion: 9370 Gradiente: [0.002760167599920275,-0.04762034020605685] Loss: 22.844876674975563\n",
      "Iteracion: 9371 Gradiente: [0.002758700210120916,-0.04759502377150729] Loss: 22.84487440026504\n",
      "Iteracion: 9372 Gradiente: [0.002757233600393268,-0.04756972079595343] Loss: 22.844872127972504\n",
      "Iteracion: 9373 Gradiente: [0.0027557677704010074,-0.04754443127223631] Loss: 22.844869858095358\n",
      "Iteracion: 9374 Gradiente: [0.002754302719594648,-0.04751915519321095] Loss: 22.844867590631047\n",
      "Iteracion: 9375 Gradiente: [0.0027528384477297626,-0.047493892551721116] Loss: 22.844865325576997\n",
      "Iteracion: 9376 Gradiente: [0.0027513749543506567,-0.04746864334062373] Loss: 22.844863062930656\n",
      "Iteracion: 9377 Gradiente: [0.0027499122389798457,-0.04744340755278434] Loss: 22.84486080268946\n",
      "Iteracion: 9378 Gradiente: [0.0027484503011805826,-0.04741818518106721] Loss: 22.844858544850844\n",
      "Iteracion: 9379 Gradiente: [0.002746989140681914,-0.047392976218332925] Loss: 22.844856289412277\n",
      "Iteracion: 9380 Gradiente: [0.0027455287569694065,-0.04736778065745639] Loss: 22.84485403637117\n",
      "Iteracion: 9381 Gradiente: [0.0027440691495778918,-0.04734259849131834] Loss: 22.84485178572502\n",
      "Iteracion: 9382 Gradiente: [0.0027426103181511508,-0.04731742971279272] Loss: 22.84484953747125\n",
      "Iteracion: 9383 Gradiente: [0.0027411522622903324,-0.047292274314760976] Loss: 22.84484729160732\n",
      "Iteracion: 9384 Gradiente: [0.0027396949816430077,-0.0472671322901076] Loss: 22.844845048130704\n",
      "Iteracion: 9385 Gradiente: [0.00273823847569948,-0.047242003631728834] Loss: 22.844842807038866\n",
      "Iteracion: 9386 Gradiente: [0.002736782744069425,-0.047216888332516284] Loss: 22.84484056832925\n",
      "Iteracion: 9387 Gradiente: [0.0027353277863710447,-0.04719178638536749] Loss: 22.844838331999334\n",
      "Iteracion: 9388 Gradiente: [0.002733873602191276,-0.04716669778318223] Loss: 22.84483609804658\n",
      "Iteracion: 9389 Gradiente: [0.0027324201910583195,-0.047141622518869754] Loss: 22.844833866468484\n",
      "Iteracion: 9390 Gradiente: [0.0027309675526206926,-0.047116560585337905] Loss: 22.84483163726251\n",
      "Iteracion: 9391 Gradiente: [0.002729515686447333,-0.047091511975499385] Loss: 22.844829410426133\n",
      "Iteracion: 9392 Gradiente: [0.002728064592171601,-0.047066476682268286] Loss: 22.84482718595683\n",
      "Iteracion: 9393 Gradiente: [0.0027266142691890613,-0.04704145469857532] Loss: 22.844824963852083\n",
      "Iteracion: 9394 Gradiente: [0.002725164717340552,-0.04701644601733257] Loss: 22.844822744109408\n",
      "Iteracion: 9395 Gradiente: [0.0027237159360576396,-0.04699145063147618] Loss: 22.844820526726263\n",
      "Iteracion: 9396 Gradiente: [0.0027222679250712644,-0.04696646853392797] Loss: 22.844818311700127\n",
      "Iteracion: 9397 Gradiente: [0.002720820683936154,-0.04694149971762727] Loss: 22.844816099028556\n",
      "Iteracion: 9398 Gradiente: [0.0027193742122231393,-0.04691654417551459] Loss: 22.844813888708977\n",
      "Iteracion: 9399 Gradiente: [0.002717928509386525,-0.04689160190054101] Loss: 22.844811680738943\n",
      "Iteracion: 9400 Gradiente: [0.002716483575178093,-0.04686667288564277] Loss: 22.844809475115923\n",
      "Iteracion: 9401 Gradiente: [0.00271503940908057,-0.04684175712377758] Loss: 22.84480727183742\n",
      "Iteracion: 9402 Gradiente: [0.002713596010822054,-0.046816854607892464] Loss: 22.844805070900975\n",
      "Iteracion: 9403 Gradiente: [0.0027121533799013757,-0.046791965330949856] Loss: 22.844802872304072\n",
      "Iteracion: 9404 Gradiente: [0.0027107115159291577,-0.04676708928591052] Loss: 22.844800676044233\n",
      "Iteracion: 9405 Gradiente: [0.00270927041849139,-0.0467422264657408] Loss: 22.84479848211897\n",
      "Iteracion: 9406 Gradiente: [0.0027078300871750116,-0.046717376863410466] Loss: 22.844796290525803\n",
      "Iteracion: 9407 Gradiente: [0.0027063905214996945,-0.04669254047189592] Loss: 22.844794101262252\n",
      "Iteracion: 9408 Gradiente: [0.0027049517212844876,-0.04666771728416137] Loss: 22.844791914325835\n",
      "Iteracion: 9409 Gradiente: [0.002703513685906008,-0.04664290729319944] Loss: 22.844789729714094\n",
      "Iteracion: 9410 Gradiente: [0.0027020764150828807,-0.04661811049198761] Loss: 22.844787547424563\n",
      "Iteracion: 9411 Gradiente: [0.00270063990830162,-0.04659332687351896] Loss: 22.844785367454737\n",
      "Iteracion: 9412 Gradiente: [0.0026992041651719015,-0.046568556430785675] Loss: 22.84478318980218\n",
      "Iteracion: 9413 Gradiente: [0.0026977691854796148,-0.04654379915677029] Loss: 22.84478101446442\n",
      "Iteracion: 9414 Gradiente: [0.0026963349685966403,-0.046519055044487466] Loss: 22.844778841439\n",
      "Iteracion: 9415 Gradiente: [0.0026949015140473875,-0.04649432408694144] Loss: 22.84477667072346\n",
      "Iteracion: 9416 Gradiente: [0.002693468821771224,-0.0464696062771188] Loss: 22.844774502315342\n",
      "Iteracion: 9417 Gradiente: [0.002692036891060449,-0.046444901608049705] Loss: 22.844772336212195\n",
      "Iteracion: 9418 Gradiente: [0.002690605721664004,-0.04642021007273686] Loss: 22.84477017241157\n",
      "Iteracion: 9419 Gradiente: [0.002689175313075983,-0.046395531664204744] Loss: 22.844768010911018\n",
      "Iteracion: 9420 Gradiente: [0.002687745664860586,-0.046370866375476896] Loss: 22.8447658517081\n",
      "Iteracion: 9421 Gradiente: [0.002686316776725069,-0.04634621419957175] Loss: 22.84476369480037\n",
      "Iteracion: 9422 Gradiente: [0.0026848886482459494,-0.04632157512951842] Loss: 22.844761540185385\n",
      "Iteracion: 9423 Gradiente: [0.002683461279071745,-0.04629694915834648] Loss: 22.844759387860698\n",
      "Iteracion: 9424 Gradiente: [0.002682034668722129,-0.04627233627909841] Loss: 22.844757237823902\n",
      "Iteracion: 9425 Gradiente: [0.002680608816753723,-0.04624773648481515] Loss: 22.844755090072542\n",
      "Iteracion: 9426 Gradiente: [0.0026791837228226237,-0.04622314976853682] Loss: 22.844752944604195\n",
      "Iteracion: 9427 Gradiente: [0.002677759386494927,-0.0461985761233129] Loss: 22.84475080141643\n",
      "Iteracion: 9428 Gradiente: [0.0026763358074267294,-0.046174015542189535] Loss: 22.844748660506845\n",
      "Iteracion: 9429 Gradiente: [0.0026749129852068638,-0.04614946801822472] Loss: 22.844746521872995\n",
      "Iteracion: 9430 Gradiente: [0.002673490919352162,-0.046124933544479066] Loss: 22.844744385512467\n",
      "Iteracion: 9431 Gradiente: [0.0026720696095281935,-0.04610041211401317] Loss: 22.844742251422847\n",
      "Iteracion: 9432 Gradiente: [0.0026706490552641073,-0.04607590371989486] Loss: 22.844740119601724\n",
      "Iteracion: 9433 Gradiente: [0.002669229256227368,-0.046051408355189595] Loss: 22.84473799004666\n",
      "Iteracion: 9434 Gradiente: [0.0026678102120399672,-0.04602692601296946] Loss: 22.84473586275529\n",
      "Iteracion: 9435 Gradiente: [0.0026663919222291574,-0.04600245668631627] Loss: 22.844733737725175\n",
      "Iteracion: 9436 Gradiente: [0.0026649743864728256,-0.045978000368305176] Loss: 22.844731614953904\n",
      "Iteracion: 9437 Gradiente: [0.0026635576042982243,-0.04595355705202486] Loss: 22.844729494439125\n",
      "Iteracion: 9438 Gradiente: [0.0026621415753207128,-0.045929126730562216] Loss: 22.8447273761784\n",
      "Iteracion: 9439 Gradiente: [0.002660726299130071,-0.04590470939700945] Loss: 22.84472526016935\n",
      "Iteracion: 9440 Gradiente: [0.0026593117753748174,-0.04588030504445844] Loss: 22.84472314640954\n",
      "Iteracion: 9441 Gradiente: [0.0026578980035604142,-0.04585591366601444] Loss: 22.84472103489663\n",
      "Iteracion: 9442 Gradiente: [0.002656484983450014,-0.04583153525477061] Loss: 22.844718925628225\n",
      "Iteracion: 9443 Gradiente: [0.002655072714485603,-0.04580716980384191] Loss: 22.844716818601913\n",
      "Iteracion: 9444 Gradiente: [0.0026536611962162246,-0.04578281730634046] Loss: 22.84471471381534\n",
      "Iteracion: 9445 Gradiente: [0.0026522504285509283,-0.04575847775536346] Loss: 22.844712611266093\n",
      "Iteracion: 9446 Gradiente: [0.0026508404107820145,-0.04573415114404457] Loss: 22.84471051095181\n",
      "Iteracion: 9447 Gradiente: [0.0026494311427247416,-0.04570983746549402] Loss: 22.84470841287013\n",
      "Iteracion: 9448 Gradiente: [0.00264802262379078,-0.045685536712846066] Loss: 22.844706317018666\n",
      "Iteracion: 9449 Gradiente: [0.002646614853655175,-0.04566124887922302] Loss: 22.844704223395038\n",
      "Iteracion: 9450 Gradiente: [0.0026452078319910773,-0.04563697395775617] Loss: 22.8447021319969\n",
      "Iteracion: 9451 Gradiente: [0.002643801558334265,-0.045612711941581924] Loss: 22.844700042821856\n",
      "Iteracion: 9452 Gradiente: [0.0026423960322716766,-0.04558846282384247] Loss: 22.844697955867574\n",
      "Iteracion: 9453 Gradiente: [0.0026409912534423557,-0.04556422659767823] Loss: 22.844695871131673\n",
      "Iteracion: 9454 Gradiente: [0.002639587221401977,-0.04554000325623709] Loss: 22.84469378861182\n",
      "Iteracion: 9455 Gradiente: [0.002638183935772531,-0.04551579279266989] Loss: 22.84469170830562\n",
      "Iteracion: 9456 Gradiente: [0.0026367813962584327,-0.04549159520012071] Loss: 22.844689630210755\n",
      "Iteracion: 9457 Gradiente: [0.002635379602283668,-0.04546741047176231] Loss: 22.844687554324846\n",
      "Iteracion: 9458 Gradiente: [0.0026339785535782314,-0.045443238600744826] Loss: 22.844685480645566\n",
      "Iteracion: 9459 Gradiente: [0.002632578249836115,-0.04541907958022844] Loss: 22.844683409170568\n",
      "Iteracion: 9460 Gradiente: [0.0026311786903827775,-0.0453949334033986] Loss: 22.844681339897498\n",
      "Iteracion: 9461 Gradiente: [0.0026297798749965297,-0.04537080006341689] Loss: 22.844679272824028\n",
      "Iteracion: 9462 Gradiente: [0.002628381803257677,-0.045346679553459404] Loss: 22.8446772079478\n",
      "Iteracion: 9463 Gradiente: [0.0026269844748242122,-0.045322571866702785] Loss: 22.844675145266514\n",
      "Iteracion: 9464 Gradiente: [0.0026255878892603355,-0.04529847699633258] Loss: 22.8446730847778\n",
      "Iteracion: 9465 Gradiente: [0.002624192046130247,-0.0452743949355361] Loss: 22.84467102647934\n",
      "Iteracion: 9466 Gradiente: [0.002622796945089097,-0.04525032567750221] Loss: 22.844668970368804\n",
      "Iteracion: 9467 Gradiente: [0.002621402585695402,-0.04522626921542591] Loss: 22.84466691644388\n",
      "Iteracion: 9468 Gradiente: [0.002620008967640312,-0.04520222554250021] Loss: 22.844664864702228\n",
      "Iteracion: 9469 Gradiente: [0.002618616090554345,-0.045178194651924033] Loss: 22.844662815141547\n",
      "Iteracion: 9470 Gradiente: [0.0026172239537999077,-0.04515417653691666] Loss: 22.844660767759486\n",
      "Iteracion: 9471 Gradiente: [0.0026158325572074167,-0.04513017119067323] Loss: 22.844658722553753\n",
      "Iteracion: 9472 Gradiente: [0.002614441900327809,-0.045106178606408184] Loss: 22.844656679522043\n",
      "Iteracion: 9473 Gradiente: [0.0026130519827679186,-0.04508219877733713] Loss: 22.844654638662018\n",
      "Iteracion: 9474 Gradiente: [0.002611662804136472,-0.0450582316966802] Loss: 22.844652599971386\n",
      "Iteracion: 9475 Gradiente: [0.0026102743639863017,-0.04503427735765998] Loss: 22.844650563447853\n",
      "Iteracion: 9476 Gradiente: [0.002608886662046454,-0.04501033575349815] Loss: 22.844648529089092\n",
      "Iteracion: 9477 Gradiente: [0.002607499697831865,-0.044986406877429864] Loss: 22.8446464968928\n",
      "Iteracion: 9478 Gradiente: [0.0026061134708821026,-0.04496249072269052] Loss: 22.844644466856703\n",
      "Iteracion: 9479 Gradiente: [0.002604727981034216,-0.04493858728250532] Loss: 22.84464243897849\n",
      "Iteracion: 9480 Gradiente: [0.002603343227699876,-0.044914696550125414] Loss: 22.844640413255874\n",
      "Iteracion: 9481 Gradiente: [0.0026019592105588647,-0.04489081851879177] Loss: 22.844638389686544\n",
      "Iteracion: 9482 Gradiente: [0.002600575929160224,-0.044866953181755535] Loss: 22.844636368268237\n",
      "Iteracion: 9483 Gradiente: [0.0025991933831041552,-0.04484310053226667] Loss: 22.844634348998646\n",
      "Iteracion: 9484 Gradiente: [0.0025978115721092838,-0.0448192605635762] Loss: 22.844632331875495\n",
      "Iteracion: 9485 Gradiente: [0.0025964304957994955,-0.044795433268940484] Loss: 22.844630316896517\n",
      "Iteracion: 9486 Gradiente: [0.002595050153674568,-0.04477161864162842] Loss: 22.844628304059402\n",
      "Iteracion: 9487 Gradiente: [0.002593670545372599,-0.044747816674903615] Loss: 22.844626293361898\n",
      "Iteracion: 9488 Gradiente: [0.002592291670465367,-0.044724027362037094] Loss: 22.84462428480174\n",
      "Iteracion: 9489 Gradiente: [0.002590913528600443,-0.044700250696300375] Loss: 22.84462227837661\n",
      "Iteracion: 9490 Gradiente: [0.0025895361194614,-0.04467648667096616] Loss: 22.844620274084264\n",
      "Iteracion: 9491 Gradiente: [0.002588159442532856,-0.04465273527932065] Loss: 22.844618271922467\n",
      "Iteracion: 9492 Gradiente: [0.0025867834975978592,-0.04462899651463654] Loss: 22.84461627188891\n",
      "Iteracion: 9493 Gradiente: [0.0025854082840453428,-0.04460527037021601] Loss: 22.84461427398134\n",
      "Iteracion: 9494 Gradiente: [0.002584033801599617,-0.044581556839341376] Loss: 22.84461227819751\n",
      "Iteracion: 9495 Gradiente: [0.002582660049966042,-0.04455785591530249] Loss: 22.844610284535158\n",
      "Iteracion: 9496 Gradiente: [0.002581287028555342,-0.04453416759140829] Loss: 22.84460829299202\n",
      "Iteracion: 9497 Gradiente: [0.002579914737156249,-0.04451049186095008] Loss: 22.844606303565854\n",
      "Iteracion: 9498 Gradiente: [0.0025785431753575947,-0.044486828717234535] Loss: 22.844604316254426\n",
      "Iteracion: 9499 Gradiente: [0.0025771723425843146,-0.04446317815357948] Loss: 22.844602331055434\n",
      "Iteracion: 9500 Gradiente: [0.0025758022386933514,-0.04443954016328462] Loss: 22.844600347966683\n",
      "Iteracion: 9501 Gradiente: [0.0025744328632470114,-0.04441591473966741] Loss: 22.844598366985917\n",
      "Iteracion: 9502 Gradiente: [0.002573064215651281,-0.04439230187605965] Loss: 22.84459638811088\n",
      "Iteracion: 9503 Gradiente: [0.002571696295668365,-0.04436870156577501] Loss: 22.844594411339344\n",
      "Iteracion: 9504 Gradiente: [0.0025703291030292046,-0.04434511380213273] Loss: 22.844592436669085\n",
      "Iteracion: 9505 Gradiente: [0.002568962637153997,-0.04432153857847491] Loss: 22.84459046409786\n",
      "Iteracion: 9506 Gradiente: [0.0025675968977964202,-0.044297975888124695] Loss: 22.844588493623416\n",
      "Iteracion: 9507 Gradiente: [0.002566231884516886,-0.04427442572442501] Loss: 22.84458652524356\n",
      "Iteracion: 9508 Gradiente: [0.002564867596847383,-0.04425088808071855] Loss: 22.844584558956054\n",
      "Iteracion: 9509 Gradiente: [0.0025635040345359053,-0.044227362950342436] Loss: 22.84458259475867\n",
      "Iteracion: 9510 Gradiente: [0.0025621411971239166,-0.044203850326649176] Loss: 22.84458063264918\n",
      "Iteracion: 9511 Gradiente: [0.002560779084111194,-0.04418035020299544] Loss: 22.84457867262537\n",
      "Iteracion: 9512 Gradiente: [0.0025594176953925778,-0.04415686257271905] Loss: 22.84457671468503\n",
      "Iteracion: 9513 Gradiente: [0.0025580570304488976,-0.044133387429186995] Loss: 22.844574758825924\n",
      "Iteracion: 9514 Gradiente: [0.002556697088835828,-0.044109924765763725] Loss: 22.84457280504587\n",
      "Iteracion: 9515 Gradiente: [0.002555337870089147,-0.04408647457582084] Loss: 22.84457085334264\n",
      "Iteracion: 9516 Gradiente: [0.002553979374013693,-0.04406303685271347] Loss: 22.844568903714023\n",
      "Iteracion: 9517 Gradiente: [0.002552621600230509,-0.04403961158981531] Loss: 22.844566956157834\n",
      "Iteracion: 9518 Gradiente: [0.002551264548210952,-0.04401619878051027] Loss: 22.844565010671833\n",
      "Iteracion: 9519 Gradiente: [0.0025499082176594356,-0.043992798418173135] Loss: 22.84456306725387\n",
      "Iteracion: 9520 Gradiente: [0.0025485526081392134,-0.04396941049618756] Loss: 22.844561125901688\n",
      "Iteracion: 9521 Gradiente: [0.0025471977194475434,-0.04394603500793141] Loss: 22.844559186613147\n",
      "Iteracion: 9522 Gradiente: [0.002545843550970517,-0.04392267194680718] Loss: 22.84455724938601\n",
      "Iteracion: 9523 Gradiente: [0.0025444901024646543,-0.04389932130619923] Loss: 22.844555314218105\n",
      "Iteracion: 9524 Gradiente: [0.0025431373734069967,-0.04387598307951433] Loss: 22.844553381107264\n",
      "Iteracion: 9525 Gradiente: [0.002541785363513327,-0.04385265726014704] Loss: 22.844551450051267\n",
      "Iteracion: 9526 Gradiente: [0.0025404340723753195,-0.043829343841501375] Loss: 22.844549521047938\n",
      "Iteracion: 9527 Gradiente: [0.0025390834995884385,-0.04380604281698742] Loss: 22.8445475940951\n",
      "Iteracion: 9528 Gradiente: [0.002537733644819203,-0.04378275418001264] Loss: 22.844545669190573\n",
      "Iteracion: 9529 Gradiente: [0.002536384507725605,-0.043759477923989466] Loss: 22.844543746332185\n",
      "Iteracion: 9530 Gradiente: [0.002535036087877529,-0.04373621404233745] Loss: 22.844541825517755\n",
      "Iteracion: 9531 Gradiente: [0.00253368838488844,-0.04371296252847922] Loss: 22.84453990674509\n",
      "Iteracion: 9532 Gradiente: [0.0025323413983490656,-0.04368972337584189] Loss: 22.844537990012068\n",
      "Iteracion: 9533 Gradiente: [0.002530995127952451,-0.043666496577848574] Loss: 22.84453607531647\n",
      "Iteracion: 9534 Gradiente: [0.0025296495732883765,-0.0436432821279323] Loss: 22.844534162656156\n",
      "Iteracion: 9535 Gradiente: [0.0025283047338784097,-0.04362008001953643] Loss: 22.844532252028962\n",
      "Iteracion: 9536 Gradiente: [0.002526960609521704,-0.04359689024608713] Loss: 22.844530343432726\n",
      "Iteracion: 9537 Gradiente: [0.0025256171997294057,-0.0435737128010345] Loss: 22.84452843686528\n",
      "Iteracion: 9538 Gradiente: [0.002524274504059084,-0.04355054767782711] Loss: 22.844526532324487\n",
      "Iteracion: 9539 Gradiente: [0.0025229325222748383,-0.04352739486990617] Loss: 22.844524629808184\n",
      "Iteracion: 9540 Gradiente: [0.0025215912539152895,-0.04350425437073018] Loss: 22.8445227293142\n",
      "Iteracion: 9541 Gradiente: [0.0025202506985446387,-0.043481126173756594] Loss: 22.8445208308404\n",
      "Iteracion: 9542 Gradiente: [0.0025189108559061425,-0.04345801027243965] Loss: 22.844518934384645\n",
      "Iteracion: 9543 Gradiente: [0.002517571725657793,-0.04343490666024105] Loss: 22.84451703994478\n",
      "Iteracion: 9544 Gradiente: [0.0025162333071771553,-0.043411815330640854] Loss: 22.84451514751867\n",
      "Iteracion: 9545 Gradiente: [0.002514895600252013,-0.04338873627709982] Loss: 22.84451325710415\n",
      "Iteracion: 9546 Gradiente: [0.0025135586045668865,-0.04336566949308806] Loss: 22.844511368699127\n",
      "Iteracion: 9547 Gradiente: [0.0025122223197399764,-0.04334261497208172] Loss: 22.844509482301437\n",
      "Iteracion: 9548 Gradiente: [0.0025108867452729557,-0.043319572707569046] Loss: 22.844507597908926\n",
      "Iteracion: 9549 Gradiente: [0.0025095518808086583,-0.043296542693033284] Loss: 22.844505715519496\n",
      "Iteracion: 9550 Gradiente: [0.002508217725971917,-0.04327352492196302] Loss: 22.84450383513101\n",
      "Iteracion: 9551 Gradiente: [0.0025068842804643054,-0.043250519387842586] Loss: 22.844501956741336\n",
      "Iteracion: 9552 Gradiente: [0.0025055515438774973,-0.043227526084169084] Loss: 22.844500080348343\n",
      "Iteracion: 9553 Gradiente: [0.0025042195157899036,-0.043204545004442235] Loss: 22.84449820594992\n",
      "Iteracion: 9554 Gradiente: [0.002502888195837727,-0.04318157614216401] Loss: 22.844496333543933\n",
      "Iteracion: 9555 Gradiente: [0.002501557583605063,-0.04315861949084111] Loss: 22.844494463128292\n",
      "Iteracion: 9556 Gradiente: [0.00250022767882759,-0.04313567504397516] Loss: 22.844492594700846\n",
      "Iteracion: 9557 Gradiente: [0.002498898480996559,-0.04311274279508458] Loss: 22.844490728259508\n",
      "Iteracion: 9558 Gradiente: [0.0024975699898372263,-0.043089822737681756] Loss: 22.84448886380216\n",
      "Iteracion: 9559 Gradiente: [0.002496242204909057,-0.043066914865286306] Loss: 22.844487001326677\n",
      "Iteracion: 9560 Gradiente: [0.002494915125942043,-0.04304401917141405] Loss: 22.844485140830987\n",
      "Iteracion: 9561 Gradiente: [0.0024935887524643856,-0.04302113564959693] Loss: 22.844483282312943\n",
      "Iteracion: 9562 Gradiente: [0.002492263084214604,-0.04299826429335809] Loss: 22.844481425770482\n",
      "Iteracion: 9563 Gradiente: [0.0024909381207028976,-0.04297540509623611] Loss: 22.844479571201486\n",
      "Iteracion: 9564 Gradiente: [0.0024896138615408364,-0.04295255805176718] Loss: 22.84447771860385\n",
      "Iteracion: 9565 Gradiente: [0.0024882903063902025,-0.042929723153488315] Loss: 22.84447586797547\n",
      "Iteracion: 9566 Gradiente: [0.0024869674548559334,-0.04290690039494376] Loss: 22.84447401931429\n",
      "Iteracion: 9567 Gradiente: [0.0024856453066111803,-0.042884089769676476] Loss: 22.844472172618193\n",
      "Iteracion: 9568 Gradiente: [0.002484323861230564,-0.04286129127124051] Loss: 22.844470327885094\n",
      "Iteracion: 9569 Gradiente: [0.002483003118415657,-0.04283850489318309] Loss: 22.84446848511292\n",
      "Iteracion: 9570 Gradiente: [0.0024816830777079227,-0.04281573062906681] Loss: 22.844466644299537\n",
      "Iteracion: 9571 Gradiente: [0.002480363738824091,-0.042792968472445264] Loss: 22.84446480544294\n",
      "Iteracion: 9572 Gradiente: [0.002479045101352047,-0.0427702184168839] Loss: 22.844462968540995\n",
      "Iteracion: 9573 Gradiente: [0.002477727164886308,-0.04274748045595101] Loss: 22.844461133591626\n",
      "Iteracion: 9574 Gradiente: [0.0024764099290602343,-0.04272475458321677] Loss: 22.84445930059277\n",
      "Iteracion: 9575 Gradiente: [0.002475093393503395,-0.04270204079225479] Loss: 22.844457469542366\n",
      "Iteracion: 9576 Gradiente: [0.002473777557892731,-0.04267933907664106] Loss: 22.84445564043833\n",
      "Iteracion: 9577 Gradiente: [0.002472462421826549,-0.042656649429955236] Loss: 22.844453813278584\n",
      "Iteracion: 9578 Gradiente: [0.0024711479849154707,-0.04263397184578146] Loss: 22.84445198806107\n",
      "Iteracion: 9579 Gradiente: [0.0024698342468089623,-0.04261130631770804] Loss: 22.844450164783733\n",
      "Iteracion: 9580 Gradiente: [0.0024685212071015408,-0.042588652839324934] Loss: 22.844448343444487\n",
      "Iteracion: 9581 Gradiente: [0.0024672088654957253,-0.04256601140422577] Loss: 22.8444465240413\n",
      "Iteracion: 9582 Gradiente: [0.0024658972215434007,-0.04254338200600903] Loss: 22.844444706572087\n",
      "Iteracion: 9583 Gradiente: [0.0024645862748855054,-0.04252076463827533] Loss: 22.84444289103482\n",
      "Iteracion: 9584 Gradiente: [0.002463276025236875,-0.04249815929462481] Loss: 22.84444107742742\n",
      "Iteracion: 9585 Gradiente: [0.0024619664720584447,-0.042475565968675376] Loss: 22.844439265747837\n",
      "Iteracion: 9586 Gradiente: [0.002460657615154105,-0.04245298465402646] Loss: 22.844437455994047\n",
      "Iteracion: 9587 Gradiente: [0.0024593494539459472,-0.042430415344304605] Loss: 22.84443564816398\n",
      "Iteracion: 9588 Gradiente: [0.002458041988377128,-0.04240785803311103] Loss: 22.844433842255604\n",
      "Iteracion: 9589 Gradiente: [0.0024567352177162624,-0.042385312714086965] Loss: 22.844432038266866\n",
      "Iteracion: 9590 Gradiente: [0.0024554291418970326,-0.04236277938083918] Loss: 22.84443023619573\n",
      "Iteracion: 9591 Gradiente: [0.002454123760389848,-0.04234025802700536] Loss: 22.844428436040157\n",
      "Iteracion: 9592 Gradiente: [0.002452819072832805,-0.04231774864621608] Loss: 22.844426637798104\n",
      "Iteracion: 9593 Gradiente: [0.002451515078963477,-0.042295251232100387] Loss: 22.84442484146756\n",
      "Iteracion: 9594 Gradiente: [0.0024502117782986945,-0.042272765778302815] Loss: 22.84442304704646\n",
      "Iteracion: 9595 Gradiente: [0.0024489091704775015,-0.0422502922784646] Loss: 22.844421254532808\n",
      "Iteracion: 9596 Gradiente: [0.00244760725514368,-0.04222783072622936] Loss: 22.84441946392454\n",
      "Iteracion: 9597 Gradiente: [0.002446306032043329,-0.04220538111523998] Loss: 22.844417675219667\n",
      "Iteracion: 9598 Gradiente: [0.0024450055007340173,-0.04218294343915095] Loss: 22.844415888416147\n",
      "Iteracion: 9599 Gradiente: [0.0024437056607032066,-0.04216051769162673] Loss: 22.84441410351197\n",
      "Iteracion: 9600 Gradiente: [0.0024424065117595243,-0.04213810386631565] Loss: 22.844412320505096\n",
      "Iteracion: 9601 Gradiente: [0.00244110805351833,-0.04211570195687872] Loss: 22.84441053939351\n",
      "Iteracion: 9602 Gradiente: [0.0024398102855656134,-0.04209331195698584] Loss: 22.844408760175217\n",
      "Iteracion: 9603 Gradiente: [0.002438513207538525,-0.04207093386030429] Loss: 22.844406982848202\n",
      "Iteracion: 9604 Gradiente: [0.00243721681901737,-0.04204856766050978] Loss: 22.844405207410457\n",
      "Iteracion: 9605 Gradiente: [0.002435921119790881,-0.042026213351268875] Loss: 22.844403433859938\n",
      "Iteracion: 9606 Gradiente: [0.002434626109344625,-0.0420038709262658] Loss: 22.84440166219468\n",
      "Iteracion: 9607 Gradiente: [0.002433331787308172,-0.041981540379186555] Loss: 22.844399892412657\n",
      "Iteracion: 9608 Gradiente: [0.0024320381534190952,-0.04195922170370909] Loss: 22.844398124511894\n",
      "Iteracion: 9609 Gradiente: [0.0024307452073837037,-0.041936914893517495] Loss: 22.844396358490354\n",
      "Iteracion: 9610 Gradiente: [0.002429452948631668,-0.04191461994231519] Loss: 22.844394594346063\n",
      "Iteracion: 9611 Gradiente: [0.0024281613768389813,-0.04189233684379635] Loss: 22.844392832077013\n",
      "Iteracion: 9612 Gradiente: [0.002426870491732795,-0.0418700655916543] Loss: 22.84439107168123\n",
      "Iteracion: 9613 Gradiente: [0.0024255802928915197,-0.04184780617959352] Loss: 22.844389313156707\n",
      "Iteracion: 9614 Gradiente: [0.002424290779984517,-0.04182555860131778] Loss: 22.844387556501445\n",
      "Iteracion: 9615 Gradiente: [0.002423001952599672,-0.041803322850539594] Loss: 22.844385801713486\n",
      "Iteracion: 9616 Gradiente: [0.0024217138103504493,-0.04178109892097114] Loss: 22.844384048790825\n",
      "Iteracion: 9617 Gradiente: [0.002420426352895788,-0.041758886806327666] Loss: 22.84438229773146\n",
      "Iteracion: 9618 Gradiente: [0.00241913957998842,-0.04173668650032027] Loss: 22.844380548533476\n",
      "Iteracion: 9619 Gradiente: [0.0024178534911375965,-0.04171449799667819] Loss: 22.844378801194843\n",
      "Iteracion: 9620 Gradiente: [0.0024165680859207822,-0.041692321289132535] Loss: 22.844377055713565\n",
      "Iteracion: 9621 Gradiente: [0.0024152833642422894,-0.04167015637139286] Loss: 22.844375312087735\n",
      "Iteracion: 9622 Gradiente: [0.00241399932542663,-0.04164800323721328] Loss: 22.844373570315316\n",
      "Iteracion: 9623 Gradiente: [0.0024127159693335897,-0.041625861880315064] Loss: 22.844371830394365\n",
      "Iteracion: 9624 Gradiente: [0.0024114332955453696,-0.04160373229443823] Loss: 22.844370092322908\n",
      "Iteracion: 9625 Gradiente: [0.0024101513035333253,-0.041581614473337206] Loss: 22.844368356098997\n",
      "Iteracion: 9626 Gradiente: [0.00240886999312598,-0.04155950841074538] Loss: 22.844366621720653\n",
      "Iteracion: 9627 Gradiente: [0.002407589363957641,-0.04153741410041114] Loss: 22.844364889185915\n",
      "Iteracion: 9628 Gradiente: [0.0024063094155025054,-0.04151533153609772] Loss: 22.84436315849283\n",
      "Iteracion: 9629 Gradiente: [0.00240503014752278,-0.041493260711550545] Loss: 22.844361429639427\n",
      "Iteracion: 9630 Gradiente: [0.0024037515597200355,-0.04147120162052798] Loss: 22.844359702623763\n",
      "Iteracion: 9631 Gradiente: [0.0024024736515485755,-0.041449154256800944] Loss: 22.84435797744388\n",
      "Iteracion: 9632 Gradiente: [0.0024011964227810267,-0.041427118614128136] Loss: 22.84435625409783\n",
      "Iteracion: 9633 Gradiente: [0.00239991987303938,-0.04140509468627857] Loss: 22.844354532583655\n",
      "Iteracion: 9634 Gradiente: [0.0023986440019768906,-0.041383082467023394] Loss: 22.844352812899412\n",
      "Iteracion: 9635 Gradiente: [0.0023973688092752353,-0.04136108195013636] Loss: 22.844351095043166\n",
      "Iteracion: 9636 Gradiente: [0.0023960942943944017,-0.041339093129404605] Loss: 22.84434937901295\n",
      "Iteracion: 9637 Gradiente: [0.0023948204571392277,-0.04131711599860175] Loss: 22.84434766480685\n",
      "Iteracion: 9638 Gradiente: [0.002393547297114651,-0.041295150551515884] Loss: 22.844345952422895\n",
      "Iteracion: 9639 Gradiente: [0.0023922748139398207,-0.041273196781934966] Loss: 22.844344241859194\n",
      "Iteracion: 9640 Gradiente: [0.0023910030071495687,-0.041251254683658915] Loss: 22.84434253311378\n",
      "Iteracion: 9641 Gradiente: [0.002389731876575259,-0.041229324250471204] Loss: 22.844340826184713\n",
      "Iteracion: 9642 Gradiente: [0.0023884614216787745,-0.041207405476180035] Loss: 22.844339121070075\n",
      "Iteracion: 9643 Gradiente: [0.0023871916422469517,-0.04118549835457917] Loss: 22.844337417767964\n",
      "Iteracion: 9644 Gradiente: [0.00238592253791694,-0.041163602879473515] Loss: 22.844335716276394\n",
      "Iteracion: 9645 Gradiente: [0.0023846541082349405,-0.041141719044676846] Loss: 22.844334016593496\n",
      "Iteracion: 9646 Gradiente: [0.0023833863529839996,-0.04111984684399346] Loss: 22.844332318717306\n",
      "Iteracion: 9647 Gradiente: [0.002382119271670528,-0.041097986271244245] Loss: 22.844330622645938\n",
      "Iteracion: 9648 Gradiente: [0.0023808528639345165,-0.04107613732025115] Loss: 22.844328928377454\n",
      "Iteracion: 9649 Gradiente: [0.0023795871294548005,-0.04105429998483009] Loss: 22.844327235909937\n",
      "Iteracion: 9650 Gradiente: [0.002378322067815475,-0.0410324742588107] Loss: 22.84432554524149\n",
      "Iteracion: 9651 Gradiente: [0.00237705767888959,-0.041010660136007716] Loss: 22.84432385637017\n",
      "Iteracion: 9652 Gradiente: [0.002375793962003551,-0.04098885761027103] Loss: 22.8443221692941\n",
      "Iteracion: 9653 Gradiente: [0.00237453091703325,-0.040967066675421056] Loss: 22.844320484011327\n",
      "Iteracion: 9654 Gradiente: [0.0023732685435220446,-0.04094528732530082] Loss: 22.844318800520004\n",
      "Iteracion: 9655 Gradiente: [0.002372006841173402,-0.04092351955374909] Loss: 22.8443171188182\n",
      "Iteracion: 9656 Gradiente: [0.0023707458094908893,-0.04090176335461872] Loss: 22.844315438903983\n",
      "Iteracion: 9657 Gradiente: [0.0023694854482708178,-0.04088001872174848] Loss: 22.844313760775503\n",
      "Iteracion: 9658 Gradiente: [0.002368225757073598,-0.04085828564899477] Loss: 22.844312084430822\n",
      "Iteracion: 9659 Gradiente: [0.0023669667355325904,-0.04083656413020999] Loss: 22.844310409868065\n",
      "Iteracion: 9660 Gradiente: [0.002365708383376841,-0.040814854159248765] Loss: 22.84430873708534\n",
      "Iteracion: 9661 Gradiente: [0.0023644507001800243,-0.04079315572997556] Loss: 22.844307066080734\n",
      "Iteracion: 9662 Gradiente: [0.0023631936856001326,-0.04077146883625401] Loss: 22.844305396852373\n",
      "Iteracion: 9663 Gradiente: [0.0023619373392923157,-0.04074979347195059] Loss: 22.84430372939837\n",
      "Iteracion: 9664 Gradiente: [0.0023606816609098285,-0.040728129630935445] Loss: 22.844302063716828\n",
      "Iteracion: 9665 Gradiente: [0.002359426650112558,-0.04070647730708075] Loss: 22.84430039980588\n",
      "Iteracion: 9666 Gradiente: [0.002358172306504495,-0.040684836494267414] Loss: 22.844298737663635\n",
      "Iteracion: 9667 Gradiente: [0.0023569186296848936,-0.04066320718637826] Loss: 22.84429707728821\n",
      "Iteracion: 9668 Gradiente: [0.0023556656194396435,-0.0406415893772877] Loss: 22.84429541867773\n",
      "Iteracion: 9669 Gradiente: [0.0023544132752751543,-0.04061998306089123] Loss: 22.844293761830315\n",
      "Iteracion: 9670 Gradiente: [0.002353161596927104,-0.040598388231075214] Loss: 22.844292106744096\n",
      "Iteracion: 9671 Gradiente: [0.0023519105839577984,-0.040576804881734437] Loss: 22.844290453417184\n",
      "Iteracion: 9672 Gradiente: [0.002350660236060283,-0.040555233006767465] Loss: 22.844288801847746\n",
      "Iteracion: 9673 Gradiente: [0.002349410552897287,-0.04053367260006837] Loss: 22.844287152033882\n",
      "Iteracion: 9674 Gradiente: [0.002348161534120171,-0.04051212365554413] Loss: 22.84428550397373\n",
      "Iteracion: 9675 Gradiente: [0.0023469131794115585,-0.040490586167095916] Loss: 22.844283857665435\n",
      "Iteracion: 9676 Gradiente: [0.0023456654883943885,-0.040469060128636866] Loss: 22.844282213107135\n",
      "Iteracion: 9677 Gradiente: [0.002344418460580755,-0.04044754553408651] Loss: 22.84428057029695\n",
      "Iteracion: 9678 Gradiente: [0.0023431720958096017,-0.04042604237735136] Loss: 22.84427892923305\n",
      "Iteracion: 9679 Gradiente: [0.002341926393513442,-0.04040455065236112] Loss: 22.844277289913578\n",
      "Iteracion: 9680 Gradiente: [0.0023406813536107998,-0.04038307035302277] Loss: 22.844275652336645\n",
      "Iteracion: 9681 Gradiente: [0.0023394369755095567,-0.040361601473277275] Loss: 22.84427401650044\n",
      "Iteracion: 9682 Gradiente: [0.002338193259006971,-0.04034014400704441] Loss: 22.844272382403066\n",
      "Iteracion: 9683 Gradiente: [0.0023369502037364023,-0.04031869794825731] Loss: 22.844270750042742\n",
      "Iteracion: 9684 Gradiente: [0.0023357078092431038,-0.04029726329085683] Loss: 22.84426911941756\n",
      "Iteracion: 9685 Gradiente: [0.0023344660753413867,-0.040275840028772096] Loss: 22.844267490525695\n",
      "Iteracion: 9686 Gradiente: [0.0023332250014694487,-0.04025442815595864] Loss: 22.844265863365308\n",
      "Iteracion: 9687 Gradiente: [0.0023319845874529696,-0.04023302766635055] Loss: 22.844264237934567\n",
      "Iteracion: 9688 Gradiente: [0.0023307448328987825,-0.04021163855389768] Loss: 22.844262614231624\n",
      "Iteracion: 9689 Gradiente: [0.002329505737369194,-0.04019026081255746] Loss: 22.84426099225463\n",
      "Iteracion: 9690 Gradiente: [0.0023282673005866174,-0.04016889443628126] Loss: 22.84425937200178\n",
      "Iteracion: 9691 Gradiente: [0.0023270295222602043,-0.0401475394190219] Loss: 22.84425775347119\n",
      "Iteracion: 9692 Gradiente: [0.0023257924019607873,-0.04012619575474593] Loss: 22.84425613666111\n",
      "Iteracion: 9693 Gradiente: [0.0023245559393340423,-0.04010486343741858] Loss: 22.844254521569642\n",
      "Iteracion: 9694 Gradiente: [0.002323320134024698,-0.04008354246100734] Loss: 22.844252908194985\n",
      "Iteracion: 9695 Gradiente: [0.002322084985677482,-0.040062232819483866] Loss: 22.844251296535308\n",
      "Iteracion: 9696 Gradiente: [0.0023208504940868124,-0.04004093450681161] Loss: 22.844249686588782\n",
      "Iteracion: 9697 Gradiente: [0.0023196166587733085,-0.04001964751697891] Loss: 22.844248078353623\n",
      "Iteracion: 9698 Gradiente: [0.0023183834793466455,-0.03999837184396462] Loss: 22.84424647182796\n",
      "Iteracion: 9699 Gradiente: [0.0023171509554515525,-0.03997710748175732] Loss: 22.844244867010016\n",
      "Iteracion: 9700 Gradiente: [0.002315919086842655,-0.03995585442433338] Loss: 22.844243263897944\n",
      "Iteracion: 9701 Gradiente: [0.002314687873170366,-0.039934612665685994] Loss: 22.844241662489946\n",
      "Iteracion: 9702 Gradiente: [0.0023134573140396243,-0.03991338219981036] Loss: 22.844240062784216\n",
      "Iteracion: 9703 Gradiente: [0.002312227409155791,-0.03989216302069885] Loss: 22.844238464778943\n",
      "Iteracion: 9704 Gradiente: [0.002310998158152226,-0.039870955122354106] Loss: 22.844236868472315\n",
      "Iteracion: 9705 Gradiente: [0.0023097695605552343,-0.03984975849878344] Loss: 22.844235273862527\n",
      "Iteracion: 9706 Gradiente: [0.002308541616166811,-0.03982857314398546] Loss: 22.84423368094776\n",
      "Iteracion: 9707 Gradiente: [0.002307314324608948,-0.03980739905197173] Loss: 22.844232089726244\n",
      "Iteracion: 9708 Gradiente: [0.00230608768545153,-0.03978623621676055] Loss: 22.844230500196165\n",
      "Iteracion: 9709 Gradiente: [0.002304861698423603,-0.03976508463236049] Loss: 22.844228912355717\n",
      "Iteracion: 9710 Gradiente: [0.002303636363239055,-0.03974394429278713] Loss: 22.844227326203107\n",
      "Iteracion: 9711 Gradiente: [0.0023024116794317707,-0.03972281519207108] Loss: 22.844225741736548\n",
      "Iteracion: 9712 Gradiente: [0.002301187646688163,-0.039701697324233835] Loss: 22.844224158954248\n",
      "Iteracion: 9713 Gradiente: [0.0022999642646700145,-0.039680590683304] Loss: 22.844222577854403\n",
      "Iteracion: 9714 Gradiente: [0.002298741533199215,-0.0396594952633027] Loss: 22.844220998435233\n",
      "Iteracion: 9715 Gradiente: [0.002297519451609749,-0.039638411058284716] Loss: 22.84421942069495\n",
      "Iteracion: 9716 Gradiente: [0.002296298019722561,-0.03961733806227509] Loss: 22.844217844631782\n",
      "Iteracion: 9717 Gradiente: [0.002295077237184273,-0.03959627626931758] Loss: 22.84421627024393\n",
      "Iteracion: 9718 Gradiente: [0.0022938571036983527,-0.039575225673454284] Loss: 22.844214697529612\n",
      "Iteracion: 9719 Gradiente: [0.002292637618932266,-0.03955418626872896] Loss: 22.844213126487066\n",
      "Iteracion: 9720 Gradiente: [0.002291418782393369,-0.039533158049203126] Loss: 22.844211557114516\n",
      "Iteracion: 9721 Gradiente: [0.0022902005937036544,-0.039512141008930496] Loss: 22.84420998941015\n",
      "Iteracion: 9722 Gradiente: [0.0022889830528763848,-0.03949113514194747] Loss: 22.844208423372226\n",
      "Iteracion: 9723 Gradiente: [0.0022877661592133336,-0.03947014044233595] Loss: 22.844206858998984\n",
      "Iteracion: 9724 Gradiente: [0.0022865499125051276,-0.0394491569041494] Loss: 22.844205296288624\n",
      "Iteracion: 9725 Gradiente: [0.002285334312348179,-0.039428184521458104] Loss: 22.84420373523938\n",
      "Iteracion: 9726 Gradiente: [0.0022841193584696383,-0.03940722328832832] Loss: 22.844202175849528\n",
      "Iteracion: 9727 Gradiente: [0.002282905050477287,-0.039386273198834115] Loss: 22.844200618117256\n",
      "Iteracion: 9728 Gradiente: [0.0022816913880774337,-0.03936533424704957] Loss: 22.84419906204083\n",
      "Iteracion: 9729 Gradiente: [0.0022804783709451234,-0.03934440642705101] Loss: 22.844197507618464\n",
      "Iteracion: 9730 Gradiente: [0.0022792659985744496,-0.039323489732931566] Loss: 22.844195954848423\n",
      "Iteracion: 9731 Gradiente: [0.0022780542708394098,-0.0393025841587626] Loss: 22.844194403728938\n",
      "Iteracion: 9732 Gradiente: [0.0022768431873335733,-0.03928168969863558] Loss: 22.844192854258274\n",
      "Iteracion: 9733 Gradiente: [0.002275632747569034,-0.03926080634665172] Loss: 22.84419130643466\n",
      "Iteracion: 9734 Gradiente: [0.0022744229513591563,-0.039239934096896904] Loss: 22.844189760256345\n",
      "Iteracion: 9735 Gradiente: [0.0022732137983211944,-0.039219072943469276] Loss: 22.84418821572159\n",
      "Iteracion: 9736 Gradiente: [0.0022720052880875604,-0.03919822288047226] Loss: 22.84418667282864\n",
      "Iteracion: 9737 Gradiente: [0.0022707974203039307,-0.0391773839020105] Loss: 22.84418513157576\n",
      "Iteracion: 9738 Gradiente: [0.002269590194708826,-0.039156556002187176] Loss: 22.84418359196118\n",
      "Iteracion: 9739 Gradiente: [0.0022683836108910783,-0.03913573917511488] Loss: 22.844182053983197\n",
      "Iteracion: 9740 Gradiente: [0.002267177668533312,-0.03911493341490733] Loss: 22.844180517640048\n",
      "Iteracion: 9741 Gradiente: [0.0022659723673389936,-0.039094138715677455] Loss: 22.84417898292999\n",
      "Iteracion: 9742 Gradiente: [0.002264767706861903,-0.03907335507155179] Loss: 22.8441774498513\n",
      "Iteracion: 9743 Gradiente: [0.0022635636867988753,-0.03905258247664989] Loss: 22.844175918402243\n",
      "Iteracion: 9744 Gradiente: [0.0022623603069073777,-0.0390318209250933] Loss: 22.844174388581084\n",
      "Iteracion: 9745 Gradiente: [0.0022611575667374002,-0.039011070411016154] Loss: 22.844172860386088\n",
      "Iteracion: 9746 Gradiente: [0.0022599554659526195,-0.03899033092855101] Loss: 22.844171333815535\n",
      "Iteracion: 9747 Gradiente: [0.0022587540043218723,-0.038969602471827375] Loss: 22.844169808867697\n",
      "Iteracion: 9748 Gradiente: [0.0022575531812558816,-0.03894888503499653] Loss: 22.84416828554084\n",
      "Iteracion: 9749 Gradiente: [0.0022563529966646454,-0.038928178612187024] Loss: 22.84416676383325\n",
      "Iteracion: 9750 Gradiente: [0.0022551534502118407,-0.03890748319754126] Loss: 22.8441652437432\n",
      "Iteracion: 9751 Gradiente: [0.0022539545414247187,-0.038886798785216455] Loss: 22.844163725268974\n",
      "Iteracion: 9752 Gradiente: [0.0022527562699622195,-0.038866125369363874] Loss: 22.844162208408843\n",
      "Iteracion: 9753 Gradiente: [0.0022515586355685477,-0.038845462944131624] Loss: 22.84416069316113\n",
      "Iteracion: 9754 Gradiente: [0.002250361637917801,-0.03882481150367726] Loss: 22.844159179524066\n",
      "Iteracion: 9755 Gradiente: [0.0022491652766158648,-0.03880417104216226] Loss: 22.84415766749599\n",
      "Iteracion: 9756 Gradiente: [0.0022479695512724143,-0.03878354155375317] Loss: 22.844156157075147\n",
      "Iteracion: 9757 Gradiente: [0.002246774461697972,-0.03876292303260958] Loss: 22.844154648259867\n",
      "Iteracion: 9758 Gradiente: [0.002245580007490844,-0.038742315472902904] Loss: 22.844153141048412\n",
      "Iteracion: 9759 Gradiente: [0.002244386188155545,-0.038721718868814745] Loss: 22.844151635439093\n",
      "Iteracion: 9760 Gradiente: [0.002243193003565125,-0.038701133214510366] Loss: 22.844150131430204\n",
      "Iteracion: 9761 Gradiente: [0.0022420004532780996,-0.0386805585041742] Loss: 22.844148629020047\n",
      "Iteracion: 9762 Gradiente: [0.0022408085370490957,-0.03865999473198111] Loss: 22.844147128206927\n",
      "Iteracion: 9763 Gradiente: [0.002239617254482103,-0.038639441892120395] Loss: 22.844145628989125\n",
      "Iteracion: 9764 Gradiente: [0.0022384266051498495,-0.03861889997878573] Loss: 22.844144131364967\n",
      "Iteracion: 9765 Gradiente: [0.0022372365888296978,-0.0385983689861605] Loss: 22.844142635332755\n",
      "Iteracion: 9766 Gradiente: [0.0022360472051663766,-0.038577848908439925] Loss: 22.844141140890784\n",
      "Iteracion: 9767 Gradiente: [0.0022348584538841956,-0.03855733973982041] Loss: 22.844139648037377\n",
      "Iteracion: 9768 Gradiente: [0.002233670334626936,-0.03853684147450013] Loss: 22.844138156770857\n",
      "Iteracion: 9769 Gradiente: [0.0022324828468147945,-0.0385163541066986] Loss: 22.844136667089515\n",
      "Iteracion: 9770 Gradiente: [0.002231295990394718,-0.03849587763060664] Loss: 22.844135178991657\n",
      "Iteracion: 9771 Gradiente: [0.002230109764949854,-0.038475412040436176] Loss: 22.844133692475612\n",
      "Iteracion: 9772 Gradiente: [0.0022289241702139863,-0.03845495733039866] Loss: 22.844132207539726\n",
      "Iteracion: 9773 Gradiente: [0.002227739205747525,-0.03843451349471347] Loss: 22.844130724182275\n",
      "Iteracion: 9774 Gradiente: [0.0022265548711544623,-0.038414080527604234] Loss: 22.844129242401614\n",
      "Iteracion: 9775 Gradiente: [0.0022253711662671093,-0.03839365842328254] Loss: 22.844127762196063\n",
      "Iteracion: 9776 Gradiente: [0.002224188090628824,-0.03837324717598196] Loss: 22.844126283563913\n",
      "Iteracion: 9777 Gradiente: [0.0022230056439610734,-0.038352846779926016] Loss: 22.844124806503537\n",
      "Iteracion: 9778 Gradiente: [0.0022218238259256395,-0.03833245722934737] Loss: 22.844123331013236\n",
      "Iteracion: 9779 Gradiente: [0.0022206426361274603,-0.038312078518481486] Loss: 22.844121857091356\n",
      "Iteracion: 9780 Gradiente: [0.002219462074284214,-0.03829171064156398] Loss: 22.84412038473621\n",
      "Iteracion: 9781 Gradiente: [0.0022182821401846315,-0.03827135359282785] Loss: 22.84411891394616\n",
      "Iteracion: 9782 Gradiente: [0.0022171028332347003,-0.038251007366532974] Loss: 22.844117444719522\n",
      "Iteracion: 9783 Gradiente: [0.0022159241533159955,-0.03823067195691105] Loss: 22.844115977054663\n",
      "Iteracion: 9784 Gradiente: [0.002214746100071352,-0.038210347358214845] Loss: 22.84411451094987\n",
      "Iteracion: 9785 Gradiente: [0.002213568673109497,-0.0381900335646994] Loss: 22.84411304640353\n",
      "Iteracion: 9786 Gradiente: [0.0022123918720817906,-0.038169730570620786] Loss: 22.844111583413966\n",
      "Iteracion: 9787 Gradiente: [0.0022112156966291726,-0.03814943837024032] Loss: 22.84411012197953\n",
      "Iteracion: 9788 Gradiente: [0.0022100401465583747,-0.038129156957811126] Loss: 22.844108662098577\n",
      "Iteracion: 9789 Gradiente: [0.002208865221267805,-0.03810888632761286] Loss: 22.84410720376943\n",
      "Iteracion: 9790 Gradiente: [0.0022076909207991474,-0.03808862647389214] Loss: 22.84410574699047\n",
      "Iteracion: 9791 Gradiente: [0.0022065172445356516,-0.03806837739093823] Loss: 22.84410429176002\n",
      "Iteracion: 9792 Gradiente: [0.002205344192277418,-0.03804813907301572] Loss: 22.844102838076466\n",
      "Iteracion: 9793 Gradiente: [0.0022041717635081187,-0.03802791151441198] Loss: 22.84410138593814\n",
      "Iteracion: 9794 Gradiente: [0.002202999958169016,-0.03800769470939116] Loss: 22.844099935343404\n",
      "Iteracion: 9795 Gradiente: [0.002201828775735256,-0.03798748865224913] Loss: 22.844098486290612\n",
      "Iteracion: 9796 Gradiente: [0.0022006582159832533,-0.03796729333726437] Loss: 22.844097038778145\n",
      "Iteracion: 9797 Gradiente: [0.002199488278617423,-0.037947108758723876] Loss: 22.844095592804365\n",
      "Iteracion: 9798 Gradiente: [0.002198318963072173,-0.037926934910933595] Loss: 22.844094148367603\n",
      "Iteracion: 9799 Gradiente: [0.0021971502692338164,-0.037906771788176266] Loss: 22.844092705466245\n",
      "Iteracion: 9800 Gradiente: [0.0021959821967158176,-0.0378866193847531] Loss: 22.84409126409866\n",
      "Iteracion: 9801 Gradiente: [0.002194814745209328,-0.03786647769496625] Loss: 22.844089824263236\n",
      "Iteracion: 9802 Gradiente: [0.0021936479143022324,-0.037846346713121905] Loss: 22.844088385958322\n",
      "Iteracion: 9803 Gradiente: [0.002192481703749157,-0.03782622643352435] Loss: 22.844086949182287\n",
      "Iteracion: 9804 Gradiente: [0.002191316113177777,-0.03780611685048451] Loss: 22.844085513933525\n",
      "Iteracion: 9805 Gradiente: [0.002190151142251769,-0.03778601795831828] Loss: 22.844084080210393\n",
      "Iteracion: 9806 Gradiente: [0.0021889867907238644,-0.03776592975133598] Loss: 22.844082648011273\n",
      "Iteracion: 9807 Gradiente: [0.002187823058106157,-0.0377458522238675] Loss: 22.844081217334562\n",
      "Iteracion: 9808 Gradiente: [0.0021866599442072737,-0.03772578537022516] Loss: 22.844079788178615\n",
      "Iteracion: 9809 Gradiente: [0.0021854974486889963,-0.03770572918473659] Loss: 22.84407836054184\n",
      "Iteracion: 9810 Gradiente: [0.002184335571109841,-0.037685683661736864] Loss: 22.844076934422613\n",
      "Iteracion: 9811 Gradiente: [0.0021831743112376975,-0.03766564879555053] Loss: 22.844075509819323\n",
      "Iteracion: 9812 Gradiente: [0.0021820136687968747,-0.03764562458051112] Loss: 22.844074086730352\n",
      "Iteracion: 9813 Gradiente: [0.002180853643324099,-0.037625611010962366] Loss: 22.844072665154098\n",
      "Iteracion: 9814 Gradiente: [0.002179694234663998,-0.03760560808123519] Loss: 22.844071245088937\n",
      "Iteracion: 9815 Gradiente: [0.0021785354423116132,-0.03758561578568352] Loss: 22.84406982653329\n",
      "Iteracion: 9816 Gradiente: [0.002177377266020623,-0.03756563411864941] Loss: 22.844068409485534\n",
      "Iteracion: 9817 Gradiente: [0.0021762197054007023,-0.037545663074485584] Loss: 22.844066993944068\n",
      "Iteracion: 9818 Gradiente: [0.0021750627602339515,-0.03752570264753814] Loss: 22.844065579907294\n",
      "Iteracion: 9819 Gradiente: [0.0021739064301461515,-0.037505752832165486] Loss: 22.84406416737361\n",
      "Iteracion: 9820 Gradiente: [0.0021727507147081345,-0.03748581362273302] Loss: 22.84406275634142\n",
      "Iteracion: 9821 Gradiente: [0.0021715956137455805,-0.03746588501359274] Loss: 22.844061346809134\n",
      "Iteracion: 9822 Gradiente: [0.002170441126827427,-0.03744596699911469] Loss: 22.844059938775125\n",
      "Iteracion: 9823 Gradiente: [0.0021692872537026156,-0.03742605957366341] Loss: 22.84405853223785\n",
      "Iteracion: 9824 Gradiente: [0.0021681339940774554,-0.03740616273160479] Loss: 22.844057127195686\n",
      "Iteracion: 9825 Gradiente: [0.002166981347427092,-0.03738627646732861] Loss: 22.844055723647063\n",
      "Iteracion: 9826 Gradiente: [0.002165829313607522,-0.03736640077519683] Loss: 22.844054321590363\n",
      "Iteracion: 9827 Gradiente: [0.0021646778922786323,-0.03734653564959108] Loss: 22.844052921024044\n",
      "Iteracion: 9828 Gradiente: [0.002163527083046309,-0.03732668108489807] Loss: 22.844051521946483\n",
      "Iteracion: 9829 Gradiente: [0.002162376885676546,-0.03730683707549775] Loss: 22.844050124356112\n",
      "Iteracion: 9830 Gradiente: [0.002161227299670069,-0.03728700361578845] Loss: 22.844048728251344\n",
      "Iteracion: 9831 Gradiente: [0.0021600783250543525,-0.03726718070014042] Loss: 22.844047333630613\n",
      "Iteracion: 9832 Gradiente: [0.0021589299610004294,-0.03724736832298016] Loss: 22.844045940492336\n",
      "Iteracion: 9833 Gradiente: [0.0021577822075405113,-0.03722756647868266] Loss: 22.844044548834937\n",
      "Iteracion: 9834 Gradiente: [0.0021566350642918527,-0.03720777516165133] Loss: 22.844043158656845\n",
      "Iteracion: 9835 Gradiente: [0.0021554885309304456,-0.037187994366289584] Loss: 22.844041769956466\n",
      "Iteracion: 9836 Gradiente: [0.0021543426070271226,-0.03716822408700947] Loss: 22.844040382732253\n",
      "Iteracion: 9837 Gradiente: [0.002153197292436933,-0.03714846431821073] Loss: 22.844038996982633\n",
      "Iteracion: 9838 Gradiente: [0.0021520525866587075,-0.03712871505431726] Loss: 22.84403761270603\n",
      "Iteracion: 9839 Gradiente: [0.002150908489484967,-0.037108976289735196] Loss: 22.84403622990089\n",
      "Iteracion: 9840 Gradiente: [0.0021497650004761227,-0.03708924801888986] Loss: 22.84403484856563\n",
      "Iteracion: 9841 Gradiente: [0.002148622119431328,-0.03706953023619602] Loss: 22.84403346869872\n",
      "Iteracion: 9842 Gradiente: [0.0021474798459668893,-0.037049822936081375] Loss: 22.84403209029857\n",
      "Iteracion: 9843 Gradiente: [0.0021463381797768005,-0.03703012611297254] Loss: 22.844030713363615\n",
      "Iteracion: 9844 Gradiente: [0.0021451971204119974,-0.03701043976130644] Loss: 22.844029337892326\n",
      "Iteracion: 9845 Gradiente: [0.00214405666786206,-0.03699076387549738] Loss: 22.84402796388314\n",
      "Iteracion: 9846 Gradiente: [0.002142916821587922,-0.03697109844999531] Loss: 22.844026591334476\n",
      "Iteracion: 9847 Gradiente: [0.0021417775811727324,-0.036951443479243194] Loss: 22.844025220244827\n",
      "Iteracion: 9848 Gradiente: [0.002140638946506594,-0.03693179895767204] Loss: 22.8440238506126\n",
      "Iteracion: 9849 Gradiente: [0.0021395009170646516,-0.03691216487973759] Loss: 22.84402248243628\n",
      "Iteracion: 9850 Gradiente: [0.0021383634927379567,-0.036892541239875364] Loss: 22.844021115714302\n",
      "Iteracion: 9851 Gradiente: [0.002137226673074603,-0.03687292803254382] Loss: 22.844019750445103\n",
      "Iteracion: 9852 Gradiente: [0.002136090457776163,-0.036853325252195764] Loss: 22.844018386627162\n",
      "Iteracion: 9853 Gradiente: [0.0021349548464750494,-0.036833732893290734] Loss: 22.844017024258925\n",
      "Iteracion: 9854 Gradiente: [0.0021338198389751523,-0.036814150950279984] Loss: 22.844015663338872\n",
      "Iteracion: 9855 Gradiente: [0.0021326854348425665,-0.03679457941763324] Loss: 22.84401430386544\n",
      "Iteracion: 9856 Gradiente: [0.0021315516337153894,-0.03677501828981893] Loss: 22.844012945837097\n",
      "Iteracion: 9857 Gradiente: [0.0021304184355282513,-0.03675546756129127] Loss: 22.84401158925231\n",
      "Iteracion: 9858 Gradiente: [0.0021292858397572445,-0.03673592722653123] Loss: 22.844010234109547\n",
      "Iteracion: 9859 Gradiente: [0.0021281538460290977,-0.03671639728001731] Loss: 22.84400888040727\n",
      "Iteracion: 9860 Gradiente: [0.0021270224541382276,-0.036696877716221606] Loss: 22.844007528143944\n",
      "Iteracion: 9861 Gradiente: [0.00212589166368294,-0.03667736852962733] Loss: 22.844006177318057\n",
      "Iteracion: 9862 Gradiente: [0.002124761474406493,-0.03665786971471476] Loss: 22.84400482792806\n",
      "Iteracion: 9863 Gradiente: [0.0021236318860151943,-0.036638381265968786] Loss: 22.84400347997244\n",
      "Iteracion: 9864 Gradiente: [0.002122502898032508,-0.036618903177886845] Loss: 22.844002133449667\n",
      "Iteracion: 9865 Gradiente: [0.0021213745103144297,-0.03659943544495062] Loss: 22.84400078835822\n",
      "Iteracion: 9866 Gradiente: [0.002120246722563479,-0.03657997806165459] Loss: 22.843999444696582\n",
      "Iteracion: 9867 Gradiente: [0.0021191195343002772,-0.03656053102250425] Loss: 22.843998102463214\n",
      "Iteracion: 9868 Gradiente: [0.0021179929453182924,-0.03654109432199573] Loss: 22.843996761656612\n",
      "Iteracion: 9869 Gradiente: [0.002116866955312465,-0.03652166795462956] Loss: 22.843995422275274\n",
      "Iteracion: 9870 Gradiente: [0.0021157415638640487,-0.036502251914921166] Loss: 22.84399408431766\n",
      "Iteracion: 9871 Gradiente: [0.002114616770651878,-0.03648284619737853] Loss: 22.843992747782274\n",
      "Iteracion: 9872 Gradiente: [0.002113492575504476,-0.03646345079650608] Loss: 22.843991412667595\n",
      "Iteracion: 9873 Gradiente: [0.002112368978012569,-0.036444065706824465] Loss: 22.843990078972105\n",
      "Iteracion: 9874 Gradiente: [0.002111245977780148,-0.03642469092285528] Loss: 22.8439887466943\n",
      "Iteracion: 9875 Gradiente: [0.0021101235746044723,-0.036405326439114205] Loss: 22.843987415832704\n",
      "Iteracion: 9876 Gradiente: [0.002109001768193745,-0.03638597225012366] Loss: 22.84398608638577\n",
      "Iteracion: 9877 Gradiente: [0.0021078805580723764,-0.036366628350419934] Loss: 22.843984758352004\n",
      "Iteracion: 9878 Gradiente: [0.0021067599440669936,-0.03634729473452344] Loss: 22.843983431729917\n",
      "Iteracion: 9879 Gradiente: [0.002105639925844116,-0.036327971396969735] Loss: 22.843982106518013\n",
      "Iteracion: 9880 Gradiente: [0.002104520503073104,-0.03630865833229287] Loss: 22.84398078271476\n",
      "Iteracion: 9881 Gradiente: [0.002103401675395844,-0.036289355535036684] Loss: 22.84397946031869\n",
      "Iteracion: 9882 Gradiente: [0.0021022834424153794,-0.03627006299974696] Loss: 22.84397813932829\n",
      "Iteracion: 9883 Gradiente: [0.0021011658039829704,-0.036250780720956284] Loss: 22.84397681974209\n",
      "Iteracion: 9884 Gradiente: [0.00210004875969787,-0.03623150869321966] Loss: 22.843975501558585\n",
      "Iteracion: 9885 Gradiente: [0.0020989323093421794,-0.03621224691107917] Loss: 22.843974184776265\n",
      "Iteracion: 9886 Gradiente: [0.0020978164525359945,-0.03619299536909277] Loss: 22.843972869393657\n",
      "Iteracion: 9887 Gradiente: [0.0020967011888965697,-0.036173754061820655] Loss: 22.843971555409286\n",
      "Iteracion: 9888 Gradiente: [0.002095586518206005,-0.0361545229838157] Loss: 22.843970242821648\n",
      "Iteracion: 9889 Gradiente: [0.002094472440070187,-0.036135302129644366] Loss: 22.84396893162926\n",
      "Iteracion: 9890 Gradiente: [0.002093358954183107,-0.0361160914938696] Loss: 22.843967621830636\n",
      "Iteracion: 9891 Gradiente: [0.002092246060367605,-0.03609689107105301] Loss: 22.843966313424303\n",
      "Iteracion: 9892 Gradiente: [0.0020911337581291417,-0.036077700855775656] Loss: 22.843965006408787\n",
      "Iteracion: 9893 Gradiente: [0.0020900220472109747,-0.03605852084260531] Loss: 22.843963700782602\n",
      "Iteracion: 9894 Gradiente: [0.0020889109273980466,-0.036039351026114254] Loss: 22.843962396544253\n",
      "Iteracion: 9895 Gradiente: [0.0020878003982071885,-0.036020191400891] Loss: 22.843961093692297\n",
      "Iteracion: 9896 Gradiente: [0.002086690459444185,-0.036001041961510556] Loss: 22.84395979222523\n",
      "Iteracion: 9897 Gradiente: [0.0020855811107461856,-0.035981902702561304] Loss: 22.84395849214162\n",
      "Iteracion: 9898 Gradiente: [0.0020844723517853938,-0.03596277361862998] Loss: 22.843957193439962\n",
      "Iteracion: 9899 Gradiente: [0.0020833641823476984,-0.03594365470430283] Loss: 22.843955896118782\n",
      "Iteracion: 9900 Gradiente: [0.002082256602023828,-0.035924545954177725] Loss: 22.843954600176634\n",
      "Iteracion: 9901 Gradiente: [0.002081149610477458,-0.03590544736285395] Loss: 22.84395330561204\n",
      "Iteracion: 9902 Gradiente: [0.002080043207463215,-0.035886358924927704] Loss: 22.84395201242355\n",
      "Iteracion: 9903 Gradiente: [0.0020789373926845657,-0.03586728063499791] Loss: 22.8439507206097\n",
      "Iteracion: 9904 Gradiente: [0.0020778321656990785,-0.03584821248767831] Loss: 22.843949430169005\n",
      "Iteracion: 9905 Gradiente: [0.0020767275263750664,-0.035829154477565336] Loss: 22.843948141100018\n",
      "Iteracion: 9906 Gradiente: [0.0020756234742378863,-0.035810106599279835] Loss: 22.84394685340129\n",
      "Iteracion: 9907 Gradiente: [0.0020745200090810084,-0.03579106884742913] Loss: 22.843945567071348\n",
      "Iteracion: 9908 Gradiente: [0.0020734171306031612,-0.035772041216629684] Loss: 22.84394428210878\n",
      "Iteracion: 9909 Gradiente: [0.002072314838373283,-0.03575302370150624] Loss: 22.843942998512073\n",
      "Iteracion: 9910 Gradiente: [0.0020712131322018954,-0.03573401629667619] Loss: 22.843941716279808\n",
      "Iteracion: 9911 Gradiente: [0.0020701120117981495,-0.035715018996761334] Loss: 22.843940435410534\n",
      "Iteracion: 9912 Gradiente: [0.0020690114766608756,-0.03569603179640073] Loss: 22.84393915590279\n",
      "Iteracion: 9913 Gradiente: [0.0020679115266877563,-0.03567705469021523] Loss: 22.84393787775514\n",
      "Iteracion: 9914 Gradiente: [0.002066812161426886,-0.03565808767284378] Loss: 22.843936600966117\n",
      "Iteracion: 9915 Gradiente: [0.002065713380674576,-0.035639130738917876] Loss: 22.843935325534314\n",
      "Iteracion: 9916 Gradiente: [0.0020646151840281847,-0.035620183883083814] Loss: 22.843934051458252\n",
      "Iteracion: 9917 Gradiente: [0.0020635175712518123,-0.03560124709997581] Loss: 22.843932778736523\n",
      "Iteracion: 9918 Gradiente: [0.0020624205419475555,-0.03558232038424691] Loss: 22.843931507367667\n",
      "Iteracion: 9919 Gradiente: [0.002061324095831196,-0.03556340373054319] Loss: 22.843930237350243\n",
      "Iteracion: 9920 Gradiente: [0.0020602282327018885,-0.03554449713350714] Loss: 22.843928968682818\n",
      "Iteracion: 9921 Gradiente: [0.002059132952169307,-0.03552560058779856] Loss: 22.843927701363963\n",
      "Iteracion: 9922 Gradiente: [0.002058038253854496,-0.03550671408807915] Loss: 22.843926435392248\n",
      "Iteracion: 9923 Gradiente: [0.002056944137550924,-0.035487837628999465] Loss: 22.843925170766237\n",
      "Iteracion: 9924 Gradiente: [0.0020558506029127936,-0.035468971205224756] Loss: 22.84392390748447\n",
      "Iteracion: 9925 Gradiente: [0.0020547576496549405,-0.03545011481141908] Loss: 22.84392264554558\n",
      "Iteracion: 9926 Gradiente: [0.0020536652774078828,-0.03543126844225265] Loss: 22.843921384948075\n",
      "Iteracion: 9927 Gradiente: [0.002052573485905403,-0.03541243209239475] Loss: 22.843920125690577\n",
      "Iteracion: 9928 Gradiente: [0.0020514822748168627,-0.035393605756519265] Loss: 22.843918867771624\n",
      "Iteracion: 9929 Gradiente: [0.00205039164391773,-0.035374789429298066] Loss: 22.843917611189816\n",
      "Iteracion: 9930 Gradiente: [0.002049301592703993,-0.035355983105421276] Loss: 22.843916355943744\n",
      "Iteracion: 9931 Gradiente: [0.0020482121210676496,-0.03533718677955946] Loss: 22.84391510203196\n",
      "Iteracion: 9932 Gradiente: [0.0020471232286922714,-0.03531840044639741] Loss: 22.84391384945307\n",
      "Iteracion: 9933 Gradiente: [0.002046034915118374,-0.035299624100631394] Loss: 22.843912598205627\n",
      "Iteracion: 9934 Gradiente: [0.0020449471801687953,-0.03528085773694549] Loss: 22.84391134828824\n",
      "Iteracion: 9935 Gradiente: [0.002043860023446579,-0.035262101350036684] Loss: 22.843910099699492\n",
      "Iteracion: 9936 Gradiente: [0.002042773444728141,-0.03524335493459615] Loss: 22.84390885243797\n",
      "Iteracion: 9937 Gradiente: [0.002041687443707474,-0.0352246184853243] Loss: 22.84390760650226\n",
      "Iteracion: 9938 Gradiente: [0.002040602019935515,-0.03520589199692867] Loss: 22.843906361890948\n",
      "Iteracion: 9939 Gradiente: [0.00203951717327584,-0.03518717546410611] Loss: 22.84390511860263\n",
      "Iteracion: 9940 Gradiente: [0.0020384329033125443,-0.03516846888156901] Loss: 22.84390387663591\n",
      "Iteracion: 9941 Gradiente: [0.002037349209786991,-0.035149772244024886] Loss: 22.843902635989362\n",
      "Iteracion: 9942 Gradiente: [0.002036266092467069,-0.03513108554618232] Loss: 22.843901396661604\n",
      "Iteracion: 9943 Gradiente: [0.0020351835509122415,-0.03511240878276448] Loss: 22.843900158651216\n",
      "Iteracion: 9944 Gradiente: [0.002034101584850608,-0.035093741948490256] Loss: 22.843898921956832\n",
      "Iteracion: 9945 Gradiente: [0.0020330201939638454,-0.03507508503807877] Loss: 22.843897686576998\n",
      "Iteracion: 9946 Gradiente: [0.0020319393780245795,-0.03505643804625187] Loss: 22.84389645251036\n",
      "Iteracion: 9947 Gradiente: [0.0020308591366775394,-0.03503780096773852] Loss: 22.843895219755492\n",
      "Iteracion: 9948 Gradiente: [0.0020297794696887196,-0.03501917379726282] Loss: 22.84389398831104\n",
      "Iteracion: 9949 Gradiente: [0.0020287003766479,-0.03500055652956498] Loss: 22.843892758175578\n",
      "Iteracion: 9950 Gradiente: [0.0020276218572178096,-0.034981949159382] Loss: 22.843891529347722\n",
      "Iteracion: 9951 Gradiente: [0.002026543911222234,-0.03496335168144545] Loss: 22.843890301826097\n",
      "Iteracion: 9952 Gradiente: [0.002025466538323902,-0.03494476409049637] Loss: 22.843889075609276\n",
      "Iteracion: 9953 Gradiente: [0.0020243897381144885,-0.03492618638128633] Loss: 22.84388785069593\n",
      "Iteracion: 9954 Gradiente: [0.0020233135104329373,-0.03490761854855163] Loss: 22.843886627084608\n",
      "Iteracion: 9955 Gradiente: [0.0020222378547922895,-0.0348890605870551] Loss: 22.843885404773975\n",
      "Iteracion: 9956 Gradiente: [0.00202116277108928,-0.034870512491534576] Loss: 22.843884183762633\n",
      "Iteracion: 9957 Gradiente: [0.002020088258890951,-0.03485197425675359] Loss: 22.843882964049204\n",
      "Iteracion: 9958 Gradiente: [0.0020190143179888765,-0.03483344587746468] Loss: 22.843881745632277\n",
      "Iteracion: 9959 Gradiente: [0.002017940947976626,-0.03481492734843409] Loss: 22.843880528510528\n",
      "Iteracion: 9960 Gradiente: [0.002016868148650512,-0.03479641866441862] Loss: 22.843879312682542\n",
      "Iteracion: 9961 Gradiente: [0.0020157959195813645,-0.034777919820191745] Loss: 22.84387809814697\n",
      "Iteracion: 9962 Gradiente: [0.002014724260664025,-0.034759430810511326] Loss: 22.843876884902418\n",
      "Iteracion: 9963 Gradiente: [0.0020136531713622692,-0.03474095163016303] Loss: 22.84387567294751\n",
      "Iteracion: 9964 Gradiente: [0.0020125826515576743,-0.03472248227390935] Loss: 22.843874462280898\n",
      "Iteracion: 9965 Gradiente: [0.0020115127008021243,-0.03470402273653607] Loss: 22.843873252901187\n",
      "Iteracion: 9966 Gradiente: [0.00201044331889193,-0.03468557301281798] Loss: 22.84387204480702\n",
      "Iteracion: 9967 Gradiente: [0.0020093745055457172,-0.03466713309753663] Loss: 22.843870837997038\n",
      "Iteracion: 9968 Gradiente: [0.0020083062604413725,-0.034648702985479334] Loss: 22.84386963246987\n",
      "Iteracion: 9969 Gradiente: [0.0020072385831809924,-0.0346302826714389] Loss: 22.843868428224155\n",
      "Iteracion: 9970 Gradiente: [0.0020061714735959413,-0.034611872150199355] Loss: 22.84386722525852\n",
      "Iteracion: 9971 Gradiente: [0.0020051049312039973,-0.03459347141656413] Loss: 22.84386602357161\n",
      "Iteracion: 9972 Gradiente: [0.0020040389558857894,-0.03457508046531928] Loss: 22.843864823162075\n",
      "Iteracion: 9973 Gradiente: [0.0020029735472216243,-0.034556699291272724] Loss: 22.84386362402854\n",
      "Iteracion: 9974 Gradiente: [0.0020019087049756006,-0.03453832788922225] Loss: 22.843862426169668\n",
      "Iteracion: 9975 Gradiente: [0.002000844428842659,-0.03451996625397153] Loss: 22.843861229584103\n",
      "Iteracion: 9976 Gradiente: [0.0019997807185433204,-0.034501614380327554] Loss: 22.843860034270463\n",
      "Iteracion: 9977 Gradiente: [0.0019987175737185225,-0.034483272263105984] Loss: 22.843858840227423\n",
      "Iteracion: 9978 Gradiente: [0.0019976549941304713,-0.034464939897114515] Loss: 22.843857647453625\n",
      "Iteracion: 9979 Gradiente: [0.0019965929794655795,-0.034446617277168204] Loss: 22.843856455947723\n",
      "Iteracion: 9980 Gradiente: [0.0019955315293818405,-0.034428304398091095] Loss: 22.843855265708367\n",
      "Iteracion: 9981 Gradiente: [0.001994470643584615,-0.034410001254702983] Loss: 22.8438540767342\n",
      "Iteracion: 9982 Gradiente: [0.0019934103217138953,-0.034391707841832306] Loss: 22.843852889023914\n",
      "Iteracion: 9983 Gradiente: [0.0019923505636294673,-0.03437342415429647] Loss: 22.84385170257611\n",
      "Iteracion: 9984 Gradiente: [0.0019912913689059527,-0.034355150186932214] Loss: 22.843850517389498\n",
      "Iteracion: 9985 Gradiente: [0.001990232737303662,-0.03433688593456831] Loss: 22.843849333462693\n",
      "Iteracion: 9986 Gradiente: [0.0019891746684436384,-0.03431863139204647] Loss: 22.84384815079438\n",
      "Iteracion: 9987 Gradiente: [0.001988117162117457,-0.03430038655419653] Loss: 22.843846969383232\n",
      "Iteracion: 9988 Gradiente: [0.001987060218024794,-0.03428215141586032] Loss: 22.843845789227878\n",
      "Iteracion: 9989 Gradiente: [0.0019860038359013287,-0.03426392597188131] Loss: 22.843844610327036\n",
      "Iteracion: 9990 Gradiente: [0.001984948015321682,-0.03424571021711008] Loss: 22.843843432679307\n",
      "Iteracion: 9991 Gradiente: [0.001983892756015848,-0.034227504146395304] Loss: 22.843842256283406\n",
      "Iteracion: 9992 Gradiente: [0.0019828380576598194,-0.03420930775458959] Loss: 22.843841081137995\n",
      "Iteracion: 9993 Gradiente: [0.00198178392004517,-0.0341911210365429] Loss: 22.84383990724173\n",
      "Iteracion: 9994 Gradiente: [0.0019807303428715766,-0.0341729439871127] Loss: 22.84383873459331\n",
      "Iteracion: 9995 Gradiente: [0.0019796773258737707,-0.034154776601155255] Loss: 22.843837563191375\n",
      "Iteracion: 9996 Gradiente: [0.0019786248686405844,-0.03413661887354067] Loss: 22.84383639303461\n",
      "Iteracion: 9997 Gradiente: [0.001977572970940855,-0.03411847079913185] Loss: 22.843835224121698\n",
      "Iteracion: 9998 Gradiente: [0.001976521632402258,-0.03410033237279914] Loss: 22.843834056451318\n",
      "Iteracion: 9999 Gradiente: [0.0019754708528807894,-0.03408220358940592] Loss: 22.843832890022156\n",
      "Iteracion: 10000 Gradiente: [0.0019744206319188607,-0.03406408444383343] Loss: 22.843831724832885\n",
      "Iteracion: 10001 Gradiente: [0.0019733709693383616,-0.03404597493095274] Loss: 22.843830560882182\n",
      "Iteracion: 10002 Gradiente: [0.0019723218647745475,-0.03402787504564439] Loss: 22.843829398168715\n",
      "Iteracion: 10003 Gradiente: [0.0019712733178844626,-0.03400978478279484] Loss: 22.8438282366912\n",
      "Iteracion: 10004 Gradiente: [0.0019702253284634708,-0.033991704137283564] Loss: 22.84382707644831\n",
      "Iteracion: 10005 Gradiente: [0.0019691778962159863,-0.03397363310399607] Loss: 22.843825917438746\n",
      "Iteracion: 10006 Gradiente: [0.001968131020794317,-0.03395557167782665] Loss: 22.84382475966118\n",
      "Iteracion: 10007 Gradiente: [0.0019670847018678234,-0.03393751985366814] Loss: 22.843823603114302\n",
      "Iteracion: 10008 Gradiente: [0.0019660389393379774,-0.03391947762640536] Loss: 22.843822447796793\n",
      "Iteracion: 10009 Gradiente: [0.0019649937326562394,-0.033901444990952004] Loss: 22.843821293707393\n",
      "Iteracion: 10010 Gradiente: [0.001963949081721239,-0.03388342194219345] Loss: 22.843820140844745\n",
      "Iteracion: 10011 Gradiente: [0.0019629049860327543,-0.03386540847504982] Loss: 22.84381898920757\n",
      "Iteracion: 10012 Gradiente: [0.001961861445518783,-0.033847404584411815] Loss: 22.843817838794557\n",
      "Iteracion: 10013 Gradiente: [0.0019608184597245783,-0.03382941026519847] Loss: 22.843816689604413\n",
      "Iteracion: 10014 Gradiente: [0.0019597760284502403,-0.03381142551231567] Loss: 22.843815541635838\n",
      "Iteracion: 10015 Gradiente: [0.001958734151389763,-0.03379345032067782] Loss: 22.843814394887524\n",
      "Iteracion: 10016 Gradiente: [0.0019576928282305063,-0.033775484685203236] Loss: 22.843813249358185\n",
      "Iteracion: 10017 Gradiente: [0.001956652058605831,-0.03375752860081616] Loss: 22.8438121050465\n",
      "Iteracion: 10018 Gradiente: [0.001955611842291205,-0.033739582062434555] Loss: 22.843810961951217\n",
      "Iteracion: 10019 Gradiente: [0.001954572179051676,-0.03372164506497863] Loss: 22.84380982007102\n",
      "Iteracion: 10020 Gradiente: [0.001953533068521551,-0.033703717603381735] Loss: 22.843808679404596\n",
      "Iteracion: 10021 Gradiente: [0.0019524945103900867,-0.03368579967257581] Loss: 22.8438075399507\n",
      "Iteracion: 10022 Gradiente: [0.001951456504399592,-0.033667891267489834] Loss: 22.843806401708\n",
      "Iteracion: 10023 Gradiente: [0.0019504190502080594,-0.03364999238306498] Loss: 22.843805264675247\n",
      "Iteracion: 10024 Gradiente: [0.0019493821475445353,-0.033632103014236844] Loss: 22.84380412885112\n",
      "Iteracion: 10025 Gradiente: [0.0019483457961456453,-0.03361422315594673] Loss: 22.843802994234352\n",
      "Iteracion: 10026 Gradiente: [0.0019473099956580123,-0.033596352803139835] Loss: 22.843801860823657\n",
      "Iteracion: 10027 Gradiente: [0.0019462747459111065,-0.033578491950755546] Loss: 22.84380072861777\n",
      "Iteracion: 10028 Gradiente: [0.0019452400465714466,-0.03356064059374712] Loss: 22.84379959761537\n",
      "Iteracion: 10029 Gradiente: [0.0019442058972212331,-0.033542798727073615] Loss: 22.843798467815223\n",
      "Iteracion: 10030 Gradiente: [0.0019431722976965679,-0.0335249663456814] Loss: 22.843797339216003\n",
      "Iteracion: 10031 Gradiente: [0.00194213924761281,-0.033507143444534176] Loss: 22.84379621181649\n",
      "Iteracion: 10032 Gradiente: [0.0019411067467899556,-0.03348933001858493] Loss: 22.843795085615362\n",
      "Iteracion: 10033 Gradiente: [0.0019400747948083107,-0.033471526062802445] Loss: 22.843793960611357\n",
      "Iteracion: 10034 Gradiente: [0.0019390433915418726,-0.033453731572144346] Loss: 22.84379283680322\n",
      "Iteracion: 10035 Gradiente: [0.0019380125365046296,-0.03343594654159018] Loss: 22.843791714189656\n",
      "Iteracion: 10036 Gradiente: [0.0019369822296056328,-0.03341817096609674] Loss: 22.843790592769412\n",
      "Iteracion: 10037 Gradiente: [0.0019359524704393985,-0.03340040484064571] Loss: 22.84378947254122\n",
      "Iteracion: 10038 Gradiente: [0.0019349232587179206,-0.03338264816021296] Loss: 22.843788353503793\n",
      "Iteracion: 10039 Gradiente: [0.00193389459410677,-0.033364900919779214] Loss: 22.84378723565588\n",
      "Iteracion: 10040 Gradiente: [0.0019328664764458382,-0.033347163114318214] Loss: 22.84378611899623\n",
      "Iteracion: 10041 Gradiente: [0.0019318389052131123,-0.03332943473882892] Loss: 22.843785003523568\n",
      "Iteracion: 10042 Gradiente: [0.0019308118803223807,-0.03331171578828413] Loss: 22.843783889236608\n",
      "Iteracion: 10043 Gradiente: [0.0019297854014098449,-0.03329400625767948] Loss: 22.84378277613414\n",
      "Iteracion: 10044 Gradiente: [0.0019287594682509735,-0.033276306142001624] Loss: 22.84378166421486\n",
      "Iteracion: 10045 Gradiente: [0.0019277340805122852,-0.033258615436247634] Loss: 22.84378055347751\n",
      "Iteracion: 10046 Gradiente: [0.0019267092379304056,-0.033240934135414575] Loss: 22.843779443920873\n",
      "Iteracion: 10047 Gradiente: [0.0019256849401405893,-0.03322326223450709] Loss: 22.84377833554366\n",
      "Iteracion: 10048 Gradiente: [0.0019246611869050412,-0.03320559972852379] Loss: 22.843777228344628\n",
      "Iteracion: 10049 Gradiente: [0.0019236379779982826,-0.03318794661246791] Loss: 22.843776122322538\n",
      "Iteracion: 10050 Gradiente: [0.0019226153129939878,-0.03317030288135273] Loss: 22.843775017476123\n",
      "Iteracion: 10051 Gradiente: [0.0019215931917339428,-0.03315266853018315] Loss: 22.843773913804128\n",
      "Iteracion: 10052 Gradiente: [0.001920571613780453,-0.033135043553980496] Loss: 22.843772811305307\n",
      "Iteracion: 10053 Gradiente: [0.0019195505789847782,-0.0331174279477537] Loss: 22.843771709978434\n",
      "Iteracion: 10054 Gradiente: [0.0019185300870087,-0.03309982170652267] Loss: 22.84377060982224\n",
      "Iteracion: 10055 Gradiente: [0.001917510137496,-0.03308222482531432] Loss: 22.84376951083549\n",
      "Iteracion: 10056 Gradiente: [0.0019164907301842504,-0.03306463729915] Loss: 22.843768413016946\n",
      "Iteracion: 10057 Gradiente: [0.0019154718649294486,-0.033047059123048686] Loss: 22.84376731636535\n",
      "Iteracion: 10058 Gradiente: [0.0019144535412673727,-0.03302949029204818] Loss: 22.843766220879466\n",
      "Iteracion: 10059 Gradiente: [0.0019134357590189666,-0.03301193080117623] Loss: 22.843765126558086\n",
      "Iteracion: 10060 Gradiente: [0.001912418517854538,-0.03299438064546851] Loss: 22.843764033399932\n",
      "Iteracion: 10061 Gradiente: [0.0019114018175059755,-0.0329768398199595] Loss: 22.843762941403764\n",
      "Iteracion: 10062 Gradiente: [0.0019103856576407452,-0.03295930831969388] Loss: 22.843761850568388\n",
      "Iteracion: 10063 Gradiente: [0.00190937003793105,-0.03294178613971669] Loss: 22.84376076089253\n",
      "Iteracion: 10064 Gradiente: [0.0019083549582404658,-0.032924273275062414] Loss: 22.843759672374986\n",
      "Iteracion: 10065 Gradiente: [0.0019073404181900362,-0.032906769720785906] Loss: 22.84375858501451\n",
      "Iteracion: 10066 Gradiente: [0.0019063264173676468,-0.032889275471942155] Loss: 22.843757498809875\n",
      "Iteracion: 10067 Gradiente: [0.0019053129557590864,-0.03287179052357132] Loss: 22.843756413759856\n",
      "Iteracion: 10068 Gradiente: [0.0019043000328935022,-0.032854314870737984] Loss: 22.843755329863225\n",
      "Iteracion: 10069 Gradiente: [0.0019032876484696242,-0.03283684850850186] Loss: 22.843754247118742\n",
      "Iteracion: 10070 Gradiente: [0.0019022758024145028,-0.03281939143191058] Loss: 22.843753165525197\n",
      "Iteracion: 10071 Gradiente: [0.0019012644941862314,-0.03280194363604328] Loss: 22.843752085081363\n",
      "Iteracion: 10072 Gradiente: [0.0019002537236777546,-0.032784505115955064] Loss: 22.843751005786025\n",
      "Iteracion: 10073 Gradiente: [0.0018992434904276934,-0.03276707586672577] Loss: 22.84374992763795\n",
      "Iteracion: 10074 Gradiente: [0.0018982337943072025,-0.03274965588341701] Loss: 22.843748850635926\n",
      "Iteracion: 10075 Gradiente: [0.0018972246349174308,-0.032732245161108627] Loss: 22.84374777477874\n",
      "Iteracion: 10076 Gradiente: [0.0018962160120594262,-0.032714843694872424] Loss: 22.843746700065136\n",
      "Iteracion: 10077 Gradiente: [0.0018952079254726565,-0.03269745147978706] Loss: 22.843745626493956\n",
      "Iteracion: 10078 Gradiente: [0.0018942003747810076,-0.032680068510939006] Loss: 22.84374455406395\n",
      "Iteracion: 10079 Gradiente: [0.001893193359735316,-0.03266269478341073] Loss: 22.843743482773917\n",
      "Iteracion: 10080 Gradiente: [0.001892186880072207,-0.032645330292287035] Loss: 22.843742412622642\n",
      "Iteracion: 10081 Gradiente: [0.001891180935401356,-0.03262797503266673] Loss: 22.843741343608926\n",
      "Iteracion: 10082 Gradiente: [0.001890175525555075,-0.032610628999631665] Loss: 22.843740275731534\n",
      "Iteracion: 10083 Gradiente: [0.001889170650170513,-0.032593292188283725] Loss: 22.843739208989284\n",
      "Iteracion: 10084 Gradiente: [0.0018881663091084042,-0.03257596459371056] Loss: 22.84373814338096\n",
      "Iteracion: 10085 Gradiente: [0.0018871625019803182,-0.03255864621101902] Loss: 22.843737078905335\n",
      "Iteracion: 10086 Gradiente: [0.0018861592284167728,-0.032541337035315955] Loss: 22.843736015561245\n",
      "Iteracion: 10087 Gradiente: [0.001885156488318292,-0.03252403706169768] Loss: 22.843734953347465\n",
      "Iteracion: 10088 Gradiente: [0.0018841542812926568,-0.03250674628527588] Loss: 22.843733892262787\n",
      "Iteracion: 10089 Gradiente: [0.0018831526070281748,-0.03248946470116465] Loss: 22.84373283230602\n",
      "Iteracion: 10090 Gradiente: [0.0018821514653126314,-0.03247219230447283] Loss: 22.843731773475977\n",
      "Iteracion: 10091 Gradiente: [0.0018811508557623332,-0.0324549290903209] Loss: 22.84373071577142\n",
      "Iteracion: 10092 Gradiente: [0.0018801507782635932,-0.03243767505381735] Loss: 22.843729659191204\n",
      "Iteracion: 10093 Gradiente: [0.0018791512324478768,-0.03242043019008918] Loss: 22.84372860373412\n",
      "Iteracion: 10094 Gradiente: [0.0018781522179419123,-0.032403194494265125] Loss: 22.84372754939893\n",
      "Iteracion: 10095 Gradiente: [0.001877153734521168,-0.032385967961466734] Loss: 22.843726496184498\n",
      "Iteracion: 10096 Gradiente: [0.0018761557820217453,-0.03236875058681683] Loss: 22.843725444089603\n",
      "Iteracion: 10097 Gradiente: [0.0018751583599519487,-0.032351542365456724] Loss: 22.84372439311308\n",
      "Iteracion: 10098 Gradiente: [0.001874161468259672,-0.03233434329250997] Loss: 22.843723343253707\n",
      "Iteracion: 10099 Gradiente: [0.001873165106481641,-0.032317153363121895] Loss: 22.84372229451031\n",
      "Iteracion: 10100 Gradiente: [0.0018721692743649025,-0.03229997257242966] Loss: 22.843721246881728\n",
      "Iteracion: 10101 Gradiente: [0.0018711739717711376,-0.032282800915567005] Loss: 22.843720200366725\n",
      "Iteracion: 10102 Gradiente: [0.0018701791982654943,-0.03226563838768648] Loss: 22.843719154964162\n",
      "Iteracion: 10103 Gradiente: [0.001869184953596914,-0.032248484983934254] Loss: 22.843718110672835\n",
      "Iteracion: 10104 Gradiente: [0.0018681912374859167,-0.03223134069945802] Loss: 22.843717067491557\n",
      "Iteracion: 10105 Gradiente: [0.0018671980496426006,-0.03221420552941131] Loss: 22.843716025419173\n",
      "Iteracion: 10106 Gradiente: [0.00186620538984054,-0.03219707946894559] Loss: 22.84371498445449\n",
      "Iteracion: 10107 Gradiente: [0.0018652132578030963,-0.032179962513215796] Loss: 22.843713944596335\n",
      "Iteracion: 10108 Gradiente: [0.0018642216531740512,-0.032162854657388114] Loss: 22.843712905843503\n",
      "Iteracion: 10109 Gradiente: [0.0018632305756815033,-0.0321457558966235] Loss: 22.843711868194855\n",
      "Iteracion: 10110 Gradiente: [0.0018622400251568176,-0.032128666226079974] Loss: 22.843710831649204\n",
      "Iteracion: 10111 Gradiente: [0.001861250001273144,-0.03211158564092855] Loss: 22.843709796205395\n",
      "Iteracion: 10112 Gradiente: [0.0018602605036155259,-0.032094514136345724] Loss: 22.84370876186222\n",
      "Iteracion: 10113 Gradiente: [0.001859271531997327,-0.032077451707498976] Loss: 22.84370772861854\n",
      "Iteracion: 10114 Gradiente: [0.0018582830861798053,-0.03206039834956146] Loss: 22.843706696473173\n",
      "Iteracion: 10115 Gradiente: [0.001857295165935587,-0.032043354057707536] Loss: 22.843705665424956\n",
      "Iteracion: 10116 Gradiente: [0.0018563077707871875,-0.032026318827128954] Loss: 22.843704635472722\n",
      "Iteracion: 10117 Gradiente: [0.0018553209005744975,-0.032009292653001606] Loss: 22.843703606615307\n",
      "Iteracion: 10118 Gradiente: [0.0018543345550474063,-0.0319922755305092] Loss: 22.84370257885154\n",
      "Iteracion: 10119 Gradiente: [0.0018533487338961172,-0.03197526745484017] Loss: 22.843701552180267\n",
      "Iteracion: 10120 Gradiente: [0.0018523634368269389,-0.031958268421188055] Loss: 22.84370052660033\n",
      "Iteracion: 10121 Gradiente: [0.0018513786635035482,-0.03194127842474745] Loss: 22.84369950211055\n",
      "Iteracion: 10122 Gradiente: [0.001850394413782889,-0.03192429746070762] Loss: 22.843698478709786\n",
      "Iteracion: 10123 Gradiente: [0.0018494106873655862,-0.031907325524264486] Loss: 22.843697456396878\n",
      "Iteracion: 10124 Gradiente: [0.0018484274839067893,-0.03189036261062661] Loss: 22.84369643517068\n",
      "Iteracion: 10125 Gradiente: [0.0018474448030644907,-0.031873408714999246] Loss: 22.843695415030002\n",
      "Iteracion: 10126 Gradiente: [0.001846462644648265,-0.03185646383258292] Loss: 22.843694395973724\n",
      "Iteracion: 10127 Gradiente: [0.0018454810084771603,-0.031839527958579576] Loss: 22.843693378000673\n",
      "Iteracion: 10128 Gradiente: [0.0018444998941636944,-0.03182260108820797] Loss: 22.8436923611097\n",
      "Iteracion: 10129 Gradiente: [0.00184351930133649,-0.03180568321668744] Loss: 22.84369134529967\n",
      "Iteracion: 10130 Gradiente: [0.0018425392298041742,-0.03178877433922622] Loss: 22.84369033056942\n",
      "Iteracion: 10131 Gradiente: [0.001841559679432218,-0.0317718744510356] Loss: 22.84368931691781\n",
      "Iteracion: 10132 Gradiente: [0.0018405806498179799,-0.03175498354734619] Loss: 22.843688304343694\n",
      "Iteracion: 10133 Gradiente: [0.0018396021406137683,-0.03173810162338242] Loss: 22.843687292845928\n",
      "Iteracion: 10134 Gradiente: [0.001838624151721054,-0.03172122867436092] Loss: 22.843686282423356\n",
      "Iteracion: 10135 Gradiente: [0.0018376466826244572,-0.03170436469552603] Loss: 22.843685273074843\n",
      "Iteracion: 10136 Gradiente: [0.0018366697332946084,-0.031687509682091886] Loss: 22.843684264799233\n",
      "Iteracion: 10137 Gradiente: [0.0018356933033212878,-0.03167066362930176] Loss: 22.84368325759541\n",
      "Iteracion: 10138 Gradiente: [0.0018347173924060675,-0.0316538265323936] Loss: 22.84368225146223\n",
      "Iteracion: 10139 Gradiente: [0.0018337420003073626,-0.031636998386603325] Loss: 22.843681246398546\n",
      "Iteracion: 10140 Gradiente: [0.001832767126758957,-0.03162017918717185] Loss: 22.84368024240321\n",
      "Iteracion: 10141 Gradiente: [0.001831792771559056,-0.03160336892933842] Loss: 22.843679239475097\n",
      "Iteracion: 10142 Gradiente: [0.0018308189342927032,-0.03158656760835742] Loss: 22.8436782376131\n",
      "Iteracion: 10143 Gradiente: [0.0018298456147732623,-0.03156977521947333] Loss: 22.843677236816042\n",
      "Iteracion: 10144 Gradiente: [0.001828872812626514,-0.03155299175794314] Loss: 22.84367623708282\n",
      "Iteracion: 10145 Gradiente: [0.001827900527699929,-0.03153621721901298] Loss: 22.843675238412292\n",
      "Iteracion: 10146 Gradiente: [0.0018269287596912895,-0.031519451597941624] Loss: 22.843674240803306\n",
      "Iteracion: 10147 Gradiente: [0.001825957508321115,-0.031502694889987616] Loss: 22.843673244254795\n",
      "Iteracion: 10148 Gradiente: [0.0018249867732388718,-0.031485947090417694] Loss: 22.843672248765564\n",
      "Iteracion: 10149 Gradiente: [0.0018240165542295016,-0.03146920819449228] Loss: 22.843671254334527\n",
      "Iteracion: 10150 Gradiente: [0.0018230468510116301,-0.03145247819747846] Loss: 22.843670260960547\n",
      "Iteracion: 10151 Gradiente: [0.0018220776633086188,-0.03143575709464415] Loss: 22.843669268642522\n",
      "Iteracion: 10152 Gradiente: [0.0018211089908694096,-0.0314190448812622] Loss: 22.843668277379287\n",
      "Iteracion: 10153 Gradiente: [0.001820140833394627,-0.03140234155260562] Loss: 22.84366728716975\n",
      "Iteracion: 10154 Gradiente: [0.0018191731906976353,-0.03138564710394658] Loss: 22.84366629801278\n",
      "Iteracion: 10155 Gradiente: [0.0018182060623634774,-0.031368961530573] Loss: 22.843665309907276\n",
      "Iteracion: 10156 Gradiente: [0.0018172394482415181,-0.03135228482775692] Loss: 22.843664322852092\n",
      "Iteracion: 10157 Gradiente: [0.0018162733479982762,-0.03133561699079147] Loss: 22.84366333684614\n",
      "Iteracion: 10158 Gradiente: [0.0018153077613400606,-0.031318958014960194] Loss: 22.843662351888277\n",
      "Iteracion: 10159 Gradiente: [0.0018143426880091813,-0.03130230789555088] Loss: 22.843661367977415\n",
      "Iteracion: 10160 Gradiente: [0.0018133781277327899,-0.0312856666278576] Loss: 22.843660385112432\n",
      "Iteracion: 10161 Gradiente: [0.0018124140803015129,-0.03126903420716971] Loss: 22.8436594032922\n",
      "Iteracion: 10162 Gradiente: [0.0018114505452956563,-0.03125241062879276] Loss: 22.843658422515634\n",
      "Iteracion: 10163 Gradiente: [0.0018104875225825859,-0.03123579588801668] Loss: 22.843657442781613\n",
      "Iteracion: 10164 Gradiente: [0.0018095250118875584,-0.031219189980145147] Loss: 22.843656464089023\n",
      "Iteracion: 10165 Gradiente: [0.001808563012850565,-0.03120259290048596] Loss: 22.843655486436756\n",
      "Iteracion: 10166 Gradiente: [0.0018076015252347587,-0.03118600464434434] Loss: 22.84365450982372\n",
      "Iteracion: 10167 Gradiente: [0.0018066405488089761,-0.031169425207027028] Loss: 22.843653534248812\n",
      "Iteracion: 10168 Gradiente: [0.0018056800832539465,-0.031152854583847756] Loss: 22.843652559710907\n",
      "Iteracion: 10169 Gradiente: [0.0018047201282882952,-0.031136292770122035] Loss: 22.843651586208924\n",
      "Iteracion: 10170 Gradiente: [0.0018037606837140174,-0.03111973976116135] Loss: 22.84365061374174\n",
      "Iteracion: 10171 Gradiente: [0.0018028017491009982,-0.03110319555229554] Loss: 22.84364964230827\n",
      "Iteracion: 10172 Gradiente: [0.001801843324402815,-0.031086660138832538] Loss: 22.843648671907427\n",
      "Iteracion: 10173 Gradiente: [0.0018008854091779844,-0.031070133516105495] Loss: 22.843647702538085\n",
      "Iteracion: 10174 Gradiente: [0.0017999280032038694,-0.03105361567943845] Loss: 22.843646734199172\n",
      "Iteracion: 10175 Gradiente: [0.0017989711061450937,-0.031037106624164798] Loss: 22.84364576688957\n",
      "Iteracion: 10176 Gradiente: [0.001798014717907866,-0.031020606345605727] Loss: 22.84364480060821\n",
      "Iteracion: 10177 Gradiente: [0.0017970588380914402,-0.031004114839103043] Loss: 22.843643835353983\n",
      "Iteracion: 10178 Gradiente: [0.0017961034664002304,-0.03098763209999286] Loss: 22.8436428711258\n",
      "Iteracion: 10179 Gradiente: [0.0017951486026807591,-0.030971158123609517] Loss: 22.843641907922574\n",
      "Iteracion: 10180 Gradiente: [0.001794194246637441,-0.030954692905296354] Loss: 22.843640945743207\n",
      "Iteracion: 10181 Gradiente: [0.001793240397882793,-0.030938236440402632] Loss: 22.843639984586623\n",
      "Iteracion: 10182 Gradiente: [0.0017922870561979684,-0.0309217887242711] Loss: 22.84363902445173\n",
      "Iteracion: 10183 Gradiente: [0.0017913342214711746,-0.03090534975224332] Loss: 22.843638065337426\n",
      "Iteracion: 10184 Gradiente: [0.0017903818932135588,-0.030888919519683] Loss: 22.843637107242646\n",
      "Iteracion: 10185 Gradiente: [0.0017894300712868016,-0.030872498021936087] Loss: 22.84363615016632\n",
      "Iteracion: 10186 Gradiente: [0.0017884787552929992,-0.030856085254366405] Loss: 22.843635194107314\n",
      "Iteracion: 10187 Gradiente: [0.0017875279450739374,-0.03083968121232535] Loss: 22.843634239064595\n",
      "Iteracion: 10188 Gradiente: [0.0017865776404107693,-0.030823285891173193] Loss: 22.843633285037058\n",
      "Iteracion: 10189 Gradiente: [0.0017856278408686422,-0.030806899286281227] Loss: 22.84363233202365\n",
      "Iteracion: 10190 Gradiente: [0.0017846785462495517,-0.030790521393013394] Loss: 22.843631380023247\n",
      "Iteracion: 10191 Gradiente: [0.0017837297563365459,-0.030774152206734465] Loss: 22.84363042903483\n",
      "Iteracion: 10192 Gradiente: [0.001782781470857723,-0.030757791722815262] Loss: 22.84362947905727\n",
      "Iteracion: 10193 Gradiente: [0.0017818336895591832,-0.030741439936628367] Loss: 22.843628530089532\n",
      "Iteracion: 10194 Gradiente: [0.0017808864120581803,-0.030725096843559167] Loss: 22.843627582130523\n",
      "Iteracion: 10195 Gradiente: [0.0017799396381557623,-0.030708762438978354] Loss: 22.843626635179167\n",
      "Iteracion: 10196 Gradiente: [0.001778993367627398,-0.0306924367182661] Loss: 22.84362568923441\n",
      "Iteracion: 10197 Gradiente: [0.001778047600111184,-0.030676119676812046] Loss: 22.84362474429516\n",
      "Iteracion: 10198 Gradiente: [0.001777102335452696,-0.030659811309994702] Loss: 22.843623800360366\n",
      "Iteracion: 10199 Gradiente: [0.0017761575733061363,-0.030643511613206442] Loss: 22.843622857428954\n",
      "Iteracion: 10200 Gradiente: [0.001775213313441289,-0.030627220581836905] Loss: 22.84362191549987\n",
      "Iteracion: 10201 Gradiente: [0.001774269555555937,-0.030610938211279404] Loss: 22.843620974572026\n",
      "Iteracion: 10202 Gradiente: [0.0017733262993895474,-0.030594664496931993] Loss: 22.843620034644363\n",
      "Iteracion: 10203 Gradiente: [0.0017723835447071678,-0.030578399434188934] Loss: 22.843619095715837\n",
      "Iteracion: 10204 Gradiente: [0.001771441291159211,-0.030562143018456684] Loss: 22.84361815778536\n",
      "Iteracion: 10205 Gradiente: [0.0017704995385893578,-0.030545895245129982] Loss: 22.84361722085189\n",
      "Iteracion: 10206 Gradiente: [0.0017695582867323387,-0.030529656109616592] Loss: 22.84361628491435\n",
      "Iteracion: 10207 Gradiente: [0.0017686175352281453,-0.030513425607328535] Loss: 22.8436153499717\n",
      "Iteracion: 10208 Gradiente: [0.0017676772837755077,-0.030497203733678663] Loss: 22.84361441602288\n",
      "Iteracion: 10209 Gradiente: [0.001766737532273055,-0.03048099048406942] Loss: 22.843613483066814\n",
      "Iteracion: 10210 Gradiente: [0.0017657982803749898,-0.030464785853921466] Loss: 22.843612551102467\n",
      "Iteracion: 10211 Gradiente: [0.0017648595277753051,-0.03044858983865429] Loss: 22.843611620128772\n",
      "Iteracion: 10212 Gradiente: [0.0017639212743214708,-0.030432402433681695] Loss: 22.843610690144708\n",
      "Iteracion: 10213 Gradiente: [0.0017629835196468471,-0.030416223634431225] Loss: 22.84360976114918\n",
      "Iteracion: 10214 Gradiente: [0.0017620462635704825,-0.03040005343632342] Loss: 22.84360883314115\n",
      "Iteracion: 10215 Gradiente: [0.0017611095056518403,-0.030383891834797203] Loss: 22.843607906119566\n",
      "Iteracion: 10216 Gradiente: [0.0017601732458151294,-0.030367738825269917] Loss: 22.843606980083404\n",
      "Iteracion: 10217 Gradiente: [0.0017592374836813936,-0.030351594403180117] Loss: 22.843606055031593\n",
      "Iteracion: 10218 Gradiente: [0.0017583022190403124,-0.03033545856395925] Loss: 22.843605130963088\n",
      "Iteracion: 10219 Gradiente: [0.001757367451674933,-0.03031933130304386] Loss: 22.843604207876854\n",
      "Iteracion: 10220 Gradiente: [0.001756433181131456,-0.03030321261588161] Loss: 22.843603285771835\n",
      "Iteracion: 10221 Gradiente: [0.001755499407371038,-0.030287102497902414] Loss: 22.84360236464699\n",
      "Iteracion: 10222 Gradiente: [0.001754566129941774,-0.03027100094456377] Loss: 22.843601444501303\n",
      "Iteracion: 10223 Gradiente: [0.0017536333487157662,-0.03025490795130151] Loss: 22.843600525333688\n",
      "Iteracion: 10224 Gradiente: [0.0017527010634864836,-0.030238823513560813] Loss: 22.84359960714314\n",
      "Iteracion: 10225 Gradiente: [0.0017517692737546516,-0.030222747626811614] Loss: 22.84359868992861\n",
      "Iteracion: 10226 Gradiente: [0.0017508379794056357,-0.030206680286495313] Loss: 22.843597773689044\n",
      "Iteracion: 10227 Gradiente: [0.0017499071801580612,-0.030190621488068576] Loss: 22.843596858423435\n",
      "Iteracion: 10228 Gradiente: [0.0017489768758584508,-0.030174571226989143] Loss: 22.843595944130747\n",
      "Iteracion: 10229 Gradiente: [0.0017480470660965846,-0.030158529498721384] Loss: 22.8435950308099\n",
      "Iteracion: 10230 Gradiente: [0.0017471177506479308,-0.030142496298728953] Loss: 22.84359411845991\n",
      "Iteracion: 10231 Gradiente: [0.0017461889291951137,-0.03012647162248167] Loss: 22.843593207079728\n",
      "Iteracion: 10232 Gradiente: [0.0017452606015429712,-0.030110455465442953] Loss: 22.843592296668326\n",
      "Iteracion: 10233 Gradiente: [0.001744332767455603,-0.030094447823083688] Loss: 22.84359138722469\n",
      "Iteracion: 10234 Gradiente: [0.001743405426510473,-0.030078448690885294] Loss: 22.843590478747736\n",
      "Iteracion: 10235 Gradiente: [0.0017424785787881092,-0.030062458064303143] Loss: 22.8435895712365\n",
      "Iteracion: 10236 Gradiente: [0.0017415522236821819,-0.030046475938836418] Loss: 22.84358866468993\n",
      "Iteracion: 10237 Gradiente: [0.001740626360966265,-0.03003050230996358] Loss: 22.84358775910699\n",
      "Iteracion: 10238 Gradiente: [0.001739700990572146,-0.030014537173156105] Loss: 22.843586854486663\n",
      "Iteracion: 10239 Gradiente: [0.0017387761121578176,-0.02999858052390299] Loss: 22.843585950827922\n",
      "Iteracion: 10240 Gradiente: [0.0017378517254077982,-0.029982632357696205] Loss: 22.843585048129764\n",
      "Iteracion: 10241 Gradiente: [0.0017369278300956618,-0.02996669267002545] Loss: 22.843584146391144\n",
      "Iteracion: 10242 Gradiente: [0.0017360044260234038,-0.029950761456376304] Loss: 22.84358324561108\n",
      "Iteracion: 10243 Gradiente: [0.001735081512758067,-0.029934838712255996] Loss: 22.843582345788494\n",
      "Iteracion: 10244 Gradiente: [0.001734159090195438,-0.02991892443315187] Loss: 22.843581446922418\n",
      "Iteracion: 10245 Gradiente: [0.001733237158002036,-0.029903018614568566] Loss: 22.843580549011822\n",
      "Iteracion: 10246 Gradiente: [0.0017323157158870117,-0.029887121252010117] Loss: 22.84357965205567\n",
      "Iteracion: 10247 Gradiente: [0.0017313947636012016,-0.029871232340978084] Loss: 22.843578756052963\n",
      "Iteracion: 10248 Gradiente: [0.0017304743010489196,-0.02985535187697271] Loss: 22.843577861002693\n",
      "Iteracion: 10249 Gradiente: [0.0017295543278578406,-0.029839479855508107] Loss: 22.843576966903857\n",
      "Iteracion: 10250 Gradiente: [0.0017286348437266952,-0.029823616272099212] Loss: 22.84357607375542\n",
      "Iteracion: 10251 Gradiente: [0.0017277158483172646,-0.029807761122263562] Loss: 22.843575181556375\n",
      "Iteracion: 10252 Gradiente: [0.0017267973415357573,-0.029791914401508635] Loss: 22.84357429030571\n",
      "Iteracion: 10253 Gradiente: [0.0017258793230278495,-0.029776076105359907] Loss: 22.843573400002445\n",
      "Iteracion: 10254 Gradiente: [0.0017249617925983785,-0.029760246229332193] Loss: 22.843572510645537\n",
      "Iteracion: 10255 Gradiente: [0.0017240447500578664,-0.029744424768946943] Loss: 22.843571622234006\n",
      "Iteracion: 10256 Gradiente: [0.0017231281948757746,-0.02972861171974633] Loss: 22.843570734766836\n",
      "Iteracion: 10257 Gradiente: [0.0017222121270663137,-0.029712807077242095] Loss: 22.843569848243025\n",
      "Iteracion: 10258 Gradiente: [0.0017212965461908425,-0.02969701083697499] Loss: 22.843568962661564\n",
      "Iteracion: 10259 Gradiente: [0.0017203814520977783,-0.029681222994472022] Loss: 22.843568078021473\n",
      "Iteracion: 10260 Gradiente: [0.0017194668444659555,-0.02966544354527289] Loss: 22.843567194321718\n",
      "Iteracion: 10261 Gradiente: [0.0017185527231400024,-0.029649672484909337] Loss: 22.843566311561343\n",
      "Iteracion: 10262 Gradiente: [0.00171763908770212,-0.02963390980893005] Loss: 22.84356542973928\n",
      "Iteracion: 10263 Gradiente: [0.0017167259381352551,-0.02961815551286359] Loss: 22.843564548854612\n",
      "Iteracion: 10264 Gradiente: [0.0017158132739761337,-0.029602409592268942] Loss: 22.84356366890629\n",
      "Iteracion: 10265 Gradiente: [0.0017149010949613814,-0.029586672042691477] Loss: 22.843562789893344\n",
      "Iteracion: 10266 Gradiente: [0.0017139894008854147,-0.02957094285967893] Loss: 22.843561911814763\n",
      "Iteracion: 10267 Gradiente: [0.0017130781915123332,-0.029555222038782365] Loss: 22.84356103466957\n",
      "Iteracion: 10268 Gradiente: [0.001712167466534235,-0.02953950957555757] Loss: 22.84356015845675\n",
      "Iteracion: 10269 Gradiente: [0.0017112572258279593,-0.029523805465554175] Loss: 22.843559283175335\n",
      "Iteracion: 10270 Gradiente: [0.0017103474688762313,-0.02950810970434752] Loss: 22.84355840882432\n",
      "Iteracion: 10271 Gradiente: [0.0017094381956411554,-0.0294924222874851] Loss: 22.843557535402727\n",
      "Iteracion: 10272 Gradiente: [0.001708529405833777,-0.029476743210533247] Loss: 22.84355666290956\n",
      "Iteracion: 10273 Gradiente: [0.0017076210992087227,-0.029461072469058654] Loss: 22.84355579134382\n",
      "Iteracion: 10274 Gradiente: [0.001706713275421142,-0.02944541005863345] Loss: 22.843554920704555\n",
      "Iteracion: 10275 Gradiente: [0.0017058059342806094,-0.0294297559748248] Loss: 22.843554050990754\n",
      "Iteracion: 10276 Gradiente: [0.0017048990754792233,-0.02941411021320993] Loss: 22.84355318220145\n",
      "Iteracion: 10277 Gradiente: [0.0017039926987431878,-0.02939847276936535] Loss: 22.843552314335643\n",
      "Iteracion: 10278 Gradiente: [0.0017030868038782879,-0.02938284363886474] Loss: 22.843551447392358\n",
      "Iteracion: 10279 Gradiente: [0.0017021813907642052,-0.02936722281728] Loss: 22.843550581370614\n",
      "Iteracion: 10280 Gradiente: [0.0017012764588704007,-0.029351610300211443] Loss: 22.843549716269433\n",
      "Iteracion: 10281 Gradiente: [0.0017003720080888721,-0.029336006083235826] Loss: 22.84354885208785\n",
      "Iteracion: 10282 Gradiente: [0.0016994680380965595,-0.029320410161942097] Loss: 22.843547988824845\n",
      "Iteracion: 10283 Gradiente: [0.0016985645487646176,-0.029304822531914947] Loss: 22.843547126479493\n",
      "Iteracion: 10284 Gradiente: [0.0016976615397197748,-0.029289243188751257] Loss: 22.843546265050797\n",
      "Iteracion: 10285 Gradiente: [0.0016967590106465498,-0.029273672128051587] Loss: 22.84354540453777\n",
      "Iteracion: 10286 Gradiente: [0.001695856961496626,-0.02925810934539849] Loss: 22.843544544939448\n",
      "Iteracion: 10287 Gradiente: [0.0016949553919052582,-0.029242554836397972] Loss: 22.843543686254872\n",
      "Iteracion: 10288 Gradiente: [0.0016940543016213875,-0.0292270085966526] Loss: 22.843542828483052\n",
      "Iteracion: 10289 Gradiente: [0.0016931536902897429,-0.02921147062177051] Loss: 22.843541971623026\n",
      "Iteracion: 10290 Gradiente: [0.0016922535578307437,-0.029195940907348] Loss: 22.843541115673823\n",
      "Iteracion: 10291 Gradiente: [0.0016913539039450144,-0.02918041944899592] Loss: 22.843540260634477\n",
      "Iteracion: 10292 Gradiente: [0.001690454728259283,-0.029164906242332953] Loss: 22.84353940650403\n",
      "Iteracion: 10293 Gradiente: [0.0016895560307176537,-0.029149401282960005] Loss: 22.8435385532815\n",
      "Iteracion: 10294 Gradiente: [0.0016886578108559055,-0.029133904566504044] Loss: 22.843537700965918\n",
      "Iteracion: 10295 Gradiente: [0.0016877600684836124,-0.029118416088580665] Loss: 22.843536849556337\n",
      "Iteracion: 10296 Gradiente: [0.0016868628034027702,-0.029102935844806173] Loss: 22.84353599905178\n",
      "Iteracion: 10297 Gradiente: [0.0016859660153661102,-0.029087463830803274] Loss: 22.843535149451302\n",
      "Iteracion: 10298 Gradiente: [0.0016850697040600457,-0.029072000042198808] Loss: 22.843534300753916\n",
      "Iteracion: 10299 Gradiente: [0.0016841738693168888,-0.029056544474615364] Loss: 22.84353345295869\n",
      "Iteracion: 10300 Gradiente: [0.0016832785108495804,-0.029041097123685588] Loss: 22.843532606064652\n",
      "Iteracion: 10301 Gradiente: [0.0016823836283303233,-0.02902565798504438] Loss: 22.843531760070835\n",
      "Iteracion: 10302 Gradiente: [0.0016814892215099538,-0.02901022705432664] Loss: 22.843530914976306\n",
      "Iteracion: 10303 Gradiente: [0.0016805952902681535,-0.02899480432715992] Loss: 22.843530070780094\n",
      "Iteracion: 10304 Gradiente: [0.0016797018341662806,-0.028979389799194757] Loss: 22.84352922748123\n",
      "Iteracion: 10305 Gradiente: [0.001678808853174966,-0.02896398346605859] Loss: 22.843528385078788\n",
      "Iteracion: 10306 Gradiente: [0.001677916346794935,-0.028948585323408695] Loss: 22.84352754357178\n",
      "Iteracion: 10307 Gradiente: [0.0016770243149750285,-0.028933195366880562] Loss: 22.8435267029593\n",
      "Iteracion: 10308 Gradiente: [0.0016761327573580804,-0.028917813592126508] Loss: 22.843525863240366\n",
      "Iteracion: 10309 Gradiente: [0.001675241673762192,-0.02890243999479362] Loss: 22.843525024414024\n",
      "Iteracion: 10310 Gradiente: [0.0016743510638614603,-0.02888707457053966] Loss: 22.84352418647935\n",
      "Iteracion: 10311 Gradiente: [0.0016734609274654606,-0.028871717315014204] Loss: 22.84352334943537\n",
      "Iteracion: 10312 Gradiente: [0.0016725712643117655,-0.02885636822387679] Loss: 22.84352251328118\n",
      "Iteracion: 10313 Gradiente: [0.0016716820740384718,-0.028841027292791803] Loss: 22.843521678015787\n",
      "Iteracion: 10314 Gradiente: [0.0016707933565205243,-0.02882569451741605] Loss: 22.843520843638252\n",
      "Iteracion: 10315 Gradiente: [0.0016699051114424416,-0.0288103698934151] Loss: 22.843520010147646\n",
      "Iteracion: 10316 Gradiente: [0.0016690173386137985,-0.028795053416452276] Loss: 22.84351917754301\n",
      "Iteracion: 10317 Gradiente: [0.0016681300378271165,-0.028779745082194687] Loss: 22.84351834582346\n",
      "Iteracion: 10318 Gradiente: [0.0016672432086503855,-0.028764444886324726] Loss: 22.843517514987973\n",
      "Iteracion: 10319 Gradiente: [0.0016663568510608684,-0.028749152824501203] Loss: 22.843516685035656\n",
      "Iteracion: 10320 Gradiente: [0.0016654709644986572,-0.028733868892418475] Loss: 22.84351585596557\n",
      "Iteracion: 10321 Gradiente: [0.0016645855490518594,-0.02871859308573299] Loss: 22.84351502777675\n",
      "Iteracion: 10322 Gradiente: [0.0016637006043528876,-0.028703325400133886] Loss: 22.84351420046829\n",
      "Iteracion: 10323 Gradiente: [0.0016628161300009955,-0.02868806583131196] Loss: 22.843513374039237\n",
      "Iteracion: 10324 Gradiente: [0.0016619321259194444,-0.028672814374942252] Loss: 22.843512548488665\n",
      "Iteracion: 10325 Gradiente: [0.0016610485917690691,-0.028657571026716035] Loss: 22.843511723815613\n",
      "Iteracion: 10326 Gradiente: [0.001660165527421024,-0.02864233578231712] Loss: 22.843510900019215\n",
      "Iteracion: 10327 Gradiente: [0.001659282932423404,-0.028627108637447187] Loss: 22.843510077098465\n",
      "Iteracion: 10328 Gradiente: [0.0016584008066511538,-0.0286118895877955] Loss: 22.843509255052457\n",
      "Iteracion: 10329 Gradiente: [0.0016575191499185848,-0.028596678629052973] Loss: 22.84350843388029\n",
      "Iteracion: 10330 Gradiente: [0.001656637961835372,-0.028581475756926978] Loss: 22.843507613580996\n",
      "Iteracion: 10331 Gradiente: [0.0016557572422006691,-0.028566280967116005] Loss: 22.84350679415365\n",
      "Iteracion: 10332 Gradiente: [0.0016548769908799462,-0.028551094255315945] Loss: 22.843505975597353\n",
      "Iteracion: 10333 Gradiente: [0.0016539972074629836,-0.028535915617240684] Loss: 22.843505157911167\n",
      "Iteracion: 10334 Gradiente: [0.0016531178918076724,-0.028520745048593216] Loss: 22.843504341094153\n",
      "Iteracion: 10335 Gradiente: [0.0016522390435634786,-0.028505582545090155] Loss: 22.84350352514541\n",
      "Iteracion: 10336 Gradiente: [0.0016513606625958724,-0.02849042810243508] Loss: 22.843502710063994\n",
      "Iteracion: 10337 Gradiente: [0.001650482748527793,-0.028475281716352423] Loss: 22.84350189584901\n",
      "Iteracion: 10338 Gradiente: [0.001649605301270185,-0.028460143382548136] Loss: 22.84350108249951\n",
      "Iteracion: 10339 Gradiente: [0.0016487283204971467,-0.02844501309674558] Loss: 22.843500270014566\n",
      "Iteracion: 10340 Gradiente: [0.0016478518058761437,-0.02842989085467226] Loss: 22.843499458393286\n",
      "Iteracion: 10341 Gradiente: [0.001646975757250857,-0.028414776652047162] Loss: 22.843498647634743\n",
      "Iteracion: 10342 Gradiente: [0.0016461001743522275,-0.028399670484595064] Loss: 22.843497837738028\n",
      "Iteracion: 10343 Gradiente: [0.0016452250569860402,-0.028384572348044774] Loss: 22.843497028702213\n",
      "Iteracion: 10344 Gradiente: [0.001644350404802708,-0.028369482238129726] Loss: 22.843496220526383\n",
      "Iteracion: 10345 Gradiente: [0.0016434762177842306,-0.028354400150569368] Loss: 22.843495413209624\n",
      "Iteracion: 10346 Gradiente: [0.0016426024953346996,-0.028339326081120574] Loss: 22.843494606751033\n",
      "Iteracion: 10347 Gradiente: [0.00164172923740864,-0.02832426002550837] Loss: 22.84349380114968\n",
      "Iteracion: 10348 Gradiente: [0.0016408564437322562,-0.02830920197947364] Loss: 22.843492996404677\n",
      "Iteracion: 10349 Gradiente: [0.0016399841140933328,-0.02829415193875562] Loss: 22.84349219251509\n",
      "Iteracion: 10350 Gradiente: [0.0016391122482114421,-0.028279109899102532] Loss: 22.84349138948003\n",
      "Iteracion: 10351 Gradiente: [0.0016382408458468945,-0.028264075856258706] Loss: 22.84349058729857\n",
      "Iteracion: 10352 Gradiente: [0.0016373699067183149,-0.028249049805974855] Loss: 22.84348978596982\n",
      "Iteracion: 10353 Gradiente: [0.0016364994305831716,-0.028234031744001933] Loss: 22.843488985492847\n",
      "Iteracion: 10354 Gradiente: [0.0016356294172718813,-0.028219021666089836] Loss: 22.843488185866782\n",
      "Iteracion: 10355 Gradiente: [0.0016347598664831744,-0.028204019567994603] Loss: 22.8434873870907\n",
      "Iteracion: 10356 Gradiente: [0.0016338907779100964,-0.02818902544547915] Loss: 22.843486589163692\n",
      "Iteracion: 10357 Gradiente: [0.0016330221514427496,-0.02817403929429491] Loss: 22.843485792084877\n",
      "Iteracion: 10358 Gradiente: [0.0016321539867021784,-0.02815906111021332] Loss: 22.84348499585333\n",
      "Iteracion: 10359 Gradiente: [0.0016312862836059595,-0.028144090888988654] Loss: 22.843484200468172\n",
      "Iteracion: 10360 Gradiente: [0.001630419041756189,-0.0281291286263946] Loss: 22.843483405928463\n",
      "Iteracion: 10361 Gradiente: [0.0016295522609207562,-0.028114174318202127] Loss: 22.843482612233355\n",
      "Iteracion: 10362 Gradiente: [0.0016286859409859744,-0.028099227960172612] Loss: 22.843481819381932\n",
      "Iteracion: 10363 Gradiente: [0.0016278200814828854,-0.028084289548093958] Loss: 22.843481027373286\n",
      "Iteracion: 10364 Gradiente: [0.0016269546824257,-0.02806935907772612] Loss: 22.84348023620653\n",
      "Iteracion: 10365 Gradiente: [0.001626089743291459,-0.028054436544862917] Loss: 22.843479445880757\n",
      "Iteracion: 10366 Gradiente: [0.0016252252640773198,-0.028039521945271536] Loss: 22.843478656395096\n",
      "Iteracion: 10367 Gradiente: [0.0016243612444649595,-0.02802461527473762] Loss: 22.843477867748636\n",
      "Iteracion: 10368 Gradiente: [0.001623497684204267,-0.028009716529046002] Loss: 22.8434770799405\n",
      "Iteracion: 10369 Gradiente: [0.0016226345829541819,-0.0279948257039905] Loss: 22.843476292969772\n",
      "Iteracion: 10370 Gradiente: [0.0016217719406351231,-0.02797994279535002] Loss: 22.843475506835585\n",
      "Iteracion: 10371 Gradiente: [0.001620909756819818,-0.02796506779892643] Loss: 22.84347472153705\n",
      "Iteracion: 10372 Gradiente: [0.0016200480314703706,-0.02795020071050208] Loss: 22.843473937073252\n",
      "Iteracion: 10373 Gradiente: [0.0016191867642229832,-0.027935341525878954] Loss: 22.843473153443348\n",
      "Iteracion: 10374 Gradiente: [0.0016183259548550192,-0.027920490240853725] Loss: 22.843472370646406\n",
      "Iteracion: 10375 Gradiente: [0.0016174656031580526,-0.027905646851226134] Loss: 22.843471588681563\n",
      "Iteracion: 10376 Gradiente: [0.0016166057087408112,-0.027890811352805044] Loss: 22.843470807547934\n",
      "Iteracion: 10377 Gradiente: [0.0016157462715720309,-0.02787598374138523] Loss: 22.843470027244635\n",
      "Iteracion: 10378 Gradiente: [0.0016148872912348604,-0.027861164012783372] Loss: 22.843469247770784\n",
      "Iteracion: 10379 Gradiente: [0.0016140287675843487,-0.027846352162802526] Loss: 22.843468469125487\n",
      "Iteracion: 10380 Gradiente: [0.0016131707003457527,-0.02783154818725535] Loss: 22.84346769130788\n",
      "Iteracion: 10381 Gradiente: [0.001612313089303067,-0.02781675208195509] Loss: 22.843466914317073\n",
      "Iteracion: 10382 Gradiente: [0.0016114559341910233,-0.02780196384271842] Loss: 22.843466138152213\n",
      "Iteracion: 10383 Gradiente: [0.0016105992346846657,-0.027787183465369125] Loss: 22.843465362812363\n",
      "Iteracion: 10384 Gradiente: [0.0016097429907489414,-0.02777241094571501] Loss: 22.843464588296705\n",
      "Iteracion: 10385 Gradiente: [0.0016088872019878408,-0.02775764627958708] Loss: 22.843463814604338\n",
      "Iteracion: 10386 Gradiente: [0.0016080318681616745,-0.027742889462810182] Loss: 22.84346304173439\n",
      "Iteracion: 10387 Gradiente: [0.0016071769890857012,-0.027728140491209766] Loss: 22.84346226968598\n",
      "Iteracion: 10388 Gradiente: [0.0016063225644453874,-0.027713399360616956] Loss: 22.84346149845824\n",
      "Iteracion: 10389 Gradiente: [0.0016054685940323074,-0.027698666066862523] Loss: 22.84346072805031\n",
      "Iteracion: 10390 Gradiente: [0.001604615077606771,-0.027683940605779965] Loss: 22.8434599584613\n",
      "Iteracion: 10391 Gradiente: [0.0016037620149603527,-0.027669222973203015] Loss: 22.843459189690353\n",
      "Iteracion: 10392 Gradiente: [0.0016029094058836791,-0.02765451316496777] Loss: 22.843458421736596\n",
      "Iteracion: 10393 Gradiente: [0.001602057250044216,-0.02763981117692076] Loss: 22.843457654599142\n",
      "Iteracion: 10394 Gradiente: [0.0016012055472979607,-0.027625117004897486] Loss: 22.843456888277146\n",
      "Iteracion: 10395 Gradiente: [0.0016003542972669038,-0.027610430644751816] Loss: 22.843456122769727\n",
      "Iteracion: 10396 Gradiente: [0.0015995034997539884,-0.027595752092329087] Loss: 22.84345535807605\n",
      "Iteracion: 10397 Gradiente: [0.0015986531546227903,-0.02758108134347014] Loss: 22.843454594195205\n",
      "Iteracion: 10398 Gradiente: [0.0015978032615966717,-0.027566418394029787] Loss: 22.84345383112635\n",
      "Iteracion: 10399 Gradiente: [0.0015969538202777283,-0.02755176323987006] Loss: 22.84345306886864\n",
      "Iteracion: 10400 Gradiente: [0.0015961048306223802,-0.027537115876835352] Loss: 22.84345230742117\n",
      "Iteracion: 10401 Gradiente: [0.0015952562923217783,-0.027522476300789123] Loss: 22.84345154678311\n",
      "Iteracion: 10402 Gradiente: [0.0015944082050433886,-0.027507844507595535] Loss: 22.843450786953596\n",
      "Iteracion: 10403 Gradiente: [0.00159356056871142,-0.027493220493107866] Loss: 22.84345002793176\n",
      "Iteracion: 10404 Gradiente: [0.001592713383055866,-0.027478604253192648] Loss: 22.84344926971675\n",
      "Iteracion: 10405 Gradiente: [0.001591866647708192,-0.02746399578372435] Loss: 22.843448512307702\n",
      "Iteracion: 10406 Gradiente: [0.0015910203625850272,-0.027449395080562173] Loss: 22.84344775570377\n",
      "Iteracion: 10407 Gradiente: [0.00159017452731689,-0.027434802139584966] Loss: 22.843446999904092\n",
      "Iteracion: 10408 Gradiente: [0.0015893291416555636,-0.027420216956666483] Loss: 22.843446244907806\n",
      "Iteracion: 10409 Gradiente: [0.001588484205529994,-0.027405639527673387] Loss: 22.843445490714092\n",
      "Iteracion: 10410 Gradiente: [0.0015876397185991208,-0.027391069848488787] Loss: 22.84344473732203\n",
      "Iteracion: 10411 Gradiente: [0.0015867956805692529,-0.027376507914996396] Loss: 22.84344398473083\n",
      "Iteracion: 10412 Gradiente: [0.0015859520912490173,-0.027361953723074706] Loss: 22.843443232939617\n",
      "Iteracion: 10413 Gradiente: [0.001585108950463147,-0.027347407268605295] Loss: 22.843442481947537\n",
      "Iteracion: 10414 Gradiente: [0.001584266257877213,-0.027332868547479332] Loss: 22.84344173175375\n",
      "Iteracion: 10415 Gradiente: [0.0015834240132193145,-0.027318337555589402] Loss: 22.8434409823574\n",
      "Iteracion: 10416 Gradiente: [0.0015825822164155549,-0.027303814288814957] Loss: 22.843440233757647\n",
      "Iteracion: 10417 Gradiente: [0.0015817408671741382,-0.02728929874305225] Loss: 22.843439485953635\n",
      "Iteracion: 10418 Gradiente: [0.0015808999651852674,-0.027274790914201694] Loss: 22.84343873894451\n",
      "Iteracion: 10419 Gradiente: [0.001580059510277465,-0.027260290798156107] Loss: 22.84343799272947\n",
      "Iteracion: 10420 Gradiente: [0.0015792195020850387,-0.027245798390822765] Loss: 22.84343724730762\n",
      "Iteracion: 10421 Gradiente: [0.0015783799405340916,-0.027231313688094616] Loss: 22.843436502678134\n",
      "Iteracion: 10422 Gradiente: [0.0015775408253053532,-0.02721683668587976] Loss: 22.843435758840187\n",
      "Iteracion: 10423 Gradiente: [0.001576702156232083,-0.02720236738007967] Loss: 22.843435015792924\n",
      "Iteracion: 10424 Gradiente: [0.0015758639330404852,-0.02718790576660576] Loss: 22.843434273535497\n",
      "Iteracion: 10425 Gradiente: [0.001575026155326024,-0.02717345184137881] Loss: 22.843433532067078\n",
      "Iteracion: 10426 Gradiente: [0.0015741888230252244,-0.02715900560030017] Loss: 22.843432791386817\n",
      "Iteracion: 10427 Gradiente: [0.0015733519359542925,-0.02714456703928197] Loss: 22.84343205149389\n",
      "Iteracion: 10428 Gradiente: [0.0015725154938328008,-0.027130136154244748] Loss: 22.84343131238746\n",
      "Iteracion: 10429 Gradiente: [0.0015716794963045308,-0.027115712941113774] Loss: 22.843430574066666\n",
      "Iteracion: 10430 Gradiente: [0.001570843943180004,-0.02710129739580793] Loss: 22.84342983653071\n",
      "Iteracion: 10431 Gradiente: [0.0015700088343929034,-0.027086889514239824] Loss: 22.843429099778735\n",
      "Iteracion: 10432 Gradiente: [0.0015691741694212169,-0.02707248929235367] Loss: 22.8434283638099\n",
      "Iteracion: 10433 Gradiente: [0.001568339948251681,-0.02705809672606385] Loss: 22.843427628623388\n",
      "Iteracion: 10434 Gradiente: [0.0015675061705934467,-0.027043711811301326] Loss: 22.84342689421838\n",
      "Iteracion: 10435 Gradiente: [0.001566672836237141,-0.027029334543999185] Loss: 22.84342616059401\n",
      "Iteracion: 10436 Gradiente: [0.0015658399449004416,-0.027014964920094777] Loss: 22.843425427749484\n",
      "Iteracion: 10437 Gradiente: [0.001565007496361659,-0.02700060293552333] Loss: 22.843424695683947\n",
      "Iteracion: 10438 Gradiente: [0.0015641754904057354,-0.02698624858622208] Loss: 22.843423964396575\n",
      "Iteracion: 10439 Gradiente: [0.0015633439266271884,-0.02697190186814069] Loss: 22.843423233886547\n",
      "Iteracion: 10440 Gradiente: [0.0015625128050193856,-0.026957562777211307] Loss: 22.843422504153036\n",
      "Iteracion: 10441 Gradiente: [0.001561682125292426,-0.026943231309380047] Loss: 22.843421775195218\n",
      "Iteracion: 10442 Gradiente: [0.0015608518870417734,-0.026928907460605462] Loss: 22.843421047012264\n",
      "Iteracion: 10443 Gradiente: [0.0015600220902844816,-0.02691459122682171] Loss: 22.843420319603357\n",
      "Iteracion: 10444 Gradiente: [0.0015591927347031742,-0.02690028260398686] Loss: 22.84341959296767\n",
      "Iteracion: 10445 Gradiente: [0.0015583638199605807,-0.02688598158805746] Loss: 22.843418867104376\n",
      "Iteracion: 10446 Gradiente: [0.00155753534593354,-0.026871688174985663] Loss: 22.843418142012663\n",
      "Iteracion: 10447 Gradiente: [0.0015567073122326747,-0.026857402360738074] Loss: 22.8434174176917\n",
      "Iteracion: 10448 Gradiente: [0.0015558797189034598,-0.026843124141258556] Loss: 22.843416694140693\n",
      "Iteracion: 10449 Gradiente: [0.0015550525655214642,-0.026828853512519284] Loss: 22.843415971358795\n",
      "Iteracion: 10450 Gradiente: [0.0015542258517967865,-0.02681459047048982] Loss: 22.8434152493452\n",
      "Iteracion: 10451 Gradiente: [0.0015533995775996346,-0.026800335011130726] Loss: 22.843414528099093\n",
      "Iteracion: 10452 Gradiente: [0.001552573742716845,-0.026786087130407785] Loss: 22.843413807619644\n",
      "Iteracion: 10453 Gradiente: [0.0015517483468490431,-0.026771846824297778] Loss: 22.843413087906065\n",
      "Iteracion: 10454 Gradiente: [0.0015509233898200136,-0.026757614088768852] Loss: 22.843412368957512\n",
      "Iteracion: 10455 Gradiente: [0.001550098871374909,-0.026743388919798387] Loss: 22.843411650773188\n",
      "Iteracion: 10456 Gradiente: [0.0015492747912039325,-0.026729171313368265] Loss: 22.843410933352278\n",
      "Iteracion: 10457 Gradiente: [0.0015484511492095028,-0.026714961265449706] Loss: 22.843410216693986\n",
      "Iteracion: 10458 Gradiente: [0.001547627945012664,-0.026700758772034306] Loss: 22.843409500797488\n",
      "Iteracion: 10459 Gradiente: [0.0015468051785499407,-0.02668656382909352] Loss: 22.84340878566194\n",
      "Iteracion: 10460 Gradiente: [0.0015459828494044814,-0.02667237643262593] Loss: 22.8434080712866\n",
      "Iteracion: 10461 Gradiente: [0.0015451609574900734,-0.026658196578611636] Loss: 22.843407357670618\n",
      "Iteracion: 10462 Gradiente: [0.00154433950251871,-0.026644024263042236] Loss: 22.84340664481319\n",
      "Iteracion: 10463 Gradiente: [0.0015435184842222801,-0.02662985948191287] Loss: 22.843405932713512\n",
      "Iteracion: 10464 Gradiente: [0.0015426979024312005,-0.026615702231212998] Loss: 22.843405221370766\n",
      "Iteracion: 10465 Gradiente: [0.001541877756822411,-0.02660155250694878] Loss: 22.843404510784193\n",
      "Iteracion: 10466 Gradiente: [0.0015410580472774882,-0.02658741030510695] Loss: 22.843403800952935\n",
      "Iteracion: 10467 Gradiente: [0.001540238773573795,-0.026573275621691532] Loss: 22.84340309187622\n",
      "Iteracion: 10468 Gradiente: [0.0015394199352935327,-0.026559148452714846] Loss: 22.84340238355325\n",
      "Iteracion: 10469 Gradiente: [0.001538601532364699,-0.026545028794173458] Loss: 22.8434016759832\n",
      "Iteracion: 10470 Gradiente: [0.0015377835645741318,-0.02653091664207281] Loss: 22.84340096916529\n",
      "Iteracion: 10471 Gradiente: [0.0015369660316982465,-0.02651681199242167] Loss: 22.843400263098708\n",
      "Iteracion: 10472 Gradiente: [0.0015361489333325076,-0.026502714841241944] Loss: 22.84339955778266\n",
      "Iteracion: 10473 Gradiente: [0.0015353322693689127,-0.026488625184539552] Loss: 22.843398853216343\n",
      "Iteracion: 10474 Gradiente: [0.0015345160396568265,-0.026474543018326694] Loss: 22.843398149398958\n",
      "Iteracion: 10475 Gradiente: [0.0015337002438836104,-0.026460468338625396] Loss: 22.84339744632974\n",
      "Iteracion: 10476 Gradiente: [0.001532884881757468,-0.02644640114145898] Loss: 22.84339674400786\n",
      "Iteracion: 10477 Gradiente: [0.0015320699531163958,-0.026432341422844986] Loss: 22.843396042432524\n",
      "Iteracion: 10478 Gradiente: [0.0015312554576837555,-0.026418289178810284] Loss: 22.843395341602953\n",
      "Iteracion: 10479 Gradiente: [0.0015304413952293317,-0.0264042444053833] Loss: 22.843394641518355\n",
      "Iteracion: 10480 Gradiente: [0.0015296277656403844,-0.026390207098583574] Loss: 22.843393942177933\n",
      "Iteracion: 10481 Gradiente: [0.0015288145685948015,-0.026376177254446513] Loss: 22.843393243580874\n",
      "Iteracion: 10482 Gradiente: [0.0015280018038708932,-0.026362154869005987] Loss: 22.843392545726424\n",
      "Iteracion: 10483 Gradiente: [0.0015271894711437048,-0.02634813993830131] Loss: 22.843391848613784\n",
      "Iteracion: 10484 Gradiente: [0.0015263775703734457,-0.02633413245835854] Loss: 22.843391152242145\n",
      "Iteracion: 10485 Gradiente: [0.0015255661012361088,-0.026320132425221486] Loss: 22.843390456610756\n",
      "Iteracion: 10486 Gradiente: [0.0015247550634038968,-0.026306139834937882] Loss: 22.843389761718775\n",
      "Iteracion: 10487 Gradiente: [0.001523944456831335,-0.02629215468353969] Loss: 22.84338906756548\n",
      "Iteracion: 10488 Gradiente: [0.0015231342811830473,-0.026278176967078556] Loss: 22.84338837415004\n",
      "Iteracion: 10489 Gradiente: [0.001522324536273345,-0.02626420668159817] Loss: 22.843387681471693\n",
      "Iteracion: 10490 Gradiente: [0.0015215152218236957,-0.026250243823152532] Loss: 22.843386989529638\n",
      "Iteracion: 10491 Gradiente: [0.001520706337644621,-0.02623628838778996] Loss: 22.843386298323125\n",
      "Iteracion: 10492 Gradiente: [0.0015198978834016922,-0.026222340371570497] Loss: 22.843385607851328\n",
      "Iteracion: 10493 Gradiente: [0.0015190898590882777,-0.02620839977053701] Loss: 22.843384918113504\n",
      "Iteracion: 10494 Gradiente: [0.0015182822643026838,-0.02619446658075759] Loss: 22.84338422910884\n",
      "Iteracion: 10495 Gradiente: [0.001517475098805221,-0.02618054079829264] Loss: 22.84338354083659\n",
      "Iteracion: 10496 Gradiente: [0.0015166683624471488,-0.026166622419199707] Loss: 22.84338285329593\n",
      "Iteracion: 10497 Gradiente: [0.0015158620549859355,-0.026152711439543806] Loss: 22.843382166486148\n",
      "Iteracion: 10498 Gradiente: [0.0015150561761553643,-0.026138807855393152] Loss: 22.8433814804064\n",
      "Iteracion: 10499 Gradiente: [0.0015142507258862755,-0.026124911662806956] Loss: 22.84338079505597\n",
      "Iteracion: 10500 Gradiente: [0.0015134457036926582,-0.026111022857869887] Loss: 22.843380110434033\n",
      "Iteracion: 10501 Gradiente: [0.0015126411095309322,-0.02609714143664566] Loss: 22.843379426539837\n",
      "Iteracion: 10502 Gradiente: [0.001511836943066669,-0.026083267395213265] Loss: 22.843378743372604\n",
      "Iteracion: 10503 Gradiente: [0.0015110332041492333,-0.02606940072964624] Loss: 22.843378060931578\n",
      "Iteracion: 10504 Gradiente: [0.0015102298925010397,-0.0260555414360257] Loss: 22.84337737921596\n",
      "Iteracion: 10505 Gradiente: [0.0015094270079724008,-0.026041689510427683] Loss: 22.84337669822498\n",
      "Iteracion: 10506 Gradiente: [0.0015086245501625703,-0.02602784494894384] Loss: 22.843376017957898\n",
      "Iteracion: 10507 Gradiente: [0.0015078225190838642,-0.02601400774764855] Loss: 22.84337533841392\n",
      "Iteracion: 10508 Gradiente: [0.0015070209143469052,-0.026000177902635948] Loss: 22.84337465959227\n",
      "Iteracion: 10509 Gradiente: [0.0015062197357688472,-0.025986355409993677] Loss: 22.843373981492213\n",
      "Iteracion: 10510 Gradiente: [0.0015054189830626303,-0.02597254026581576] Loss: 22.84337330411295\n",
      "Iteracion: 10511 Gradiente: [0.0015046186561950966,-0.025958732466186164] Loss: 22.84337262745373\n",
      "Iteracion: 10512 Gradiente: [0.0015038187547929739,-0.025944932007206253] Loss: 22.843371951513795\n",
      "Iteracion: 10513 Gradiente: [0.0015030192785275176,-0.02593113888498024] Loss: 22.843371276292356\n",
      "Iteracion: 10514 Gradiente: [0.0015022202272689355,-0.02591735309560394] Loss: 22.843370601788664\n",
      "Iteracion: 10515 Gradiente: [0.0015014216008694347,-0.025903574635172313] Loss: 22.84336992800196\n",
      "Iteracion: 10516 Gradiente: [0.0015006233990845885,-0.02588980349979136] Loss: 22.843369254931478\n",
      "Iteracion: 10517 Gradiente: [0.0014998256216169163,-0.025876039685569077] Loss: 22.84336858257645\n",
      "Iteracion: 10518 Gradiente: [0.0014990282682854664,-0.02586228318861347] Loss: 22.843367910936106\n",
      "Iteracion: 10519 Gradiente: [0.0014982313388704446,-0.025848534005032182] Loss: 22.843367240009727\n",
      "Iteracion: 10520 Gradiente: [0.0014974348331700563,-0.025834792130937128] Loss: 22.843366569796515\n",
      "Iteracion: 10521 Gradiente: [0.0014966387507996614,-0.02582105756245028] Loss: 22.843365900295744\n",
      "Iteracion: 10522 Gradiente: [0.0014958430917469438,-0.025807330295675376] Loss: 22.843365231506613\n",
      "Iteracion: 10523 Gradiente: [0.0014950478555647352,-0.025793610326745882] Loss: 22.843364563428402\n",
      "Iteracion: 10524 Gradiente: [0.0014942530422833518,-0.02577989765176388] Loss: 22.843363896060346\n",
      "Iteracion: 10525 Gradiente: [0.0014934586515029955,-0.02576619226686496] Loss: 22.8433632294017\n",
      "Iteracion: 10526 Gradiente: [0.0014926646830749253,-0.0257524941681672] Loss: 22.84336256345167\n",
      "Iteracion: 10527 Gradiente: [0.0014918711366902926,-0.025738803351802512] Loss: 22.84336189820956\n",
      "Iteracion: 10528 Gradiente: [0.001491078012220252,-0.025725119813893353] Loss: 22.843361233674564\n",
      "Iteracion: 10529 Gradiente: [0.0014902853094326928,-0.02571144355057129] Loss: 22.84336056984596\n",
      "Iteracion: 10530 Gradiente: [0.0014894930279353957,-0.025697774557977126] Loss: 22.843359906723\n",
      "Iteracion: 10531 Gradiente: [0.0014887011676734119,-0.02568411283223829] Loss: 22.843359244304914\n",
      "Iteracion: 10532 Gradiente: [0.001487909728425052,-0.025670458369489068] Loss: 22.843358582590962\n",
      "Iteracion: 10533 Gradiente: [0.001487118710010312,-0.025656811165867665] Loss: 22.843357921580427\n",
      "Iteracion: 10534 Gradiente: [0.0014863281120824467,-0.025643171217520213] Loss: 22.8433572612725\n",
      "Iteracion: 10535 Gradiente: [0.001485537934361029,-0.025629538520595928] Loss: 22.843356601666482\n",
      "Iteracion: 10536 Gradiente: [0.0014847481767484775,-0.025615913071228034] Loss: 22.843355942761605\n",
      "Iteracion: 10537 Gradiente: [0.0014839588390240502,-0.02560229486556646] Loss: 22.843355284557145\n",
      "Iteracion: 10538 Gradiente: [0.001483169921070271,-0.025588683899753425] Loss: 22.843354627052324\n",
      "Iteracion: 10539 Gradiente: [0.0014823814223338635,-0.025575080169958634] Loss: 22.84335397024642\n",
      "Iteracion: 10540 Gradiente: [0.0014815933429161987,-0.02556148367231721] Loss: 22.843353314138692\n",
      "Iteracion: 10541 Gradiente: [0.001480805682451584,-0.025547894402990916] Loss: 22.843352658728392\n",
      "Iteracion: 10542 Gradiente: [0.001480018440648223,-0.025534312358142942] Loss: 22.84335200401478\n",
      "Iteracion: 10543 Gradiente: [0.0014792316174170614,-0.025520737533922973] Loss: 22.843351349997096\n",
      "Iteracion: 10544 Gradiente: [0.0014784452124947242,-0.025507169926495977] Loss: 22.84335069667464\n",
      "Iteracion: 10545 Gradiente: [0.0014776592256358376,-0.02549360953202703] Loss: 22.843350044046662\n",
      "Iteracion: 10546 Gradiente: [0.001476873656640502,-0.02548005634667933] Loss: 22.843349392112383\n",
      "Iteracion: 10547 Gradiente: [0.0014760885052917652,-0.025466510366618778] Loss: 22.843348740871104\n",
      "Iteracion: 10548 Gradiente: [0.0014753037713774119,-0.025452971588017213] Loss: 22.843348090322106\n",
      "Iteracion: 10549 Gradiente: [0.0014745194545999615,-0.025439440007048002] Loss: 22.8433474404646\n",
      "Iteracion: 10550 Gradiente: [0.0014737355548049892,-0.02542591561988227] Loss: 22.84334679129789\n",
      "Iteracion: 10551 Gradiente: [0.001472952071784069,-0.025412398422694573] Loss: 22.843346142821236\n",
      "Iteracion: 10552 Gradiente: [0.001472169005150666,-0.025398888411670838] Loss: 22.843345495033887\n",
      "Iteracion: 10553 Gradiente: [0.0014713863549350966,-0.025385385582978516] Loss: 22.84334484793514\n",
      "Iteracion: 10554 Gradiente: [0.0014706041207792472,-0.025371889932803645] Loss: 22.843344201524232\n",
      "Iteracion: 10555 Gradiente: [0.0014698223025220614,-0.02535840145732943] Loss: 22.843343555800445\n",
      "Iteracion: 10556 Gradiente: [0.0014690408998433214,-0.02534492015274618] Loss: 22.84334291076304\n",
      "Iteracion: 10557 Gradiente: [0.0014682599126009184,-0.0253314460152378] Loss: 22.8433422664113\n",
      "Iteracion: 10558 Gradiente: [0.0014674793405390573,-0.025317979040994844] Loss: 22.843341622744497\n",
      "Iteracion: 10559 Gradiente: [0.001466699183547841,-0.0253045192262037] Loss: 22.8433409797619\n",
      "Iteracion: 10560 Gradiente: [0.001465919441288103,-0.025291066567062852] Loss: 22.843340337462784\n",
      "Iteracion: 10561 Gradiente: [0.0014651401134405735,-0.025277621059777645] Loss: 22.843339695846417\n",
      "Iteracion: 10562 Gradiente: [0.001464361199955988,-0.025264182700533412] Loss: 22.843339054912068\n",
      "Iteracion: 10563 Gradiente: [0.0014635827006334997,-0.025250751485529695] Loss: 22.843338414659005\n",
      "Iteracion: 10564 Gradiente: [0.0014628046151547855,-0.025237327410973027] Loss: 22.843337775086546\n",
      "Iteracion: 10565 Gradiente: [0.0014620269432782606,-0.025223910473071123] Loss: 22.843337136193927\n",
      "Iteracion: 10566 Gradiente: [0.0014612496849167656,-0.025210500668018554] Loss: 22.84333649798044\n",
      "Iteracion: 10567 Gradiente: [0.0014604728397320818,-0.025197097992033454] Loss: 22.843335860445357\n",
      "Iteracion: 10568 Gradiente: [0.0014596964075110463,-0.025183702441323418] Loss: 22.843335223587967\n",
      "Iteracion: 10569 Gradiente: [0.0014589203880944978,-0.02517031401209865] Loss: 22.84333458740753\n",
      "Iteracion: 10570 Gradiente: [0.0014581447812664313,-0.025156932700570185] Loss: 22.843333951903343\n",
      "Iteracion: 10571 Gradiente: [0.0014573695866573644,-0.02514355850296598] Loss: 22.843333317074684\n",
      "Iteracion: 10572 Gradiente: [0.001456594804346878,-0.025130191415485115] Loss: 22.843332682920842\n",
      "Iteracion: 10573 Gradiente: [0.0014558204338148547,-0.025116831434363482] Loss: 22.843332049441084\n",
      "Iteracion: 10574 Gradiente: [0.001455046475035715,-0.025103478555816138] Loss: 22.843331416634708\n",
      "Iteracion: 10575 Gradiente: [0.001454272927621029,-0.025090132776072357] Loss: 22.84333078450099\n",
      "Iteracion: 10576 Gradiente: [0.001453499791477005,-0.025076794091354295] Loss: 22.843330153039208\n",
      "Iteracion: 10577 Gradiente: [0.0014527270664302706,-0.025063462497885068] Loss: 22.843329522248663\n",
      "Iteracion: 10578 Gradiente: [0.0014519547521151329,-0.025050137991904364] Loss: 22.843328892128643\n",
      "Iteracion: 10579 Gradiente: [0.0014511828483459036,-0.02503682056964275] Loss: 22.84332826267841\n",
      "Iteracion: 10580 Gradiente: [0.0014504113550316334,-0.025023510227328197] Loss: 22.843327633897278\n",
      "Iteracion: 10581 Gradiente: [0.0014496402718018923,-0.025010206961202406] Loss: 22.843327005784527\n",
      "Iteracion: 10582 Gradiente: [0.0014488695985524676,-0.024996910767497955] Loss: 22.84332637833944\n",
      "Iteracion: 10583 Gradiente: [0.0014480993350285112,-0.024983621642458567] Loss: 22.843325751561334\n",
      "Iteracion: 10584 Gradiente: [0.0014473294809249637,-0.024970339582329852] Loss: 22.84332512544946\n",
      "Iteracion: 10585 Gradiente: [0.0014465600361508754,-0.024957064583347944] Loss: 22.84332450000313\n",
      "Iteracion: 10586 Gradiente: [0.0014457910004210817,-0.024943796641765204] Loss: 22.843323875221646\n",
      "Iteracion: 10587 Gradiente: [0.0014450223735574734,-0.02493053575382582] Loss: 22.843323251104284\n",
      "Iteracion: 10588 Gradiente: [0.0014442541553080445,-0.02491728191578275] Loss: 22.843322627650352\n",
      "Iteracion: 10589 Gradiente: [0.0014434863454492112,-0.024904035123886458] Loss: 22.843322004859136\n",
      "Iteracion: 10590 Gradiente: [0.0014427189439212877,-0.02489079537438421] Loss: 22.843321382729936\n",
      "Iteracion: 10591 Gradiente: [0.0014419519502041566,-0.024877562663548503] Loss: 22.843320761262042\n",
      "Iteracion: 10592 Gradiente: [0.0014411853642665544,-0.024864336987627193] Loss: 22.843320140454757\n",
      "Iteracion: 10593 Gradiente: [0.0014404191859388978,-0.024851118342877142] Loss: 22.843319520307375\n",
      "Iteracion: 10594 Gradiente: [0.0014396534148535995,-0.02483790672556777] Loss: 22.843318900819206\n",
      "Iteracion: 10595 Gradiente: [0.001438888050848656,-0.02482470213196303] Loss: 22.843318281989536\n",
      "Iteracion: 10596 Gradiente: [0.0014381230937819585,-0.024811504558321453] Loss: 22.843317663817654\n",
      "Iteracion: 10597 Gradiente: [0.0014373585434119226,-0.024798314000914465] Loss: 22.843317046302886\n",
      "Iteracion: 10598 Gradiente: [0.0014365943994789632,-0.024785130456013486] Loss: 22.843316429444524\n",
      "Iteracion: 10599 Gradiente: [0.001435830661865604,-0.024771953919886278] Loss: 22.843315813241873\n",
      "Iteracion: 10600 Gradiente: [0.001435067330144572,-0.024758784388816936] Loss: 22.84331519769422\n",
      "Iteracion: 10601 Gradiente: [0.0014343044043092354,-0.02474562185907049] Loss: 22.843314582800883\n",
      "Iteracion: 10602 Gradiente: [0.0014335418841019039,-0.02473246632692761] Loss: 22.843313968561183\n",
      "Iteracion: 10603 Gradiente: [0.0014327797692449925,-0.024719317788669908] Loss: 22.843313354974384\n",
      "Iteracion: 10604 Gradiente: [0.001432018059517759,-0.02470617624058112] Loss: 22.84331274203983\n",
      "Iteracion: 10605 Gradiente: [0.0014312567547960953,-0.024693041678938717] Loss: 22.843312129756804\n",
      "Iteracion: 10606 Gradiente: [0.001430495854743678,-0.024679914100037573] Loss: 22.843311518124622\n",
      "Iteracion: 10607 Gradiente: [0.0014297353592506094,-0.02466679350015705] Loss: 22.843310907142598\n",
      "Iteracion: 10608 Gradiente: [0.0014289752680677263,-0.024653679875589772] Loss: 22.84331029681003\n",
      "Iteracion: 10609 Gradiente: [0.0014282155809733391,-0.024640573222627183] Loss: 22.843309687126244\n",
      "Iteracion: 10610 Gradiente: [0.0014274562977211265,-0.02462747353756652] Loss: 22.84330907809053\n",
      "Iteracion: 10611 Gradiente: [0.0014266974180903466,-0.024614380816702426] Loss: 22.84330846970221\n",
      "Iteracion: 10612 Gradiente: [0.0014259389419900496,-0.024601295056324683] Loss: 22.84330786196058\n",
      "Iteracion: 10613 Gradiente: [0.0014251808690384374,-0.024588216252743916] Loss: 22.843307254864975\n",
      "Iteracion: 10614 Gradiente: [0.0014244231991180337,-0.02457514440225642] Loss: 22.843306648414718\n",
      "Iteracion: 10615 Gradiente: [0.001423665932055466,-0.02456207950116216] Loss: 22.843306042609086\n",
      "Iteracion: 10616 Gradiente: [0.0014229090675314637,-0.02454902154577283] Loss: 22.843305437447402\n",
      "Iteracion: 10617 Gradiente: [0.0014221526054560248,-0.024535970532388873] Loss: 22.84330483292901\n",
      "Iteracion: 10618 Gradiente: [0.0014213965454501932,-0.02452292645733015] Loss: 22.843304229053214\n",
      "Iteracion: 10619 Gradiente: [0.001420640887456178,-0.024509889316897217] Loss: 22.843303625819303\n",
      "Iteracion: 10620 Gradiente: [0.0014198856311547084,-0.024496859107412068] Loss: 22.843303023226632\n",
      "Iteracion: 10621 Gradiente: [0.0014191307763998869,-0.024483835825184267] Loss: 22.84330242127452\n",
      "Iteracion: 10622 Gradiente: [0.0014183763228307574,-0.024470819466539596] Loss: 22.843301819962235\n",
      "Iteracion: 10623 Gradiente: [0.001417622270417951,-0.02445781002778619] Loss: 22.84330121928916\n",
      "Iteracion: 10624 Gradiente: [0.0014168686189170406,-0.024444807505247943] Loss: 22.84330061925456\n",
      "Iteracion: 10625 Gradiente: [0.0014161153680750734,-0.024431811895250042] Loss: 22.8433000198578\n",
      "Iteracion: 10626 Gradiente: [0.001415362517651412,-0.024418823194119928] Loss: 22.843299421098195\n",
      "Iteracion: 10627 Gradiente: [0.0014146100675323698,-0.024405841398177468] Loss: 22.84329882297505\n",
      "Iteracion: 10628 Gradiente: [0.0014138580174090976,-0.024392866503758624] Loss: 22.843298225487697\n",
      "Iteracion: 10629 Gradiente: [0.0014131063670930644,-0.024379898507191435] Loss: 22.843297628635458\n",
      "Iteracion: 10630 Gradiente: [0.0014123551164364776,-0.024366937404807013] Loss: 22.843297032417674\n",
      "Iteracion: 10631 Gradiente: [0.0014116042650366958,-0.024353983192948667] Loss: 22.843296436833647\n",
      "Iteracion: 10632 Gradiente: [0.0014108538128804564,-0.024341035867943953] Loss: 22.843295841882703\n",
      "Iteracion: 10633 Gradiente: [0.0014101037597024893,-0.024328095426133227] Loss: 22.84329524756419\n",
      "Iteracion: 10634 Gradiente: [0.0014093541052166833,-0.02431516186386157] Loss: 22.84329465387742\n",
      "Iteracion: 10635 Gradiente: [0.0014086048493472465,-0.024302235177464954] Loss: 22.843294060821737\n",
      "Iteracion: 10636 Gradiente: [0.0014078559917019599,-0.02428931536329711] Loss: 22.843293468396443\n",
      "Iteracion: 10637 Gradiente: [0.001407107532237243,-0.02427640241769685] Loss: 22.8432928766009\n",
      "Iteracion: 10638 Gradiente: [0.0014063594706925642,-0.02426349633701257] Loss: 22.84329228543442\n",
      "Iteracion: 10639 Gradiente: [0.0014056118068869713,-0.024250597117593385] Loss: 22.843291694896326\n",
      "Iteracion: 10640 Gradiente: [0.0014048645404784565,-0.024237704755800848] Loss: 22.84329110498597\n",
      "Iteracion: 10641 Gradiente: [0.0014041176713533333,-0.02422481924798383] Loss: 22.843290515702684\n",
      "Iteracion: 10642 Gradiente: [0.0014033711993647557,-0.024211940590492868] Loss: 22.843289927045785\n",
      "Iteracion: 10643 Gradiente: [0.001402625124099662,-0.02419906877969768] Loss: 22.84328933901462\n",
      "Iteracion: 10644 Gradiente: [0.0014018794455608941,-0.024186203811948914] Loss: 22.84328875160851\n",
      "Iteracion: 10645 Gradiente: [0.0014011341633789697,-0.024173345683614993] Loss: 22.843288164826806\n",
      "Iteracion: 10646 Gradiente: [0.0014003892774856772,-0.024160494391051893] Loss: 22.843287578668836\n",
      "Iteracion: 10647 Gradiente: [0.0013996447875892196,-0.024147649930631114] Loss: 22.843286993133937\n",
      "Iteracion: 10648 Gradiente: [0.001398900693448013,-0.024134812298721896] Loss: 22.843286408221456\n",
      "Iteracion: 10649 Gradiente: [0.0013981569948631053,-0.024121981491694436] Loss: 22.843285823930742\n",
      "Iteracion: 10650 Gradiente: [0.0013974136916971246,-0.024109157505914424] Loss: 22.84328524026109\n",
      "Iteracion: 10651 Gradiente: [0.0013966707836999604,-0.02409634033775857] Loss: 22.84328465721187\n",
      "Iteracion: 10652 Gradiente: [0.0013959282706499229,-0.02408352998360274] Loss: 22.843284074782424\n",
      "Iteracion: 10653 Gradiente: [0.0013951861522713215,-0.024070726439830045] Loss: 22.8432834929721\n",
      "Iteracion: 10654 Gradiente: [0.0013944444285452088,-0.02405792970280641] Loss: 22.8432829117802\n",
      "Iteracion: 10655 Gradiente: [0.001393703099096418,-0.02404513976892216] Loss: 22.843282331206115\n",
      "Iteracion: 10656 Gradiente: [0.0013929621637193653,-0.024032356634562883] Loss: 22.843281751249165\n",
      "Iteracion: 10657 Gradiente: [0.0013922216223439439,-0.024019580296104927] Loss: 22.843281171908686\n",
      "Iteracion: 10658 Gradiente: [0.0013914814745220384,-0.024006810749948097] Loss: 22.843280593184044\n",
      "Iteracion: 10659 Gradiente: [0.0013907417202934389,-0.02399404799246826] Loss: 22.84328001507458\n",
      "Iteracion: 10660 Gradiente: [0.0013900023593104531,-0.02398129202006345] Loss: 22.843279437579607\n",
      "Iteracion: 10661 Gradiente: [0.0013892633914508678,-0.023968542829122086] Loss: 22.84327886069854\n",
      "Iteracion: 10662 Gradiente: [0.0013885248163433062,-0.02395580041604776] Loss: 22.84327828443065\n",
      "Iteracion: 10663 Gradiente: [0.0013877866339546093,-0.02394306477722855] Loss: 22.84327770877532\n",
      "Iteracion: 10664 Gradiente: [0.001387048843901084,-0.023930335909070285] Loss: 22.843277133731927\n",
      "Iteracion: 10665 Gradiente: [0.0013863114461012552,-0.0239176138079691] Loss: 22.843276559299778\n",
      "Iteracion: 10666 Gradiente: [0.0013855744404234352,-0.023904898470320526] Loss: 22.84327598547824\n",
      "Iteracion: 10667 Gradiente: [0.0013848378265038264,-0.02389218989254059] Loss: 22.843275412266664\n",
      "Iteracion: 10668 Gradiente: [0.0013841016042600055,-0.023879488071026127] Loss: 22.8432748396644\n",
      "Iteracion: 10669 Gradiente: [0.00138336577339165,-0.023866793002190197] Loss: 22.843274267670783\n",
      "Iteracion: 10670 Gradiente: [0.0013826303336145428,-0.023854104682449064] Loss: 22.843273696285205\n",
      "Iteracion: 10671 Gradiente: [0.001381895284911631,-0.023841423108202874] Loss: 22.843273125506993\n",
      "Iteracion: 10672 Gradiente: [0.0013811606270328033,-0.0238287482758673] Loss: 22.843272555335513\n",
      "Iteracion: 10673 Gradiente: [0.00138042635965121,-0.023816080181866064] Loss: 22.843271985770105\n",
      "Iteracion: 10674 Gradiente: [0.0013796924826010581,-0.023803418822614237] Loss: 22.843271416810115\n",
      "Iteracion: 10675 Gradiente: [0.001378958995657816,-0.023790764194531987] Loss: 22.84327084845494\n",
      "Iteracion: 10676 Gradiente: [0.0013782258987400078,-0.0237781162940338] Loss: 22.843270280703916\n",
      "Iteracion: 10677 Gradiente: [0.0013774931915908914,-0.02376547511754623] Loss: 22.843269713556392\n",
      "Iteracion: 10678 Gradiente: [0.001376760873926249,-0.023752840661498454] Loss: 22.84326914701173\n",
      "Iteracion: 10679 Gradiente: [0.0013760289455651295,-0.023740212922315842] Loss: 22.84326858106929\n",
      "Iteracion: 10680 Gradiente: [0.0013752974063910037,-0.02372759189642295] Loss: 22.843268015728437\n",
      "Iteracion: 10681 Gradiente: [0.0013745662560637583,-0.02371497758025877] Loss: 22.843267450988545\n",
      "Iteracion: 10682 Gradiente: [0.0013738354943541252,-0.023702369970257327] Loss: 22.843266886848948\n",
      "Iteracion: 10683 Gradiente: [0.0013731051213795808,-0.023689769062834168] Loss: 22.843266323309017\n",
      "Iteracion: 10684 Gradiente: [0.0013723751364769518,-0.02367717485445316] Loss: 22.843265760368116\n",
      "Iteracion: 10685 Gradiente: [0.0013716455397466613,-0.023664587341534708] Loss: 22.843265198025616\n",
      "Iteracion: 10686 Gradiente: [0.0013709163308798604,-0.023652006520524675] Loss: 22.84326463628086\n",
      "Iteracion: 10687 Gradiente: [0.0013701875095989634,-0.023639432387870467] Loss: 22.843264075133238\n",
      "Iteracion: 10688 Gradiente: [0.001369459075909655,-0.02362686494000291] Loss: 22.843263514582112\n",
      "Iteracion: 10689 Gradiente: [0.0013687310294320317,-0.023614304173380417] Loss: 22.843262954626834\n",
      "Iteracion: 10690 Gradiente: [0.0013680033700183003,-0.02360175008444578] Loss: 22.84326239526677\n",
      "Iteracion: 10691 Gradiente: [0.001367276097481825,-0.02358920266964771] Loss: 22.843261836501295\n",
      "Iteracion: 10692 Gradiente: [0.001366549211531757,-0.023576661925444022] Loss: 22.843261278329784\n",
      "Iteracion: 10693 Gradiente: [0.0013658227119644076,-0.023564127848287222] Loss: 22.84326072075158\n",
      "Iteracion: 10694 Gradiente: [0.0013650965987096696,-0.02355160043462708] Loss: 22.843260163766093\n",
      "Iteracion: 10695 Gradiente: [0.0013643708714672206,-0.023539079680923437] Loss: 22.84325960737265\n",
      "Iteracion: 10696 Gradiente: [0.0013636455301432686,-0.023526565583631864] Loss: 22.84325905157066\n",
      "Iteracion: 10697 Gradiente: [0.0013629205742669607,-0.023514058139229375] Loss: 22.84325849635946\n",
      "Iteracion: 10698 Gradiente: [0.001362196003847771,-0.023501557344165154] Loss: 22.843257941738443\n",
      "Iteracion: 10699 Gradiente: [0.001361471818651694,-0.023489063194905433] Loss: 22.843257387706977\n",
      "Iteracion: 10700 Gradiente: [0.0013607480183793542,-0.023476575687923665] Loss: 22.843256834264427\n",
      "Iteracion: 10701 Gradiente: [0.0013600246028943274,-0.023464094819684315] Loss: 22.843256281410184\n",
      "Iteracion: 10702 Gradiente: [0.0013593015721056645,-0.023451620586651126] Loss: 22.84325572914362\n",
      "Iteracion: 10703 Gradiente: [0.0013585789256618833,-0.023439152985305373] Loss: 22.843255177464094\n",
      "Iteracion: 10704 Gradiente: [0.0013578566634350863,-0.023426692012117202] Loss: 22.84325462637098\n",
      "Iteracion: 10705 Gradiente: [0.0013571347851988472,-0.02341423766356492] Loss: 22.843254075863687\n",
      "Iteracion: 10706 Gradiente: [0.0013564132906509485,-0.02340178993613063] Loss: 22.84325352594157\n",
      "Iteracion: 10707 Gradiente: [0.0013556921797762318,-0.02338934882628436] Loss: 22.843252976603996\n",
      "Iteracion: 10708 Gradiente: [0.0013549714521133183,-0.0233769143305229] Loss: 22.843252427850363\n",
      "Iteracion: 10709 Gradiente: [0.0013542511077294724,-0.023364486445314015] Loss: 22.843251879680054\n",
      "Iteracion: 10710 Gradiente: [0.0013535311462684755,-0.023352065167152602] Loss: 22.843251332092414\n",
      "Iteracion: 10711 Gradiente: [0.0013528115675692713,-0.02333965049252361] Loss: 22.843250785086862\n",
      "Iteracion: 10712 Gradiente: [0.0013520923713798538,-0.02332724241791778] Loss: 22.843250238662765\n",
      "Iteracion: 10713 Gradiente: [0.0013513735576178001,-0.02331484093982148] Loss: 22.843249692819498\n",
      "Iteracion: 10714 Gradiente: [0.0013506551259117336,-0.023302446054737656] Loss: 22.843249147556467\n",
      "Iteracion: 10715 Gradiente: [0.0013499370761091238,-0.02329005775915564] Loss: 22.84324860287302\n",
      "Iteracion: 10716 Gradiente: [0.001349219408197655,-0.023277676049563923] Loss: 22.84324805876857\n",
      "Iteracion: 10717 Gradiente: [0.0013485021217510015,-0.023265300922472795] Loss: 22.843247515242503\n",
      "Iteracion: 10718 Gradiente: [0.001347785216659266,-0.023252932374376912] Loss: 22.84324697229417\n",
      "Iteracion: 10719 Gradiente: [0.0013470686926334945,-0.023240570401786207] Loss: 22.843246429922985\n",
      "Iteracion: 10720 Gradiente: [0.001346352549613054,-0.023228215001194385] Loss: 22.843245888128344\n",
      "Iteracion: 10721 Gradiente: [0.0013456367872313043,-0.02321586616911766] Loss: 22.843245346909605\n",
      "Iteracion: 10722 Gradiente: [0.0013449214053916118,-0.023203523902057672] Loss: 22.843244806266178\n",
      "Iteracion: 10723 Gradiente: [0.0013442064039281831,-0.023191188196521988] Loss: 22.843244266197434\n",
      "Iteracion: 10724 Gradiente: [0.001343491782519853,-0.023178859049030245] Loss: 22.843243726702788\n",
      "Iteracion: 10725 Gradiente: [0.0013427775410984092,-0.023166536456087055] Loss: 22.8432431877816\n",
      "Iteracion: 10726 Gradiente: [0.0013420636793057383,-0.023154220414217854] Loss: 22.843242649433282\n",
      "Iteracion: 10727 Gradiente: [0.001341350197100155,-0.023141910919929388] Loss: 22.843242111657208\n",
      "Iteracion: 10728 Gradiente: [0.0013406370940970192,-0.023129607969753132] Loss: 22.84324157445276\n",
      "Iteracion: 10729 Gradiente: [0.0013399243703285416,-0.023117311560194764] Loss: 22.84324103781938\n",
      "Iteracion: 10730 Gradiente: [0.001339212025394924,-0.023105021687789908] Loss: 22.843240501756426\n",
      "Iteracion: 10731 Gradiente: [0.0013385000591256357,-0.023092738349060938] Loss: 22.843239966263297\n",
      "Iteracion: 10732 Gradiente: [0.0013377884713927793,-0.023080461540529267] Loss: 22.84323943133937\n",
      "Iteracion: 10733 Gradiente: [0.001337077261995508,-0.02306819125872354] Loss: 22.843238896984072\n",
      "Iteracion: 10734 Gradiente: [0.0013363664306998166,-0.02305592750017643] Loss: 22.843238363196768\n",
      "Iteracion: 10735 Gradiente: [0.0013356559773342268,-0.02304367026141847] Loss: 22.84323782997688\n",
      "Iteracion: 10736 Gradiente: [0.0013349459015624158,-0.023031419538991096] Loss: 22.84323729732378\n",
      "Iteracion: 10737 Gradiente: [0.001334236203359751,-0.023019175329419165] Loss: 22.843236765236895\n",
      "Iteracion: 10738 Gradiente: [0.0013335268824514893,-0.023006937629246237] Loss: 22.84323623371559\n",
      "Iteracion: 10739 Gradiente: [0.0013328179386500475,-0.0229947064350083] Loss: 22.84323570275931\n",
      "Iteracion: 10740 Gradiente: [0.0013321093716882615,-0.02298248174325117] Loss: 22.843235172367397\n",
      "Iteracion: 10741 Gradiente: [0.001331401181499814,-0.02297026355051308] Loss: 22.843234642539294\n",
      "Iteracion: 10742 Gradiente: [0.001330693367745539,-0.022958051853343875] Loss: 22.843234113274377\n",
      "Iteracion: 10743 Gradiente: [0.0013299859303174343,-0.022945846648285813] Loss: 22.843233584572054\n",
      "Iteracion: 10744 Gradiente: [0.0013292788689445465,-0.022933647931891937] Loss: 22.843233056431735\n",
      "Iteracion: 10745 Gradiente: [0.001328572183513188,-0.022921455700707347] Loss: 22.843232528852827\n",
      "Iteracion: 10746 Gradiente: [0.0013278658737268265,-0.02290926995129065] Loss: 22.84323200183473\n",
      "Iteracion: 10747 Gradiente: [0.0013271599394888275,-0.02289709068018908] Loss: 22.843231475376818\n",
      "Iteracion: 10748 Gradiente: [0.0013264543805386589,-0.02288491788396148] Loss: 22.84323094947854\n",
      "Iteracion: 10749 Gradiente: [0.0013257491966660003,-0.02287275155916776] Loss: 22.84323042413927\n",
      "Iteracion: 10750 Gradiente: [0.001325044387705058,-0.022860591702364038] Loss: 22.84322989935843\n",
      "Iteracion: 10751 Gradiente: [0.00132433995343888,-0.022848438310114953] Loss: 22.84322937513541\n",
      "Iteracion: 10752 Gradiente: [0.0013236358936230394,-0.02283629137898468] Loss: 22.84322885146964\n",
      "Iteracion: 10753 Gradiente: [0.0013229322081883764,-0.022824150905531106] Loss: 22.843228328360507\n",
      "Iteracion: 10754 Gradiente: [0.001322228896808042,-0.02281201688632848] Loss: 22.84322780580743\n",
      "Iteracion: 10755 Gradiente: [0.0013215259593664542,-0.022799889317940844] Loss: 22.843227283809824\n",
      "Iteracion: 10756 Gradiente: [0.0013208233956352917,-0.022787768196940186] Loss: 22.843226762367085\n",
      "Iteracion: 10757 Gradiente: [0.001320121205415603,-0.022775653519899564] Loss: 22.843226241478625\n",
      "Iteracion: 10758 Gradiente: [0.0013194193884051705,-0.02276354528339842] Loss: 22.843225721143867\n",
      "Iteracion: 10759 Gradiente: [0.0013187179445414662,-0.022751443484004714] Loss: 22.843225201362205\n",
      "Iteracion: 10760 Gradiente: [0.0013180168735904849,-0.022739348118298607] Loss: 22.843224682133066\n",
      "Iteracion: 10761 Gradiente: [0.0013173161753456952,-0.022727259182861795] Loss: 22.84322416345585\n",
      "Iteracion: 10762 Gradiente: [0.001316615849669726,-0.022715176674270646] Loss: 22.843223645329985\n",
      "Iteracion: 10763 Gradiente: [0.0013159158962755175,-0.022703100589112544] Loss: 22.843223127754868\n",
      "Iteracion: 10764 Gradiente: [0.0013152163149944347,-0.022691030923973326] Loss: 22.843222610729924\n",
      "Iteracion: 10765 Gradiente: [0.001314517105631315,-0.02267896767543765] Loss: 22.843222094254575\n",
      "Iteracion: 10766 Gradiente: [0.0013138182680118387,-0.022666910840094444] Loss: 22.84322157832822\n",
      "Iteracion: 10767 Gradiente: [0.0013131198019228425,-0.02265486041453452] Loss: 22.84322106295029\n",
      "Iteracion: 10768 Gradiente: [0.0013124217070744255,-0.022642816395356154] Loss: 22.843220548120186\n",
      "Iteracion: 10769 Gradiente: [0.0013117239833936385,-0.022630778779145544] Loss: 22.843220033837344\n",
      "Iteracion: 10770 Gradiente: [0.0013110266306625817,-0.022618747562499073] Loss: 22.843219520101172\n",
      "Iteracion: 10771 Gradiente: [0.0013103296487268306,-0.022606722742014185] Loss: 22.843219006911095\n",
      "Iteracion: 10772 Gradiente: [0.0013096330372415347,-0.022594704314298043] Loss: 22.843218494266516\n",
      "Iteracion: 10773 Gradiente: [0.0013089367960532172,-0.022582692275949393] Loss: 22.843217982166877\n",
      "Iteracion: 10774 Gradiente: [0.001308240925084192,-0.02257068662356403] Loss: 22.843217470611584\n",
      "Iteracion: 10775 Gradiente: [0.0013075454240170832,-0.022558687353755737] Loss: 22.843216959600053\n",
      "Iteracion: 10776 Gradiente: [0.0013068502927704154,-0.022546694463123393] Loss: 22.84321644913175\n",
      "Iteracion: 10777 Gradiente: [0.0013061555310230233,-0.02253470794828196] Loss: 22.84321593920604\n",
      "Iteracion: 10778 Gradiente: [0.001305461138665957,-0.0225227278058379] Loss: 22.843215429822376\n",
      "Iteracion: 10779 Gradiente: [0.0013047671154159464,-0.022510754032409017] Loss: 22.843214920980174\n",
      "Iteracion: 10780 Gradiente: [0.001304073461173516,-0.02249878662460295] Loss: 22.843214412678858\n",
      "Iteracion: 10781 Gradiente: [0.0013033801756857125,-0.022486825579039287] Loss: 22.843213904917864\n",
      "Iteracion: 10782 Gradiente: [0.0013026872587715843,-0.022474870892333954] Loss: 22.843213397696577\n",
      "Iteracion: 10783 Gradiente: [0.0013019947102558642,-0.02246292256110595] Loss: 22.84321289101449\n",
      "Iteracion: 10784 Gradiente: [0.0013013025299396,-0.022450980581976764] Loss: 22.84321238487099\n",
      "Iteracion: 10785 Gradiente: [0.001300610717586892,-0.022439044951570845] Loss: 22.843211879265507\n",
      "Iteracion: 10786 Gradiente: [0.0012999192729637344,-0.022427115666516072] Loss: 22.843211374197477\n",
      "Iteracion: 10787 Gradiente: [0.0012992281960578112,-0.02241519272342837] Loss: 22.843210869666315\n",
      "Iteracion: 10788 Gradiente: [0.0012985374864503759,-0.022403276118947702] Loss: 22.843210365671453\n",
      "Iteracion: 10789 Gradiente: [0.001297847144089322,-0.02239136584969875] Loss: 22.843209862212337\n",
      "Iteracion: 10790 Gradiente: [0.0012971571686411683,-0.022379461912318634] Loss: 22.843209359288398\n",
      "Iteracion: 10791 Gradiente: [0.0012964675600822299,-0.02236756430343263] Loss: 22.843208856899036\n",
      "Iteracion: 10792 Gradiente: [0.001295778318223976,-0.02235567301967727] Loss: 22.843208355043707\n",
      "Iteracion: 10793 Gradiente: [0.0012950894426694503,-0.022343788057699498] Loss: 22.843207853721857\n",
      "Iteracion: 10794 Gradiente: [0.0012944009333798097,-0.02233190941413028] Loss: 22.843207352932886\n",
      "Iteracion: 10795 Gradiente: [0.0012937127900670476,-0.022320037085616324] Loss: 22.84320685267624\n",
      "Iteracion: 10796 Gradiente: [0.0012930250126724256,-0.0223081710687912] Loss: 22.843206352951363\n",
      "Iteracion: 10797 Gradiente: [0.001292337600914569,-0.022296311360305292] Loss: 22.84320585375768\n",
      "Iteracion: 10798 Gradiente: [0.0012916505545509456,-0.022284457956807557] Loss: 22.84320535509464\n",
      "Iteracion: 10799 Gradiente: [0.0012909638734261838,-0.022272610854944357] Loss: 22.84320485696166\n",
      "Iteracion: 10800 Gradiente: [0.0012902775574758607,-0.022260770051356595] Loss: 22.843204359358165\n",
      "Iteracion: 10801 Gradiente: [0.0012895916063210204,-0.022248935542708518] Loss: 22.84320386228364\n",
      "Iteracion: 10802 Gradiente: [0.0012889060197475526,-0.022237107325654055] Loss: 22.84320336573749\n",
      "Iteracion: 10803 Gradiente: [0.001288220797783879,-0.022225285396835423] Loss: 22.84320286971915\n",
      "Iteracion: 10804 Gradiente: [0.0012875359400320956,-0.022213469752920288] Loss: 22.84320237422808\n",
      "Iteracion: 10805 Gradiente: [0.001286851446418306,-0.022201660390560936] Loss: 22.843201879263688\n",
      "Iteracion: 10806 Gradiente: [0.0012861673166582933,-0.022189857306423734] Loss: 22.843201384825438\n",
      "Iteracion: 10807 Gradiente: [0.0012854835506573182,-0.022178060497166287] Loss: 22.843200890912765\n",
      "Iteracion: 10808 Gradiente: [0.001284800148136848,-0.02216626995945615] Loss: 22.84320039752512\n",
      "Iteracion: 10809 Gradiente: [0.0012841171089547742,-0.022154485689956736] Loss: 22.843199904661933\n",
      "Iteracion: 10810 Gradiente: [0.0012834344329424616,-0.022142707685333936] Loss: 22.843199412322647\n",
      "Iteracion: 10811 Gradiente: [0.0012827521197474804,-0.022130935942265968] Loss: 22.843198920506705\n",
      "Iteracion: 10812 Gradiente: [0.0012820701693300406,-0.022119170457417424] Loss: 22.84319842921355\n",
      "Iteracion: 10813 Gradiente: [0.0012813885814580317,-0.02210741122745965] Loss: 22.84319793844264\n",
      "Iteracion: 10814 Gradiente: [0.0012807073559940817,-0.022095658249066473] Loss: 22.843197448193397\n",
      "Iteracion: 10815 Gradiente: [0.0012800264926842904,-0.022083911518919072] Loss: 22.84319695846528\n",
      "Iteracion: 10816 Gradiente: [0.0012793459913439166,-0.022072171033693168] Loss: 22.84319646925774\n",
      "Iteracion: 10817 Gradiente: [0.0012786658517076907,-0.022060436790074196] Loss: 22.84319598057021\n",
      "Iteracion: 10818 Gradiente: [0.0012779860736562417,-0.02204870878473931] Loss: 22.843195492402153\n",
      "Iteracion: 10819 Gradiente: [0.0012773066570635667,-0.022036987014368136] Loss: 22.843195004753003\n",
      "Iteracion: 10820 Gradiente: [0.0012766276015696576,-0.022025271475654997] Loss: 22.84319451762222\n",
      "Iteracion: 10821 Gradiente: [0.0012759489071612507,-0.02201356216527787] Loss: 22.84319403100924\n",
      "Iteracion: 10822 Gradiente: [0.001275270573561708,-0.02200185907992882] Loss: 22.843193544913518\n",
      "Iteracion: 10823 Gradiente: [0.0012745926005824988,-0.021990162216300627] Loss: 22.843193059334507\n",
      "Iteracion: 10824 Gradiente: [0.0012739149880114078,-0.02197847157108477] Loss: 22.843192574271658\n",
      "Iteracion: 10825 Gradiente: [0.0012732377357518013,-0.021966787140971413] Loss: 22.843192089724432\n",
      "Iteracion: 10826 Gradiente: [0.001272560843519462,-0.02195510892265986] Loss: 22.84319160569225\n",
      "Iteracion: 10827 Gradiente: [0.0012718843110633316,-0.021943436912852596] Loss: 22.8431911221746\n",
      "Iteracion: 10828 Gradiente: [0.0012712081383530933,-0.02193177110823991] Loss: 22.843190639170903\n",
      "Iteracion: 10829 Gradiente: [0.0012705323251414786,-0.02192011150552465] Loss: 22.843190156680638\n",
      "Iteracion: 10830 Gradiente: [0.0012698568711300596,-0.021908458101417238] Loss: 22.843189674703243\n",
      "Iteracion: 10831 Gradiente: [0.0012691817761985172,-0.02189681089261827] Loss: 22.843189193238196\n",
      "Iteracion: 10832 Gradiente: [0.0012685070402577972,-0.02188516987582716] Loss: 22.843188712284924\n",
      "Iteracion: 10833 Gradiente: [0.0012678326629052587,-0.02187353504776581] Loss: 22.84318823184289\n",
      "Iteracion: 10834 Gradiente: [0.001267158644193008,-0.02186190640512843] Loss: 22.843187751911568\n",
      "Iteracion: 10835 Gradiente: [0.0012664849836861928,-0.02185028394464226] Loss: 22.84318727249039\n",
      "Iteracion: 10836 Gradiente: [0.0012658116814141826,-0.021838667663007526] Loss: 22.843186793578845\n",
      "Iteracion: 10837 Gradiente: [0.0012651387370586538,-0.02182705755694497] Loss: 22.843186315176343\n",
      "Iteracion: 10838 Gradiente: [0.0012644661505021304,-0.021815453623169983] Loss: 22.843185837282405\n",
      "Iteracion: 10839 Gradiente: [0.001263793921525765,-0.021803855858400802] Loss: 22.843185359896438\n",
      "Iteracion: 10840 Gradiente: [0.0012631220498784993,-0.02179226425936278] Loss: 22.843184883017933\n",
      "Iteracion: 10841 Gradiente: [0.0012624505354419094,-0.02178067882277297] Loss: 22.843184406646333\n",
      "Iteracion: 10842 Gradiente: [0.0012617793780075697,-0.021769099545356597] Loss: 22.84318393078111\n",
      "Iteracion: 10843 Gradiente: [0.0012611085772884203,-0.021757526423845163] Loss: 22.84318345542171\n",
      "Iteracion: 10844 Gradiente: [0.0012604381332333029,-0.021745959454957504] Loss: 22.843182980567622\n",
      "Iteracion: 10845 Gradiente: [0.0012597680456906345,-0.02173439863542145] Loss: 22.843182506218287\n",
      "Iteracion: 10846 Gradiente: [0.0012590983143165129,-0.0217228439619749] Loss: 22.843182032373175\n",
      "Iteracion: 10847 Gradiente: [0.001258428939060726,-0.021711295431344625] Loss: 22.843181559031752\n",
      "Iteracion: 10848 Gradiente: [0.0012577599195878974,-0.021699753040271236] Loss: 22.84318108619349\n",
      "Iteracion: 10849 Gradiente: [0.0012570912557900253,-0.021688216785487078] Loss: 22.843180613857825\n",
      "Iteracion: 10850 Gradiente: [0.0012564229475060529,-0.02167668666372909] Loss: 22.843180142024263\n",
      "Iteracion: 10851 Gradiente: [0.001255754994501975,-0.021665162671737428] Loss: 22.843179670692226\n",
      "Iteracion: 10852 Gradiente: [0.0012550873966451566,-0.02165364480625082] Loss: 22.843179199861233\n",
      "Iteracion: 10853 Gradiente: [0.001254420153583169,-0.021642133064023385] Loss: 22.8431787295307\n",
      "Iteracion: 10854 Gradiente: [0.0012537532653396966,-0.02163062744178603] Loss: 22.84317825970013\n",
      "Iteracion: 10855 Gradiente: [0.0012530867315897847,-0.021619127936292296] Loss: 22.843177790368976\n",
      "Iteracion: 10856 Gradiente: [0.0012524205523144853,-0.021607634544281613] Loss: 22.843177321536707\n",
      "Iteracion: 10857 Gradiente: [0.0012517547270334717,-0.021596147262521608] Loss: 22.84317685320281\n",
      "Iteracion: 10858 Gradiente: [0.0012510892558395882,-0.021584666087745556] Loss: 22.84317638536674\n",
      "Iteracion: 10859 Gradiente: [0.001250424138421143,-0.021573191016714215] Loss: 22.843175918027956\n",
      "Iteracion: 10860 Gradiente: [0.0012497593745327625,-0.021561722046188454] Loss: 22.84317545118596\n",
      "Iteracion: 10861 Gradiente: [0.0012490949641630778,-0.021550259172911743] Loss: 22.8431749848402\n",
      "Iteracion: 10862 Gradiente: [0.001248430906902816,-0.021538802393657262] Loss: 22.843174518990146\n",
      "Iteracion: 10863 Gradiente: [0.0012477672026780815,-0.021527351705177958] Loss: 22.843174053635284\n",
      "Iteracion: 10864 Gradiente: [0.0012471038513429751,-0.021515907104233515] Loss: 22.843173588775084\n",
      "Iteracion: 10865 Gradiente: [0.0012464408526398074,-0.02150446858759286] Loss: 22.843173124409038\n",
      "Iteracion: 10866 Gradiente: [0.0012457782064548913,-0.021493036152015556] Loss: 22.843172660536585\n",
      "Iteracion: 10867 Gradiente: [0.0012451159125845379,-0.021481609794269475] Loss: 22.84317219715723\n",
      "Iteracion: 10868 Gradiente: [0.0012444539706554754,-0.02147018951113514] Loss: 22.84317173427042\n",
      "Iteracion: 10869 Gradiente: [0.0012437923806762304,-0.02145877529937247] Loss: 22.843171271875676\n",
      "Iteracion: 10870 Gradiente: [0.001243131142463009,-0.021447367155751115] Loss: 22.843170809972435\n",
      "Iteracion: 10871 Gradiente: [0.0012424702557922273,-0.02143596507704982] Loss: 22.84317034856018\n",
      "Iteracion: 10872 Gradiente: [0.0012418097204952498,-0.021424569060040474] Loss: 22.843169887638407\n",
      "Iteracion: 10873 Gradiente: [0.0012411495363304917,-0.021413179101506093] Loss: 22.843169427206572\n",
      "Iteracion: 10874 Gradiente: [0.0012404897031823717,-0.02140179519822046] Loss: 22.843168967264173\n",
      "Iteracion: 10875 Gradiente: [0.001239830220775199,-0.02139041734697097] Loss: 22.843168507810685\n",
      "Iteracion: 10876 Gradiente: [0.0012391710889289697,-0.021379045544537924] Loss: 22.843168048845573\n",
      "Iteracion: 10877 Gradiente: [0.0012385123075849454,-0.021367679787699483] Loss: 22.843167590368353\n",
      "Iteracion: 10878 Gradiente: [0.0012378538763992235,-0.0213563200732505] Loss: 22.843167132378476\n",
      "Iteracion: 10879 Gradiente: [0.0012371957953054866,-0.021344966397971277] Loss: 22.843166674875416\n",
      "Iteracion: 10880 Gradiente: [0.0012365380640034119,-0.021333618758658208] Loss: 22.843166217858695\n",
      "Iteracion: 10881 Gradiente: [0.001235880682414366,-0.021322277152095618] Loss: 22.843165761327764\n",
      "Iteracion: 10882 Gradiente: [0.0012352236503280285,-0.021310941575078245] Loss: 22.843165305282124\n",
      "Iteracion: 10883 Gradiente: [0.001234566967530289,-0.021299612024401428] Loss: 22.84316484972124\n",
      "Iteracion: 10884 Gradiente: [0.0012339106338193535,-0.02128828849686452] Loss: 22.843164394644607\n",
      "Iteracion: 10885 Gradiente: [0.0012332546490265864,-0.021276970989261675] Loss: 22.843163940051724\n",
      "Iteracion: 10886 Gradiente: [0.0012325990129208246,-0.02126565949839569] Loss: 22.84316348594205\n",
      "Iteracion: 10887 Gradiente: [0.0012319437255712273,-0.021254354021056205] Loss: 22.843163032315093\n",
      "Iteracion: 10888 Gradiente: [0.0012312887864936784,-0.021243054554060354] Loss: 22.843162579170322\n",
      "Iteracion: 10889 Gradiente: [0.0012306341955697537,-0.021231761094209035] Loss: 22.84316212650724\n",
      "Iteracion: 10890 Gradiente: [0.00122997995259387,-0.021220473638311314] Loss: 22.843161674325337\n",
      "Iteracion: 10891 Gradiente: [0.0012293260575290787,-0.0212091921831662] Loss: 22.843161222624083\n",
      "Iteracion: 10892 Gradiente: [0.0012286725100561094,-0.021197916725590578] Loss: 22.84316077140297\n",
      "Iteracion: 10893 Gradiente: [0.0012280193100006425,-0.021186647262395535] Loss: 22.843160320661507\n",
      "Iteracion: 10894 Gradiente: [0.0012273664573304662,-0.021175383790387414] Loss: 22.843159870399177\n",
      "Iteracion: 10895 Gradiente: [0.0012267139515775701,-0.021164126306394474] Loss: 22.84315942061545\n",
      "Iteracion: 10896 Gradiente: [0.001226061792827219,-0.02115287480722105] Loss: 22.843158971309855\n",
      "Iteracion: 10897 Gradiente: [0.0012254099807194052,-0.021141629289692432] Loss: 22.84315852248184\n",
      "Iteracion: 10898 Gradiente: [0.0012247585151290727,-0.02113038975062705] Loss: 22.84315807413095\n",
      "Iteracion: 10899 Gradiente: [0.0012241073959747457,-0.02111915618683895] Loss: 22.843157626256623\n",
      "Iteracion: 10900 Gradiente: [0.0012234566229399964,-0.02110792859516053] Loss: 22.843157178858384\n",
      "Iteracion: 10901 Gradiente: [0.0012228061958012403,-0.021096706972419454] Loss: 22.843156731935714\n",
      "Iteracion: 10902 Gradiente: [0.001222156114447633,-0.0210854913154377] Loss: 22.843156285488117\n",
      "Iteracion: 10903 Gradiente: [0.0012215063787465396,-0.021074281621039977] Loss: 22.843155839515084\n",
      "Iteracion: 10904 Gradiente: [0.0012208569884667971,-0.021063077886059864] Loss: 22.84315539401612\n",
      "Iteracion: 10905 Gradiente: [0.001220207943493771,-0.021051880107324786] Loss: 22.843154948990694\n",
      "Iteracion: 10906 Gradiente: [0.0012195592434333473,-0.021040688281681715] Loss: 22.84315450443834\n",
      "Iteracion: 10907 Gradiente: [0.001218910888298789,-0.021029502405952034] Loss: 22.843154060358525\n",
      "Iteracion: 10908 Gradiente: [0.001218262877906303,-0.021018322476974424] Loss: 22.843153616750765\n",
      "Iteracion: 10909 Gradiente: [0.0012176152119593552,-0.02100714849159407] Loss: 22.843153173614535\n",
      "Iteracion: 10910 Gradiente: [0.0012169678903129959,-0.020995980446647404] Loss: 22.843152730949353\n",
      "Iteracion: 10911 Gradiente: [0.0012163209128345897,-0.020984818338975586] Loss: 22.843152288754734\n",
      "Iteracion: 10912 Gradiente: [0.0012156742792550782,-0.02097366216542606] Loss: 22.84315184703014\n",
      "Iteracion: 10913 Gradiente: [0.0012150279895270918,-0.02096251192283489] Loss: 22.843151405775092\n",
      "Iteracion: 10914 Gradiente: [0.0012143820433569393,-0.020951367608057343] Loss: 22.843150964989096\n",
      "Iteracion: 10915 Gradiente: [0.001213736440582617,-0.020940229217939907] Loss: 22.843150524671632\n",
      "Iteracion: 10916 Gradiente: [0.0012130911810307528,-0.02092909674933156] Loss: 22.843150084822238\n",
      "Iteracion: 10917 Gradiente: [0.0012124462644464984,-0.02091797019909135] Loss: 22.843149645440384\n",
      "Iteracion: 10918 Gradiente: [0.0012118016908213272,-0.020906849564060317] Loss: 22.843149206525577\n",
      "Iteracion: 10919 Gradiente: [0.0012111574598947072,-0.020895734841098936] Loss: 22.843148768077334\n",
      "Iteracion: 10920 Gradiente: [0.0012105135714515806,-0.02088462602706483] Loss: 22.843148330095154\n",
      "Iteracion: 10921 Gradiente: [0.001209870025282574,-0.020873523118820122] Loss: 22.843147892578536\n",
      "Iteracion: 10922 Gradiente: [0.0012092268212001045,-0.020862426113224804] Loss: 22.843147455526992\n",
      "Iteracion: 10923 Gradiente: [0.0012085839591568022,-0.020851335007132595] Loss: 22.843147018940012\n",
      "Iteracion: 10924 Gradiente: [0.0012079414388049751,-0.020840249797416513] Loss: 22.84314658281713\n",
      "Iteracion: 10925 Gradiente: [0.0012072992600536737,-0.020829170480938804] Loss: 22.843146147157835\n",
      "Iteracion: 10926 Gradiente: [0.0012066574226698398,-0.020818097054566446] Loss: 22.84314571196163\n",
      "Iteracion: 10927 Gradiente: [0.0012060159265426288,-0.020807029515166064] Loss: 22.84314527722802\n",
      "Iteracion: 10928 Gradiente: [0.001205374771519511,-0.020795967859606298] Loss: 22.843144842956537\n",
      "Iteracion: 10929 Gradiente: [0.0012047339572584784,-0.020784912084766915] Loss: 22.843144409146678\n",
      "Iteracion: 10930 Gradiente: [0.0012040934837159512,-0.020773862187513596] Loss: 22.843143975797943\n",
      "Iteracion: 10931 Gradiente: [0.0012034533506020276,-0.020762818164729305] Loss: 22.843143542909868\n",
      "Iteracion: 10932 Gradiente: [0.00120281355794134,-0.02075178001327842] Loss: 22.843143110481922\n",
      "Iteracion: 10933 Gradiente: [0.0012021741053966177,-0.02074074773004876] Loss: 22.84314267851364\n",
      "Iteracion: 10934 Gradiente: [0.0012015349927215386,-0.020729721311923775] Loss: 22.84314224700453\n",
      "Iteracion: 10935 Gradiente: [0.0012008962198365226,-0.020718700755780497] Loss: 22.8431418159541\n",
      "Iteracion: 10936 Gradiente: [0.0012002577865141953,-0.02070768605850558] Loss: 22.84314138536188\n",
      "Iteracion: 10937 Gradiente: [0.0011996196926200278,-0.020696677216979965] Loss: 22.843140955227362\n",
      "Iteracion: 10938 Gradiente: [0.0011989819379114883,-0.020685674228095387] Loss: 22.84314052555008\n",
      "Iteracion: 10939 Gradiente: [0.0011983445223127849,-0.020674677088734463] Loss: 22.843140096329524\n",
      "Iteracion: 10940 Gradiente: [0.0011977074456154923,-0.02066368579578738] Loss: 22.84313966756523\n",
      "Iteracion: 10941 Gradiente: [0.0011970707075259193,-0.020652700346154993] Loss: 22.843139239256697\n",
      "Iteracion: 10942 Gradiente: [0.0011964343080544874,-0.020641720736716825] Loss: 22.843138811403442\n",
      "Iteracion: 10943 Gradiente: [0.0011957982468355037,-0.020630746964381785] Loss: 22.84313838400501\n",
      "Iteracion: 10944 Gradiente: [0.001195162523771387,-0.020619779026040418] Loss: 22.84313795706085\n",
      "Iteracion: 10945 Gradiente: [0.0011945271386783437,-0.020608816918591445] Loss: 22.84313753057054\n",
      "Iteracion: 10946 Gradiente: [0.0011938920913365791,-0.02059786063893772] Loss: 22.843137104533596\n",
      "Iteracion: 10947 Gradiente: [0.0011932573816409331,-0.020586910183977142] Loss: 22.84313667894952\n",
      "Iteracion: 10948 Gradiente: [0.001192623009435086,-0.02057596555061032] Loss: 22.843136253817825\n",
      "Iteracion: 10949 Gradiente: [0.0011919889744120837,-0.02056502673575101] Loss: 22.84313582913802\n",
      "Iteracion: 10950 Gradiente: [0.0011913552765378198,-0.02055409373629651] Loss: 22.84313540490965\n",
      "Iteracion: 10951 Gradiente: [0.0011907219155546044,-0.020543166549160694] Loss: 22.84313498113224\n",
      "Iteracion: 10952 Gradiente: [0.001190088891237906,-0.0205322451712559] Loss: 22.843134557805293\n",
      "Iteracion: 10953 Gradiente: [0.0011894562034569845,-0.02052132959948961] Loss: 22.84313413492833\n",
      "Iteracion: 10954 Gradiente: [0.0011888238519465707,-0.020510419830782448] Loss: 22.843133712500876\n",
      "Iteracion: 10955 Gradiente: [0.0011881918366365577,-0.02049951586204107] Loss: 22.84313329052243\n",
      "Iteracion: 10956 Gradiente: [0.0011875601574776813,-0.02048861769017556] Loss: 22.843132868992573\n",
      "Iteracion: 10957 Gradiente: [0.0011869288139848776,-0.020477725312121938] Loss: 22.843132447910776\n",
      "Iteracion: 10958 Gradiente: [0.0011862978062595175,-0.020466838724783543] Loss: 22.84313202727657\n",
      "Iteracion: 10959 Gradiente: [0.0011856671339728563,-0.020455957925089762] Loss: 22.8431316070895\n",
      "Iteracion: 10960 Gradiente: [0.001185036796890889,-0.020445082909967976] Loss: 22.843131187349098\n",
      "Iteracion: 10961 Gradiente: [0.0011844067949548767,-0.020434213676336316] Loss: 22.843130768054856\n",
      "Iteracion: 10962 Gradiente: [0.001183777127999027,-0.020423350221120393] Loss: 22.843130349206305\n",
      "Iteracion: 10963 Gradiente: [0.001183147795673752,-0.020412492541256693] Loss: 22.84312993080299\n",
      "Iteracion: 10964 Gradiente: [0.0011825187979601045,-0.020401640633667027] Loss: 22.84312951284443\n",
      "Iteracion: 10965 Gradiente: [0.0011818901346525004,-0.020390794495284106] Loss: 22.84312909533015\n",
      "Iteracion: 10966 Gradiente: [0.001181261805587989,-0.0203799541230385] Loss: 22.843128678259678\n",
      "Iteracion: 10967 Gradiente: [0.0011806338106273037,-0.020369119513866232] Loss: 22.843128261632547\n",
      "Iteracion: 10968 Gradiente: [0.0011800061494085412,-0.020358290664708532] Loss: 22.843127845448286\n",
      "Iteracion: 10969 Gradiente: [0.0011793788219013853,-0.020347467572500003] Loss: 22.843127429706406\n",
      "Iteracion: 10970 Gradiente: [0.0011787518279618326,-0.020336650234175007] Loss: 22.84312701440646\n",
      "Iteracion: 10971 Gradiente: [0.0011781251672658755,-0.020325838646685194] Loss: 22.843126599547965\n",
      "Iteracion: 10972 Gradiente: [0.001177498839818251,-0.020315032806961262] Loss: 22.84312618513046\n",
      "Iteracion: 10973 Gradiente: [0.0011768728452940043,-0.02030423271195687] Loss: 22.843125771153474\n",
      "Iteracion: 10974 Gradiente: [0.0011762471834989203,-0.02029343835861681] Loss: 22.84312535761652\n",
      "Iteracion: 10975 Gradiente: [0.0011756218544424731,-0.020282649743879944] Loss: 22.84312494451916\n",
      "Iteracion: 10976 Gradiente: [0.0011749968577997076,-0.02027186686470349] Loss: 22.843124531860916\n",
      "Iteracion: 10977 Gradiente: [0.00117437219336504,-0.020261089718040755] Loss: 22.84312411964131\n",
      "Iteracion: 10978 Gradiente: [0.0011737478610996277,-0.020250318300834992] Loss: 22.84312370785989\n",
      "Iteracion: 10979 Gradiente: [0.0011731238606690416,-0.0202395526100504] Loss: 22.843123296516186\n",
      "Iteracion: 10980 Gradiente: [0.001172500191998438,-0.020228792642635676] Loss: 22.843122885609734\n",
      "Iteracion: 10981 Gradiente: [0.0011718768549333922,-0.02021803839554662] Loss: 22.843122475140053\n",
      "Iteracion: 10982 Gradiente: [0.001171253849233267,-0.02020728986574755] Loss: 22.84312206510671\n",
      "Iteracion: 10983 Gradiente: [0.0011706311747749017,-0.020196547050193914] Loss: 22.84312165550922\n",
      "Iteracion: 10984 Gradiente: [0.0011700088311973407,-0.02018580994586069] Loss: 22.843121246347106\n",
      "Iteracion: 10985 Gradiente: [0.0011693868186862725,-0.02017507854968912] Loss: 22.843120837619963\n",
      "Iteracion: 10986 Gradiente: [0.0011687651367680019,-0.020164352858662464] Loss: 22.843120429327264\n",
      "Iteracion: 10987 Gradiente: [0.0011681437853866329,-0.02015363286974008] Loss: 22.843120021468554\n",
      "Iteracion: 10988 Gradiente: [0.0011675227642247894,-0.020142918579900027] Loss: 22.843119614043413\n",
      "Iteracion: 10989 Gradiente: [0.0011669020732171021,-0.020132209986104026] Loss: 22.84311920705134\n",
      "Iteracion: 10990 Gradiente: [0.0011662817123446227,-0.020121507085317823] Loss: 22.843118800491897\n",
      "Iteracion: 10991 Gradiente: [0.0011656616812047104,-0.02011080987452587] Loss: 22.843118394364627\n",
      "Iteracion: 10992 Gradiente: [0.001165041979623993,-0.020100118350703276] Loss: 22.84311798866906\n",
      "Iteracion: 10993 Gradiente: [0.0011644226075958387,-0.02008943251081708] Loss: 22.84311758340473\n",
      "Iteracion: 10994 Gradiente: [0.001163803564746028,-0.02007875235185696] Loss: 22.843117178571188\n",
      "Iteracion: 10995 Gradiente: [0.001163184851003507,-0.020068077870796706] Loss: 22.843116774167978\n",
      "Iteracion: 10996 Gradiente: [0.001162566466238483,-0.020057409064615093] Loss: 22.84311637019464\n",
      "Iteracion: 10997 Gradiente: [0.0011619484102766363,-0.02004674593029563] Loss: 22.843115966650714\n",
      "Iteracion: 10998 Gradiente: [0.001161330682799644,-0.02003608846483047] Loss: 22.84311556353575\n",
      "Iteracion: 10999 Gradiente: [0.0011607132838169795,-0.02002543666519602] Loss: 22.843115160849287\n",
      "Iteracion: 11000 Gradiente: [0.0011600962130387416,-0.0200147905283868] Loss: 22.84311475859088\n",
      "Iteracion: 11001 Gradiente: [0.001159479470286821,-0.020004150051392954] Loss: 22.843114356760065\n",
      "Iteracion: 11002 Gradiente: [0.001158863055419109,-0.019993515231201187] Loss: 22.84311395535637\n",
      "Iteracion: 11003 Gradiente: [0.0011582469681798101,-0.019982886064812532] Loss: 22.843113554379375\n",
      "Iteracion: 11004 Gradiente: [0.0011576312085566087,-0.01997226254921003] Loss: 22.843113153828607\n",
      "Iteracion: 11005 Gradiente: [0.0011570157763448681,-0.019961644681390448] Loss: 22.843112753703608\n",
      "Iteracion: 11006 Gradiente: [0.0011564006712433184,-0.01995103245835968] Loss: 22.84311235400394\n",
      "Iteracion: 11007 Gradiente: [0.001155785893126904,-0.0199404258771145] Loss: 22.84311195472913\n",
      "Iteracion: 11008 Gradiente: [0.001155171441763514,-0.019929824934657957] Loss: 22.843111555878764\n",
      "Iteracion: 11009 Gradiente: [0.0011545573172232555,-0.01991922962797747] Loss: 22.843111157452363\n",
      "Iteracion: 11010 Gradiente: [0.0011539435190760134,-0.019908639954093133] Loss: 22.843110759449466\n",
      "Iteracion: 11011 Gradiente: [0.0011533300472279962,-0.01989805591000599] Loss: 22.843110361869655\n",
      "Iteracion: 11012 Gradiente: [0.0011527169016280444,-0.01988747749271482] Loss: 22.84310996471244\n",
      "Iteracion: 11013 Gradiente: [0.0011521040819350977,-0.019876904699238432] Loss: 22.843109567977425\n",
      "Iteracion: 11014 Gradiente: [0.0011514915880506274,-0.01986633752658259] Loss: 22.8431091716641\n",
      "Iteracion: 11015 Gradiente: [0.001150879419805051,-0.019855775971759117] Loss: 22.843108775772066\n",
      "Iteracion: 11016 Gradiente: [0.0011502675769795208,-0.019845220031786088] Loss: 22.84310838030086\n",
      "Iteracion: 11017 Gradiente: [0.0011496560594101387,-0.019834669703673195] Loss: 22.843107985250025\n",
      "Iteracion: 11018 Gradiente: [0.0011490448670116392,-0.019824124984434376] Loss: 22.843107590619116\n",
      "Iteracion: 11019 Gradiente: [0.001148433999471384,-0.019813585871096488] Loss: 22.843107196407697\n",
      "Iteracion: 11020 Gradiente: [0.0011478234567022128,-0.019803052360674773] Loss: 22.84310680261532\n",
      "Iteracion: 11021 Gradiente: [0.0011472132385118054,-0.019792524450190523] Loss: 22.843106409241518\n",
      "Iteracion: 11022 Gradiente: [0.0011466033447665798,-0.019782002136664426] Loss: 22.843106016285894\n",
      "Iteracion: 11023 Gradiente: [0.0011459937752268464,-0.019771485417125825] Loss: 22.843105623747945\n",
      "Iteracion: 11024 Gradiente: [0.0011453845297959711,-0.01976097428859447] Loss: 22.84310523162728\n",
      "Iteracion: 11025 Gradiente: [0.0011447756081328939,-0.019750468748109876] Loss: 22.84310483992341\n",
      "Iteracion: 11026 Gradiente: [0.0011441670103238266,-0.019739968792684692] Loss: 22.84310444863592\n",
      "Iteracion: 11027 Gradiente: [0.0011435587360419202,-0.019729474419359624] Loss: 22.84310405776436\n",
      "Iteracion: 11028 Gradiente: [0.001142950785060748,-0.019718985625170642] Loss: 22.8431036673083\n",
      "Iteracion: 11029 Gradiente: [0.0011423431573045187,-0.01970850240714507] Loss: 22.843103277267264\n",
      "Iteracion: 11030 Gradiente: [0.001141735852611229,-0.019698024762317936] Loss: 22.84310288764085\n",
      "Iteracion: 11031 Gradiente: [0.001141128870843507,-0.019687552687724975] Loss: 22.843102498428593\n",
      "Iteracion: 11032 Gradiente: [0.0011405222116906088,-0.019677086180411862] Loss: 22.843102109630063\n",
      "Iteracion: 11033 Gradiente: [0.0011399158750748483,-0.019666625237415403] Loss: 22.84310172124481\n",
      "Iteracion: 11034 Gradiente: [0.0011393098608010633,-0.019656169855776895] Loss: 22.84310133327243\n",
      "Iteracion: 11035 Gradiente: [0.001138704168695881,-0.019645720032540837] Loss: 22.843100945712436\n",
      "Iteracion: 11036 Gradiente: [0.0011380987985991928,-0.01963527576475137] Loss: 22.8431005585644\n",
      "Iteracion: 11037 Gradiente: [0.001137493750415312,-0.019624837049451452] Loss: 22.843100171827903\n",
      "Iteracion: 11038 Gradiente: [0.0011368890237937043,-0.019614403883697187] Loss: 22.84309978550251\n",
      "Iteracion: 11039 Gradiente: [0.0011362846186803684,-0.019603976264533193] Loss: 22.843099399587746\n",
      "Iteracion: 11040 Gradiente: [0.0011356805349464593,-0.019593554189009647] Loss: 22.84309901408322\n",
      "Iteracion: 11041 Gradiente: [0.0011350767722850227,-0.01958313765418455] Loss: 22.843098628988482\n",
      "Iteracion: 11042 Gradiente: [0.0011344733306060562,-0.019572726657109622] Loss: 22.84309824430309\n",
      "Iteracion: 11043 Gradiente: [0.0011338702097949257,-0.01956232119483623] Loss: 22.84309786002661\n",
      "Iteracion: 11044 Gradiente: [0.0011332674095901514,-0.019551921264428528] Loss: 22.843097476158583\n",
      "Iteracion: 11045 Gradiente: [0.0011326649298050975,-0.0195415268629457] Loss: 22.84309709269863\n",
      "Iteracion: 11046 Gradiente: [0.001132062770363973,-0.019531137987442666] Loss: 22.84309670964627\n",
      "Iteracion: 11047 Gradiente: [0.001131460930976876,-0.01952075463499021] Loss: 22.8430963270011\n",
      "Iteracion: 11048 Gradiente: [0.001130859411748967,-0.019510376802631887] Loss: 22.843095944762663\n",
      "Iteracion: 11049 Gradiente: [0.0011302582121620237,-0.019500004487458256] Loss: 22.84309556293054\n",
      "Iteracion: 11050 Gradiente: [0.0011296573322634154,-0.01948963768652175] Loss: 22.843095181504292\n",
      "Iteracion: 11051 Gradiente: [0.0011290567716694493,-0.019479276396901925] Loss: 22.843094800483513\n",
      "Iteracion: 11052 Gradiente: [0.0011284565304255996,-0.019468920615658074] Loss: 22.843094419867725\n",
      "Iteracion: 11053 Gradiente: [0.0011278566083196514,-0.01945857033986324] Loss: 22.843094039656545\n",
      "Iteracion: 11054 Gradiente: [0.0011272570051024408,-0.019448225566596613] Loss: 22.8430936598495\n",
      "Iteracion: 11055 Gradiente: [0.0011266577207180717,-0.019437886292924488] Loss: 22.843093280446194\n",
      "Iteracion: 11056 Gradiente: [0.001126058754891801,-0.019427552515929727] Loss: 22.843092901446184\n",
      "Iteracion: 11057 Gradiente: [0.0011254601074957313,-0.019417224232687977] Loss: 22.84309252284904\n",
      "Iteracion: 11058 Gradiente: [0.0011248617782636454,-0.019406901440285295] Loss: 22.843092144654342\n",
      "Iteracion: 11059 Gradiente: [0.0011242637673338626,-0.01939658413578146] Loss: 22.843091766861654\n",
      "Iteracion: 11060 Gradiente: [0.0011236660741758442,-0.01938627231628279] Loss: 22.843091389470544\n",
      "Iteracion: 11061 Gradiente: [0.0011230686987687478,-0.019375965978864804] Loss: 22.843091012480606\n",
      "Iteracion: 11062 Gradiente: [0.001122471641000781,-0.0193656651206093] Loss: 22.843090635891407\n",
      "Iteracion: 11063 Gradiente: [0.0011218749006502548,-0.01935536973860413] Loss: 22.843090259702493\n",
      "Iteracion: 11064 Gradiente: [0.001121278477464216,-0.01934507982994565] Loss: 22.843089883913468\n",
      "Iteracion: 11065 Gradiente: [0.0011206823714312957,-0.019334795391714246] Loss: 22.843089508523903\n",
      "Iteracion: 11066 Gradiente: [0.001120086582333594,-0.01932451642100439] Loss: 22.843089133533375\n",
      "Iteracion: 11067 Gradiente: [0.0011194911098489986,-0.01931424291491813] Loss: 22.843088758941427\n",
      "Iteracion: 11068 Gradiente: [0.0011188959540286683,-0.01930397487053798] Loss: 22.843088384747684\n",
      "Iteracion: 11069 Gradiente: [0.0011183011145485958,-0.01929371228496777] Loss: 22.8430880109517\n",
      "Iteracion: 11070 Gradiente: [0.0011177065913689907,-0.019283455155301017] Loss: 22.843087637553058\n",
      "Iteracion: 11071 Gradiente: [0.0011171123842662685,-0.019273203478637767] Loss: 22.843087264551308\n",
      "Iteracion: 11072 Gradiente: [0.0011165184929874765,-0.019262957252084215] Loss: 22.843086891946072\n",
      "Iteracion: 11073 Gradiente: [0.0011159249175117718,-0.0192527164727359] Loss: 22.843086519736904\n",
      "Iteracion: 11074 Gradiente: [0.0011153316575824117,-0.019242481137700076] Loss: 22.84308614792338\n",
      "Iteracion: 11075 Gradiente: [0.0011147387130838145,-0.01923225124408141] Loss: 22.8430857765051\n",
      "Iteracion: 11076 Gradiente: [0.0011141460837563954,-0.019222026788991542] Loss: 22.84308540548162\n",
      "Iteracion: 11077 Gradiente: [0.001113553769479836,-0.019211807769535957] Loss: 22.843085034852518\n",
      "Iteracion: 11078 Gradiente: [0.0011129617701366593,-0.01920159418282239] Loss: 22.8430846646174\n",
      "Iteracion: 11079 Gradiente: [0.0011123700855705464,-0.019191386025962605] Loss: 22.843084294775835\n",
      "Iteracion: 11080 Gradiente: [0.0011117787153750668,-0.019181183296082812] Loss: 22.8430839253274\n",
      "Iteracion: 11081 Gradiente: [0.001111187659718856,-0.01917098599027926] Loss: 22.843083556271683\n",
      "Iteracion: 11082 Gradiente: [0.0011105969182731692,-0.019160794105675904] Loss: 22.84308318760826\n",
      "Iteracion: 11083 Gradiente: [0.001110006490843792,-0.019150607639395054] Loss: 22.843082819336722\n",
      "Iteracion: 11084 Gradiente: [0.001109416377263983,-0.01914042658855332] Loss: 22.843082451456645\n",
      "Iteracion: 11085 Gradiente: [0.0011088265774854258,-0.01913025095026697] Loss: 22.843082083967623\n",
      "Iteracion: 11086 Gradiente: [0.0011082370912589568,-0.019120080721663275] Loss: 22.84308171686924\n",
      "Iteracion: 11087 Gradiente: [0.0011076479184633095,-0.019109915899861583] Loss: 22.843081350161054\n",
      "Iteracion: 11088 Gradiente: [0.0011070590588000565,-0.019099756481995675] Loss: 22.84308098384269\n",
      "Iteracion: 11089 Gradiente: [0.001106470512182985,-0.019089602465190106] Loss: 22.843080617913714\n",
      "Iteracion: 11090 Gradiente: [0.0011058822785590414,-0.019079453846564575] Loss: 22.843080252373696\n",
      "Iteracion: 11091 Gradiente: [0.0011052943575199,-0.019069310623264713] Loss: 22.843079887222256\n",
      "Iteracion: 11092 Gradiente: [0.00110470674909872,-0.019059172792410924] Loss: 22.843079522458954\n",
      "Iteracion: 11093 Gradiente: [0.0011041194530861276,-0.019049040351136644] Loss: 22.843079158083395\n",
      "Iteracion: 11094 Gradiente: [0.0011035324692803292,-0.01903891329658206] Loss: 22.843078794095163\n",
      "Iteracion: 11095 Gradiente: [0.0011029457976690082,-0.01902879162587124] Loss: 22.843078430493836\n",
      "Iteracion: 11096 Gradiente: [0.0011023594378504716,-0.019018675336157996] Loss: 22.84307806727901\n",
      "Iteracion: 11097 Gradiente: [0.0011017733896683997,-0.019008564424578957] Loss: 22.84307770445027\n",
      "Iteracion: 11098 Gradiente: [0.001101187653072581,-0.01899845888826993] Loss: 22.843077342007202\n",
      "Iteracion: 11099 Gradiente: [0.0011006022279370124,-0.01898835872437014] Loss: 22.84307697994942\n",
      "Iteracion: 11100 Gradiente: [0.0011000171139490553,-0.018978263930033278] Loss: 22.84307661827649\n",
      "Iteracion: 11101 Gradiente: [0.0010994323111456576,-0.01896817450239278] Loss: 22.843076256988017\n",
      "Iteracion: 11102 Gradiente: [0.0010988478191867066,-0.018958090438602446] Loss: 22.84307589608358\n",
      "Iteracion: 11103 Gradiente: [0.0010982636379263037,-0.018948011735814063] Loss: 22.843075535562782\n",
      "Iteracion: 11104 Gradiente: [0.0010976797672384464,-0.01893793839117149] Loss: 22.84307517542521\n",
      "Iteracion: 11105 Gradiente: [0.001097096206963973,-0.018927870401828764] Loss: 22.843074815670448\n",
      "Iteracion: 11106 Gradiente: [0.0010965129569418272,-0.018917807764936376] Loss: 22.84307445629809\n",
      "Iteracion: 11107 Gradiente: [0.0010959300170460058,-0.01890775047764824] Loss: 22.843074097307753\n",
      "Iteracion: 11108 Gradiente: [0.0010953473870832415,-0.01889769853712101] Loss: 22.843073738699008\n",
      "Iteracion: 11109 Gradiente: [0.0010947650667352113,-0.018887651940520556] Loss: 22.84307338047147\n",
      "Iteracion: 11110 Gradiente: [0.0010941830560454945,-0.01887761068499678] Loss: 22.843073022624708\n",
      "Iteracion: 11111 Gradiente: [0.001093601354766823,-0.018867574767711896] Loss: 22.84307266515833\n",
      "Iteracion: 11112 Gradiente: [0.001093019962714455,-0.018857544185829293] Loss: 22.843072308071932\n",
      "Iteracion: 11113 Gradiente: [0.0010924388797093343,-0.018847518936514856] Loss: 22.84307195136511\n",
      "Iteracion: 11114 Gradiente: [0.0010918581056273524,-0.01883749901693032] Loss: 22.84307159503745\n",
      "Iteracion: 11115 Gradiente: [0.0010912776403188217,-0.01882748442424287] Loss: 22.843071239088584\n",
      "Iteracion: 11116 Gradiente: [0.0010906974837164777,-0.018817475155611992] Loss: 22.843070883518074\n",
      "Iteracion: 11117 Gradiente: [0.0010901176354659963,-0.018807471208220742] Loss: 22.843070528325516\n",
      "Iteracion: 11118 Gradiente: [0.0010895380954536904,-0.018797472579236658] Loss: 22.843070173510522\n",
      "Iteracion: 11119 Gradiente: [0.0010889588634986089,-0.01878747926583273] Loss: 22.84306981907269\n",
      "Iteracion: 11120 Gradiente: [0.0010883799396007514,-0.018777491265174244] Loss: 22.84306946501162\n",
      "Iteracion: 11121 Gradiente: [0.0010878013234777958,-0.018767508574443426] Loss: 22.843069111326916\n",
      "Iteracion: 11122 Gradiente: [0.0010872230148568937,-0.01875753119082309] Loss: 22.84306875801818\n",
      "Iteracion: 11123 Gradiente: [0.0010866450137238342,-0.01874755911148552] Loss: 22.843068405084974\n",
      "Iteracion: 11124 Gradiente: [0.0010860673198313483,-0.01873759233361151] Loss: 22.84306805252695\n",
      "Iteracion: 11125 Gradiente: [0.0010854899330648019,-0.01872763085438341] Loss: 22.84306770034368\n",
      "Iteracion: 11126 Gradiente: [0.00108491285336072,-0.018717674670978585] Loss: 22.84306734853477\n",
      "Iteracion: 11127 Gradiente: [0.0010843360804301482,-0.018707723780586723] Loss: 22.843066997099843\n",
      "Iteracion: 11128 Gradiente: [0.0010837596140153968,-0.018697778180399638] Loss: 22.843066646038455\n",
      "Iteracion: 11129 Gradiente: [0.001083183454180888,-0.01868783786759233] Loss: 22.843066295350273\n",
      "Iteracion: 11130 Gradiente: [0.0010826076005628238,-0.018677902839365858] Loss: 22.843065945034834\n",
      "Iteracion: 11131 Gradiente: [0.0010820320531953105,-0.01866797309289924] Loss: 22.84306559509178\n",
      "Iteracion: 11132 Gradiente: [0.0010814568116794968,-0.01865804862539804] Loss: 22.84306524552072\n",
      "Iteracion: 11133 Gradiente: [0.0010808818760949634,-0.01864812943404092] Loss: 22.84306489632124\n",
      "Iteracion: 11134 Gradiente: [0.0010803072461404402,-0.01863821551603273] Loss: 22.84306454749295\n",
      "Iteracion: 11135 Gradiente: [0.0010797329216558182,-0.01862830686856801] Loss: 22.843064199035464\n",
      "Iteracion: 11136 Gradiente: [0.0010791589024468824,-0.018618403488847926] Loss: 22.84306385094838\n",
      "Iteracion: 11137 Gradiente: [0.001078585188443526,-0.01860850537406513] Loss: 22.84306350323129\n",
      "Iteracion: 11138 Gradiente: [0.0010780117794752186,-0.018598612521423284] Loss: 22.843063155883833\n",
      "Iteracion: 11139 Gradiente: [0.0010774386753818514,-0.018588724928123194] Loss: 22.84306280890559\n",
      "Iteracion: 11140 Gradiente: [0.001076865875849838,-0.018578842591378476] Loss: 22.843062462296178\n",
      "Iteracion: 11141 Gradiente: [0.0010762933809398115,-0.018568965508383193] Loss: 22.843062116055208\n",
      "Iteracion: 11142 Gradiente: [0.0010757211902955532,-0.018559093676350714] Loss: 22.843061770182278\n",
      "Iteracion: 11143 Gradiente: [0.0010751493038886413,-0.018549227092485174] Loss: 22.84306142467701\n",
      "Iteracion: 11144 Gradiente: [0.0010745777215608618,-0.018539365753995205] Loss: 22.843061079539\n",
      "Iteracion: 11145 Gradiente: [0.0010740064431018937,-0.018529509658097017] Loss: 22.843060734767878\n",
      "Iteracion: 11146 Gradiente: [0.0010734354682729948,-0.018519658802005998] Loss: 22.843060390363252\n",
      "Iteracion: 11147 Gradiente: [0.0010728647969945845,-0.018509813182932197] Loss: 22.843060046324684\n",
      "Iteracion: 11148 Gradiente: [0.0010722944291217118,-0.01849997279809088] Loss: 22.843059702651843\n",
      "Iteracion: 11149 Gradiente: [0.0010717243645321636,-0.018490137644697077] Loss: 22.843059359344313\n",
      "Iteracion: 11150 Gradiente: [0.0010711546029076166,-0.018480307719977537] Loss: 22.84305901640171\n",
      "Iteracion: 11151 Gradiente: [0.001070585144199754,-0.018470483021147406] Loss: 22.843058673823652\n",
      "Iteracion: 11152 Gradiente: [0.0010700159883109941,-0.018460663545424676] Loss: 22.843058331609743\n",
      "Iteracion: 11153 Gradiente: [0.0010694471350423858,-0.018450849290033964] Loss: 22.843057989759604\n",
      "Iteracion: 11154 Gradiente: [0.001068878584052868,-0.01844104025221114] Loss: 22.843057648272847\n",
      "Iteracion: 11155 Gradiente: [0.0010683103353613887,-0.01843123642917064] Loss: 22.843057307149085\n",
      "Iteracion: 11156 Gradiente: [0.0010677423888277341,-0.018421437818141467] Loss: 22.843056966387923\n",
      "Iteracion: 11157 Gradiente: [0.001067174744264321,-0.01841164441635108] Loss: 22.84305662598898\n",
      "Iteracion: 11158 Gradiente: [0.0010666074013689316,-0.0184018562210408] Loss: 22.84305628595188\n",
      "Iteracion: 11159 Gradiente: [0.0010660403601965148,-0.01839207322942921] Loss: 22.843055946276234\n",
      "Iteracion: 11160 Gradiente: [0.001065473620374746,-0.018382295438762717] Loss: 22.84305560696166\n",
      "Iteracion: 11161 Gradiente: [0.001064907181860993,-0.018372522846269133] Loss: 22.843055268007753\n",
      "Iteracion: 11162 Gradiente: [0.0010643410445709378,-0.0183627554491796] Loss: 22.843054929414155\n",
      "Iteracion: 11163 Gradiente: [0.001063775208149309,-0.01835299324474538] Loss: 22.84305459118047\n",
      "Iteracion: 11164 Gradiente: [0.0010632096725961067,-0.018343236230195833] Loss: 22.84305425330633\n",
      "Iteracion: 11165 Gradiente: [0.0010626444377190107,-0.018333484402774523] Loss: 22.843053915791337\n",
      "Iteracion: 11166 Gradiente: [0.001062079503290647,-0.01832373775972561] Loss: 22.843053578635136\n",
      "Iteracion: 11167 Gradiente: [0.0010615148692399619,-0.018313996298288397] Loss: 22.8430532418373\n",
      "Iteracion: 11168 Gradiente: [0.0010609505353149492,-0.018304260015715102] Loss: 22.84305290539748\n",
      "Iteracion: 11169 Gradiente: [0.001060386501389606,-0.01829452890924834] Loss: 22.843052569315283\n",
      "Iteracion: 11170 Gradiente: [0.001059822767366351,-0.01828480297613595] Loss: 22.843052233590342\n",
      "Iteracion: 11171 Gradiente: [0.0010592593331476034,-0.01827508221362043] Loss: 22.843051898222274\n",
      "Iteracion: 11172 Gradiente: [0.0010586961983345114,-0.01826536661896808] Loss: 22.843051563210697\n",
      "Iteracion: 11173 Gradiente: [0.0010581333629791818,-0.018255656189420103] Loss: 22.843051228555215\n",
      "Iteracion: 11174 Gradiente: [0.0010575708268684518,-0.018245950922232742] Loss: 22.843050894255473\n",
      "Iteracion: 11175 Gradiente: [0.0010570085897209462,-0.018236250814669338] Loss: 22.84305056031109\n",
      "Iteracion: 11176 Gradiente: [0.0010564466514836114,-0.018226555863980327] Loss: 22.84305022672166\n",
      "Iteracion: 11177 Gradiente: [0.0010558850120266545,-0.018216866067421716] Loss: 22.843049893486853\n",
      "Iteracion: 11178 Gradiente: [0.0010553236711084916,-0.01820718142226004] Loss: 22.84304956060625\n",
      "Iteracion: 11179 Gradiente: [0.0010547626286779633,-0.018197501925749295] Loss: 22.843049228079504\n",
      "Iteracion: 11180 Gradiente: [0.0010542018845039062,-0.0181878275751572] Loss: 22.84304889590623\n",
      "Iteracion: 11181 Gradiente: [0.0010536414384243167,-0.01817815836774687] Loss: 22.843048564086036\n",
      "Iteracion: 11182 Gradiente: [0.0010530812903084552,-0.018168494300784963] Loss: 22.843048232618575\n",
      "Iteracion: 11183 Gradiente: [0.0010525214399062103,-0.01815883537154015] Loss: 22.84304790150345\n",
      "Iteracion: 11184 Gradiente: [0.0010519618872081083,-0.018149181577275422] Loss: 22.843047570740282\n",
      "Iteracion: 11185 Gradiente: [0.0010514026320303552,-0.01813953291526064] Loss: 22.84304724032872\n",
      "Iteracion: 11186 Gradiente: [0.0010508436740887344,-0.018129889382774895] Loss: 22.84304691026839\n",
      "Iteracion: 11187 Gradiente: [0.0010502850133785083,-0.018120250977084494] Loss: 22.843046580558887\n",
      "Iteracion: 11188 Gradiente: [0.0010497266496050392,-0.018110617695468654] Loss: 22.843046251199855\n",
      "Iteracion: 11189 Gradiente: [0.0010491685826868509,-0.018100989535200895] Loss: 22.843045922190942\n",
      "Iteracion: 11190 Gradiente: [0.0010486108124865723,-0.018091366493556884] Loss: 22.84304559353175\n",
      "Iteracion: 11191 Gradiente: [0.0010480533388090407,-0.018081748567817493] Loss: 22.843045265221907\n",
      "Iteracion: 11192 Gradiente: [0.0010474961615026738,-0.018072135755263594] Loss: 22.843044937261073\n",
      "Iteracion: 11193 Gradiente: [0.001046939280306939,-0.018062528053180316] Loss: 22.84304460964885\n",
      "Iteracion: 11194 Gradiente: [0.0010463826951915205,-0.018052925458845456] Loss: 22.843044282384845\n",
      "Iteracion: 11195 Gradiente: [0.001045826406126101,-0.018043327969536567] Loss: 22.843043955468747\n",
      "Iteracion: 11196 Gradiente: [0.0010452704126682497,-0.018033735582553827] Loss: 22.843043628900137\n",
      "Iteracion: 11197 Gradiente: [0.001044714714836914,-0.018024148295177157] Loss: 22.843043302678655\n",
      "Iteracion: 11198 Gradiente: [0.0010441593124672485,-0.01801456610469465] Loss: 22.84304297680396\n",
      "Iteracion: 11199 Gradiente: [0.0010436042053034575,-0.018004989008401515] Loss: 22.843042651275645\n",
      "Iteracion: 11200 Gradiente: [0.0010430493932934344,-0.01799541700358418] Loss: 22.843042326093368\n",
      "Iteracion: 11201 Gradiente: [0.0010424948762202272,-0.017985850087539983] Loss: 22.84304200125676\n",
      "Iteracion: 11202 Gradiente: [0.0010419406539284638,-0.01797628825756282] Loss: 22.84304167676543\n",
      "Iteracion: 11203 Gradiente: [0.00104138672627793,-0.017966731510948363] Loss: 22.843041352619032\n",
      "Iteracion: 11204 Gradiente: [0.0010408330931454656,-0.01795717984499182] Loss: 22.843041028817204\n",
      "Iteracion: 11205 Gradiente: [0.001040279754374751,-0.017947633256992647] Loss: 22.843040705359567\n",
      "Iteracion: 11206 Gradiente: [0.0010397267097014644,-0.017938091744256705] Loss: 22.843040382245757\n",
      "Iteracion: 11207 Gradiente: [0.0010391739591057102,-0.0179285553040773] Loss: 22.843040059475413\n",
      "Iteracion: 11208 Gradiente: [0.0010386215023117985,-0.01791902393376681] Loss: 22.843039737048173\n",
      "Iteracion: 11209 Gradiente: [0.00103806933917762,-0.017909497630626346] Loss: 22.843039414963645\n",
      "Iteracion: 11210 Gradiente: [0.001037517469663385,-0.01789997639195799] Loss: 22.843039093221492\n",
      "Iteracion: 11211 Gradiente: [0.0010369658935151923,-0.017890460215072915] Loss: 22.843038771821355\n",
      "Iteracion: 11212 Gradiente: [0.0010364146106041972,-0.017880949097279598] Loss: 22.843038450762847\n",
      "Iteracion: 11213 Gradiente: [0.0010358636208233445,-0.01787144303588626] Loss: 22.843038130045645\n",
      "Iteracion: 11214 Gradiente: [0.001035312923897891,-0.017861942028210245] Loss: 22.843037809669323\n",
      "Iteracion: 11215 Gradiente: [0.0010347625198117308,-0.017852446071557063] Loss: 22.84303748963358\n",
      "Iteracion: 11216 Gradiente: [0.0010342124082256988,-0.017842955163253175] Loss: 22.84303716993801\n",
      "Iteracion: 11217 Gradiente: [0.0010336625892629551,-0.017833469300598162] Loss: 22.843036850582283\n",
      "Iteracion: 11218 Gradiente: [0.0010331130624460154,-0.01782398848092702] Loss: 22.843036531566035\n",
      "Iteracion: 11219 Gradiente: [0.0010325638278231963,-0.017814512701547377] Loss: 22.843036212888865\n",
      "Iteracion: 11220 Gradiente: [0.0010320148852429156,-0.017805041959780134] Loss: 22.84303589455045\n",
      "Iteracion: 11221 Gradiente: [0.001031466234422851,-0.01779557625295425] Loss: 22.843035576550434\n",
      "Iteracion: 11222 Gradiente: [0.001030917875299527,-0.01778611557838848] Loss: 22.84303525888844\n",
      "Iteracion: 11223 Gradiente: [0.0010303698077365198,-0.017776659933405388] Loss: 22.8430349415641\n",
      "Iteracion: 11224 Gradiente: [0.0010298220315121398,-0.017767209315334754] Loss: 22.843034624577097\n",
      "Iteracion: 11225 Gradiente: [0.0010292745465231216,-0.017757763721502447] Loss: 22.84303430792703\n",
      "Iteracion: 11226 Gradiente: [0.00102872735259704,-0.01774832314923683] Loss: 22.843033991613552\n",
      "Iteracion: 11227 Gradiente: [0.0010281804495207326,-0.017738887595872655] Loss: 22.843033675636306\n",
      "Iteracion: 11228 Gradiente: [0.0010276338372714614,-0.017729457058732596] Loss: 22.843033359994955\n",
      "Iteracion: 11229 Gradiente: [0.0010270875155318512,-0.01772003153516266] Loss: 22.8430330446891\n",
      "Iteracion: 11230 Gradiente: [0.0010265414842782169,-0.017710611022486944] Loss: 22.843032729718427\n",
      "Iteracion: 11231 Gradiente: [0.0010259957433333966,-0.01770119551804363] Loss: 22.843032415082554\n",
      "Iteracion: 11232 Gradiente: [0.0010254502925268602,-0.017691785019171156] Loss: 22.843032100781116\n",
      "Iteracion: 11233 Gradiente: [0.0010249051316909192,-0.017682379523209] Loss: 22.843031786813803\n",
      "Iteracion: 11234 Gradiente: [0.001024360260655044,-0.01767297902749855] Loss: 22.843031473180215\n",
      "Iteracion: 11235 Gradiente: [0.001023815679268599,-0.0176635835293812] Loss: 22.84303115988001\n",
      "Iteracion: 11236 Gradiente: [0.001023271387375265,-0.017654193026202] Loss: 22.843030846912846\n",
      "Iteracion: 11237 Gradiente: [0.0010227273849447254,-0.017644807515294167] Loss: 22.843030534278366\n",
      "Iteracion: 11238 Gradiente: [0.0010221836716832891,-0.017635426994014835] Loss: 22.843030221976196\n",
      "Iteracion: 11239 Gradiente: [0.0010216402474100051,-0.01762605145971333] Loss: 22.843029910005995\n",
      "Iteracion: 11240 Gradiente: [0.0010210971121125568,-0.01761668090972878] Loss: 22.843029598367423\n",
      "Iteracion: 11241 Gradiente: [0.0010205542655654654,-0.017607315341416078] Loss: 22.843029287060105\n",
      "Iteracion: 11242 Gradiente: [0.0010200117075783055,-0.01759795475212916] Loss: 22.843028976083698\n",
      "Iteracion: 11243 Gradiente: [0.0010194694380620225,-0.017588599139216517] Loss: 22.843028665437867\n",
      "Iteracion: 11244 Gradiente: [0.0010189274568811395,-0.017579248500032445] Loss: 22.843028355122236\n",
      "Iteracion: 11245 Gradiente: [0.0010183857637448075,-0.017569902831940945] Loss: 22.843028045136467\n",
      "Iteracion: 11246 Gradiente: [0.0010178443586482898,-0.017560562132289567] Loss: 22.843027735480213\n",
      "Iteracion: 11247 Gradiente: [0.001017303241350002,-0.017551226398443257] Loss: 22.843027426153114\n",
      "Iteracion: 11248 Gradiente: [0.0010167624116083592,-0.017541895627765907] Loss: 22.843027117154804\n",
      "Iteracion: 11249 Gradiente: [0.0010162218695446276,-0.01753256981760245] Loss: 22.843026808484975\n",
      "Iteracion: 11250 Gradiente: [0.001015681614821536,-0.01752324896532874] Loss: 22.843026500143246\n",
      "Iteracion: 11251 Gradiente: [0.0010151416472704493,-0.017513933068307945] Loss: 22.843026192129262\n",
      "Iteracion: 11252 Gradiente: [0.001014601966880946,-0.017504622123898982] Loss: 22.843025884442707\n",
      "Iteracion: 11253 Gradiente: [0.0010140625733508083,-0.017495316129478293] Loss: 22.84302557708323\n",
      "Iteracion: 11254 Gradiente: [0.0010135234665114012,-0.017486015082410707] Loss: 22.843025270050457\n",
      "Iteracion: 11255 Gradiente: [0.0010129846463210394,-0.01747671898006485] Loss: 22.843024963344046\n",
      "Iteracion: 11256 Gradiente: [0.0010124461125845603,-0.017467427819811594] Loss: 22.84302465696367\n",
      "Iteracion: 11257 Gradiente: [0.0010119078651968038,-0.017458141599020868] Loss: 22.843024350908966\n",
      "Iteracion: 11258 Gradiente: [0.0010113699038640789,-0.01744886031507704] Loss: 22.843024045179593\n",
      "Iteracion: 11259 Gradiente: [0.0010108322286394393,-0.01743958396534045] Loss: 22.8430237397752\n",
      "Iteracion: 11260 Gradiente: [0.0010102948391505606,-0.01743031254720305] Loss: 22.843023434695446\n",
      "Iteracion: 11261 Gradiente: [0.0010097577354637602,-0.01742104605802979] Loss: 22.843023129939983\n",
      "Iteracion: 11262 Gradiente: [0.0010092209172417673,-0.017411784495211434] Loss: 22.84302282550846\n",
      "Iteracion: 11263 Gradiente: [0.0010086843843964744,-0.017402527856123957] Loss: 22.843022521400545\n",
      "Iteracion: 11264 Gradiente: [0.0010081481367781937,-0.017393276138151493] Loss: 22.8430222176159\n",
      "Iteracion: 11265 Gradiente: [0.0010076121743310296,-0.017384029338671187] Loss: 22.84302191415419\n",
      "Iteracion: 11266 Gradiente: [0.0010070764966817098,-0.017374787455081513] Loss: 22.843021611015015\n",
      "Iteracion: 11267 Gradiente: [0.001006541103985607,-0.017365550484750376] Loss: 22.84302130819809\n",
      "Iteracion: 11268 Gradiente: [0.0010060059958400794,-0.017356318425080987] Loss: 22.843021005703044\n",
      "Iteracion: 11269 Gradiente: [0.0010054711721541783,-0.017347091273459132] Loss: 22.843020703529557\n",
      "Iteracion: 11270 Gradiente: [0.0010049366327924266,-0.01733786902727618] Loss: 22.84302040167727\n",
      "Iteracion: 11271 Gradiente: [0.0010044023776657695,-0.017328651683917565] Loss: 22.843020100145846\n",
      "Iteracion: 11272 Gradiente: [0.0010038684064928324,-0.01731943924078602] Loss: 22.843019798934943\n",
      "Iteracion: 11273 Gradiente: [0.0010033347192783518,-0.017310231695265917] Loss: 22.843019498044203\n",
      "Iteracion: 11274 Gradiente: [0.001002801315750427,-0.017301029044762828] Loss: 22.84301919747333\n",
      "Iteracion: 11275 Gradiente: [0.0010022681957745287,-0.01729183128667226] Loss: 22.84301889722195\n",
      "Iteracion: 11276 Gradiente: [0.0010017353592057059,-0.01728263841839374] Loss: 22.84301859728972\n",
      "Iteracion: 11277 Gradiente: [0.00100120280603259,-0.01727345043731816] Loss: 22.84301829767632\n",
      "Iteracion: 11278 Gradiente: [0.001000670535860119,-0.01726426734086317] Loss: 22.84301799838139\n",
      "Iteracion: 11279 Gradiente: [0.0010001385487119782,-0.017255089126421198] Loss: 22.84301769940464\n",
      "Iteracion: 11280 Gradiente: [0.0009996068443295296,-0.01724591579140243] Loss: 22.843017400745666\n",
      "Iteracion: 11281 Gradiente: [0.0009990754225725595,-0.01723674733321374] Loss: 22.843017102404158\n",
      "Iteracion: 11282 Gradiente: [0.0009985442834183308,-0.017227583749253008] Loss: 22.843016804379786\n",
      "Iteracion: 11283 Gradiente: [0.000998013426606311,-0.017218425036936817] Loss: 22.843016506672214\n",
      "Iteracion: 11284 Gradiente: [0.000997482852086288,-0.01720927119366801] Loss: 22.843016209281082\n",
      "Iteracion: 11285 Gradiente: [0.0009969525595986777,-0.01720012221686673] Loss: 22.84301591220609\n",
      "Iteracion: 11286 Gradiente: [0.0009964225490080025,-0.01719097810394222] Loss: 22.843015615446873\n",
      "Iteracion: 11287 Gradiente: [0.0009958928202308925,-0.017181838852307507] Loss: 22.843015319003104\n",
      "Iteracion: 11288 Gradiente: [0.000995363373064606,-0.017172704459379417] Loss: 22.843015022874468\n",
      "Iteracion: 11289 Gradiente: [0.0009948342073139809,-0.017163574922577376] Loss: 22.84301472706058\n",
      "Iteracion: 11290 Gradiente: [0.0009943053229354367,-0.01715445023931655] Loss: 22.843014431561162\n",
      "Iteracion: 11291 Gradiente: [0.00099377671965802,-0.017145330407020036] Loss: 22.843014136375828\n",
      "Iteracion: 11292 Gradiente: [0.0009932483974855208,-0.017136215423101932] Loss: 22.843013841504284\n",
      "Iteracion: 11293 Gradiente: [0.0009927203560996152,-0.01712710528499398] Loss: 22.843013546946192\n",
      "Iteracion: 11294 Gradiente: [0.0009921925954444078,-0.017117999990114777] Loss: 22.843013252701194\n",
      "Iteracion: 11295 Gradiente: [0.0009916651154270538,-0.017108899535886242] Loss: 22.84301295876897\n",
      "Iteracion: 11296 Gradiente: [0.0009911379158874449,-0.01709980391973443] Loss: 22.843012665149214\n",
      "Iteracion: 11297 Gradiente: [0.0009906109964807305,-0.01709071313910056] Loss: 22.843012371841553\n",
      "Iteracion: 11298 Gradiente: [0.000990084357277965,-0.01708162719139909] Loss: 22.84301207884566\n",
      "Iteracion: 11299 Gradiente: [0.0009895579979873521,-0.017072546074070515] Loss: 22.843011786161227\n",
      "Iteracion: 11300 Gradiente: [0.000989031918615524,-0.01706346978453818] Loss: 22.843011493787913\n",
      "Iteracion: 11301 Gradiente: [0.000988506118876368,-0.017054398320243063] Loss: 22.84301120172538\n",
      "Iteracion: 11302 Gradiente: [0.0009879805986694615,-0.0170453316786156] Loss: 22.843010909973305\n",
      "Iteracion: 11303 Gradiente: [0.000987455357892486,-0.017036269857090856] Loss: 22.84301061853135\n",
      "Iteracion: 11304 Gradiente: [0.0009869303963559635,-0.017027212853107678] Loss: 22.84301032739921\n",
      "Iteracion: 11305 Gradiente: [0.0009864057139102064,-0.01701816066410539] Loss: 22.84301003657653\n",
      "Iteracion: 11306 Gradiente: [0.0009858813102558391,-0.017009113287533614] Loss: 22.843009746062975\n",
      "Iteracion: 11307 Gradiente: [0.0009853571855425495,-0.017000070720816277] Loss: 22.843009455858247\n",
      "Iteracion: 11308 Gradiente: [0.000984833339361065,-0.016991032961411415] Loss: 22.84300916596199\n",
      "Iteracion: 11309 Gradiente: [0.0009843097717748607,-0.016982000006751246] Loss: 22.843008876373883\n",
      "Iteracion: 11310 Gradiente: [0.0009837864825347727,-0.016972971854286218] Loss: 22.843008587093607\n",
      "Iteracion: 11311 Gradiente: [0.0009832634714390073,-0.01696394850146786] Loss: 22.84300829812084\n",
      "Iteracion: 11312 Gradiente: [0.0009827407383558768,-0.01695492994574188] Loss: 22.843008009455243\n",
      "Iteracion: 11313 Gradiente: [0.0009822182831698,-0.0169459161845572] Loss: 22.84300772109647\n",
      "Iteracion: 11314 Gradiente: [0.0009816961058296176,-0.016936907215360363] Loss: 22.843007433044246\n",
      "Iteracion: 11315 Gradiente: [0.000981174206097535,-0.016927903035607508] Loss: 22.84300714529819\n",
      "Iteracion: 11316 Gradiente: [0.000980652583725335,-0.016918903642758682] Loss: 22.84300685785802\n",
      "Iteracion: 11317 Gradiente: [0.0009801312386912286,-0.01690990903426138] Loss: 22.84300657072338\n",
      "Iteracion: 11318 Gradiente: [0.0009796101708104744,-0.01690091920757541] Loss: 22.84300628389396\n",
      "Iteracion: 11319 Gradiente: [0.0009790893801209677,-0.01689193416014637] Loss: 22.843005997369442\n",
      "Iteracion: 11320 Gradiente: [0.0009785688661243816,-0.0168829538894542] Loss: 22.843005711149484\n",
      "Iteracion: 11321 Gradiente: [0.0009780486288197684,-0.01687397839295054] Loss: 22.843005425233784\n",
      "Iteracion: 11322 Gradiente: [0.0009775286681758643,-0.01686500766809284] Loss: 22.843005139621994\n",
      "Iteracion: 11323 Gradiente: [0.0009770089838506617,-0.016856041712354042] Loss: 22.84300485431381\n",
      "Iteracion: 11324 Gradiente: [0.0009764895758441601,-0.016847080523189343] Loss: 22.843004569308892\n",
      "Iteracion: 11325 Gradiente: [0.0009759704440720422,-0.01683812409806258] Loss: 22.843004284606938\n",
      "Iteracion: 11326 Gradiente: [0.0009754515882860915,-0.01682917243444635] Loss: 22.843004000207618\n",
      "Iteracion: 11327 Gradiente: [0.0009749330081424054,-0.016820225529819766] Loss: 22.8430037161106\n",
      "Iteracion: 11328 Gradiente: [0.000974414703928043,-0.016811283381630085] Loss: 22.843003432315577\n",
      "Iteracion: 11329 Gradiente: [0.000973896675146572,-0.01680234598736661] Loss: 22.843003148822223\n",
      "Iteracion: 11330 Gradiente: [0.0009733789218008345,-0.01679341334449482] Loss: 22.843002865630222\n",
      "Iteracion: 11331 Gradiente: [0.000972861443652088,-0.016784485450493256] Loss: 22.843002582739235\n",
      "Iteracion: 11332 Gradiente: [0.000972344240686122,-0.01677556230282929] Loss: 22.843002300148953\n",
      "Iteracion: 11333 Gradiente: [0.000971827312669878,-0.016766643898984412] Loss: 22.84300201785907\n",
      "Iteracion: 11334 Gradiente: [0.000971310659408194,-0.016757730236442232] Loss: 22.843001735869255\n",
      "Iteracion: 11335 Gradiente: [0.0009707942808025412,-0.016748821312676892] Loss: 22.84300145417919\n",
      "Iteracion: 11336 Gradiente: [0.0009702781768169188,-0.016739917125162762] Loss: 22.84300117278855\n",
      "Iteracion: 11337 Gradiente: [0.0009697623471150034,-0.01673101767139329] Loss: 22.843000891697027\n",
      "Iteracion: 11338 Gradiente: [0.0009692467916939525,-0.016722122948843676] Loss: 22.843000610904287\n",
      "Iteracion: 11339 Gradiente: [0.0009687315103500774,-0.016713232955002866] Loss: 22.843000330410028\n",
      "Iteracion: 11340 Gradiente: [0.0009682165029554805,-0.016704347687353998] Loss: 22.843000050213927\n",
      "Iteracion: 11341 Gradiente: [0.0009677017693737374,-0.016695467143385538] Loss: 22.842999770315682\n",
      "Iteracion: 11342 Gradiente: [0.0009671873093784219,-0.016686591320589864] Loss: 22.84299949071494\n",
      "Iteracion: 11343 Gradiente: [0.0009666731229477439,-0.01667772021645059] Loss: 22.842999211411428\n",
      "Iteracion: 11344 Gradiente: [0.0009661592098628564,-0.016668853828464118] Loss: 22.842998932404807\n",
      "Iteracion: 11345 Gradiente: [0.0009656455699020701,-0.01665999215412626] Loss: 22.842998653694757\n",
      "Iteracion: 11346 Gradiente: [0.0009651322030094888,-0.016651135190927136] Loss: 22.842998375280988\n",
      "Iteracion: 11347 Gradiente: [0.0009646191091803758,-0.01664228293635297] Loss: 22.842998097163143\n",
      "Iteracion: 11348 Gradiente: [0.000964106288041459,-0.016633435387913547] Loss: 22.842997819340937\n",
      "Iteracion: 11349 Gradiente: [0.0009635937394979995,-0.01662459254310396] Loss: 22.842997541814054\n",
      "Iteracion: 11350 Gradiente: [0.0009630814635765242,-0.016615754399414467] Loss: 22.842997264582177\n",
      "Iteracion: 11351 Gradiente: [0.0009625694599094458,-0.016606920954359932] Loss: 22.842996987644998\n",
      "Iteracion: 11352 Gradiente: [0.0009620577284114991,-0.016598092205436406] Loss: 22.842996711002186\n",
      "Iteracion: 11353 Gradiente: [0.0009615462690495254,-0.016589268150142663] Loss: 22.842996434653443\n",
      "Iteracion: 11354 Gradiente: [0.0009610350814758324,-0.01658044878599393] Loss: 22.842996158598446\n",
      "Iteracion: 11355 Gradiente: [0.000960524165718842,-0.016571634110486972] Loss: 22.8429958828369\n",
      "Iteracion: 11356 Gradiente: [0.0009600135216089711,-0.016562824121129437] Loss: 22.842995607368476\n",
      "Iteracion: 11357 Gradiente: [0.0009595031489899005,-0.01655401881543206] Loss: 22.842995332192874\n",
      "Iteracion: 11358 Gradiente: [0.0009589930477024685,-0.016545218190906635] Loss: 22.842995057309775\n",
      "Iteracion: 11359 Gradiente: [0.0009584832175325649,-0.016536422245065907] Loss: 22.842994782718872\n",
      "Iteracion: 11360 Gradiente: [0.0009579736584801897,-0.01652763097541623] Loss: 22.842994508419864\n",
      "Iteracion: 11361 Gradiente: [0.0009574643702573364,-0.016518844379478873] Loss: 22.842994234412423\n",
      "Iteracion: 11362 Gradiente: [0.0009569553527910557,-0.01651006245476528] Loss: 22.842993960696234\n",
      "Iteracion: 11363 Gradiente: [0.0009564466059780822,-0.016501285198791158] Loss: 22.842993687271004\n",
      "Iteracion: 11364 Gradiente: [0.0009559381295938844,-0.016492512609076353] Loss: 22.842993414136423\n",
      "Iteracion: 11365 Gradiente: [0.0009554299235058276,-0.01648374468314155] Loss: 22.842993141292183\n",
      "Iteracion: 11366 Gradiente: [0.0009549219877120171,-0.01647498141849913] Loss: 22.84299286873797\n",
      "Iteracion: 11367 Gradiente: [0.0009544143218012854,-0.016466222812685415] Loss: 22.84299259647347\n",
      "Iteracion: 11368 Gradiente: [0.0009539069258333181,-0.01645746886321291] Loss: 22.842992324498386\n",
      "Iteracion: 11369 Gradiente: [0.0009533997996110581,-0.01644871956760845] Loss: 22.842992052812402\n",
      "Iteracion: 11370 Gradiente: [0.0009528929430141867,-0.016439974923396505] Loss: 22.84299178141521\n",
      "Iteracion: 11371 Gradiente: [0.0009523863558428047,-0.016431234928108416] Loss: 22.84299151030651\n",
      "Iteracion: 11372 Gradiente: [0.0009518800380703852,-0.016422499579265924] Loss: 22.84299123948601\n",
      "Iteracion: 11373 Gradiente: [0.0009513739893558675,-0.016413768874408537] Loss: 22.84299096895336\n",
      "Iteracion: 11374 Gradiente: [0.0009508682097845167,-0.016405042811056227] Loss: 22.842990698708302\n",
      "Iteracion: 11375 Gradiente: [0.0009503626990692738,-0.016396321386748625] Loss: 22.8429904287505\n",
      "Iteracion: 11376 Gradiente: [0.0009498574571267682,-0.01638760459901588] Loss: 22.84299015907966\n",
      "Iteracion: 11377 Gradiente: [0.0009493524837438372,-0.016378892445397024] Loss: 22.842989889695463\n",
      "Iteracion: 11378 Gradiente: [0.0009488477788001622,-0.01637018492342861] Loss: 22.842989620597635\n",
      "Iteracion: 11379 Gradiente: [0.0009483433421905829,-0.016361482030644345] Loss: 22.84298935178584\n",
      "Iteracion: 11380 Gradiente: [0.0009478391736488826,-0.016352783764591077] Loss: 22.842989083259784\n",
      "Iteracion: 11381 Gradiente: [0.0009473352733872768,-0.016344090122789603] Loss: 22.84298881501917\n",
      "Iteracion: 11382 Gradiente: [0.0009468316408894376,-0.01633540110280182] Loss: 22.8429885470637\n",
      "Iteracion: 11383 Gradiente: [0.0009463282760947322,-0.016326716702166167] Loss: 22.842988279393033\n",
      "Iteracion: 11384 Gradiente: [0.0009458251788961055,-0.016318036918424993] Loss: 22.842988012006913\n",
      "Iteracion: 11385 Gradiente: [0.0009453223492300822,-0.016309361749119945] Loss: 22.84298774490503\n",
      "Iteracion: 11386 Gradiente: [0.0009448197868114979,-0.016300691191804145] Loss: 22.842987478087057\n",
      "Iteracion: 11387 Gradiente: [0.0009443174915882461,-0.01629202524402231] Loss: 22.842987211552707\n",
      "Iteracion: 11388 Gradiente: [0.0009438154634608508,-0.01628336390332071] Loss: 22.842986945301664\n",
      "Iteracion: 11389 Gradiente: [0.0009433137021754116,-0.01627470716725649] Loss: 22.842986679333656\n",
      "Iteracion: 11390 Gradiente: [0.000942812207656137,-0.016266055033377923] Loss: 22.84298641364837\n",
      "Iteracion: 11391 Gradiente: [0.0009423109797798664,-0.01625740749923755] Loss: 22.842986148245494\n",
      "Iteracion: 11392 Gradiente: [0.0009418100183324896,-0.016248764562394057] Loss: 22.84298588312474\n",
      "Iteracion: 11393 Gradiente: [0.0009413093232012671,-0.01624012622040117] Loss: 22.842985618285795\n",
      "Iteracion: 11394 Gradiente: [0.0009408088943104076,-0.016231492470814027] Loss: 22.842985353728384\n",
      "Iteracion: 11395 Gradiente: [0.0009403087313709572,-0.01622286331119843] Loss: 22.842985089452178\n",
      "Iteracion: 11396 Gradiente: [0.0009398088344227063,-0.016214238739104304] Loss: 22.8429848254569\n",
      "Iteracion: 11397 Gradiente: [0.0009393092031558581,-0.016205618752099582] Loss: 22.84298456174223\n",
      "Iteracion: 11398 Gradiente: [0.0009388098376319931,-0.016197003347738815] Loss: 22.842984298307897\n",
      "Iteracion: 11399 Gradiente: [0.0009383107374418386,-0.016188392523599878] Loss: 22.842984035153577\n",
      "Iteracion: 11400 Gradiente: [0.0009378119026403434,-0.016179786277236728] Loss: 22.842983772279\n",
      "Iteracion: 11401 Gradiente: [0.0009373133330835041,-0.01617118460621517] Loss: 22.842983509683854\n",
      "Iteracion: 11402 Gradiente: [0.0009368150284937353,-0.01616258750811023] Loss: 22.84298324736783\n",
      "Iteracion: 11403 Gradiente: [0.000936316988939249,-0.016153994980479543] Loss: 22.84298298533064\n",
      "Iteracion: 11404 Gradiente: [0.0009358192141358283,-0.016145407020902405] Loss: 22.842982723571986\n",
      "Iteracion: 11405 Gradiente: [0.0009353217038854685,-0.016136823626952908] Loss: 22.842982462091587\n",
      "Iteracion: 11406 Gradiente: [0.0009348244581701692,-0.016128244796196376] Loss: 22.842982200889132\n",
      "Iteracion: 11407 Gradiente: [0.0009343274768402428,-0.01611967052620713] Loss: 22.842981939964343\n",
      "Iteracion: 11408 Gradiente: [0.00093383075958684,-0.016111100814570515] Loss: 22.842981679316885\n",
      "Iteracion: 11409 Gradiente: [0.0009333343064857521,-0.01610253565885221] Loss: 22.84298141894652\n",
      "Iteracion: 11410 Gradiente: [0.000932838117320974,-0.01609397505663246] Loss: 22.842981158852904\n",
      "Iteracion: 11411 Gradiente: [0.0009323421918689215,-0.01608541900549625] Loss: 22.842980899035766\n",
      "Iteracion: 11412 Gradiente: [0.0009318465301674905,-0.01607686750301281] Loss: 22.842980639494805\n",
      "Iteracion: 11413 Gradiente: [0.000931351131976991,-0.016068320546770563] Loss: 22.842980380229722\n",
      "Iteracion: 11414 Gradiente: [0.0009308559971069977,-0.01605977813435615] Loss: 22.842980121240245\n",
      "Iteracion: 11415 Gradiente: [0.0009303611254874038,-0.01605124026334875] Loss: 22.842979862526075\n",
      "Iteracion: 11416 Gradiente: [0.0009298665170793659,-0.01604270693132932] Loss: 22.842979604086906\n",
      "Iteracion: 11417 Gradiente: [0.0009293721713968732,-0.016034178135901557] Loss: 22.842979345922448\n",
      "Iteracion: 11418 Gradiente: [0.0009288780886341405,-0.01602565387463907] Loss: 22.842979088032425\n",
      "Iteracion: 11419 Gradiente: [0.0009283842685315828,-0.01601713414513585] Loss: 22.842978830416527\n",
      "Iteracion: 11420 Gradiente: [0.0009278907109319334,-0.016008618944983392] Loss: 22.842978573074465\n",
      "Iteracion: 11421 Gradiente: [0.0009273974157508746,-0.01600010827177177] Loss: 22.842978316005947\n",
      "Iteracion: 11422 Gradiente: [0.0009269043828159814,-0.015991602123095087] Loss: 22.842978059210687\n",
      "Iteracion: 11423 Gradiente: [0.0009264116120154616,-0.01598310049654768] Loss: 22.84297780268842\n",
      "Iteracion: 11424 Gradiente: [0.0009259191031484685,-0.015974603389726973] Loss: 22.8429775464388\n",
      "Iteracion: 11425 Gradiente: [0.0009254268561051049,-0.015966110800230738] Loss: 22.84297729046159\n",
      "Iteracion: 11426 Gradiente: [0.0009249348708474751,-0.015957622725649875] Loss: 22.84297703475647\n",
      "Iteracion: 11427 Gradiente: [0.0009244431470563086,-0.015949139163594122] Loss: 22.842976779323145\n",
      "Iteracion: 11428 Gradiente: [0.0009239516846416033,-0.01594066011166433] Loss: 22.84297652416136\n",
      "Iteracion: 11429 Gradiente: [0.0009234604835531476,-0.01593218556745472] Loss: 22.84297626927081\n",
      "Iteracion: 11430 Gradiente: [0.0009229695436327271,-0.015923715528571827] Loss: 22.84297601465118\n",
      "Iteracion: 11431 Gradiente: [0.0009224788646889692,-0.015915249992621956] Loss: 22.84297576030222\n",
      "Iteracion: 11432 Gradiente: [0.0009219884466839782,-0.01590678895720643] Loss: 22.842975506223624\n",
      "Iteracion: 11433 Gradiente: [0.0009214982893581691,-0.015898332419940195] Loss: 22.84297525241512\n",
      "Iteracion: 11434 Gradiente: [0.0009210083925078531,-0.015889880378434758] Loss: 22.842974998876397\n",
      "Iteracion: 11435 Gradiente: [0.0009205187562571382,-0.0158814328302847] Loss: 22.84297474560719\n",
      "Iteracion: 11436 Gradiente: [0.0009200293801626458,-0.015872989773118698] Loss: 22.842974492607183\n",
      "Iteracion: 11437 Gradiente: [0.000919540264266061,-0.01586455120454081] Loss: 22.842974239876124\n",
      "Iteracion: 11438 Gradiente: [0.0009190514084271702,-0.015856117122161707] Loss: 22.842973987413714\n",
      "Iteracion: 11439 Gradiente: [0.0009185628124716535,-0.015847687523601432] Loss: 22.842973735219665\n",
      "Iteracion: 11440 Gradiente: [0.0009180744762924557,-0.015839262406472324] Loss: 22.84297348329368\n",
      "Iteracion: 11441 Gradiente: [0.0009175863998242069,-0.01583084176838921] Loss: 22.842973231635515\n",
      "Iteracion: 11442 Gradiente: [0.0009170985827646898,-0.015822425606978878] Loss: 22.842972980244834\n",
      "Iteracion: 11443 Gradiente: [0.0009166110249187417,-0.015814013919865744] Loss: 22.842972729121378\n",
      "Iteracion: 11444 Gradiente: [0.0009161237264341556,-0.015805606704654453] Loss: 22.842972478264873\n",
      "Iteracion: 11445 Gradiente: [0.0009156366869262911,-0.015797203958981616] Loss: 22.842972227675016\n",
      "Iteracion: 11446 Gradiente: [0.0009151499063714633,-0.01578880568046538] Loss: 22.84297197735153\n",
      "Iteracion: 11447 Gradiente: [0.0009146633845887209,-0.01578041186673218] Loss: 22.84297172729413\n",
      "Iteracion: 11448 Gradiente: [0.0009141771214369025,-0.015772022515409153] Loss: 22.842971477502545\n",
      "Iteracion: 11449 Gradiente: [0.0009136911168297956,-0.01576363762412143] Loss: 22.842971227976467\n",
      "Iteracion: 11450 Gradiente: [0.0009132053706177127,-0.015755257190498403] Loss: 22.84297097871565\n",
      "Iteracion: 11451 Gradiente: [0.0009127198825855961,-0.015746881212172662] Loss: 22.84297072971979\n",
      "Iteracion: 11452 Gradiente: [0.0009122346526889184,-0.01573850968677289] Loss: 22.842970480988587\n",
      "Iteracion: 11453 Gradiente: [0.0009117496807543072,-0.015730142611933336] Loss: 22.842970232521786\n",
      "Iteracion: 11454 Gradiente: [0.0009112649665685997,-0.015721779985291444] Loss: 22.842969984319108\n",
      "Iteracion: 11455 Gradiente: [0.000910780510110006,-0.015713421804477552] Loss: 22.84296973638026\n",
      "Iteracion: 11456 Gradiente: [0.0009102963112828396,-0.015705068067123542] Loss: 22.842969488704963\n",
      "Iteracion: 11457 Gradiente: [0.0009098123698824641,-0.01569671877087148] Loss: 22.842969241292945\n",
      "Iteracion: 11458 Gradiente: [0.0009093286856701374,-0.015688373913367925] Loss: 22.842968994143924\n",
      "Iteracion: 11459 Gradiente: [0.0009088452586335431,-0.015680033492244404] Loss: 22.842968747257608\n",
      "Iteracion: 11460 Gradiente: [0.0009083620886154146,-0.015671697505144522] Loss: 22.84296850063373\n",
      "Iteracion: 11461 Gradiente: [0.0009078791754082734,-0.015663365949713536] Loss: 22.84296825427202\n",
      "Iteracion: 11462 Gradiente: [0.0009073965189969613,-0.015655038823588777] Loss: 22.84296800817217\n",
      "Iteracion: 11463 Gradiente: [0.0009069141191484202,-0.01564671612442273] Loss: 22.842967762333917\n",
      "Iteracion: 11464 Gradiente: [0.0009064319757423315,-0.0156383978498603] Loss: 22.842967516757003\n",
      "Iteracion: 11465 Gradiente: [0.0009059500887114307,-0.015630083997545206] Loss: 22.842967271441125\n",
      "Iteracion: 11466 Gradiente: [0.0009054684578198173,-0.015621774565131474] Loss: 22.84296702638601\n",
      "Iteracion: 11467 Gradiente: [0.000904987082998332,-0.015613469550265317] Loss: 22.842966781591375\n",
      "Iteracion: 11468 Gradiente: [0.0009045059640736023,-0.015605168950602178] Loss: 22.842966537056974\n",
      "Iteracion: 11469 Gradiente: [0.0009040251009148885,-0.015596872763792173] Loss: 22.84296629278249\n",
      "Iteracion: 11470 Gradiente: [0.0009035444933961875,-0.015588580987489683] Loss: 22.842966048767675\n",
      "Iteracion: 11471 Gradiente: [0.0009030641414350763,-0.015580293619347192] Loss: 22.842965805012227\n",
      "Iteracion: 11472 Gradiente: [0.0009025840448003919,-0.015572010657027372] Loss: 22.8429655615159\n",
      "Iteracion: 11473 Gradiente: [0.0009021042034163429,-0.015563732098182943] Loss: 22.842965318278395\n",
      "Iteracion: 11474 Gradiente: [0.0009016246171986116,-0.015555457940470419] Loss: 22.842965075299453\n",
      "Iteracion: 11475 Gradiente: [0.0009011452858809813,-0.01554718818155815] Loss: 22.842964832578787\n",
      "Iteracion: 11476 Gradiente: [0.000900666209361134,-0.01553892281910431] Loss: 22.842964590116136\n",
      "Iteracion: 11477 Gradiente: [0.0009001873875414883,-0.015530661850770974] Loss: 22.842964347911206\n",
      "Iteracion: 11478 Gradiente: [0.0008997088202969887,-0.015522405274221522] Loss: 22.842964105963745\n",
      "Iteracion: 11479 Gradiente: [0.0008992305074627893,-0.015514153087122768] Loss: 22.842963864273475\n",
      "Iteracion: 11480 Gradiente: [0.0008987524489990998,-0.01550590528713336] Loss: 22.842963622840106\n",
      "Iteracion: 11481 Gradiente: [0.0008982746446198083,-0.015497661871932541] Loss: 22.84296338166337\n",
      "Iteracion: 11482 Gradiente: [0.0008977970941865958,-0.015489422839187483] Loss: 22.842963140743002\n",
      "Iteracion: 11483 Gradiente: [0.0008973197976899882,-0.015481188186561922] Loss: 22.842962900078728\n",
      "Iteracion: 11484 Gradiente: [0.0008968427549225073,-0.0154729579117312] Loss: 22.842962659670278\n",
      "Iteracion: 11485 Gradiente: [0.0008963659658149936,-0.015464732012364852] Loss: 22.842962419517384\n",
      "Iteracion: 11486 Gradiente: [0.0008958894301182833,-0.01545651048614225] Loss: 22.84296217961977\n",
      "Iteracion: 11487 Gradiente: [0.0008954131477707961,-0.015448293330735888] Loss: 22.84296193997715\n",
      "Iteracion: 11488 Gradiente: [0.0008949371186380025,-0.015440080543820874] Loss: 22.84296170058926\n",
      "Iteracion: 11489 Gradiente: [0.0008944613426412691,-0.015431872123071481] Loss: 22.842961461455854\n",
      "Iteracion: 11490 Gradiente: [0.000893985819600592,-0.01542366806616992] Loss: 22.842961222576623\n",
      "Iteracion: 11491 Gradiente: [0.0008935105492668072,-0.015415468370801478] Loss: 22.842960983951325\n",
      "Iteracion: 11492 Gradiente: [0.0008930355315849662,-0.015407273034644575] Loss: 22.842960745579678\n",
      "Iteracion: 11493 Gradiente: [0.0008925607665711747,-0.015399082055371474] Loss: 22.84296050746142\n",
      "Iteracion: 11494 Gradiente: [0.0008920862537934226,-0.015390895430684164] Loss: 22.842960269596265\n",
      "Iteracion: 11495 Gradiente: [0.0008916119933985555,-0.015382713158251704] Loss: 22.842960031983964\n",
      "Iteracion: 11496 Gradiente: [0.0008911379851194094,-0.015374535235765544] Loss: 22.842959794624242\n",
      "Iteracion: 11497 Gradiente: [0.0008906642287968225,-0.015366361660917486] Loss: 22.842959557516828\n",
      "Iteracion: 11498 Gradiente: [0.000890190724348372,-0.015358192431393055] Loss: 22.84295932066145\n",
      "Iteracion: 11499 Gradiente: [0.0008897174716660553,-0.015350027544877775] Loss: 22.842959084057835\n",
      "Iteracion: 11500 Gradiente: [0.0008892444705073406,-0.015341866999072806] Loss: 22.84295884770574\n",
      "Iteracion: 11501 Gradiente: [0.0008887717208646488,-0.015333710791659527] Loss: 22.84295861160488\n",
      "Iteracion: 11502 Gradiente: [0.000888299222532396,-0.015325558920338505] Loss: 22.842958375754982\n",
      "Iteracion: 11503 Gradiente: [0.000887826975332473,-0.015317411382806274] Loss: 22.842958140155798\n",
      "Iteracion: 11504 Gradiente: [0.0008873549793131966,-0.015309268176748011] Loss: 22.842957904807054\n",
      "Iteracion: 11505 Gradiente: [0.0008868832341988764,-0.01530112929986925] Loss: 22.842957669708465\n",
      "Iteracion: 11506 Gradiente: [0.0008864117398568775,-0.015292994749868664] Loss: 22.84295743485979\n",
      "Iteracion: 11507 Gradiente: [0.000885940496201935,-0.015284864524442198] Loss: 22.842957200260752\n",
      "Iteracion: 11508 Gradiente: [0.0008854695029507791,-0.015276738621301196] Loss: 22.842956965911085\n",
      "Iteracion: 11509 Gradiente: [0.0008849987601621479,-0.015268617038135682] Loss: 22.842956731810542\n",
      "Iteracion: 11510 Gradiente: [0.0008845282677195125,-0.01526049977264871] Loss: 22.842956497958824\n",
      "Iteracion: 11511 Gradiente: [0.0008840580252808649,-0.015252386822556118] Loss: 22.84295626435569\n",
      "Iteracion: 11512 Gradiente: [0.0008835880329437865,-0.015244278185550778] Loss: 22.842956031000874\n",
      "Iteracion: 11513 Gradiente: [0.000883118290394691,-0.01523617385934936] Loss: 22.8429557978941\n",
      "Iteracion: 11514 Gradiente: [0.0008826487975776824,-0.015228073841656926] Loss: 22.842955565035123\n",
      "Iteracion: 11515 Gradiente: [0.0008821795543913898,-0.015219978130180323] Loss: 22.842955332423674\n",
      "Iteracion: 11516 Gradiente: [0.0008817105607107578,-0.015211886722629705] Loss: 22.842955100059477\n",
      "Iteracion: 11517 Gradiente: [0.0008812418162695697,-0.015203799616725532] Loss: 22.842954867942282\n",
      "Iteracion: 11518 Gradiente: [0.0008807733211474064,-0.015195716810167426] Loss: 22.84295463607183\n",
      "Iteracion: 11519 Gradiente: [0.0008803050749833119,-0.01518763830068508] Loss: 22.842954404447838\n",
      "Iteracion: 11520 Gradiente: [0.0008798370777337065,-0.015179564085986167] Loss: 22.84295417307006\n",
      "Iteracion: 11521 Gradiente: [0.0008793693294061692,-0.015171494163781319] Loss: 22.842953941938244\n",
      "Iteracion: 11522 Gradiente: [0.0008789018296852191,-0.01516342853179798] Loss: 22.84295371105211\n",
      "Iteracion: 11523 Gradiente: [0.0008784345784912754,-0.015155367187752233] Loss: 22.842953480411403\n",
      "Iteracion: 11524 Gradiente: [0.0008779675757049669,-0.015147310129365129] Loss: 22.84295325001585\n",
      "Iteracion: 11525 Gradiente: [0.0008775008211690268,-0.01513925735435855] Loss: 22.84295301986522\n",
      "Iteracion: 11526 Gradiente: [0.0008770343148057691,-0.015131208860452954] Loss: 22.842952789959224\n",
      "Iteracion: 11527 Gradiente: [0.0008765680564910857,-0.015123164645371527] Loss: 22.842952560297615\n",
      "Iteracion: 11528 Gradiente: [0.0008761020459691812,-0.015115124706845862] Loss: 22.84295233088014\n",
      "Iteracion: 11529 Gradiente: [0.0008756362831813173,-0.01510708904259855] Loss: 22.842952101706523\n",
      "Iteracion: 11530 Gradiente: [0.0008751707680535977,-0.015099057650355381] Loss: 22.842951872776514\n",
      "Iteracion: 11531 Gradiente: [0.000874705500432545,-0.015091030527845462] Loss: 22.84295164408986\n",
      "Iteracion: 11532 Gradiente: [0.0008742404801381553,-0.015083007672800974] Loss: 22.842951415646272\n",
      "Iteracion: 11533 Gradiente: [0.0008737757070586364,-0.015074989082953157] Loss: 22.842951187445536\n",
      "Iteracion: 11534 Gradiente: [0.0008733111810547219,-0.015066974756034432] Loss: 22.842950959487382\n",
      "Iteracion: 11535 Gradiente: [0.0008728469019767241,-0.015058964689779585] Loss: 22.842950731771523\n",
      "Iteracion: 11536 Gradiente: [0.0008723828698767496,-0.015050958881912399] Loss: 22.84295050429772\n",
      "Iteracion: 11537 Gradiente: [0.0008719190843673156,-0.015042957330185184] Loss: 22.84295027706573\n",
      "Iteracion: 11538 Gradiente: [0.0008714555454389483,-0.015034960032327878] Loss: 22.84295005007526\n",
      "Iteracion: 11539 Gradiente: [0.000870992252869011,-0.015026966986082494] Loss: 22.84294982332609\n",
      "Iteracion: 11540 Gradiente: [0.0008705292067200314,-0.015018978189179204] Loss: 22.842949596817956\n",
      "Iteracion: 11541 Gradiente: [0.0008700664066518964,-0.015010993639371506] Loss: 22.842949370550592\n",
      "Iteracion: 11542 Gradiente: [0.0008696038525878672,-0.015003013334397271] Loss: 22.842949144523747\n",
      "Iteracion: 11543 Gradiente: [0.0008691415445772084,-0.014995037271989038] Loss: 22.842948918737157\n",
      "Iteracion: 11544 Gradiente: [0.0008686794822523325,-0.014987065449906349] Loss: 22.84294869319057\n",
      "Iteracion: 11545 Gradiente: [0.000868217665491026,-0.01497909786589382] Loss: 22.84294846788374\n",
      "Iteracion: 11546 Gradiente: [0.0008677560943359216,-0.014971134517687546] Loss: 22.84294824281641\n",
      "Iteracion: 11547 Gradiente: [0.0008672947685748037,-0.014963175403041973] Loss: 22.842948017988324\n",
      "Iteracion: 11548 Gradiente: [0.0008668336880077732,-0.014955220519709538] Loss: 22.842947793399215\n",
      "Iteracion: 11549 Gradiente: [0.0008663728526556724,-0.014947269865431424] Loss: 22.84294756904885\n",
      "Iteracion: 11550 Gradiente: [0.0008659122623508134,-0.014939323437962788] Loss: 22.842947344936967\n",
      "Iteracion: 11551 Gradiente: [0.0008654519167833996,-0.01493138123506507] Loss: 22.842947121063307\n",
      "Iteracion: 11552 Gradiente: [0.0008649918159449044,-0.01492344325448715] Loss: 22.84294689742761\n",
      "Iteracion: 11553 Gradiente: [0.0008645319597254305,-0.014915509493982525] Loss: 22.84294667402964\n",
      "Iteracion: 11554 Gradiente: [0.0008640723479876063,-0.014907579951308491] Loss: 22.842946450869125\n",
      "Iteracion: 11555 Gradiente: [0.0008636129806139555,-0.014899654624221033] Loss: 22.84294622794584\n",
      "Iteracion: 11556 Gradiente: [0.0008631538574301582,-0.014891733510482178] Loss: 22.842946005259517\n",
      "Iteracion: 11557 Gradiente: [0.0008626949783357911,-0.014883816607850164] Loss: 22.8429457828099\n",
      "Iteracion: 11558 Gradiente: [0.0008622363432266411,-0.014875903914085005] Loss: 22.842945560596743\n",
      "Iteracion: 11559 Gradiente: [0.0008617779519302834,-0.014867995426951571] Loss: 22.84294533861981\n",
      "Iteracion: 11560 Gradiente: [0.000861319804333031,-0.014860091144212243] Loss: 22.842945116878806\n",
      "Iteracion: 11561 Gradiente: [0.000860861900221721,-0.014852191063637103] Loss: 22.84294489537353\n",
      "Iteracion: 11562 Gradiente: [0.0008604042396077224,-0.014844295182984031] Loss: 22.842944674103688\n",
      "Iteracion: 11563 Gradiente: [0.0008599468223413471,-0.014836403500019439] Loss: 22.84294445306906\n",
      "Iteracion: 11564 Gradiente: [0.000859489648168695,-0.014828516012522759] Loss: 22.842944232269407\n",
      "Iteracion: 11565 Gradiente: [0.0008590327171598725,-0.014820632718248206] Loss: 22.842944011704436\n",
      "Iteracion: 11566 Gradiente: [0.0008585760289823459,-0.014812753614980304] Loss: 22.842943791373933\n",
      "Iteracion: 11567 Gradiente: [0.0008581195836294835,-0.014804878700483333] Loss: 22.842943571277615\n",
      "Iteracion: 11568 Gradiente: [0.0008576633809307548,-0.014797007972533057] Loss: 22.842943351415286\n",
      "Iteracion: 11569 Gradiente: [0.0008572074207232087,-0.014789141428906187] Loss: 22.842943131786633\n",
      "Iteracion: 11570 Gradiente: [0.0008567517029140011,-0.014781279067375171] Loss: 22.84294291239148\n",
      "Iteracion: 11571 Gradiente: [0.0008562962273809187,-0.014773420885716959] Loss: 22.84294269322951\n",
      "Iteracion: 11572 Gradiente: [0.0008558409940254327,-0.014765566881707434] Loss: 22.842942474300514\n",
      "Iteracion: 11573 Gradiente: [0.0008553860026144851,-0.014757717053132662] Loss: 22.842942255604225\n",
      "Iteracion: 11574 Gradiente: [0.0008549312532418678,-0.014749871397756801] Loss: 22.842942037140414\n",
      "Iteracion: 11575 Gradiente: [0.0008544767454973604,-0.014742029913380488] Loss: 22.842941818908834\n",
      "Iteracion: 11576 Gradiente: [0.0008540224794150693,-0.014734192597776996] Loss: 22.84294160090921\n",
      "Iteracion: 11577 Gradiente: [0.0008535684549305718,-0.014726359448724462] Loss: 22.842941383141337\n",
      "Iteracion: 11578 Gradiente: [0.0008531146717141761,-0.014718530464020555] Loss: 22.842941165604923\n",
      "Iteracion: 11579 Gradiente: [0.0008526611297668297,-0.014710705641444121] Loss: 22.842940948299752\n",
      "Iteracion: 11580 Gradiente: [0.0008522078290080041,-0.014702884978779807] Loss: 22.842940731225585\n",
      "Iteracion: 11581 Gradiente: [0.0008517547691942203,-0.014695068473821493] Loss: 22.84294051438216\n",
      "Iteracion: 11582 Gradiente: [0.0008513019502136861,-0.014687256124359394] Loss: 22.842940297769236\n",
      "Iteracion: 11583 Gradiente: [0.0008508493719214509,-0.014679447928183839] Loss: 22.842940081386544\n",
      "Iteracion: 11584 Gradiente: [0.0008503970342729871,-0.01467164388308433] Loss: 22.842939865233877\n",
      "Iteracion: 11585 Gradiente: [0.0008499449371119756,-0.014663843986852736] Loss: 22.84293964931099\n",
      "Iteracion: 11586 Gradiente: [0.0008494930803038869,-0.014656048237285783] Loss: 22.84293943361761\n",
      "Iteracion: 11587 Gradiente: [0.0008490414636971384,-0.014648256632179486] Loss: 22.84293921815351\n",
      "Iteracion: 11588 Gradiente: [0.0008485900871894121,-0.014640469169329033] Loss: 22.842939002918452\n",
      "Iteracion: 11589 Gradiente: [0.0008481389506840742,-0.014632685846530318] Loss: 22.842938787912182\n",
      "Iteracion: 11590 Gradiente: [0.0008476880540428056,-0.014624906661583736] Loss: 22.842938573134443\n",
      "Iteracion: 11591 Gradiente: [0.0008472373970372852,-0.014617131612295135] Loss: 22.842938358585013\n",
      "Iteracion: 11592 Gradiente: [0.0008467869795500368,-0.014609360696465975] Loss: 22.842938144263655\n",
      "Iteracion: 11593 Gradiente: [0.0008463368015886393,-0.01460159391188931] Loss: 22.84293793017012\n",
      "Iteracion: 11594 Gradiente: [0.0008458868630109843,-0.014593831256370393] Loss: 22.842937716304156\n",
      "Iteracion: 11595 Gradiente: [0.0008454371636048563,-0.014586072727718976] Loss: 22.84293750266551\n",
      "Iteracion: 11596 Gradiente: [0.0008449877032347785,-0.01457831832374244] Loss: 22.842937289253978\n",
      "Iteracion: 11597 Gradiente: [0.0008445384818912771,-0.014570568042240358] Loss: 22.84293707606929\n",
      "Iteracion: 11598 Gradiente: [0.0008440894993327674,-0.014562821881027924] Loss: 22.842936863111227\n",
      "Iteracion: 11599 Gradiente: [0.0008436407553887193,-0.014555079837915604] Loss: 22.8429366503795\n",
      "Iteracion: 11600 Gradiente: [0.0008431922500430271,-0.014547341910710544] Loss: 22.842936437873934\n",
      "Iteracion: 11601 Gradiente: [0.0008427439831611613,-0.014539608097223914] Loss: 22.842936225594244\n",
      "Iteracion: 11602 Gradiente: [0.0008422959545526965,-0.014531878395271747] Loss: 22.842936013540207\n",
      "Iteracion: 11603 Gradiente: [0.0008418481641854214,-0.0145241528026632] Loss: 22.84293580171158\n",
      "Iteracion: 11604 Gradiente: [0.0008414006118291202,-0.01451643131721975] Loss: 22.84293559010812\n",
      "Iteracion: 11605 Gradiente: [0.00084095329748853,-0.014508713936749847] Loss: 22.842935378729592\n",
      "Iteracion: 11606 Gradiente: [0.0008405062208993285,-0.014501000659079702] Loss: 22.842935167575757\n",
      "Iteracion: 11607 Gradiente: [0.0008400593820340419,-0.01449329148202132] Loss: 22.842934956646367\n",
      "Iteracion: 11608 Gradiente: [0.0008396127806842439,-0.014485586403401148] Loss: 22.842934745941182\n",
      "Iteracion: 11609 Gradiente: [0.00083916641673909,-0.014477885421038767] Loss: 22.842934535459975\n",
      "Iteracion: 11610 Gradiente: [0.0008387202900886829,-0.014470188532755775] Loss: 22.842934325202513\n",
      "Iteracion: 11611 Gradiente: [0.0008382744006761793,-0.01446249573637021] Loss: 22.842934115168553\n",
      "Iteracion: 11612 Gradiente: [0.0008378287481804137,-0.014454807029719538] Loss: 22.84293390535786\n",
      "Iteracion: 11613 Gradiente: [0.0008373833327501264,-0.01444712241061256] Loss: 22.842933695770185\n",
      "Iteracion: 11614 Gradiente: [0.0008369381541067848,-0.014439441876886267] Loss: 22.842933486405297\n",
      "Iteracion: 11615 Gradiente: [0.0008364932120789111,-0.01443176542636948] Loss: 22.842933277262947\n",
      "Iteracion: 11616 Gradiente: [0.0008360485066380836,-0.014424093056887462] Loss: 22.842933068342926\n",
      "Iteracion: 11617 Gradiente: [0.0008356040375796663,-0.014416424766274716] Loss: 22.84293285964499\n",
      "Iteracion: 11618 Gradiente: [0.0008351598048828161,-0.014408760552356152] Loss: 22.842932651168887\n",
      "Iteracion: 11619 Gradiente: [0.0008347158082642637,-0.014401100412975272] Loss: 22.842932442914375\n",
      "Iteracion: 11620 Gradiente: [0.0008342720477259036,-0.01439344434595616] Loss: 22.842932234881243\n",
      "Iteracion: 11621 Gradiente: [0.0008338285230744684,-0.014385792349139592] Loss: 22.84293202706925\n",
      "Iteracion: 11622 Gradiente: [0.0008333852341867972,-0.014378144420361257] Loss: 22.842931819478146\n",
      "Iteracion: 11623 Gradiente: [0.0008329421810733113,-0.014370500557450564] Loss: 22.842931612107723\n",
      "Iteracion: 11624 Gradiente: [0.0008324993634156878,-0.014362860758256938] Loss: 22.84293140495773\n",
      "Iteracion: 11625 Gradiente: [0.0008320567812120317,-0.014355225020612513] Loss: 22.84293119802792\n",
      "Iteracion: 11626 Gradiente: [0.0008316144343088657,-0.014347593342359133] Loss: 22.842930991318067\n",
      "Iteracion: 11627 Gradiente: [0.0008311723225394493,-0.014339965721343616] Loss: 22.842930784827953\n",
      "Iteracion: 11628 Gradiente: [0.000830730445815675,-0.014332342155404021] Loss: 22.842930578557333\n",
      "Iteracion: 11629 Gradiente: [0.0008302888040153297,-0.014324722642386215] Loss: 22.842930372505975\n",
      "Iteracion: 11630 Gradiente: [0.0008298473970342002,-0.014317107180133822] Loss: 22.842930166673646\n",
      "Iteracion: 11631 Gradiente: [0.0008294062246344917,-0.014309495766500055] Loss: 22.842929961060122\n",
      "Iteracion: 11632 Gradiente: [0.0008289652868095724,-0.014301888399324744] Loss: 22.842929755665143\n",
      "Iteracion: 11633 Gradiente: [0.0008285245834088073,-0.014294285076459683] Loss: 22.8429295504885\n",
      "Iteracion: 11634 Gradiente: [0.000828084114283456,-0.014286685795754412] Loss: 22.842929345529956\n",
      "Iteracion: 11635 Gradiente: [0.0008276438793378323,-0.014279090555060136] Loss: 22.842929140789277\n",
      "Iteracion: 11636 Gradiente: [0.0008272038784250905,-0.014271499352229473] Loss: 22.84292893626624\n",
      "Iteracion: 11637 Gradiente: [0.0008267641114893346,-0.014263912185111256] Loss: 22.842928731960612\n",
      "Iteracion: 11638 Gradiente: [0.000826324578342034,-0.014256329051564503] Loss: 22.84292852787214\n",
      "Iteracion: 11639 Gradiente: [0.0008258852788353958,-0.014248749949444316] Loss: 22.8429283240006\n",
      "Iteracion: 11640 Gradiente: [0.0008254462128832075,-0.01424117487660972] Loss: 22.8429281203458\n",
      "Iteracion: 11641 Gradiente: [0.0008250073802732534,-0.014233603830919487] Loss: 22.842927916907477\n",
      "Iteracion: 11642 Gradiente: [0.000824568781112589,-0.014226036810221032] Loss: 22.842927713685388\n",
      "Iteracion: 11643 Gradiente: [0.0008241304150904701,-0.014218473812384029] Loss: 22.842927510679328\n",
      "Iteracion: 11644 Gradiente: [0.0008236922820335242,-0.014210914835275545] Loss: 22.84292730788905\n",
      "Iteracion: 11645 Gradiente: [0.0008232543819559623,-0.014203359876746428] Loss: 22.842927105314352\n",
      "Iteracion: 11646 Gradiente: [0.0008228167145858834,-0.014195808934671324] Loss: 22.84292690295496\n",
      "Iteracion: 11647 Gradiente: [0.0008223792800436059,-0.014188262006899658] Loss: 22.842926700810704\n",
      "Iteracion: 11648 Gradiente: [0.0008219420779473315,-0.014180719091313184] Loss: 22.84292649888131\n",
      "Iteracion: 11649 Gradiente: [0.0008215051082875864,-0.014173180185771154] Loss: 22.84292629716656\n",
      "Iteracion: 11650 Gradiente: [0.00082106837087584,-0.014165645288146796] Loss: 22.842926095666225\n",
      "Iteracion: 11651 Gradiente: [0.0008206318657850413,-0.01415811439629735] Loss: 22.842925894380077\n",
      "Iteracion: 11652 Gradiente: [0.0008201955926267601,-0.014150587508108122] Loss: 22.842925693307905\n",
      "Iteracion: 11653 Gradiente: [0.000819759551591422,-0.014143064621433155] Loss: 22.842925492449464\n",
      "Iteracion: 11654 Gradiente: [0.0008193237422062793,-0.014135545734165096] Loss: 22.84292529180453\n",
      "Iteracion: 11655 Gradiente: [0.0008188881645954401,-0.014128030844162845] Loss: 22.84292509137288\n",
      "Iteracion: 11656 Gradiente: [0.0008184528185002667,-0.014120519949309695] Loss: 22.842924891154276\n",
      "Iteracion: 11657 Gradiente: [0.0008180177038961271,-0.014113013047476504] Loss: 22.842924691148504\n",
      "Iteracion: 11658 Gradiente: [0.0008175828205717532,-0.014105510136544552] Loss: 22.842924491355348\n",
      "Iteracion: 11659 Gradiente: [0.0008171481684134582,-0.01409801121439358] Loss: 22.842924291774544\n",
      "Iteracion: 11660 Gradiente: [0.0008167137472838704,-0.014090516278901788] Loss: 22.8429240924059\n",
      "Iteracion: 11661 Gradiente: [0.0008162795572047799,-0.014083025327941456] Loss: 22.842923893249186\n",
      "Iteracion: 11662 Gradiente: [0.0008158455979459708,-0.014075538359401918] Loss: 22.842923694304186\n",
      "Iteracion: 11663 Gradiente: [0.0008154118693463867,-0.014068055371166939] Loss: 22.842923495570638\n",
      "Iteracion: 11664 Gradiente: [0.0008149783714022381,-0.014060576361114012] Loss: 22.84292329704833\n",
      "Iteracion: 11665 Gradiente: [0.0008145451038927831,-0.014053101327133888] Loss: 22.842923098737067\n",
      "Iteracion: 11666 Gradiente: [0.0008141120667715995,-0.014045630267107256] Loss: 22.842922900636605\n",
      "Iteracion: 11667 Gradiente: [0.0008136792597431016,-0.014038163178933635] Loss: 22.842922702746705\n",
      "Iteracion: 11668 Gradiente: [0.0008132466828935018,-0.014030700060488026] Loss: 22.84292250506717\n",
      "Iteracion: 11669 Gradiente: [0.0008128143360238482,-0.0140232409096645] Loss: 22.842922307597757\n",
      "Iteracion: 11670 Gradiente: [0.0008123822189446628,-0.014015785724358664] Loss: 22.842922110338243\n",
      "Iteracion: 11671 Gradiente: [0.0008119503315915229,-0.01400833450245796] Loss: 22.842921913288436\n",
      "Iteracion: 11672 Gradiente: [0.0008115186738763214,-0.014000887241852904] Loss: 22.842921716448068\n",
      "Iteracion: 11673 Gradiente: [0.0008110872456796869,-0.01399344394043863] Loss: 22.84292151981695\n",
      "Iteracion: 11674 Gradiente: [0.0008106560468187732,-0.013986004596114298] Loss: 22.842921323394833\n",
      "Iteracion: 11675 Gradiente: [0.0008102250772121048,-0.013978569206772325] Loss: 22.842921127181526\n",
      "Iteracion: 11676 Gradiente: [0.0008097943366540979,-0.013971137770314949] Loss: 22.842920931176764\n",
      "Iteracion: 11677 Gradiente: [0.0008093638250907513,-0.013963710284637661] Loss: 22.84292073538038\n",
      "Iteracion: 11678 Gradiente: [0.0008089335424424841,-0.0139562867476369] Loss: 22.8429205397921\n",
      "Iteracion: 11679 Gradiente: [0.0008085034886060309,-0.013948867157213959] Loss: 22.842920344411734\n",
      "Iteracion: 11680 Gradiente: [0.0008080736632791741,-0.01394145151127925] Loss: 22.84292014923906\n",
      "Iteracion: 11681 Gradiente: [0.0008076440665291784,-0.013934039807725895] Loss: 22.842919954273853\n",
      "Iteracion: 11682 Gradiente: [0.0008072146981125646,-0.013926632044463953] Loss: 22.842919759515876\n",
      "Iteracion: 11683 Gradiente: [0.0008067855580009109,-0.013919228219393532] Loss: 22.842919564964923\n",
      "Iteracion: 11684 Gradiente: [0.0008063566460909517,-0.013911828330421254] Loss: 22.842919370620788\n",
      "Iteracion: 11685 Gradiente: [0.0008059279621060492,-0.013904432375462506] Loss: 22.842919176483214\n",
      "Iteracion: 11686 Gradiente: [0.0008054995060831516,-0.013897040352417752] Loss: 22.842918982552025\n",
      "Iteracion: 11687 Gradiente: [0.000805071277843202,-0.01388965225919847] Loss: 22.84291878882698\n",
      "Iteracion: 11688 Gradiente: [0.0008046432772346179,-0.013882268093719338] Loss: 22.842918595307857\n",
      "Iteracion: 11689 Gradiente: [0.0008042155041115014,-0.013874887853891] Loss: 22.84291840199443\n",
      "Iteracion: 11690 Gradiente: [0.000803787958417009,-0.013867511537623993] Loss: 22.8429182088865\n",
      "Iteracion: 11691 Gradiente: [0.0008033606400999815,-0.01386013914282979] Loss: 22.84291801598384\n",
      "Iteracion: 11692 Gradiente: [0.0008029335489766254,-0.013852770667423898] Loss: 22.84291782328622\n",
      "Iteracion: 11693 Gradiente: [0.000802506684848936,-0.013845406109329635] Loss: 22.84291763079345\n",
      "Iteracion: 11694 Gradiente: [0.0008020800476373324,-0.013838045466462152] Loss: 22.84291743850529\n",
      "Iteracion: 11695 Gradiente: [0.0008016536372556023,-0.013830688736736717] Loss: 22.842917246421525\n",
      "Iteracion: 11696 Gradiente: [0.0008012274535957431,-0.013823335918074164] Loss: 22.842917054541946\n",
      "Iteracion: 11697 Gradiente: [0.0008008014965251201,-0.013815987008393668] Loss: 22.842916862866332\n",
      "Iteracion: 11698 Gradiente: [0.0008003757658997301,-0.013808642005619258] Loss: 22.842916671394462\n",
      "Iteracion: 11699 Gradiente: [0.0007999502615556746,-0.013801300907677927] Loss: 22.842916480126117\n",
      "Iteracion: 11700 Gradiente: [0.0007995249834304256,-0.01379396371248814] Loss: 22.842916289061105\n",
      "Iteracion: 11701 Gradiente: [0.0007990999313932434,-0.013786630417977837] Loss: 22.84291609819917\n",
      "Iteracion: 11702 Gradiente: [0.0007986751054024428,-0.01377930102206643] Loss: 22.842915907540124\n",
      "Iteracion: 11703 Gradiente: [0.0007982505051946494,-0.01377197552269062] Loss: 22.842915717083738\n",
      "Iteracion: 11704 Gradiente: [0.0007978261306864927,-0.013764653917777281] Loss: 22.842915526829813\n",
      "Iteracion: 11705 Gradiente: [0.000797401981759549,-0.013757336205255063] Loss: 22.84291533677812\n",
      "Iteracion: 11706 Gradiente: [0.0007969780584393978,-0.013750022383047521] Loss: 22.842915146928444\n",
      "Iteracion: 11707 Gradiente: [0.0007965543603963473,-0.013742712449096691] Loss: 22.842914957280588\n",
      "Iteracion: 11708 Gradiente: [0.0007961308876701878,-0.013735406401329324] Loss: 22.842914767834312\n",
      "Iteracion: 11709 Gradiente: [0.0007957076400420723,-0.013728104237681412] Loss: 22.842914578589408\n",
      "Iteracion: 11710 Gradiente: [0.0007952846174665259,-0.013720805956086105] Loss: 22.842914389545683\n",
      "Iteracion: 11711 Gradiente: [0.0007948618197663866,-0.013713511554482475] Loss: 22.8429142007029\n",
      "Iteracion: 11712 Gradiente: [0.0007944392467872301,-0.013706221030809828] Loss: 22.84291401206086\n",
      "Iteracion: 11713 Gradiente: [0.000794016898529056,-0.013698934382998237] Loss: 22.84291382361933\n",
      "Iteracion: 11714 Gradiente: [0.0007935947748147024,-0.013691651608991625] Loss: 22.842913635378125\n",
      "Iteracion: 11715 Gradiente: [0.0007931728754774288,-0.01368437270673392] Loss: 22.842913447336993\n",
      "Iteracion: 11716 Gradiente: [0.0007927512004499704,-0.0136770976741636] Loss: 22.842913259495752\n",
      "Iteracion: 11717 Gradiente: [0.0007923297495731655,-0.013669826509224355] Loss: 22.842913071854195\n",
      "Iteracion: 11718 Gradiente: [0.0007919085227295378,-0.013662559209861768] Loss: 22.842912884412097\n",
      "Iteracion: 11719 Gradiente: [0.0007914875198309801,-0.013655295774017754] Loss: 22.842912697169236\n",
      "Iteracion: 11720 Gradiente: [0.0007910667408007536,-0.013648036199636119] Loss: 22.842912510125412\n",
      "Iteracion: 11721 Gradiente: [0.000790646185400116,-0.013640780484672276] Loss: 22.842912323280412\n",
      "Iteracion: 11722 Gradiente: [0.0007902258536338044,-0.013633528627064113] Loss: 22.842912136634023\n",
      "Iteracion: 11723 Gradiente: [0.0007898057452886557,-0.013626280624769766] Loss: 22.842911950186032\n",
      "Iteracion: 11724 Gradiente: [0.0007893858603533014,-0.013619036475731624] Loss: 22.84291176393624\n",
      "Iteracion: 11725 Gradiente: [0.0007889661985264714,-0.013611796177912083] Loss: 22.84291157788443\n",
      "Iteracion: 11726 Gradiente: [0.0007885467598678513,-0.013604559729253651] Loss: 22.842911392030373\n",
      "Iteracion: 11727 Gradiente: [0.0007881275442647014,-0.013597327127709728] Loss: 22.842911206373888\n",
      "Iteracion: 11728 Gradiente: [0.0007877085513769089,-0.013590098371247214] Loss: 22.84291102091476\n",
      "Iteracion: 11729 Gradiente: [0.0007872897813939516,-0.01358287345780494] Loss: 22.84291083565275\n",
      "Iteracion: 11730 Gradiente: [0.0007868712340069805,-0.013575652385350405] Loss: 22.842910650587676\n",
      "Iteracion: 11731 Gradiente: [0.0007864529090937822,-0.013568435151841503] Loss: 22.842910465719328\n",
      "Iteracion: 11732 Gradiente: [0.0007860348066373035,-0.013561221755232111] Loss: 22.84291028104749\n",
      "Iteracion: 11733 Gradiente: [0.00078561692643954,-0.013554012193486998] Loss: 22.84291009657196\n",
      "Iteracion: 11734 Gradiente: [0.0007851992683110136,-0.01354680646457093] Loss: 22.842909912292512\n",
      "Iteracion: 11735 Gradiente: [0.0007847818322735141,-0.01353960456643885] Loss: 22.842909728208962\n",
      "Iteracion: 11736 Gradiente: [0.000784364618169775,-0.013532406497055878] Loss: 22.84290954432107\n",
      "Iteracion: 11737 Gradiente: [0.0007839476258775827,-0.013525212254387497] Loss: 22.842909360628664\n",
      "Iteracion: 11738 Gradiente: [0.0007835308552008276,-0.013518021836402501] Loss: 22.8429091771315\n",
      "Iteracion: 11739 Gradiente: [0.0007831143061366675,-0.013510835241063054] Loss: 22.842908993829415\n",
      "Iteracion: 11740 Gradiente: [0.000782697978587521,-0.013503652466333449] Loss: 22.84290881072216\n",
      "Iteracion: 11741 Gradiente: [0.0007822818722559077,-0.013496473510194324] Loss: 22.842908627809543\n",
      "Iteracion: 11742 Gradiente: [0.000781865987268778,-0.013489298370601333] Loss: 22.84290844509137\n",
      "Iteracion: 11743 Gradiente: [0.0007814503232803342,-0.013482127045538897] Loss: 22.842908262567416\n",
      "Iteracion: 11744 Gradiente: [0.0007810348802744708,-0.013474959532972737] Loss: 22.842908080237482\n",
      "Iteracion: 11745 Gradiente: [0.0007806196581147636,-0.013467795830876976] Loss: 22.84290789810136\n",
      "Iteracion: 11746 Gradiente: [0.0007802046568135286,-0.013460635937218631] Loss: 22.842907716158837\n",
      "Iteracion: 11747 Gradiente: [0.0007797898760638115,-0.013453479849982842] Loss: 22.842907534409733\n",
      "Iteracion: 11748 Gradiente: [0.0007793753158305587,-0.013446327567143613] Loss: 22.84290735285381\n",
      "Iteracion: 11749 Gradiente: [0.0007789609760266103,-0.013439179086674831] Loss: 22.842907171490868\n",
      "Iteracion: 11750 Gradiente: [0.0007785468564729096,-0.01343203440655903] Loss: 22.84290699032074\n",
      "Iteracion: 11751 Gradiente: [0.0007781329570586119,-0.013424893524774955] Loss: 22.842906809343166\n",
      "Iteracion: 11752 Gradiente: [0.0007777192776946625,-0.013417756439303711] Loss: 22.84290662855798\n",
      "Iteracion: 11753 Gradiente: [0.0007773058183033754,-0.013410623148122388] Loss: 22.84290644796497\n",
      "Iteracion: 11754 Gradiente: [0.0007768925785986388,-0.013403493649224885] Loss: 22.842906267563915\n",
      "Iteracion: 11755 Gradiente: [0.0007764795587329824,-0.01339636794057905] Loss: 22.842906087354624\n",
      "Iteracion: 11756 Gradiente: [0.0007760667583359766,-0.01338924602018589] Loss: 22.842905907336895\n",
      "Iteracion: 11757 Gradiente: [0.0007756541773630942,-0.013382127886025568] Loss: 22.842905727510516\n",
      "Iteracion: 11758 Gradiente: [0.0007752418157963348,-0.013375013536079786] Loss: 22.842905547875297\n",
      "Iteracion: 11759 Gradiente: [0.0007748296734263249,-0.013367902968342567] Loss: 22.842905368431015\n",
      "Iteracion: 11760 Gradiente: [0.0007744177501166405,-0.013360796180804376] Loss: 22.842905189177483\n",
      "Iteracion: 11761 Gradiente: [0.0007740060459080193,-0.013353693171445968] Loss: 22.842905010114485\n",
      "Iteracion: 11762 Gradiente: [0.000773594560561719,-0.013346593938265865] Loss: 22.842904831241853\n",
      "Iteracion: 11763 Gradiente: [0.000773183293960263,-0.013339498479257018] Loss: 22.84290465255935\n",
      "Iteracion: 11764 Gradiente: [0.0007727722459587009,-0.013332406792413565] Loss: 22.842904474066778\n",
      "Iteracion: 11765 Gradiente: [0.0007723614165276634,-0.01332531887572692] Loss: 22.842904295763926\n",
      "Iteracion: 11766 Gradiente: [0.000771950805481462,-0.013318234727196435] Loss: 22.842904117650633\n",
      "Iteracion: 11767 Gradiente: [0.0007715404127490425,-0.013311154344815179] Loss: 22.842903939726646\n",
      "Iteracion: 11768 Gradiente: [0.0007711302382157706,-0.013304077726580843] Loss: 22.8429037619918\n",
      "Iteracion: 11769 Gradiente: [0.0007707202817063793,-0.013297004870497156] Loss: 22.84290358444588\n",
      "Iteracion: 11770 Gradiente: [0.0007703105431763409,-0.013289935774558259] Loss: 22.842903407088684\n",
      "Iteracion: 11771 Gradiente: [0.0007699010224949158,-0.01328287043676705] Loss: 22.842903229920037\n",
      "Iteracion: 11772 Gradiente: [0.0007694917194375724,-0.013275808855129748] Loss: 22.842903052939683\n",
      "Iteracion: 11773 Gradiente: [0.0007690826340033633,-0.013268751027645465] Loss: 22.842902876147487\n",
      "Iteracion: 11774 Gradiente: [0.000768673766089023,-0.013261696952316509] Loss: 22.842902699543195\n",
      "Iteracion: 11775 Gradiente: [0.0007682651154501248,-0.013254646627155491] Loss: 22.842902523126632\n",
      "Iteracion: 11776 Gradiente: [0.0007678566821150905,-0.01324760005016176] Loss: 22.842902346897606\n",
      "Iteracion: 11777 Gradiente: [0.0007674484660043391,-0.013240557219339758] Loss: 22.842902170855893\n",
      "Iteracion: 11778 Gradiente: [0.0007670404667701784,-0.013233518132709203] Loss: 22.84290199500131\n",
      "Iteracion: 11779 Gradiente: [0.0007666326845395588,-0.01322648278826956] Loss: 22.842901819333676\n",
      "Iteracion: 11780 Gradiente: [0.0007662251191694244,-0.013219451184030125] Loss: 22.842901643852777\n",
      "Iteracion: 11781 Gradiente: [0.0007658177703452414,-0.013212423318014525] Loss: 22.842901468558377\n",
      "Iteracion: 11782 Gradiente: [0.0007654106380878526,-0.013205399188228146] Loss: 22.842901293450332\n",
      "Iteracion: 11783 Gradiente: [0.0007650037223385197,-0.01319837879268088] Loss: 22.842901118528424\n",
      "Iteracion: 11784 Gradiente: [0.0007645970228888169,-0.013191362129392321] Loss: 22.842900943792458\n",
      "Iteracion: 11785 Gradiente: [0.0007641905396288469,-0.013184349196379112] Loss: 22.84290076924221\n",
      "Iteracion: 11786 Gradiente: [0.0007637842724259751,-0.013177339991659428] Loss: 22.842900594877523\n",
      "Iteracion: 11787 Gradiente: [0.0007633782212678852,-0.013170334513243986] Loss: 22.842900420698186\n",
      "Iteracion: 11788 Gradiente: [0.0007629723859622573,-0.013163332759156769] Loss: 22.842900246703984\n",
      "Iteracion: 11789 Gradiente: [0.0007625667664219311,-0.013156334727415953] Loss: 22.84290007289474\n",
      "Iteracion: 11790 Gradiente: [0.0007621613625587997,-0.013149340416042913] Loss: 22.842899899270254\n",
      "Iteracion: 11791 Gradiente: [0.000761756174229807,-0.013142349823058907] Loss: 22.84289972583034\n",
      "Iteracion: 11792 Gradiente: [0.000761351201321266,-0.01313536294648839] Loss: 22.842899552574753\n",
      "Iteracion: 11793 Gradiente: [0.0007609464435252752,-0.013128379784366236] Loss: 22.842899379503365\n",
      "Iteracion: 11794 Gradiente: [0.0007605419009829953,-0.013121400334703636] Loss: 22.842899206615932\n",
      "Iteracion: 11795 Gradiente: [0.0007601375735655817,-0.013114424595527177] Loss: 22.84289903391228\n",
      "Iteracion: 11796 Gradiente: [0.0007597334611432416,-0.013107452564865696] Loss: 22.842898861392214\n",
      "Iteracion: 11797 Gradiente: [0.0007593295634478637,-0.013100484240755843] Loss: 22.842898689055527\n",
      "Iteracion: 11798 Gradiente: [0.0007589258805988189,-0.01309351962121461] Loss: 22.842898516902032\n",
      "Iteracion: 11799 Gradiente: [0.0007585224123630496,-0.013086558704278299] Loss: 22.842898344931537\n",
      "Iteracion: 11800 Gradiente: [0.0007581191585539197,-0.013079601487982373] Loss: 22.842898173143837\n",
      "Iteracion: 11801 Gradiente: [0.0007577161190605845,-0.013072647970362065] Loss: 22.84289800153874\n",
      "Iteracion: 11802 Gradiente: [0.0007573132939834674,-0.01306569814943496] Loss: 22.84289783011607\n",
      "Iteracion: 11803 Gradiente: [0.0007569106829530862,-0.013058752023251212] Loss: 22.842897658875597\n",
      "Iteracion: 11804 Gradiente: [0.0007565082860073365,-0.013051809589840365] Loss: 22.842897487817172\n",
      "Iteracion: 11805 Gradiente: [0.0007561061029927411,-0.013044870847239902] Loss: 22.84289731694057\n",
      "Iteracion: 11806 Gradiente: [0.0007557041337700336,-0.01303793579348896] Loss: 22.842897146245587\n",
      "Iteracion: 11807 Gradiente: [0.0007553023782899496,-0.013031004426621352] Loss: 22.84289697573206\n",
      "Iteracion: 11808 Gradiente: [0.0007549008364037491,-0.013024076744682488] Loss: 22.842896805399807\n",
      "Iteracion: 11809 Gradiente: [0.0007544995079001637,-0.013017152745717197] Loss: 22.842896635248596\n",
      "Iteracion: 11810 Gradiente: [0.0007540983927649829,-0.013010232427760707] Loss: 22.84289646527825\n",
      "Iteracion: 11811 Gradiente: [0.0007536974908864143,-0.013003315788857961] Loss: 22.842896295488586\n",
      "Iteracion: 11812 Gradiente: [0.0007532968022104569,-0.012996402827047978] Loss: 22.842896125879395\n",
      "Iteracion: 11813 Gradiente: [0.0007528963264547883,-0.012989493540386594] Loss: 22.8428959564505\n",
      "Iteracion: 11814 Gradiente: [0.000752496063758675,-0.012982587926904661] Loss: 22.842895787201712\n",
      "Iteracion: 11815 Gradiente: [0.0007520960137952671,-0.012975685984662633] Loss: 22.842895618132818\n",
      "Iteracion: 11816 Gradiente: [0.0007516961765020369,-0.012968787711704977] Loss: 22.84289544924366\n",
      "Iteracion: 11817 Gradiente: [0.0007512965516914013,-0.012961893106087292] Loss: 22.842895280534005\n",
      "Iteracion: 11818 Gradiente: [0.0007508971393737814,-0.012955002165850734] Loss: 22.842895112003703\n",
      "Iteracion: 11819 Gradiente: [0.0007504979394203322,-0.012948114889047228] Loss: 22.842894943652528\n",
      "Iteracion: 11820 Gradiente: [0.00075009895169084,-0.012941231273732494] Loss: 22.842894775480328\n",
      "Iteracion: 11821 Gradiente: [0.0007497001760867761,-0.012934351317957631] Loss: 22.84289460748687\n",
      "Iteracion: 11822 Gradiente: [0.0007493016123703456,-0.012927475019784869] Loss: 22.842894439672\n",
      "Iteracion: 11823 Gradiente: [0.0007489032606514456,-0.012920602377258205] Loss: 22.842894272035505\n",
      "Iteracion: 11824 Gradiente: [0.0007485051206212271,-0.012913733388444608] Loss: 22.842894104577205\n",
      "Iteracion: 11825 Gradiente: [0.0007481071924075877,-0.01290686805138582] Loss: 22.84289393729692\n",
      "Iteracion: 11826 Gradiente: [0.0007477094756202026,-0.0129000063641584] Loss: 22.842893770194447\n",
      "Iteracion: 11827 Gradiente: [0.0007473119702950725,-0.012893148324814281] Loss: 22.842893603269594\n",
      "Iteracion: 11828 Gradiente: [0.0007469146763289321,-0.012886293931411785] Loss: 22.84289343652217\n",
      "Iteracion: 11829 Gradiente: [0.0007465175935521984,-0.01287944318201634] Loss: 22.842893269952018\n",
      "Iteracion: 11830 Gradiente: [0.0007461207218815009,-0.012872596074690297] Loss: 22.842893103558918\n",
      "Iteracion: 11831 Gradiente: [0.0007457240612249431,-0.012865752607493874] Loss: 22.84289293734268\n",
      "Iteracion: 11832 Gradiente: [0.0007453276114461005,-0.012858912778495461] Loss: 22.84289277130313\n",
      "Iteracion: 11833 Gradiente: [0.0007449313724094964,-0.012852076585760723] Loss: 22.842892605440078\n",
      "Iteracion: 11834 Gradiente: [0.0007445353440251286,-0.012845244027356392] Loss: 22.842892439753335\n",
      "Iteracion: 11835 Gradiente: [0.0007441395262011004,-0.012838415101348961] Loss: 22.84289227424272\n",
      "Iteracion: 11836 Gradiente: [0.0007437439187318281,-0.01283158980581239] Loss: 22.84289210890803\n",
      "Iteracion: 11837 Gradiente: [0.0007433485216855237,-0.012824768138805709] Loss: 22.842891943749102\n",
      "Iteracion: 11838 Gradiente: [0.0007429533348120761,-0.012817950098409626] Loss: 22.84289177876572\n",
      "Iteracion: 11839 Gradiente: [0.0007425583579570608,-0.01281113568269608] Loss: 22.84289161395771\n",
      "Iteracion: 11840 Gradiente: [0.0007421635911659526,-0.012804324889731097] Loss: 22.842891449324892\n",
      "Iteracion: 11841 Gradiente: [0.0007417690341507447,-0.01279751771759822] Loss: 22.842891284867097\n",
      "Iteracion: 11842 Gradiente: [0.0007413746870014393,-0.01279071416436075] Loss: 22.84289112058408\n",
      "Iteracion: 11843 Gradiente: [0.000740980549452767,-0.012783914228103654] Loss: 22.842890956475717\n",
      "Iteracion: 11844 Gradiente: [0.0007405866214838852,-0.012777117906898639] Loss: 22.842890792541777\n",
      "Iteracion: 11845 Gradiente: [0.0007401929028854208,-0.012770325198829487] Loss: 22.84289062878212\n",
      "Iteracion: 11846 Gradiente: [0.0007397993935825298,-0.012763536101973946] Loss: 22.842890465196522\n",
      "Iteracion: 11847 Gradiente: [0.000739406093560054,-0.012756750614405614] Loss: 22.842890301784823\n",
      "Iteracion: 11848 Gradiente: [0.0007390130026029358,-0.01274996873421325] Loss: 22.842890138546803\n",
      "Iteracion: 11849 Gradiente: [0.0007386201206088572,-0.01274319045947673] Loss: 22.842889975482326\n",
      "Iteracion: 11850 Gradiente: [0.0007382274474821316,-0.012736415788278303] Loss: 22.842889812591164\n",
      "Iteracion: 11851 Gradiente: [0.0007378349831697051,-0.012729644718701158] Loss: 22.842889649873168\n",
      "Iteracion: 11852 Gradiente: [0.0007374427273873607,-0.012722877248838315] Loss: 22.84288948732813\n",
      "Iteracion: 11853 Gradiente: [0.0007370506802431009,-0.012716113376763734] Loss: 22.84288932495587\n",
      "Iteracion: 11854 Gradiente: [0.0007366588414271291,-0.012709353100577895] Loss: 22.842889162756226\n",
      "Iteracion: 11855 Gradiente: [0.0007362672109138657,-0.012702596418362806] Loss: 22.84288900072897\n",
      "Iteracion: 11856 Gradiente: [0.0007358757886739416,-0.012695843328202845] Loss: 22.842888838873964\n",
      "Iteracion: 11857 Gradiente: [0.0007354845744866149,-0.01268909382819648] Loss: 22.842888677191006\n",
      "Iteracion: 11858 Gradiente: [0.0007350935683329377,-0.012682347916426669] Loss: 22.842888515679917\n",
      "Iteracion: 11859 Gradiente: [0.0007347027701115394,-0.012675605590987734] Loss: 22.842888354340495\n",
      "Iteracion: 11860 Gradiente: [0.0007343121795173602,-0.012668866849983711] Loss: 22.842888193172588\n",
      "Iteracion: 11861 Gradiente: [0.0007339217966044013,-0.012662131691499094] Loss: 22.842888032175985\n",
      "Iteracion: 11862 Gradiente: [0.0007335316212447651,-0.012655400113631761] Loss: 22.842887871350534\n",
      "Iteracion: 11863 Gradiente: [0.0007331416533171857,-0.012648672114477223] Loss: 22.842887710696026\n",
      "Iteracion: 11864 Gradiente: [0.0007327518926833439,-0.012641947692133945] Loss: 22.84288755021228\n",
      "Iteracion: 11865 Gradiente: [0.000732362339311976,-0.012635226844698385] Loss: 22.84288738989915\n",
      "Iteracion: 11866 Gradiente: [0.0007319729930799213,-0.012628509570269012] Loss: 22.842887229756425\n",
      "Iteracion: 11867 Gradiente: [0.000731583853706752,-0.01262179586695531] Loss: 22.84288706978391\n",
      "Iteracion: 11868 Gradiente: [0.0007311949212957339,-0.012615085732848759] Loss: 22.842886909981456\n",
      "Iteracion: 11869 Gradiente: [0.0007308061956573889,-0.012608379166054225] Loss: 22.842886750348875\n",
      "Iteracion: 11870 Gradiente: [0.0007304176766638193,-0.012601676164677993] Loss: 22.842886590885957\n",
      "Iteracion: 11871 Gradiente: [0.000730029364237339,-0.012594976726821965] Loss: 22.842886431592554\n",
      "Iteracion: 11872 Gradiente: [0.0007296412581875226,-0.012588280850595506] Loss: 22.842886272468498\n",
      "Iteracion: 11873 Gradiente: [0.0007292533584916328,-0.012581588534102057] Loss: 22.84288611351357\n",
      "Iteracion: 11874 Gradiente: [0.0007288656649450332,-0.012574899775452763] Loss: 22.842885954727592\n",
      "Iteracion: 11875 Gradiente: [0.0007284781776661475,-0.012568214572743723] Loss: 22.842885796110433\n",
      "Iteracion: 11876 Gradiente: [0.0007280908963186524,-0.012561532924098274] Loss: 22.84288563766186\n",
      "Iteracion: 11877 Gradiente: [0.0007277038208864421,-0.012554854827619503] Loss: 22.84288547938172\n",
      "Iteracion: 11878 Gradiente: [0.0007273169511203529,-0.012548180281430513] Loss: 22.842885321269815\n",
      "Iteracion: 11879 Gradiente: [0.0007269302871037552,-0.0125415092836293] Loss: 22.842885163325995\n",
      "Iteracion: 11880 Gradiente: [0.000726543828630118,-0.012534841832336241] Loss: 22.842885005550066\n",
      "Iteracion: 11881 Gradiente: [0.0007261575756435453,-0.012528177925662949] Loss: 22.84288484794183\n",
      "Iteracion: 11882 Gradiente: [0.0007257715280256131,-0.012521517561725896] Loss: 22.842884690501144\n",
      "Iteracion: 11883 Gradiente: [0.0007253856856190547,-0.012514860738644629] Loss: 22.842884533227842\n",
      "Iteracion: 11884 Gradiente: [0.000725000048352816,-0.01250820745453313] Loss: 22.84288437612168\n",
      "Iteracion: 11885 Gradiente: [0.000724614615994786,-0.012501557707519003] Loss: 22.842884219182526\n",
      "Iteracion: 11886 Gradiente: [0.0007242293885857028,-0.012494911495711374] Loss: 22.842884062410203\n",
      "Iteracion: 11887 Gradiente: [0.0007238443660270377,-0.012488268817230856] Loss: 22.842883905804513\n",
      "Iteracion: 11888 Gradiente: [0.0007234595481643661,-0.012481629670201144] Loss: 22.84288374936531\n",
      "Iteracion: 11889 Gradiente: [0.0007230749347939991,-0.012474994052752564] Loss: 22.84288359309239\n",
      "Iteracion: 11890 Gradiente: [0.0007226905259850961,-0.012468361962995899] Loss: 22.84288343698558\n",
      "Iteracion: 11891 Gradiente: [0.0007223063214572297,-0.012461733399066215] Loss: 22.8428832810447\n",
      "Iteracion: 11892 Gradiente: [0.000721922321368614,-0.012455108359074766] Loss: 22.8428831252696\n",
      "Iteracion: 11893 Gradiente: [0.0007215385252597647,-0.012448486841166092] Loss: 22.84288296966007\n",
      "Iteracion: 11894 Gradiente: [0.000721154933267106,-0.012441868843456423] Loss: 22.842882814215955\n",
      "Iteracion: 11895 Gradiente: [0.0007207715451897911,-0.012435254364077507] Loss: 22.842882658937093\n",
      "Iteracion: 11896 Gradiente: [0.0007203883609160282,-0.012428643401160263] Loss: 22.842882503823258\n",
      "Iteracion: 11897 Gradiente: [0.0007200053803444462,-0.012422035952834065] Loss: 22.842882348874333\n",
      "Iteracion: 11898 Gradiente: [0.0007196226033073571,-0.01241543201723445] Loss: 22.84288219409011\n",
      "Iteracion: 11899 Gradiente: [0.0007192400298833945,-0.012408831592482865] Loss: 22.842882039470393\n",
      "Iteracion: 11900 Gradiente: [0.0007188576597476034,-0.012402234676726328] Loss: 22.842881885015057\n",
      "Iteracion: 11901 Gradiente: [0.0007184754930174601,-0.01239564126808368] Loss: 22.8428817307239\n",
      "Iteracion: 11902 Gradiente: [0.0007180935293121138,-0.012389051364709283] Loss: 22.842881576596753\n",
      "Iteracion: 11903 Gradiente: [0.0007177117688267269,-0.012382464964719493] Loss: 22.84288142263343\n",
      "Iteracion: 11904 Gradiente: [0.0007173302111946593,-0.012375882066269384] Loss: 22.842881268833782\n",
      "Iteracion: 11905 Gradiente: [0.0007169488564462274,-0.012369302667487385] Loss: 22.842881115197617\n",
      "Iteracion: 11906 Gradiente: [0.0007165677044260595,-0.012362726766516492] Loss: 22.842880961724763\n",
      "Iteracion: 11907 Gradiente: [0.000716186755007205,-0.012356154361498757] Loss: 22.842880808415032\n",
      "Iteracion: 11908 Gradiente: [0.0007158060081934537,-0.01234958545056865] Loss: 22.84288065526829\n",
      "Iteracion: 11909 Gradiente: [0.0007154254636882721,-0.012343020031877809] Loss: 22.842880502284327\n",
      "Iteracion: 11910 Gradiente: [0.0007150451215589252,-0.012336458103564022] Loss: 22.842880349462977\n",
      "Iteracion: 11911 Gradiente: [0.0007146649816509883,-0.012329899663768984] Loss: 22.84288019680407\n",
      "Iteracion: 11912 Gradiente: [0.0007142850438195107,-0.012323344710644335] Loss: 22.842880044307435\n",
      "Iteracion: 11913 Gradiente: [0.0007139053080474393,-0.012316793242329283] Loss: 22.84287989197293\n",
      "Iteracion: 11914 Gradiente: [0.0007135257740524518,-0.012310245256981862] Loss: 22.842879739800328\n",
      "Iteracion: 11915 Gradiente: [0.0007131464418336009,-0.012303700752743769] Loss: 22.84287958778949\n",
      "Iteracion: 11916 Gradiente: [0.0007127673113075161,-0.012297159727763447] Loss: 22.842879435940237\n",
      "Iteracion: 11917 Gradiente: [0.0007123883822572452,-0.012290622180198104] Loss: 22.842879284252398\n",
      "Iteracion: 11918 Gradiente: [0.000712009654748158,-0.012284088108187537] Loss: 22.842879132725788\n",
      "Iteracion: 11919 Gradiente: [0.0007116311285348804,-0.012277557509893455] Loss: 22.842878981360265\n",
      "Iteracion: 11920 Gradiente: [0.0007112528036268865,-0.01227103038346217] Loss: 22.842878830155623\n",
      "Iteracion: 11921 Gradiente: [0.0007108746798508037,-0.012264506727052074] Loss: 22.84287867911172\n",
      "Iteracion: 11922 Gradiente: [0.0007104967570616812,-0.012257986538818837] Loss: 22.842878528228365\n",
      "Iteracion: 11923 Gradiente: [0.0007101190351875176,-0.01225146981691824] Loss: 22.842878377505393\n",
      "Iteracion: 11924 Gradiente: [0.0007097415141259944,-0.012244956559506548] Loss: 22.84287822694266\n",
      "Iteracion: 11925 Gradiente: [0.0007093641936554225,-0.012238446764748664] Loss: 22.842878076539954\n",
      "Iteracion: 11926 Gradiente: [0.0007089870739501217,-0.012231940430788532] Loss: 22.842877926297135\n",
      "Iteracion: 11927 Gradiente: [0.0007086101546140829,-0.012225437555801359] Loss: 22.842877776214003\n",
      "Iteracion: 11928 Gradiente: [0.0007082334357770985,-0.012218938137935117] Loss: 22.842877626290413\n",
      "Iteracion: 11929 Gradiente: [0.0007078569171729517,-0.01221244217536146] Loss: 22.842877476526187\n",
      "Iteracion: 11930 Gradiente: [0.000707480598686061,-0.01220594966624411] Loss: 22.842877326921155\n",
      "Iteracion: 11931 Gradiente: [0.0007071044802235822,-0.01219946060874643] Loss: 22.842877177475152\n",
      "Iteracion: 11932 Gradiente: [0.000706728561675618,-0.012192975001033446] Loss: 22.842877028188013\n",
      "Iteracion: 11933 Gradiente: [0.0007063528431397496,-0.012186492841259049] Loss: 22.842876879059563\n",
      "Iteracion: 11934 Gradiente: [0.0007059773242948116,-0.012180014127602827] Loss: 22.842876730089632\n",
      "Iteracion: 11935 Gradiente: [0.0007056020050451177,-0.012173538858232054] Loss: 22.842876581278045\n",
      "Iteracion: 11936 Gradiente: [0.0007052268853575091,-0.012167067031311282] Loss: 22.842876432624646\n",
      "Iteracion: 11937 Gradiente: [0.000704851965140089,-0.012160598645009438] Loss: 22.84287628412926\n",
      "Iteracion: 11938 Gradiente: [0.000704477244180642,-0.012154133697504222] Loss: 22.842876135791723\n",
      "Iteracion: 11939 Gradiente: [0.0007041027225360116,-0.012147672186957455] Loss: 22.842875987611876\n",
      "Iteracion: 11940 Gradiente: [0.0007037283998641896,-0.01214121411155548] Loss: 22.842875839589528\n",
      "Iteracion: 11941 Gradiente: [0.0007033542763300223,-0.01213475946945716] Loss: 22.842875691724544\n",
      "Iteracion: 11942 Gradiente: [0.0007029803515232895,-0.012128308258854404] Loss: 22.842875544016717\n",
      "Iteracion: 11943 Gradiente: [0.0007026066255974683,-0.01212186047790856] Loss: 22.84287539646591\n",
      "Iteracion: 11944 Gradiente: [0.0007022330983346592,-0.012115416124802063] Loss: 22.84287524907194\n",
      "Iteracion: 11945 Gradiente: [0.0007018597696562286,-0.012108975197710474] Loss: 22.842875101834657\n",
      "Iteracion: 11946 Gradiente: [0.0007014866395100701,-0.01210253769480995] Loss: 22.842874954753874\n",
      "Iteracion: 11947 Gradiente: [0.0007011137076725996,-0.012096103614286709] Loss: 22.842874807829425\n",
      "Iteracion: 11948 Gradiente: [0.0007007409740566572,-0.012089672954321411] Loss: 22.842874661061167\n",
      "Iteracion: 11949 Gradiente: [0.0007003684386328739,-0.012083245713089734] Loss: 22.842874514448926\n",
      "Iteracion: 11950 Gradiente: [0.000699996101283773,-0.012076821888775768] Loss: 22.842874367992522\n",
      "Iteracion: 11951 Gradiente: [0.0006996239618880887,-0.012070401479563012] Loss: 22.842874221691783\n",
      "Iteracion: 11952 Gradiente: [0.0006992520203340291,-0.012063984483637213] Loss: 22.842874075546582\n",
      "Iteracion: 11953 Gradiente: [0.0006988802765022228,-0.012057570899185185] Loss: 22.842873929556735\n",
      "Iteracion: 11954 Gradiente: [0.0006985087302685618,-0.012051160724392437] Loss: 22.84287378372205\n",
      "Iteracion: 11955 Gradiente: [0.0006981373815875714,-0.012044753957443888] Loss: 22.842873638042406\n",
      "Iteracion: 11956 Gradiente: [0.0006977662302877737,-0.012038350596531917] Loss: 22.84287349251759\n",
      "Iteracion: 11957 Gradiente: [0.0006973952763672742,-0.012031950639838248] Loss: 22.842873347147492\n",
      "Iteracion: 11958 Gradiente: [0.0006970245195778564,-0.012025554085564256] Loss: 22.842873201931905\n",
      "Iteracion: 11959 Gradiente: [0.000696653959925205,-0.012019160931892259] Loss: 22.842873056870676\n",
      "Iteracion: 11960 Gradiente: [0.0006962835973714239,-0.012012771177011198] Loss: 22.842872911963642\n",
      "Iteracion: 11961 Gradiente: [0.000695913431670192,-0.012006384819121745] Loss: 22.842872767210654\n",
      "Iteracion: 11962 Gradiente: [0.000695543462706875,-0.012000001856420065] Loss: 22.84287262261154\n",
      "Iteracion: 11963 Gradiente: [0.0006951736904776833,-0.011993622287095225] Loss: 22.84287247816611\n",
      "Iteracion: 11964 Gradiente: [0.0006948041148082969,-0.011987246109344459] Loss: 22.842872333874237\n",
      "Iteracion: 11965 Gradiente: [0.0006944347355821871,-0.01198087332136879] Loss: 22.842872189735733\n",
      "Iteracion: 11966 Gradiente: [0.000694065552787985,-0.011974503921359769] Loss: 22.842872045750457\n",
      "Iteracion: 11967 Gradiente: [0.0006936965662229492,-0.011968137907521737] Loss: 22.842871901918226\n",
      "Iteracion: 11968 Gradiente: [0.0006933277758956062,-0.011961775278046953] Loss: 22.842871758238893\n",
      "Iteracion: 11969 Gradiente: [0.0006929591815757401,-0.011955416031144257] Loss: 22.84287161471228\n",
      "Iteracion: 11970 Gradiente: [0.0006925907831591379,-0.011949060165016335] Loss: 22.84287147133824\n",
      "Iteracion: 11971 Gradiente: [0.000692222580612641,-0.011942707677860891] Loss: 22.84287132811659\n",
      "Iteracion: 11972 Gradiente: [0.0006918545738775114,-0.01193635856787812] Loss: 22.842871185047194\n",
      "Iteracion: 11973 Gradiente: [0.0006914867627595338,-0.011930012833279354] Loss: 22.842871042129886\n",
      "Iteracion: 11974 Gradiente: [0.0006911191471393371,-0.011923670472269521] Loss: 22.842870899364478\n",
      "Iteracion: 11975 Gradiente: [0.0006907517269221823,-0.011917331483056277] Loss: 22.842870756750834\n",
      "Iteracion: 11976 Gradiente: [0.0006903845021232276,-0.011910995863838517] Loss: 22.842870614288778\n",
      "Iteracion: 11977 Gradiente: [0.000690017472506573,-0.0119046636128342] Loss: 22.842870471978173\n",
      "Iteracion: 11978 Gradiente: [0.0006896506380485335,-0.011898334728246477] Loss: 22.84287032981884\n",
      "Iteracion: 11979 Gradiente: [0.0006892839985946845,-0.011892009208288338] Loss: 22.842870187810597\n",
      "Iteracion: 11980 Gradiente: [0.0006889175541213414,-0.011885687051167675] Loss: 22.842870045953322\n",
      "Iteracion: 11981 Gradiente: [0.0006885513044020778,-0.011879368255102682] Loss: 22.84286990424683\n",
      "Iteracion: 11982 Gradiente: [0.0006881852493772082,-0.01187305281830516] Loss: 22.842869762690984\n",
      "Iteracion: 11983 Gradiente: [0.0006878193889197822,-0.011866740738989278] Loss: 22.842869621285583\n",
      "Iteracion: 11984 Gradiente: [0.0006874537230023255,-0.011860432015367076] Loss: 22.84286948003052\n",
      "Iteracion: 11985 Gradiente: [0.0006870882515888373,-0.011854126645649169] Loss: 22.842869338925603\n",
      "Iteracion: 11986 Gradiente: [0.0006867229743467836,-0.011847824628067135] Loss: 22.842869197970664\n",
      "Iteracion: 11987 Gradiente: [0.0006863578914125886,-0.0118415259608233] Loss: 22.84286905716556\n",
      "Iteracion: 11988 Gradiente: [0.0006859930024499287,-0.011835230642151846] Loss: 22.84286891651015\n",
      "Iteracion: 11989 Gradiente: [0.00068562830753649,-0.01182893867026209] Loss: 22.842868776004238\n",
      "Iteracion: 11990 Gradiente: [0.0006852638065026895,-0.011822650043377436] Loss: 22.84286863564768\n",
      "Iteracion: 11991 Gradiente: [0.0006848994991353644,-0.011816364759726384] Loss: 22.84286849544031\n",
      "Iteracion: 11992 Gradiente: [0.0006845353855794656,-0.011810082817517772] Loss: 22.842868355382\n",
      "Iteracion: 11993 Gradiente: [0.0006841714655820396,-0.011803804214982587] Loss: 22.842868215472553\n",
      "Iteracion: 11994 Gradiente: [0.0006838077390834012,-0.011797528950343998] Loss: 22.842868075711824\n",
      "Iteracion: 11995 Gradiente: [0.0006834442058732293,-0.011791257021833583] Loss: 22.84286793609966\n",
      "Iteracion: 11996 Gradiente: [0.0006830808659022599,-0.011784988427673681] Loss: 22.8428677966359\n",
      "Iteracion: 11997 Gradiente: [0.0006827177191477556,-0.011778723166088294] Loss: 22.84286765732039\n",
      "Iteracion: 11998 Gradiente: [0.0006823547654268698,-0.011772461235309705] Loss: 22.842867518152975\n",
      "Iteracion: 11999 Gradiente: [0.0006819920047481295,-0.01176620263356097] Loss: 22.842867379133487\n",
      "Iteracion: 12000 Gradiente: [0.0006816294369040558,-0.011759947359076624] Loss: 22.842867240261764\n",
      "Iteracion: 12001 Gradiente: [0.0006812670617544352,-0.011753695410093811] Loss: 22.84286710153767\n",
      "Iteracion: 12002 Gradiente: [0.0006809048793021096,-0.011747446784834045] Loss: 22.842866962961033\n",
      "Iteracion: 12003 Gradiente: [0.0006805428893111791,-0.011741201481541926] Loss: 22.842866824531708\n",
      "Iteracion: 12004 Gradiente: [0.0006801810917873278,-0.011734959498443942] Loss: 22.84286668624952\n",
      "Iteracion: 12005 Gradiente: [0.0006798194865856052,-0.011728720833775578] Loss: 22.84286654811433\n",
      "Iteracion: 12006 Gradiente: [0.0006794580737363276,-0.011722485485766399] Loss: 22.84286641012597\n",
      "Iteracion: 12007 Gradiente: [0.000679096852921172,-0.0117162534526661] Loss: 22.842866272284294\n",
      "Iteracion: 12008 Gradiente: [0.0006787358242007713,-0.011710024732702587] Loss: 22.842866134589134\n",
      "Iteracion: 12009 Gradiente: [0.0006783749874747021,-0.011703799324113954] Loss: 22.842865997040352\n",
      "Iteracion: 12010 Gradiente: [0.0006780143424161149,-0.011697577225154394] Loss: 22.842865859637776\n",
      "Iteracion: 12011 Gradiente: [0.0006776538891898554,-0.011691358434046725] Loss: 22.842865722381248\n",
      "Iteracion: 12012 Gradiente: [0.0006772936276073931,-0.011685142949037915] Loss: 22.842865585270623\n",
      "Iteracion: 12013 Gradiente: [0.0006769335575275667,-0.011678930768372453] Loss: 22.842865448305762\n",
      "Iteracion: 12014 Gradiente: [0.0006765736789200598,-0.011672721890291152] Loss: 22.84286531148648\n",
      "Iteracion: 12015 Gradiente: [0.0006762139916114999,-0.01166651631304217] Loss: 22.84286517481262\n",
      "Iteracion: 12016 Gradiente: [0.0006758544954417781,-0.011660314034872835] Loss: 22.842865038284053\n",
      "Iteracion: 12017 Gradiente: [0.000675495190458264,-0.011654115054020645] Loss: 22.84286490190062\n",
      "Iteracion: 12018 Gradiente: [0.0006751360764771637,-0.011647919368736836] Loss: 22.84286476566214\n",
      "Iteracion: 12019 Gradiente: [0.0006747771534643713,-0.011641726977267198] Loss: 22.842864629568492\n",
      "Iteracion: 12020 Gradiente: [0.0006744184212029344,-0.01163553787786687] Loss: 22.842864493619512\n",
      "Iteracion: 12021 Gradiente: [0.0006740598796729576,-0.011629352068781528] Loss: 22.84286435781503\n",
      "Iteracion: 12022 Gradiente: [0.00067370152871149,-0.011623169548264655] Loss: 22.842864222154933\n",
      "Iteracion: 12023 Gradiente: [0.0006733433682730568,-0.011616990314563935] Loss: 22.842864086639008\n",
      "Iteracion: 12024 Gradiente: [0.0006729853983102885,-0.011610814365928472] Loss: 22.842863951267148\n",
      "Iteracion: 12025 Gradiente: [0.0006726276186204435,-0.011604641700621225] Loss: 22.842863816039188\n",
      "Iteracion: 12026 Gradiente: [0.000672270029080361,-0.011598472316894497] Loss: 22.842863680954967\n",
      "Iteracion: 12027 Gradiente: [0.0006719126297610956,-0.01159230621299443] Loss: 22.842863546014335\n",
      "Iteracion: 12028 Gradiente: [0.0006715554203680085,-0.011586143387190025] Loss: 22.84286341121715\n",
      "Iteracion: 12029 Gradiente: [0.0006711984009048895,-0.011579983837729676] Loss: 22.842863276563243\n",
      "Iteracion: 12030 Gradiente: [0.0006708415712466831,-0.011573827562875276] Loss: 22.842863142052472\n",
      "Iteracion: 12031 Gradiente: [0.0006704849312844393,-0.011567674560888246] Loss: 22.842863007684684\n",
      "Iteracion: 12032 Gradiente: [0.0006701284808571017,-0.011561524830027517] Loss: 22.842862873459733\n",
      "Iteracion: 12033 Gradiente: [0.0006697722199883552,-0.011555378368548948] Loss: 22.842862739377452\n",
      "Iteracion: 12034 Gradiente: [0.0006694161485332491,-0.011549235174718812] Loss: 22.842862605437706\n",
      "Iteracion: 12035 Gradiente: [0.0006690602663762017,-0.01154309524679699] Loss: 22.842862471640327\n",
      "Iteracion: 12036 Gradiente: [0.0006687045734632117,-0.0115369585830481] Loss: 22.842862337985185\n",
      "Iteracion: 12037 Gradiente: [0.000668349069536589,-0.011530825181743273] Loss: 22.842862204472098\n",
      "Iteracion: 12038 Gradiente: [0.0006679937545508589,-0.011524695041146771] Loss: 22.842862071100946\n",
      "Iteracion: 12039 Gradiente: [0.0006676386286746568,-0.011518568159508883] Loss: 22.842861937871557\n",
      "Iteracion: 12040 Gradiente: [0.0006672836915091314,-0.011512444535114241] Loss: 22.842861804783773\n",
      "Iteracion: 12041 Gradiente: [0.0006669289430324928,-0.011506324166226515] Loss: 22.842861671837476\n",
      "Iteracion: 12042 Gradiente: [0.0006665743831424227,-0.011500207051116243] Loss: 22.842861539032512\n",
      "Iteracion: 12043 Gradiente: [0.0006662200116503906,-0.011494093188055861] Loss: 22.842861406368694\n",
      "Iteracion: 12044 Gradiente: [0.000665865828693768,-0.011487982575305722] Loss: 22.842861273845905\n",
      "Iteracion: 12045 Gradiente: [0.0006655118339570739,-0.011481875211148799] Loss: 22.842861141463988\n",
      "Iteracion: 12046 Gradiente: [0.0006651580274620982,-0.011475771093852434] Loss: 22.842861009222773\n",
      "Iteracion: 12047 Gradiente: [0.0006648044090193632,-0.011469670221693325] Loss: 22.84286087712214\n",
      "Iteracion: 12048 Gradiente: [0.0006644509785322346,-0.011463572592947694] Loss: 22.84286074516191\n",
      "Iteracion: 12049 Gradiente: [0.0006640977360518718,-0.011457478205882884] Loss: 22.842860613341987\n",
      "Iteracion: 12050 Gradiente: [0.0006637446812495303,-0.011451387058787788] Loss: 22.84286048166217\n",
      "Iteracion: 12051 Gradiente: [0.0006633918142256335,-0.011445299149929393] Loss: 22.842860350122315\n",
      "Iteracion: 12052 Gradiente: [0.000663039134800177,-0.011439214477589606] Loss: 22.842860218722297\n",
      "Iteracion: 12053 Gradiente: [0.0006626866428424212,-0.01143313304004856] Loss: 22.842860087461947\n",
      "Iteracion: 12054 Gradiente: [0.0006623343382557323,-0.011427054835589463] Loss: 22.842859956341133\n",
      "Iteracion: 12055 Gradiente: [0.0006619822209908458,-0.011420979862488542] Loss: 22.842859825359675\n",
      "Iteracion: 12056 Gradiente: [0.0006616302909274433,-0.011414908119029121] Loss: 22.84285969451748\n",
      "Iteracion: 12057 Gradiente: [0.0006612785479423641,-0.011408839603495717] Loss: 22.842859563814336\n",
      "Iteracion: 12058 Gradiente: [0.0006609269919816067,-0.011402774314170945] Loss: 22.84285943325015\n",
      "Iteracion: 12059 Gradiente: [0.0006605756228954836,-0.011396712249340387] Loss: 22.842859302824728\n",
      "Iteracion: 12060 Gradiente: [0.0006602244406517836,-0.011390653407286896] Loss: 22.842859172537974\n",
      "Iteracion: 12061 Gradiente: [0.0006598734450543967,-0.011384597786303867] Loss: 22.842859042389687\n",
      "Iteracion: 12062 Gradiente: [0.0006595226361317449,-0.011378545384668233] Loss: 22.84285891237976\n",
      "Iteracion: 12063 Gradiente: [0.0006591720136327695,-0.011372496200682274] Loss: 22.842858782508024\n",
      "Iteracion: 12064 Gradiente: [0.0006588215775697866,-0.01136645023262588] Loss: 22.842858652774343\n",
      "Iteracion: 12065 Gradiente: [0.0006584713277921612,-0.011360407478792448] Loss: 22.842858523178563\n",
      "Iteracion: 12066 Gradiente: [0.0006581212642648401,-0.011354367937469098] Loss: 22.84285839372054\n",
      "Iteracion: 12067 Gradiente: [0.000657771386740554,-0.01134833160695609] Loss: 22.842858264400128\n",
      "Iteracion: 12068 Gradiente: [0.0006574216953149896,-0.011342298485537465] Loss: 22.84285813521719\n",
      "Iteracion: 12069 Gradiente: [0.0006570721897285618,-0.011336268571515262] Loss: 22.84285800617156\n",
      "Iteracion: 12070 Gradiente: [0.0006567228699594807,-0.011330241863180863] Loss: 22.842857877263096\n",
      "Iteracion: 12071 Gradiente: [0.0006563737359821668,-0.011324218358823164] Loss: 22.842857748491674\n",
      "Iteracion: 12072 Gradiente: [0.0006560247875446142,-0.011318198056750125] Loss: 22.842857619857128\n",
      "Iteracion: 12073 Gradiente: [0.0006556760245947165,-0.011312180955256324] Loss: 22.84285749135932\n",
      "Iteracion: 12074 Gradiente: [0.0006553274471078415,-0.011306167052634208] Loss: 22.842857362998107\n",
      "Iteracion: 12075 Gradiente: [0.000654979054848089,-0.011300156347192095] Loss: 22.842857234773327\n",
      "Iteracion: 12076 Gradiente: [0.0006546308479632519,-0.011294148837218406] Loss: 22.842857106684846\n",
      "Iteracion: 12077 Gradiente: [0.0006542828261141646,-0.011288144521025481] Loss: 22.84285697873254\n",
      "Iteracion: 12078 Gradiente: [0.0006539349892629313,-0.011282143396912166] Loss: 22.84285685091623\n",
      "Iteracion: 12079 Gradiente: [0.0006535873373280765,-0.011276145463180735] Loss: 22.842856723235784\n",
      "Iteracion: 12080 Gradiente: [0.0006532398702537042,-0.011270150718133583] Loss: 22.84285659569107\n",
      "Iteracion: 12081 Gradiente: [0.0006528925878986532,-0.011264159160077013] Loss: 22.842856468281926\n",
      "Iteracion: 12082 Gradiente: [0.0006525454901246046,-0.011258170787318516] Loss: 22.842856341008215\n",
      "Iteracion: 12083 Gradiente: [0.0006521985769182947,-0.011252185598160599] Loss: 22.84285621386979\n",
      "Iteracion: 12084 Gradiente: [0.0006518518480978249,-0.011246203590916319] Loss: 22.84285608686651\n",
      "Iteracion: 12085 Gradiente: [0.0006515053036963536,-0.011240224763884636] Loss: 22.84285595999823\n",
      "Iteracion: 12086 Gradiente: [0.0006511589434817703,-0.01123424911538417] Loss: 22.842855833264824\n",
      "Iteracion: 12087 Gradiente: [0.0006508127674199689,-0.011228276643721221] Loss: 22.84285570666612\n",
      "Iteracion: 12088 Gradiente: [0.0006504667753404192,-0.0112223073472092] Loss: 22.842855580201995\n",
      "Iteracion: 12089 Gradiente: [0.0006501209672687007,-0.011216341224156304] Loss: 22.842855453872293\n",
      "Iteracion: 12090 Gradiente: [0.0006497753429963875,-0.011210378272879021] Loss: 22.842855327676883\n",
      "Iteracion: 12091 Gradiente: [0.00064942990245432,-0.011204418491690404] Loss: 22.84285520161561\n",
      "Iteracion: 12092 Gradiente: [0.0006490846455723916,-0.011198461878903861] Loss: 22.842855075688338\n",
      "Iteracion: 12093 Gradiente: [0.0006487395722624948,-0.011192508432834458] Loss: 22.842854949894924\n",
      "Iteracion: 12094 Gradiente: [0.0006483946823863107,-0.011186558151799867] Loss: 22.842854824235225\n",
      "Iteracion: 12095 Gradiente: [0.0006480499759419445,-0.011180611034113852] Loss: 22.84285469870911\n",
      "Iteracion: 12096 Gradiente: [0.0006477054526802324,-0.011174667078101663] Loss: 22.842854573316416\n",
      "Iteracion: 12097 Gradiente: [0.0006473611125888586,-0.011168726282078248] Loss: 22.842854448057018\n",
      "Iteracion: 12098 Gradiente: [0.0006470169555569782,-0.011162788644365425] Loss: 22.84285432293075\n",
      "Iteracion: 12099 Gradiente: [0.0006466729814472198,-0.01115685416328489] Loss: 22.842854197937513\n",
      "Iteracion: 12100 Gradiente: [0.0006463291902832679,-0.011150922837152659] Loss: 22.842854073077124\n",
      "Iteracion: 12101 Gradiente: [0.0006459855817818531,-0.011144994664300611] Loss: 22.84285394834948\n",
      "Iteracion: 12102 Gradiente: [0.0006456421560595042,-0.011139069643041922] Loss: 22.842853823754393\n",
      "Iteracion: 12103 Gradiente: [0.0006452989128907424,-0.011133147771707404] Loss: 22.84285369929177\n",
      "Iteracion: 12104 Gradiente: [0.0006449558521770389,-0.011127229048622903] Loss: 22.842853574961435\n",
      "Iteracion: 12105 Gradiente: [0.0006446129737933385,-0.01112131347211592] Loss: 22.842853450763265\n",
      "Iteracion: 12106 Gradiente: [0.0006442702777169037,-0.01111540104050898] Loss: 22.842853326697103\n",
      "Iteracion: 12107 Gradiente: [0.0006439277638454163,-0.011109491752132428] Loss: 22.842853202762832\n",
      "Iteracion: 12108 Gradiente: [0.0006435854321040324,-0.011103585605312579] Loss: 22.8428530789603\n",
      "Iteracion: 12109 Gradiente: [0.0006432432823601175,-0.011097682598382145] Loss: 22.842852955289384\n",
      "Iteracion: 12110 Gradiente: [0.0006429013144658787,-0.011091782729673127] Loss: 22.842852831749905\n",
      "Iteracion: 12111 Gradiente: [0.0006425595284535272,-0.01108588599751125] Loss: 22.84285270834174\n",
      "Iteracion: 12112 Gradiente: [0.0006422179240644255,-0.011079992400237871] Loss: 22.842852585064772\n",
      "Iteracion: 12113 Gradiente: [0.0006418765012616253,-0.011074101936181672] Loss: 22.84285246191884\n",
      "Iteracion: 12114 Gradiente: [0.0006415352600138628,-0.011068214603675368] Loss: 22.842852338903807\n",
      "Iteracion: 12115 Gradiente: [0.0006411942001771346,-0.011062330401055576] Loss: 22.842852216019537\n",
      "Iteracion: 12116 Gradiente: [0.0006408533217078608,-0.0110564493266556] Loss: 22.842852093265893\n",
      "Iteracion: 12117 Gradiente: [0.000640512624372036,-0.011050571378821298] Loss: 22.84285197064274\n",
      "Iteracion: 12118 Gradiente: [0.0006401721082378724,-0.011044696555879814] Loss: 22.842851848149916\n",
      "Iteracion: 12119 Gradiente: [0.000639831773121576,-0.01103882485617452] Loss: 22.84285172578731\n",
      "Iteracion: 12120 Gradiente: [0.0006394916189056706,-0.011032956278046176] Loss: 22.84285160355477\n",
      "Iteracion: 12121 Gradiente: [0.0006391516454887854,-0.011027090819837326] Loss: 22.842851481452158\n",
      "Iteracion: 12122 Gradiente: [0.0006388118528813417,-0.01102122847988222] Loss: 22.842851359479333\n",
      "Iteracion: 12123 Gradiente: [0.0006384722408474393,-0.011015369256531689] Loss: 22.84285123763619\n",
      "Iteracion: 12124 Gradiente: [0.0006381328093709726,-0.011009513148122825] Loss: 22.84285111592253\n",
      "Iteracion: 12125 Gradiente: [0.000637793558486995,-0.011003660152993196] Loss: 22.84285099433828\n",
      "Iteracion: 12126 Gradiente: [0.0006374544878781308,-0.01099781026949896] Loss: 22.842850872883258\n",
      "Iteracion: 12127 Gradiente: [0.0006371155974979577,-0.010991963495984672] Loss: 22.84285075155733\n",
      "Iteracion: 12128 Gradiente: [0.0006367768873114225,-0.01098611983079394] Loss: 22.84285063036037\n",
      "Iteracion: 12129 Gradiente: [0.0006364383571991538,-0.010980279272273682] Loss: 22.842850509292262\n",
      "Iteracion: 12130 Gradiente: [0.0006361000070048325,-0.0109744418187771] Loss: 22.842850388352822\n",
      "Iteracion: 12131 Gradiente: [0.0006357618366365613,-0.010968607468652535] Loss: 22.842850267541955\n",
      "Iteracion: 12132 Gradiente: [0.0006354238461246572,-0.010962776220242531] Loss: 22.842850146859494\n",
      "Iteracion: 12133 Gradiente: [0.0006350860352711152,-0.010956948071906088] Loss: 22.842850026305324\n",
      "Iteracion: 12134 Gradiente: [0.0006347484040655142,-0.010951123021988944] Loss: 22.84284990587928\n",
      "Iteracion: 12135 Gradiente: [0.0006344109523145865,-0.010945301068849151] Loss: 22.842849785581283\n",
      "Iteracion: 12136 Gradiente: [0.000634073680054333,-0.010939482210832806] Loss: 22.84284966541114\n",
      "Iteracion: 12137 Gradiente: [0.0006337365870185371,-0.010933666446305305] Loss: 22.842849545368733\n",
      "Iteracion: 12138 Gradiente: [0.0006333996731891982,-0.010927853773615819] Loss: 22.84284942545393\n",
      "Iteracion: 12139 Gradiente: [0.0006330629384838933,-0.010922044191121453] Loss: 22.842849305666597\n",
      "Iteracion: 12140 Gradiente: [0.0006327263828192523,-0.010916237697178251] Loss: 22.842849186006603\n",
      "Iteracion: 12141 Gradiente: [0.0006323900059991653,-0.010910434290150307] Loss: 22.842849066473796\n",
      "Iteracion: 12142 Gradiente: [0.0006320538080482644,-0.010904633968389633] Loss: 22.842848947068052\n",
      "Iteracion: 12143 Gradiente: [0.0006317177889153905,-0.010898836730252508] Loss: 22.84284882778923\n",
      "Iteracion: 12144 Gradiente: [0.0006313819483106423,-0.01089304257411167] Loss: 22.842848708637195\n",
      "Iteracion: 12145 Gradiente: [0.0006310462863145479,-0.010887251498317596] Loss: 22.842848589611823\n",
      "Iteracion: 12146 Gradiente: [0.0006307108027054179,-0.010881463501241721] Loss: 22.842848470712987\n",
      "Iteracion: 12147 Gradiente: [0.0006303754974576729,-0.010875678581242453] Loss: 22.842848351940518\n",
      "Iteracion: 12148 Gradiente: [0.000630040370552365,-0.010869896736678441] Loss: 22.842848233294305\n",
      "Iteracion: 12149 Gradiente: [0.0006297054217147509,-0.01086411796592645] Loss: 22.842848114774206\n",
      "Iteracion: 12150 Gradiente: [0.0006293706510016743,-0.01085834226734311] Loss: 22.842847996380097\n",
      "Iteracion: 12151 Gradiente: [0.0006290360582719738,-0.010852569639297134] Loss: 22.842847878111858\n",
      "Iteracion: 12152 Gradiente: [0.0006287016433086971,-0.01084680008016446] Loss: 22.842847759969292\n",
      "Iteracion: 12153 Gradiente: [0.0006283674062065832,-0.010841033588302195] Loss: 22.842847641952336\n",
      "Iteracion: 12154 Gradiente: [0.0006280333468756302,-0.010835270162079287] Loss: 22.842847524060836\n",
      "Iteracion: 12155 Gradiente: [0.000627699465063832,-0.01082950979987558] Loss: 22.842847406294634\n",
      "Iteracion: 12156 Gradiente: [0.0006273657607115032,-0.010823752500059906] Loss: 22.842847288653623\n",
      "Iteracion: 12157 Gradiente: [0.0006270322338593814,-0.010817998260996357] Loss: 22.842847171137674\n",
      "Iteracion: 12158 Gradiente: [0.0006266988842166181,-0.01081224708106845] Loss: 22.84284705374662\n",
      "Iteracion: 12159 Gradiente: [0.0006263657119442693,-0.010806498958634355] Loss: 22.842846936480367\n",
      "Iteracion: 12160 Gradiente: [0.0006260327166576948,-0.010800753892089196] Loss: 22.842846819338757\n",
      "Iteracion: 12161 Gradiente: [0.0006256998984904764,-0.010795011879791852] Loss: 22.842846702321662\n",
      "Iteracion: 12162 Gradiente: [0.0006253672571479759,-0.010789272920129633] Loss: 22.84284658542897\n",
      "Iteracion: 12163 Gradiente: [0.0006250347927618805,-0.010783537011468287] Loss: 22.84284646866052\n",
      "Iteracion: 12164 Gradiente: [0.0006247025050451308,-0.010777804152197727] Loss: 22.842846352016196\n",
      "Iteracion: 12165 Gradiente: [0.0006243703940128853,-0.01077207434068903] Loss: 22.842846235495866\n",
      "Iteracion: 12166 Gradiente: [0.0006240384595362987,-0.010766347575324057] Loss: 22.84284611909938\n",
      "Iteracion: 12167 Gradiente: [0.0006237067015111582,-0.010760623854484308] Loss: 22.842846002826644\n",
      "Iteracion: 12168 Gradiente: [0.0006233751198607251,-0.010754903176549391] Loss: 22.842845886677487\n",
      "Iteracion: 12169 Gradiente: [0.00062304371449405,-0.010749185539901873] Loss: 22.8428457706518\n",
      "Iteracion: 12170 Gradiente: [0.000622712485435765,-0.01074347094292006] Loss: 22.84284565474944\n",
      "Iteracion: 12171 Gradiente: [0.0006223814323770209,-0.010737759383997177] Loss: 22.84284553897028\n",
      "Iteracion: 12172 Gradiente: [0.0006220505553007645,-0.010732050861515555] Loss: 22.842845423314195\n",
      "Iteracion: 12173 Gradiente: [0.0006217198541691004,-0.010726345373858] Loss: 22.84284530778105\n",
      "Iteracion: 12174 Gradiente: [0.0006213893287584445,-0.010720642919417381] Loss: 22.84284519237072\n",
      "Iteracion: 12175 Gradiente: [0.0006210589791436405,-0.010714943496572005] Loss: 22.842845077083055\n",
      "Iteracion: 12176 Gradiente: [0.0006207288051048939,-0.010709247103718648] Loss: 22.842844961917955\n",
      "Iteracion: 12177 Gradiente: [0.0006203988066431521,-0.010703553739238814] Loss: 22.842844846875263\n",
      "Iteracion: 12178 Gradiente: [0.0006200689836295699,-0.01069786340152632] Loss: 22.84284473195487\n",
      "Iteracion: 12179 Gradiente: [0.0006197393358737221,-0.01069217608897605] Loss: 22.842844617156615\n",
      "Iteracion: 12180 Gradiente: [0.0006194098634201358,-0.010686491799973653] Loss: 22.842844502480407\n",
      "Iteracion: 12181 Gradiente: [0.0006190805661295448,-0.010680810532913062] Loss: 22.84284438792609\n",
      "Iteracion: 12182 Gradiente: [0.0006187514438806829,-0.010675132286189519] Loss: 22.842844273493533\n",
      "Iteracion: 12183 Gradiente: [0.0006184224966072331,-0.01066945705819696] Loss: 22.842844159182633\n",
      "Iteracion: 12184 Gradiente: [0.000618093724284563,-0.010663784847325056] Loss: 22.842844044993228\n",
      "Iteracion: 12185 Gradiente: [0.0006177651266786673,-0.010658115651978288] Loss: 22.842843930925213\n",
      "Iteracion: 12186 Gradiente: [0.0006174367038217572,-0.010652449470546325] Loss: 22.842843816978455\n",
      "Iteracion: 12187 Gradiente: [0.0006171084555016174,-0.01064678630143471] Loss: 22.842843703152802\n",
      "Iteracion: 12188 Gradiente: [0.0006167803816396145,-0.010641126143040698] Loss: 22.842843589448144\n",
      "Iteracion: 12189 Gradiente: [0.0006164524822764861,-0.010635468993754553] Loss: 22.842843475864377\n",
      "Iteracion: 12190 Gradiente: [0.000616124757290019,-0.010629814851980276] Loss: 22.84284336240133\n",
      "Iteracion: 12191 Gradiente: [0.0006157972064481025,-0.01062416371612578] Loss: 22.842843249058895\n",
      "Iteracion: 12192 Gradiente: [0.0006154698297791584,-0.010618515584586537] Loss: 22.842843135836926\n",
      "Iteracion: 12193 Gradiente: [0.000615142627080445,-0.010612870455770817] Loss: 22.842843022735334\n",
      "Iteracion: 12194 Gradiente: [0.0006148155983690155,-0.010607228328077293] Loss: 22.84284290975395\n",
      "Iteracion: 12195 Gradiente: [0.0006144887434819187,-0.010601589199914467] Loss: 22.84284279689267\n",
      "Iteracion: 12196 Gradiente: [0.0006141620623793642,-0.010595953069683854] Loss: 22.842842684151346\n",
      "Iteracion: 12197 Gradiente: [0.0006138355550149299,-0.01059031993578934] Loss: 22.84284257152988\n",
      "Iteracion: 12198 Gradiente: [0.0006135092211858743,-0.010584689796644871] Loss: 22.84284245902812\n",
      "Iteracion: 12199 Gradiente: [0.0006131830608457751,-0.010579062650654336] Loss: 22.842842346645956\n",
      "Iteracion: 12200 Gradiente: [0.0006128570739264205,-0.010573438496227302] Loss: 22.84284223438325\n",
      "Iteracion: 12201 Gradiente: [0.0006125312602354901,-0.010567817331776651] Loss: 22.842842122239855\n",
      "Iteracion: 12202 Gradiente: [0.0006122056198629859,-0.010562199155702955] Loss: 22.842842010215684\n",
      "Iteracion: 12203 Gradiente: [0.0006118801526307986,-0.010556583966423243] Loss: 22.842841898310596\n",
      "Iteracion: 12204 Gradiente: [0.0006115548583361867,-0.010550971762357145] Loss: 22.84284178652446\n",
      "Iteracion: 12205 Gradiente: [0.0006112297369810449,-0.01054536254190938] Loss: 22.842841674857144\n",
      "Iteracion: 12206 Gradiente: [0.0006109047884583182,-0.010539756303496618] Loss: 22.84284156330853\n",
      "Iteracion: 12207 Gradiente: [0.0006105800126761096,-0.010534153045533757] Loss: 22.8428414518785\n",
      "Iteracion: 12208 Gradiente: [0.000610255409568102,-0.010528552766432971] Loss: 22.842841340566906\n",
      "Iteracion: 12209 Gradiente: [0.000609930979087873,-0.01052295546460987] Loss: 22.84284122937363\n",
      "Iteracion: 12210 Gradiente: [0.0006096067210895247,-0.01051736113848314] Loss: 22.84284111829855\n",
      "Iteracion: 12211 Gradiente: [0.0006092826354584228,-0.010511769786473479] Loss: 22.842841007341555\n",
      "Iteracion: 12212 Gradiente: [0.0006089587220988808,-0.010506181406998275] Loss: 22.842840896502484\n",
      "Iteracion: 12213 Gradiente: [0.0006086349809568977,-0.010500595998476924] Loss: 22.84284078578124\n",
      "Iteracion: 12214 Gradiente: [0.0006083114119708929,-0.010495013559329061] Loss: 22.842840675177705\n",
      "Iteracion: 12215 Gradiente: [0.0006079880149532831,-0.010489434087979059] Loss: 22.842840564691738\n",
      "Iteracion: 12216 Gradiente: [0.0006076647897685916,-0.010483857582852826] Loss: 22.842840454323206\n",
      "Iteracion: 12217 Gradiente: [0.0006073417365617691,-0.01047828404235851] Loss: 22.84284034407199\n",
      "Iteracion: 12218 Gradiente: [0.0006070188550713359,-0.010472713464932207] Loss: 22.842840233937977\n",
      "Iteracion: 12219 Gradiente: [0.0006066961451731838,-0.010467145849000067] Loss: 22.842840123921032\n",
      "Iteracion: 12220 Gradiente: [0.0006063736068985766,-0.010461581192978405] Loss: 22.84284001402102\n",
      "Iteracion: 12221 Gradiente: [0.0006060512400495099,-0.01045601949530237] Loss: 22.84283990423784\n",
      "Iteracion: 12222 Gradiente: [0.0006057290445994568,-0.010450460754394323] Loss: 22.842839794571358\n",
      "Iteracion: 12223 Gradiente: [0.0006054070204186246,-0.010444904968684198] Loss: 22.842839685021445\n",
      "Iteracion: 12224 Gradiente: [0.000605085167476697,-0.010439352136598265] Loss: 22.842839575587995\n",
      "Iteracion: 12225 Gradiente: [0.0006047634856353549,-0.010433802256568706] Loss: 22.84283946627086\n",
      "Iteracion: 12226 Gradiente: [0.0006044419747392263,-0.010428255327029963] Loss: 22.842839357069916\n",
      "Iteracion: 12227 Gradiente: [0.00060412063493042,-0.01042271134639871] Loss: 22.84283924798506\n",
      "Iteracion: 12228 Gradiente: [0.000603799465894402,-0.010417170313122644] Loss: 22.842839139016156\n",
      "Iteracion: 12229 Gradiente: [0.0006034784675288545,-0.010411632225633483] Loss: 22.8428390301631\n",
      "Iteracion: 12230 Gradiente: [0.0006031576398764098,-0.010406097082357728] Loss: 22.842838921425734\n",
      "Iteracion: 12231 Gradiente: [0.0006028369828044334,-0.01040056488173337] Loss: 22.842838812803958\n",
      "Iteracion: 12232 Gradiente: [0.0006025164961357632,-0.010395035622200765] Loss: 22.842838704297648\n",
      "Iteracion: 12233 Gradiente: [0.0006021961798846102,-0.010389509302190802] Loss: 22.84283859590668\n",
      "Iteracion: 12234 Gradiente: [0.0006018760339638144,-0.0103839859201391] Loss: 22.84283848763092\n",
      "Iteracion: 12235 Gradiente: [0.0006015560581422127,-0.010378465474492771] Loss: 22.84283837947027\n",
      "Iteracion: 12236 Gradiente: [0.0006012362524586478,-0.010372947963683406] Loss: 22.842838271424572\n",
      "Iteracion: 12237 Gradiente: [0.0006009166167473268,-0.010367433386153972] Loss: 22.842838163493738\n",
      "Iteracion: 12238 Gradiente: [0.0006005971510859353,-0.010361921740336772] Loss: 22.84283805567762\n",
      "Iteracion: 12239 Gradiente: [0.0006002778551902566,-0.010356413024684603] Loss: 22.84283794797613\n",
      "Iteracion: 12240 Gradiente: [0.0005999587289882887,-0.01035090723763569] Loss: 22.842837840389105\n",
      "Iteracion: 12241 Gradiente: [0.0005996397725927712,-0.010345404377625773] Loss: 22.842837732916454\n",
      "Iteracion: 12242 Gradiente: [0.0005993209857355926,-0.010339904443105041] Loss: 22.84283762555804\n",
      "Iteracion: 12243 Gradiente: [0.0005990023683163296,-0.010334407432522142] Loss: 22.842837518313754\n",
      "Iteracion: 12244 Gradiente: [0.0005986839202014001,-0.010328913344322288] Loss: 22.842837411183453\n",
      "Iteracion: 12245 Gradiente: [0.0005983656414467001,-0.010323422176945722] Loss: 22.842837304167023\n",
      "Iteracion: 12246 Gradiente: [0.0005980475319063317,-0.010317933928840261] Loss: 22.842837197264373\n",
      "Iteracion: 12247 Gradiente: [0.0005977295914694499,-0.01031244859845811] Loss: 22.842837090475335\n",
      "Iteracion: 12248 Gradiente: [0.0005974118200953171,-0.010306966184243175] Loss: 22.842836983799828\n",
      "Iteracion: 12249 Gradiente: [0.0005970942175811918,-0.010301486684652398] Loss: 22.842836877237694\n",
      "Iteracion: 12250 Gradiente: [0.0005967767840009704,-0.01029601009812886] Loss: 22.84283677078886\n",
      "Iteracion: 12251 Gradiente: [0.0005964595191197002,-0.010290536423128197] Loss: 22.842836664453166\n",
      "Iteracion: 12252 Gradiente: [0.0005961424229781187,-0.010285065658097281] Loss: 22.8428365582305\n",
      "Iteracion: 12253 Gradiente: [0.0005958254953242204,-0.010279597801498378] Loss: 22.842836452120757\n",
      "Iteracion: 12254 Gradiente: [0.0005955087362063219,-0.010274132851776585] Loss: 22.842836346123786\n",
      "Iteracion: 12255 Gradiente: [0.0005951921455183159,-0.010268670807389194] Loss: 22.84283624023952\n",
      "Iteracion: 12256 Gradiente: [0.00059487572310483,-0.010263211666792789] Loss: 22.842836134467785\n",
      "Iteracion: 12257 Gradiente: [0.0005945594689014418,-0.010257755428445137] Loss: 22.8428360288085\n",
      "Iteracion: 12258 Gradiente: [0.0005942433828048858,-0.010252302090801161] Loss: 22.842835923261514\n",
      "Iteracion: 12259 Gradiente: [0.0005939274647630555,-0.010246851652319222] Loss: 22.842835817826725\n",
      "Iteracion: 12260 Gradiente: [0.0005936117147380553,-0.010241404111454007] Loss: 22.842835712504016\n",
      "Iteracion: 12261 Gradiente: [0.0005932961325347227,-0.01023595946666956] Loss: 22.842835607293246\n",
      "Iteracion: 12262 Gradiente: [0.0005929807181378995,-0.01023051771642495] Loss: 22.842835502194326\n",
      "Iteracion: 12263 Gradiente: [0.0005926654714196881,-0.010225078859181854] Loss: 22.842835397207107\n",
      "Iteracion: 12264 Gradiente: [0.000592350392245559,-0.010219642893403484] Loss: 22.842835292331507\n",
      "Iteracion: 12265 Gradiente: [0.0005920354805529844,-0.010214209817555305] Loss: 22.84283518756739\n",
      "Iteracion: 12266 Gradiente: [0.0005917207362718576,-0.01020877963009532] Loss: 22.842835082914625\n",
      "Iteracion: 12267 Gradiente: [0.0005914061594078627,-0.010203352329486038] Loss: 22.842834978373105\n",
      "Iteracion: 12268 Gradiente: [0.0005910917497468896,-0.01019792791419718] Loss: 22.842834873942707\n",
      "Iteracion: 12269 Gradiente: [0.0005907775071970415,-0.010192506382697412] Loss: 22.842834769623327\n",
      "Iteracion: 12270 Gradiente: [0.0005904634317317914,-0.01018708773344888] Loss: 22.84283466541482\n",
      "Iteracion: 12271 Gradiente: [0.000590149523297138,-0.010181671964916934] Loss: 22.842834561317105\n",
      "Iteracion: 12272 Gradiente: [0.0005898357816723395,-0.010176259075577931] Loss: 22.842834457330024\n",
      "Iteracion: 12273 Gradiente: [0.0005895222068232897,-0.010170849063897928] Loss: 22.842834353453494\n",
      "Iteracion: 12274 Gradiente: [0.0005892087987471466,-0.01016544192834156] Loss: 22.842834249687368\n",
      "Iteracion: 12275 Gradiente: [0.0005888955573103279,-0.010160037667383767] Loss: 22.842834146031564\n",
      "Iteracion: 12276 Gradiente: [0.0005885824823868309,-0.010154636279498419] Loss: 22.842834042485926\n",
      "Iteracion: 12277 Gradiente: [0.0005882695739008644,-0.010149237763155839] Loss: 22.84283393905036\n",
      "Iteracion: 12278 Gradiente: [0.000587956831742531,-0.01014384211683191] Loss: 22.84283383572475\n",
      "Iteracion: 12279 Gradiente: [0.0005876442557687748,-0.010138449339004178] Loss: 22.842833732508954\n",
      "Iteracion: 12280 Gradiente: [0.0005873318460525449,-0.010133059428138106] Loss: 22.842833629402893\n",
      "Iteracion: 12281 Gradiente: [0.0005870196023929945,-0.010127672382715858] Loss: 22.842833526406427\n",
      "Iteracion: 12282 Gradiente: [0.0005867075247058058,-0.010122288201213437] Loss: 22.842833423519448\n",
      "Iteracion: 12283 Gradiente: [0.0005863956129578203,-0.010116906882106137] Loss: 22.842833320741832\n",
      "Iteracion: 12284 Gradiente: [0.0005860838670694572,-0.010111528423870435] Loss: 22.84283321807347\n",
      "Iteracion: 12285 Gradiente: [0.0005857722869289243,-0.010106152824987783] Loss: 22.84283311551424\n",
      "Iteracion: 12286 Gradiente: [0.0005854608723941131,-0.01010078008393987] Loss: 22.842833013064023\n",
      "Iteracion: 12287 Gradiente: [0.0005851496233968116,-0.01009541019920756] Loss: 22.8428329107227\n",
      "Iteracion: 12288 Gradiente: [0.0005848385399104927,-0.010090043169270046] Loss: 22.842832808490186\n",
      "Iteracion: 12289 Gradiente: [0.0005845276218394702,-0.010084678992606892] Loss: 22.842832706366323\n",
      "Iteracion: 12290 Gradiente: [0.0005842168689705811,-0.01007931766770973] Loss: 22.84283260435103\n",
      "Iteracion: 12291 Gradiente: [0.0005839062814686713,-0.010073959193047936] Loss: 22.84283250244417\n",
      "Iteracion: 12292 Gradiente: [0.0005835958587994128,-0.010068603567133039] Loss: 22.842832400645623\n",
      "Iteracion: 12293 Gradiente: [0.0005832856013948155,-0.01006325078842032] Loss: 22.842832298955315\n",
      "Iteracion: 12294 Gradiente: [0.0005829755088901341,-0.010057900855410414] Loss: 22.84283219737308\n",
      "Iteracion: 12295 Gradiente: [0.0005826655811773662,-0.010052553766592946] Loss: 22.842832095898835\n",
      "Iteracion: 12296 Gradiente: [0.000582355818305776,-0.010047209520447116] Loss: 22.842831994532446\n",
      "Iteracion: 12297 Gradiente: [0.0005820462201010439,-0.01004186811546776] Loss: 22.84283189327383\n",
      "Iteracion: 12298 Gradiente: [0.0005817367864589566,-0.01003652955014509] Loss: 22.842831792122812\n",
      "Iteracion: 12299 Gradiente: [0.0005814275172951966,-0.010031193822968612] Loss: 22.842831691079343\n",
      "Iteracion: 12300 Gradiente: [0.000581118412590816,-0.010025860932427123] Loss: 22.84283159014327\n",
      "Iteracion: 12301 Gradiente: [0.0005808094722122329,-0.010020530877013082] Loss: 22.842831489314506\n",
      "Iteracion: 12302 Gradiente: [0.0005805006959946013,-0.010015203655226183] Loss: 22.84283138859289\n",
      "Iteracion: 12303 Gradiente: [0.0005801920840648715,-0.0100098792655461] Loss: 22.842831287978363\n",
      "Iteracion: 12304 Gradiente: [0.0005798836360601929,-0.010004557706482469] Loss: 22.84283118747077\n",
      "Iteracion: 12305 Gradiente: [0.000579575352206992,-0.009999238976513425] Loss: 22.842831087070042\n",
      "Iteracion: 12306 Gradiente: [0.0005792672321518922,-0.009993923074149317] Loss: 22.842830986776008\n",
      "Iteracion: 12307 Gradiente: [0.0005789592758948932,-0.009988609997883202] Loss: 22.842830886588608\n",
      "Iteracion: 12308 Gradiente: [0.0005786514834217845,-0.009983299746207309] Loss: 22.84283078650769\n",
      "Iteracion: 12309 Gradiente: [0.0005783438544862444,-0.009977992317629851] Loss: 22.842830686533162\n",
      "Iteracion: 12310 Gradiente: [0.0005780363890835361,-0.009972687710646137] Loss: 22.842830586664906\n",
      "Iteracion: 12311 Gradiente: [0.000577729087256292,-0.009967385923746146] Loss: 22.8428304869028\n",
      "Iteracion: 12312 Gradiente: [0.000577421948785665,-0.009962086955439512] Loss: 22.842830387246742\n",
      "Iteracion: 12313 Gradiente: [0.0005771149736318648,-0.009956790804224033] Loss: 22.842830287696625\n",
      "Iteracion: 12314 Gradiente: [0.0005768081614936212,-0.00995149746861846] Loss: 22.842830188252307\n",
      "Iteracion: 12315 Gradiente: [0.0005765015125556753,-0.009946206947106262] Loss: 22.84283008891373\n",
      "Iteracion: 12316 Gradiente: [0.0005761950266882347,-0.009940919238194705] Loss: 22.842829989680727\n",
      "Iteracion: 12317 Gradiente: [0.0005758887037529802,-0.009935634340392123] Loss: 22.8428298905532\n",
      "Iteracion: 12318 Gradiente: [0.0005755825435860136,-0.009930352252207796] Loss: 22.84282979153105\n",
      "Iteracion: 12319 Gradiente: [0.0005752765462204934,-0.009925072972141418] Loss: 22.84282969261416\n",
      "Iteracion: 12320 Gradiente: [0.0005749707115389432,-0.009919796498702501] Loss: 22.842829593802424\n",
      "Iteracion: 12321 Gradiente: [0.0005746650395167307,-0.009914522830394882] Loss: 22.842829495095707\n",
      "Iteracion: 12322 Gradiente: [0.0005743595299302721,-0.009909251965735659] Loss: 22.84282939649393\n",
      "Iteracion: 12323 Gradiente: [0.0005740541827956729,-0.009903983903226176] Loss: 22.842829297996957\n",
      "Iteracion: 12324 Gradiente: [0.0005737489978988227,-0.009898718641387442] Loss: 22.84282919960467\n",
      "Iteracion: 12325 Gradiente: [0.0005734439753571981,-0.009893456178716656] Loss: 22.842829101316983\n",
      "Iteracion: 12326 Gradiente: [0.0005731391148988981,-0.009888196513737076] Loss: 22.84282900313378\n",
      "Iteracion: 12327 Gradiente: [0.0005728344166224512,-0.009882939644950402] Loss: 22.842828905054947\n",
      "Iteracion: 12328 Gradiente: [0.0005725298802531142,-0.009877685570881074] Loss: 22.84282880708036\n",
      "Iteracion: 12329 Gradiente: [0.0005722255058259407,-0.009872434290035769] Loss: 22.842828709209915\n",
      "Iteracion: 12330 Gradiente: [0.0005719212931618738,-0.009867185800936677] Loss: 22.842828611443505\n",
      "Iteracion: 12331 Gradiente: [0.000571617242211649,-0.009861940102096038] Loss: 22.84282851378102\n",
      "Iteracion: 12332 Gradiente: [0.0005713133529440029,-0.009856697192027279] Loss: 22.842828416222353\n",
      "Iteracion: 12333 Gradiente: [0.0005710096252443009,-0.009851457069250695] Loss: 22.842828318767385\n",
      "Iteracion: 12334 Gradiente: [0.0005707060590149619,-0.009846219732284093] Loss: 22.84282822141601\n",
      "Iteracion: 12335 Gradiente: [0.0005704026541451413,-0.009840985179647533] Loss: 22.842828124168133\n",
      "Iteracion: 12336 Gradiente: [0.0005700994105562056,-0.009835753409859887] Loss: 22.842828027023604\n",
      "Iteracion: 12337 Gradiente: [0.0005697963282396283,-0.009830524421439554] Loss: 22.842827929982345\n",
      "Iteracion: 12338 Gradiente: [0.0005694934069310875,-0.009825298212916896] Loss: 22.842827833044243\n",
      "Iteracion: 12339 Gradiente: [0.00056919064680964,-0.009820074782799063] Loss: 22.84282773620918\n",
      "Iteracion: 12340 Gradiente: [0.0005688880476014902,-0.009814854129618666] Loss: 22.84282763947705\n",
      "Iteracion: 12341 Gradiente: [0.0005685856092820055,-0.009809636251897302] Loss: 22.842827542847758\n",
      "Iteracion: 12342 Gradiente: [0.0005682833316711822,-0.00980442114816391] Loss: 22.842827446321163\n",
      "Iteracion: 12343 Gradiente: [0.0005679812148438638,-0.009799208816934761] Loss: 22.842827349897185\n",
      "Iteracion: 12344 Gradiente: [0.000567679258536676,-0.009793999256747317] Loss: 22.84282725357569\n",
      "Iteracion: 12345 Gradiente: [0.0005673774628519368,-0.009788792466116074] Loss: 22.842827157356602\n",
      "Iteracion: 12346 Gradiente: [0.0005670758275935366,-0.00978358844357518] Loss: 22.842827061239785\n",
      "Iteracion: 12347 Gradiente: [0.0005667743526619991,-0.009778387187653928] Loss: 22.842826965225132\n",
      "Iteracion: 12348 Gradiente: [0.0005664730380923781,-0.009773188696875934] Loss: 22.84282686931255\n",
      "Iteracion: 12349 Gradiente: [0.0005661718836715105,-0.009767992969776174] Loss: 22.84282677350192\n",
      "Iteracion: 12350 Gradiente: [0.0005658708892629723,-0.009762800004891286] Loss: 22.84282667779312\n",
      "Iteracion: 12351 Gradiente: [0.000565570054914133,-0.009757609800742036] Loss: 22.84282658218606\n",
      "Iteracion: 12352 Gradiente: [0.0005652693804914104,-0.00975242235586613] Loss: 22.842826486680643\n",
      "Iteracion: 12353 Gradiente: [0.000564968865981541,-0.009747237668791205] Loss: 22.84282639127674\n",
      "Iteracion: 12354 Gradiente: [0.0005646685111628357,-0.009742055738057805] Loss: 22.842826295974245\n",
      "Iteracion: 12355 Gradiente: [0.0005643683160864535,-0.009736876562195107] Loss: 22.84282620077305\n",
      "Iteracion: 12356 Gradiente: [0.0005640682805676533,-0.009731700139744249] Loss: 22.84282610567306\n",
      "Iteracion: 12357 Gradiente: [0.000563768404560013,-0.00972652646923772] Loss: 22.842826010674173\n",
      "Iteracion: 12358 Gradiente: [0.0005634686879043709,-0.009721355549216545] Loss: 22.842825915776245\n",
      "Iteracion: 12359 Gradiente: [0.0005631691306642021,-0.009716187378211316] Loss: 22.842825820979208\n",
      "Iteracion: 12360 Gradiente: [0.0005628697326415022,-0.009711021954766608] Loss: 22.842825726282925\n",
      "Iteracion: 12361 Gradiente: [0.0005625704938609033,-0.009705859277414793] Loss: 22.8428256316873\n",
      "Iteracion: 12362 Gradiente: [0.0005622714140552413,-0.009700699344707667] Loss: 22.842825537192248\n",
      "Iteracion: 12363 Gradiente: [0.000561972493313571,-0.009695542155175948] Loss: 22.84282544279761\n",
      "Iteracion: 12364 Gradiente: [0.0005616737315572588,-0.009690387707359112] Loss: 22.842825348503347\n",
      "Iteracion: 12365 Gradiente: [0.0005613751285371411,-0.009685235999811562] Loss: 22.842825254309293\n",
      "Iteracion: 12366 Gradiente: [0.0005610766842693238,-0.009680087031069344] Loss: 22.84282516021538\n",
      "Iteracion: 12367 Gradiente: [0.0005607783986761206,-0.009674940799676434] Loss: 22.842825066221472\n",
      "Iteracion: 12368 Gradiente: [0.0005604802717376363,-0.00966979730417267] Loss: 22.84282497232749\n",
      "Iteracion: 12369 Gradiente: [0.0005601823032075497,-0.009664656543114348] Loss: 22.84282487853331\n",
      "Iteracion: 12370 Gradiente: [0.0005598844930602808,-0.009659518515043786] Loss: 22.84282478483882\n",
      "Iteracion: 12371 Gradiente: [0.0005595868413195149,-0.009654383218502479] Loss: 22.842824691243944\n",
      "Iteracion: 12372 Gradiente: [0.0005592893477607201,-0.009649250652045657] Loss: 22.842824597748553\n",
      "Iteracion: 12373 Gradiente: [0.0005589920123706331,-0.009644120814217652] Loss: 22.84282450435255\n",
      "Iteracion: 12374 Gradiente: [0.0005586948350251457,-0.00963899370357133] Loss: 22.842824411055815\n",
      "Iteracion: 12375 Gradiente: [0.0005583978156664671,-0.009633869318653865] Loss: 22.842824317858256\n",
      "Iteracion: 12376 Gradiente: [0.0005581009542036478,-0.009628747658017882] Loss: 22.842824224759756\n",
      "Iteracion: 12377 Gradiente: [0.0005578042507162688,-0.009623628720203807] Loss: 22.842824131760228\n",
      "Iteracion: 12378 Gradiente: [0.0005575077047884255,-0.009618512503784633] Loss: 22.842824038859543\n",
      "Iteracion: 12379 Gradiente: [0.0005572113165982273,-0.00961339900729854] Loss: 22.84282394605763\n",
      "Iteracion: 12380 Gradiente: [0.0005569150859476697,-0.009608288229306794] Loss: 22.842823853354357\n",
      "Iteracion: 12381 Gradiente: [0.0005566190127533825,-0.009603180168362375] Loss: 22.842823760749617\n",
      "Iteracion: 12382 Gradiente: [0.0005563230970456819,-0.009598074823014714] Loss: 22.842823668243327\n",
      "Iteracion: 12383 Gradiente: [0.0005560273385763518,-0.009592972191829929] Loss: 22.84282357583536\n",
      "Iteracion: 12384 Gradiente: [0.0005557317374353943,-0.009587872273354966] Loss: 22.842823483525628\n",
      "Iteracion: 12385 Gradiente: [0.0005554362933689087,-0.009582775066158433] Loss: 22.842823391314013\n",
      "Iteracion: 12386 Gradiente: [0.0005551410063702634,-0.009577680568792364] Loss: 22.842823299200415\n",
      "Iteracion: 12387 Gradiente: [0.0005548458764659851,-0.009572588779812463] Loss: 22.842823207184733\n",
      "Iteracion: 12388 Gradiente: [0.0005545509033036448,-0.00956749969779196] Loss: 22.842823115266853\n",
      "Iteracion: 12389 Gradiente: [0.0005542560870622993,-0.009562413321278503] Loss: 22.842823023446687\n",
      "Iteracion: 12390 Gradiente: [0.0005539614275259434,-0.009557329648840233] Loss: 22.84282293172413\n",
      "Iteracion: 12391 Gradiente: [0.0005536669245922592,-0.009552248679040313] Loss: 22.84282284009907\n",
      "Iteracion: 12392 Gradiente: [0.0005533725782621938,-0.009547170410438828] Loss: 22.842822748571404\n",
      "Iteracion: 12393 Gradiente: [0.0005530783884694302,-0.00954209484159693] Loss: 22.84282265714102\n",
      "Iteracion: 12394 Gradiente: [0.0005527843550131214,-0.009537021971087375] Loss: 22.842822565807847\n",
      "Iteracion: 12395 Gradiente: [0.0005524904779178996,-0.009531951797469417] Loss: 22.84282247457174\n",
      "Iteracion: 12396 Gradiente: [0.0005521967569601809,-0.009526884319316645] Loss: 22.84282238343263\n",
      "Iteracion: 12397 Gradiente: [0.0005519031922062822,-0.009521819535186774] Loss: 22.842822292390398\n",
      "Iteracion: 12398 Gradiente: [0.0005516097834837789,-0.009516757443654456] Loss: 22.84282220144492\n",
      "Iteracion: 12399 Gradiente: [0.0005513165308703567,-0.009511698043278234] Loss: 22.842822110596135\n",
      "Iteracion: 12400 Gradiente: [0.0005510234340590614,-0.009506641332640342] Loss: 22.842822019843926\n",
      "Iteracion: 12401 Gradiente: [0.0005507304930802093,-0.009501587310304652] Loss: 22.842821929188162\n",
      "Iteracion: 12402 Gradiente: [0.0005504377078712726,-0.009496535974840015] Loss: 22.842821838628783\n",
      "Iteracion: 12403 Gradiente: [0.000550145078290143,-0.00949148732482108] Loss: 22.842821748165655\n",
      "Iteracion: 12404 Gradiente: [0.0005498526042013433,-0.009486441358824985] Loss: 22.842821657798687\n",
      "Iteracion: 12405 Gradiente: [0.0005495602857299293,-0.009481398075412173] Loss: 22.842821567527785\n",
      "Iteracion: 12406 Gradiente: [0.0005492681226125266,-0.0094763574731657] Loss: 22.84282147735283\n",
      "Iteracion: 12407 Gradiente: [0.0005489761148377662,-0.009471319550657498] Loss: 22.84282138727374\n",
      "Iteracion: 12408 Gradiente: [0.0005486842622957511,-0.009466284306463754] Loss: 22.842821297290396\n",
      "Iteracion: 12409 Gradiente: [0.0005483925648566886,-0.009461251739164093] Loss: 22.842821207402697\n",
      "Iteracion: 12410 Gradiente: [0.0005481010225366845,-0.009456221847327957] Loss: 22.84282111761055\n",
      "Iteracion: 12411 Gradiente: [0.0005478096351756297,-0.009451194629540775] Loss: 22.842821027913864\n",
      "Iteracion: 12412 Gradiente: [0.0005475184027394183,-0.009446170084374591] Loss: 22.842820938312506\n",
      "Iteracion: 12413 Gradiente: [0.0005472273251276268,-0.009441148210411516] Loss: 22.8428208488064\n",
      "Iteracion: 12414 Gradiente: [0.0005469364022701484,-0.009436129006229758] Loss: 22.84282075939544\n",
      "Iteracion: 12415 Gradiente: [0.0005466456341660357,-0.009431112470405505] Loss: 22.842820670079508\n",
      "Iteracion: 12416 Gradiente: [0.0005463550205680197,-0.009426098601531051] Loss: 22.842820580858533\n",
      "Iteracion: 12417 Gradiente: [0.000546064561532944,-0.009421087398177382] Loss: 22.84282049173238\n",
      "Iteracion: 12418 Gradiente: [0.0005457742568239609,-0.009416078858940698] Loss: 22.84282040270098\n",
      "Iteracion: 12419 Gradiente: [0.0005454841064135962,-0.009411072982398376] Loss: 22.84282031376422\n",
      "Iteracion: 12420 Gradiente: [0.0005451941103757462,-0.009406069767127672] Loss: 22.842820224922\n",
      "Iteracion: 12421 Gradiente: [0.0005449042684527209,-0.009401069211722301] Loss: 22.842820136174215\n",
      "Iteracion: 12422 Gradiente: [0.0005446145806757841,-0.009396071314762248] Loss: 22.842820047520764\n",
      "Iteracion: 12423 Gradiente: [0.0005443250467540868,-0.009391076074846794] Loss: 22.842819958961538\n",
      "Iteracion: 12424 Gradiente: [0.0005440356668316327,-0.009386083490549642] Loss: 22.842819870496474\n",
      "Iteracion: 12425 Gradiente: [0.0005437464408629466,-0.009381093560456933] Loss: 22.84281978212542\n",
      "Iteracion: 12426 Gradiente: [0.0005434573685676014,-0.009376106283169131] Loss: 22.842819693848323\n",
      "Iteracion: 12427 Gradiente: [0.0005431684498840166,-0.00937112165727605] Loss: 22.842819605665063\n",
      "Iteracion: 12428 Gradiente: [0.0005428796848680879,-0.009366139681358732] Loss: 22.842819517575517\n",
      "Iteracion: 12429 Gradiente: [0.0005425910734032868,-0.009361160354010778] Loss: 22.842819429579627\n",
      "Iteracion: 12430 Gradiente: [0.0005423026152925559,-0.009356183673830643] Loss: 22.842819341677274\n",
      "Iteracion: 12431 Gradiente: [0.0005420143105851593,-0.009351209639403635] Loss: 22.842819253868353\n",
      "Iteracion: 12432 Gradiente: [0.0005417261591550945,-0.00934623824932525] Loss: 22.842819166152772\n",
      "Iteracion: 12433 Gradiente: [0.0005414381608498313,-0.00934126950219453] Loss: 22.842819078530436\n",
      "Iteracion: 12434 Gradiente: [0.0005411503157243184,-0.009336303396597856] Loss: 22.842818991001245\n",
      "Iteracion: 12435 Gradiente: [0.0005408626236506583,-0.009331339931134627] Loss: 22.842818903565085\n",
      "Iteracion: 12436 Gradiente: [0.0005405750845000057,-0.009326379104401757] Loss: 22.84281881622187\n",
      "Iteracion: 12437 Gradiente: [0.0005402876981814112,-0.009321420915000071] Loss: 22.842818728971505\n",
      "Iteracion: 12438 Gradiente: [0.0005400004646446632,-0.009316465361523049] Loss: 22.84281864181386\n",
      "Iteracion: 12439 Gradiente: [0.0005397133838073387,-0.009311512442571985] Loss: 22.842818554748884\n",
      "Iteracion: 12440 Gradiente: [0.0005394264556448055,-0.009306562156740009] Loss: 22.842818467776468\n",
      "Iteracion: 12441 Gradiente: [0.0005391396799401112,-0.009301614502637178] Loss: 22.842818380896485\n",
      "Iteracion: 12442 Gradiente: [0.0005388530567027298,-0.009296669478859106] Loss: 22.842818294108856\n",
      "Iteracion: 12443 Gradiente: [0.0005385665859667673,-0.00929172708399939] Loss: 22.842818207413476\n",
      "Iteracion: 12444 Gradiente: [0.000538280267344741,-0.009286787316680053] Loss: 22.842818120810264\n",
      "Iteracion: 12445 Gradiente: [0.0005379941010810777,-0.00928185017548427] Loss: 22.842818034299103\n",
      "Iteracion: 12446 Gradiente: [0.0005377080869341928,-0.009276915659023999] Loss: 22.842817947879894\n",
      "Iteracion: 12447 Gradiente: [0.0005374222247231349,-0.009271983765912732] Loss: 22.842817861552554\n",
      "Iteracion: 12448 Gradiente: [0.0005371365145862228,-0.009267054494741581] Loss: 22.842817775316977\n",
      "Iteracion: 12449 Gradiente: [0.0005368509563140833,-0.009262127844122265] Loss: 22.84281768917306\n",
      "Iteracion: 12450 Gradiente: [0.0005365655498508204,-0.009257203812663069] Loss: 22.84281760312073\n",
      "Iteracion: 12451 Gradiente: [0.0005362802952049606,-0.009252282398964933] Loss: 22.842817517159858\n",
      "Iteracion: 12452 Gradiente: [0.0005359951920960763,-0.009247363601647034] Loss: 22.842817431290364\n",
      "Iteracion: 12453 Gradiente: [0.0005357102407013296,-0.009242447419304986] Loss: 22.842817345512152\n",
      "Iteracion: 12454 Gradiente: [0.0005354254406455539,-0.009237533850564953] Loss: 22.84281725982511\n",
      "Iteracion: 12455 Gradiente: [0.0005351407920500151,-0.009232622894025273] Loss: 22.84281717422916\n",
      "Iteracion: 12456 Gradiente: [0.0005348562947261826,-0.009227714548304889] Loss: 22.84281708872419\n",
      "Iteracion: 12457 Gradiente: [0.0005345719486731089,-0.009222808812010541] Loss: 22.84281700331011\n",
      "Iteracion: 12458 Gradiente: [0.0005342877538017395,-0.00921790568375549] Loss: 22.842816917986827\n",
      "Iteracion: 12459 Gradiente: [0.0005340037100504939,-0.009213005162151452] Loss: 22.842816832754238\n",
      "Iteracion: 12460 Gradiente: [0.0005337198172592632,-0.009208107245818197] Loss: 22.842816747612247\n",
      "Iteracion: 12461 Gradiente: [0.0005334360754792063,-0.009203211933362591] Loss: 22.842816662560768\n",
      "Iteracion: 12462 Gradiente: [0.0005331524844583176,-0.009198319223409968] Loss: 22.8428165775997\n",
      "Iteracion: 12463 Gradiente: [0.0005328690442657565,-0.009193429114568849] Loss: 22.842816492728943\n",
      "Iteracion: 12464 Gradiente: [0.0005325857547793096,-0.009188541605458648] Loss: 22.842816407948384\n",
      "Iteracion: 12465 Gradiente: [0.0005323026157640243,-0.009183656694704941] Loss: 22.842816323257964\n",
      "Iteracion: 12466 Gradiente: [0.0005320196274103257,-0.009178774380912812] Loss: 22.842816238657566\n",
      "Iteracion: 12467 Gradiente: [0.0005317367894141019,-0.009173894662712694] Loss: 22.842816154147084\n",
      "Iteracion: 12468 Gradiente: [0.0005314541017933531,-0.009169017538721866] Loss: 22.84281606972645\n",
      "Iteracion: 12469 Gradiente: [0.0005311715644287081,-0.009164143007559982] Loss: 22.842815985395543\n",
      "Iteracion: 12470 Gradiente: [0.0005308891773078509,-0.009159271067847167] Loss: 22.842815901154275\n",
      "Iteracion: 12471 Gradiente: [0.0005306069403957281,-0.009154401718201536] Loss: 22.842815817002574\n",
      "Iteracion: 12472 Gradiente: [0.0005303248534914929,-0.009149534957254228] Loss: 22.842815732940306\n",
      "Iteracion: 12473 Gradiente: [0.0005300429164786162,-0.009144670783630222] Loss: 22.8428156489674\n",
      "Iteracion: 12474 Gradiente: [0.0005297611294537319,-0.009139809195945148] Loss: 22.842815565083757\n",
      "Iteracion: 12475 Gradiente: [0.0005294794921714659,-0.009134950192831918] Loss: 22.842815481289264\n",
      "Iteracion: 12476 Gradiente: [0.000529198004604344,-0.009130093772915278] Loss: 22.842815397583866\n",
      "Iteracion: 12477 Gradiente: [0.0005289166667220495,-0.009125239934818197] Loss: 22.84281531396744\n",
      "Iteracion: 12478 Gradiente: [0.0005286354784023691,-0.009120388677172523] Loss: 22.842815230439893\n",
      "Iteracion: 12479 Gradiente: [0.0005283544394918256,-0.009115539998606082] Loss: 22.84281514700113\n",
      "Iteracion: 12480 Gradiente: [0.000528073550060526,-0.009110693897742788] Loss: 22.842815063651063\n",
      "Iteracion: 12481 Gradiente: [0.0005277928099701512,-0.009105850373215556] Loss: 22.842814980389598\n",
      "Iteracion: 12482 Gradiente: [0.0005275122191126987,-0.009101009423655644] Loss: 22.842814897216623\n",
      "Iteracion: 12483 Gradiente: [0.0005272317774493255,-0.009096171047691353] Loss: 22.842814814132073\n",
      "Iteracion: 12484 Gradiente: [0.0005269514849184513,-0.009091335243953225] Loss: 22.842814731135846\n",
      "Iteracion: 12485 Gradiente: [0.0005266713413419666,-0.009086502011081403] Loss: 22.842814648227833\n",
      "Iteracion: 12486 Gradiente: [0.0005263913465768155,-0.009081671347710577] Loss: 22.842814565407956\n",
      "Iteracion: 12487 Gradiente: [0.0005261115008276344,-0.009076843252459336] Loss: 22.842814482676104\n",
      "Iteracion: 12488 Gradiente: [0.0005258318037770475,-0.009072017723976226] Loss: 22.842814400032204\n",
      "Iteracion: 12489 Gradiente: [0.00052555225537958,-0.009067194760896412] Loss: 22.842814317476154\n",
      "Iteracion: 12490 Gradiente: [0.0005252728557877617,-0.009062374361839313] Loss: 22.842814235007857\n",
      "Iteracion: 12491 Gradiente: [0.0005249936045477929,-0.009057556525464012] Loss: 22.842814152627216\n",
      "Iteracion: 12492 Gradiente: [0.000524714501826414,-0.009052741250396797] Loss: 22.84281407033415\n",
      "Iteracion: 12493 Gradiente: [0.0005244355474663583,-0.009047928535277567] Loss: 22.842813988128558\n",
      "Iteracion: 12494 Gradiente: [0.0005241567415168902,-0.009043118378740899] Loss: 22.842813906010353\n",
      "Iteracion: 12495 Gradiente: [0.0005238780836426334,-0.009038310779436998] Loss: 22.842813823979434\n",
      "Iteracion: 12496 Gradiente: [0.0005235995739610644,-0.009033505735998423] Loss: 22.842813742035705\n",
      "Iteracion: 12497 Gradiente: [0.0005233212123982867,-0.009028703247066143] Loss: 22.842813660179093\n",
      "Iteracion: 12498 Gradiente: [0.0005230429987828227,-0.009023903311285981] Loss: 22.84281357840948\n",
      "Iteracion: 12499 Gradiente: [0.0005227649331089879,-0.009019105927296541] Loss: 22.842813496726798\n",
      "Iteracion: 12500 Gradiente: [0.0005224870152005679,-0.009014311093747552] Loss: 22.84281341513093\n",
      "Iteracion: 12501 Gradiente: [0.0005222092450651416,-0.00900951880927856] Loss: 22.842813333621816\n",
      "Iteracion: 12502 Gradiente: [0.0005219316226288129,-0.009004729072533142] Loss: 22.842813252199328\n",
      "Iteracion: 12503 Gradiente: [0.0005216541477390517,-0.008999941882160556] Loss: 22.842813170863398\n",
      "Iteracion: 12504 Gradiente: [0.0005213768203996476,-0.008995157236804255] Loss: 22.842813089613916\n",
      "Iteracion: 12505 Gradiente: [0.0005210996404504916,-0.008990375135115988] Loss: 22.842813008450815\n",
      "Iteracion: 12506 Gradiente: [0.0005208226078442143,-0.008985595575737785] Loss: 22.842812927373984\n",
      "Iteracion: 12507 Gradiente: [0.000520545722559973,-0.008980818557319144] Loss: 22.842812846383325\n",
      "Iteracion: 12508 Gradiente: [0.0005202689845096605,-0.008976044078507073] Loss: 22.842812765478776\n",
      "Iteracion: 12509 Gradiente: [0.0005199923935426417,-0.00897127213795829] Loss: 22.84281268466021\n",
      "Iteracion: 12510 Gradiente: [0.0005197159495584932,-0.008966502734322408] Loss: 22.842812603927563\n",
      "Iteracion: 12511 Gradiente: [0.0005194396526386906,-0.008961735866241938] Loss: 22.842812523280728\n",
      "Iteracion: 12512 Gradiente: [0.0005191635025804923,-0.008956971532377385] Loss: 22.842812442719623\n",
      "Iteracion: 12513 Gradiente: [0.0005188874993128441,-0.008952209731378957] Loss: 22.842812362244157\n",
      "Iteracion: 12514 Gradiente: [0.000518611642741007,-0.008947450461903254] Loss: 22.84281228185423\n",
      "Iteracion: 12515 Gradiente: [0.000518335932911403,-0.00894269372259539] Loss: 22.842812201549748\n",
      "Iteracion: 12516 Gradiente: [0.0005180603697131877,-0.008937939512112318] Loss: 22.84281212133063\n",
      "Iteracion: 12517 Gradiente: [0.0005177849529208818,-0.008933187829120353] Loss: 22.842812041196794\n",
      "Iteracion: 12518 Gradiente: [0.0005175096825003797,-0.008928438672271] Loss: 22.842811961148126\n",
      "Iteracion: 12519 Gradiente: [0.0005172345585208405,-0.008923692040213756] Loss: 22.842811881184563\n",
      "Iteracion: 12520 Gradiente: [0.0005169595806724677,-0.008918947931618959] Loss: 22.84281180130599\n",
      "Iteracion: 12521 Gradiente: [0.0005166847491011595,-0.00891420634513267] Loss: 22.842811721512312\n",
      "Iteracion: 12522 Gradiente: [0.0005164100636534386,-0.008909467279419067] Loss: 22.842811641803483\n",
      "Iteracion: 12523 Gradiente: [0.0005161355242231972,-0.008904730733138185] Loss: 22.84281156217937\n",
      "Iteracion: 12524 Gradiente: [0.0005158611306579057,-0.008899996704956574] Loss: 22.84281148263989\n",
      "Iteracion: 12525 Gradiente: [0.0005155868830778824,-0.008895265193523134] Loss: 22.842811403184953\n",
      "Iteracion: 12526 Gradiente: [0.0005153127812349112,-0.00889053619751022] Loss: 22.842811323814484\n",
      "Iteracion: 12527 Gradiente: [0.0005150388250816225,-0.00888580971557822] Loss: 22.84281124452839\n",
      "Iteracion: 12528 Gradiente: [0.0005147650146644385,-0.008881085746383969] Loss: 22.842811165326566\n",
      "Iteracion: 12529 Gradiente: [0.0005144913498393559,-0.008876364288595914] Loss: 22.842811086208947\n",
      "Iteracion: 12530 Gradiente: [0.0005142178304974247,-0.008871645340879178] Loss: 22.8428110071754\n",
      "Iteracion: 12531 Gradiente: [0.0005139444564368508,-0.0088669289019073] Loss: 22.842810928225887\n",
      "Iteracion: 12532 Gradiente: [0.0005136712278689023,-0.008862214970330958] Loss: 22.842810849360287\n",
      "Iteracion: 12533 Gradiente: [0.0005133981445207306,-0.008857503544826055] Loss: 22.842810770578517\n",
      "Iteracion: 12534 Gradiente: [0.0005131252063108604,-0.008852794624062691] Loss: 22.842810691880498\n",
      "Iteracion: 12535 Gradiente: [0.0005128524131578161,-0.008848088206708364] Loss: 22.84281061326613\n",
      "Iteracion: 12536 Gradiente: [0.0005125797650540183,-0.008843384291428672] Loss: 22.842810534735335\n",
      "Iteracion: 12537 Gradiente: [0.0005123072619009387,-0.008838682876895139] Loss: 22.842810456288003\n",
      "Iteracion: 12538 Gradiente: [0.0005120349036407864,-0.008833983961775852] Loss: 22.84281037792407\n",
      "Iteracion: 12539 Gradiente: [0.0005117626902081914,-0.008829287544742215] Loss: 22.84281029964342\n",
      "Iteracion: 12540 Gradiente: [0.0005114906214259918,-0.008824593624472143] Loss: 22.842810221445994\n",
      "Iteracion: 12541 Gradiente: [0.0005112186973140827,-0.008819902199632423] Loss: 22.842810143331693\n",
      "Iteracion: 12542 Gradiente: [0.0005109469177284609,-0.00881521326890038] Loss: 22.842810065300423\n",
      "Iteracion: 12543 Gradiente: [0.0005106752826899689,-0.008810526830944928] Loss: 22.842809987352098\n",
      "Iteracion: 12544 Gradiente: [0.0005104037920100761,-0.008805842884445999] Loss: 22.842809909486622\n",
      "Iteracion: 12545 Gradiente: [0.0005101324456584659,-0.008801161428076416] Loss: 22.84280983170392\n",
      "Iteracion: 12546 Gradiente: [0.0005098612436446122,-0.00879648246050806] Loss: 22.84280975400391\n",
      "Iteracion: 12547 Gradiente: [0.0005095901857155619,-0.008791805980429501] Loss: 22.84280967638647\n",
      "Iteracion: 12548 Gradiente: [0.0005093192719082632,-0.008787131986509777] Loss: 22.84280959885156\n",
      "Iteracion: 12549 Gradiente: [0.0005090485022378743,-0.008782460477421712] Loss: 22.84280952139907\n",
      "Iteracion: 12550 Gradiente: [0.000508777876407862,-0.008777791451856842] Loss: 22.842809444028898\n",
      "Iteracion: 12551 Gradiente: [0.0005085073944229634,-0.008773124908490715] Loss: 22.842809366740973\n",
      "Iteracion: 12552 Gradiente: [0.0005082370563419166,-0.008768460845997576] Loss: 22.842809289535197\n",
      "Iteracion: 12553 Gradiente: [0.0005079668618520828,-0.008763799263070145] Loss: 22.842809212411495\n",
      "Iteracion: 12554 Gradiente: [0.0005076968111211499,-0.008759140158375563] Loss: 22.84280913536976\n",
      "Iteracion: 12555 Gradiente: [0.0005074269039596402,-0.008754483530605246] Loss: 22.84280905840994\n",
      "Iteracion: 12556 Gradiente: [0.0005071571401894441,-0.008749829378445876] Loss: 22.842808981531913\n",
      "Iteracion: 12557 Gradiente: [0.0005068875198531942,-0.0087451777005775] Loss: 22.84280890473561\n",
      "Iteracion: 12558 Gradiente: [0.0005066180428419405,-0.008740528495682772] Loss: 22.84280882802094\n",
      "Iteracion: 12559 Gradiente: [0.0005063487092096845,-0.008735881762441503] Loss: 22.84280875138781\n",
      "Iteracion: 12560 Gradiente: [0.0005060795187148415,-0.00873123749955044] Loss: 22.842808674836146\n",
      "Iteracion: 12561 Gradiente: [0.0005058104713109894,-0.008726595705690817] Loss: 22.84280859836586\n",
      "Iteracion: 12562 Gradiente: [0.0005055415669545482,-0.008721956379553338] Loss: 22.84280852197685\n",
      "Iteracion: 12563 Gradiente: [0.0005052728055242521,-0.008717319519824447] Loss: 22.84280844566905\n",
      "Iteracion: 12564 Gradiente: [0.0005050041869642048,-0.008712685125192838] Loss: 22.84280836944235\n",
      "Iteracion: 12565 Gradiente: [0.000504735711258301,-0.008708053194345073] Loss: 22.842808293296684\n",
      "Iteracion: 12566 Gradiente: [0.0005044673782786428,-0.00870342372597482] Loss: 22.842808217231962\n",
      "Iteracion: 12567 Gradiente: [0.0005041991879380703,-0.008698796718773138] Loss: 22.842808141248092\n",
      "Iteracion: 12568 Gradiente: [0.000503931140193951,-0.008694172171427657] Loss: 22.842808065344993\n",
      "Iteracion: 12569 Gradiente: [0.0005036632349098606,-0.008689550082636307] Loss: 22.842807989522573\n",
      "Iteracion: 12570 Gradiente: [0.0005033954720744305,-0.008684930451089319] Loss: 22.842807913780756\n",
      "Iteracion: 12571 Gradiente: [0.0005031278516507124,-0.008680313275476336] Loss: 22.84280783811946\n",
      "Iteracion: 12572 Gradiente: [0.0005028603734122801,-0.008675698554501092] Loss: 22.842807762538566\n",
      "Iteracion: 12573 Gradiente: [0.0005025930374112401,-0.008671086286850382] Loss: 22.84280768703804\n",
      "Iteracion: 12574 Gradiente: [0.000502325843477062,-0.008666476471226167] Loss: 22.84280761161775\n",
      "Iteracion: 12575 Gradiente: [0.0005020587916135355,-0.008661869106319268] Loss: 22.84280753627764\n",
      "Iteracion: 12576 Gradiente: [0.0005017918817581328,-0.008657264190828566] Loss: 22.84280746101761\n",
      "Iteracion: 12577 Gradiente: [0.0005015251137611661,-0.008652661723454002] Loss: 22.842807385837588\n",
      "Iteracion: 12578 Gradiente: [0.000501258487696532,-0.008648061702885575] Loss: 22.842807310737452\n",
      "Iteracion: 12579 Gradiente: [0.0005009920033321199,-0.008643464127832108] Loss: 22.842807235717178\n",
      "Iteracion: 12580 Gradiente: [0.0005007256605116102,-0.008638868996998046] Loss: 22.842807160776648\n",
      "Iteracion: 12581 Gradiente: [0.0005004594593704799,-0.008634276309072675] Loss: 22.842807085915766\n",
      "Iteracion: 12582 Gradiente: [0.0005001933998111477,-0.008629686062755937] Loss: 22.842807011134457\n",
      "Iteracion: 12583 Gradiente: [0.0004999274815929766,-0.008625098256762342] Loss: 22.842806936432638\n",
      "Iteracion: 12584 Gradiente: [0.0004996617047964946,-0.008620512889784255] Loss: 22.842806861810246\n",
      "Iteracion: 12585 Gradiente: [0.0004993960693136994,-0.008615929960526004] Loss: 22.84280678726715\n",
      "Iteracion: 12586 Gradiente: [0.0004991305750242721,-0.008611349467696173] Loss: 22.842806712803302\n",
      "Iteracion: 12587 Gradiente: [0.0004988652218827383,-0.008606771409995299] Loss: 22.84280663841861\n",
      "Iteracion: 12588 Gradiente: [0.0004986000098625709,-0.008602195786128893] Loss: 22.842806564112987\n",
      "Iteracion: 12589 Gradiente: [0.0004983349387468176,-0.008597622594809689] Loss: 22.842806489886353\n",
      "Iteracion: 12590 Gradiente: [0.0004980700086434809,-0.00859305183473289] Loss: 22.842806415738607\n",
      "Iteracion: 12591 Gradiente: [0.0004978052193507665,-0.008588483504615852] Loss: 22.842806341669693\n",
      "Iteracion: 12592 Gradiente: [0.0004975405708516216,-0.008583917603160766] Loss: 22.842806267679507\n",
      "Iteracion: 12593 Gradiente: [0.0004972760630645704,-0.008579354129079538] Loss: 22.842806193767977\n",
      "Iteracion: 12594 Gradiente: [0.0004970116957982403,-0.008574793081085967] Loss: 22.842806119935002\n",
      "Iteracion: 12595 Gradiente: [0.0004967474691369489,-0.00857023445788106] Loss: 22.842806046180517\n",
      "Iteracion: 12596 Gradiente: [0.000496483382905429,-0.00856567825818407] Loss: 22.842805972504426\n",
      "Iteracion: 12597 Gradiente: [0.0004962194370771537,-0.008561124480702276] Loss: 22.842805898906658\n",
      "Iteracion: 12598 Gradiente: [0.0004959556315905426,-0.008556573124146165] Loss: 22.842805825387117\n",
      "Iteracion: 12599 Gradiente: [0.000495691966322435,-0.00855202418723285] Loss: 22.842805751945722\n",
      "Iteracion: 12600 Gradiente: [0.0004954284412823048,-0.008547477668671278] Loss: 22.84280567858241\n",
      "Iteracion: 12601 Gradiente: [0.0004951650562531995,-0.008542933567183771] Loss: 22.842805605297077\n",
      "Iteracion: 12602 Gradiente: [0.000494901811375333,-0.008538391881472525] Loss: 22.842805532089642\n",
      "Iteracion: 12603 Gradiente: [0.0004946387063919625,-0.008533852610263773] Loss: 22.842805458960015\n",
      "Iteracion: 12604 Gradiente: [0.0004943757412130859,-0.008529315752274513] Loss: 22.842805385908136\n",
      "Iteracion: 12605 Gradiente: [0.000494112915888915,-0.008524781306216292] Loss: 22.842805312933894\n",
      "Iteracion: 12606 Gradiente: [0.0004938502303010258,-0.008520249270808715] Loss: 22.84280524003724\n",
      "Iteracion: 12607 Gradiente: [0.0004935876843859433,-0.008515719644767472] Loss: 22.842805167218064\n",
      "Iteracion: 12608 Gradiente: [0.0004933252780517705,-0.008511192426814418] Loss: 22.842805094476283\n",
      "Iteracion: 12609 Gradiente: [0.0004930630112236638,-0.008506667615670456] Loss: 22.842805021811856\n",
      "Iteracion: 12610 Gradiente: [0.0004928008837623565,-0.008502145210057084] Loss: 22.84280494922464\n",
      "Iteracion: 12611 Gradiente: [0.0004925388957000602,-0.008497625208690943] Loss: 22.842804876714588\n",
      "Iteracion: 12612 Gradiente: [0.0004922770468832974,-0.008493107610299333] Loss: 22.84280480428162\n",
      "Iteracion: 12613 Gradiente: [0.0004920153373111209,-0.008488592413598539] Loss: 22.84280473192565\n",
      "Iteracion: 12614 Gradiente: [0.0004917537668546856,-0.008484079617316453] Loss: 22.842804659646585\n",
      "Iteracion: 12615 Gradiente: [0.0004914923354173577,-0.008479569220177533] Loss: 22.842804587444355\n",
      "Iteracion: 12616 Gradiente: [0.0004912310430351378,-0.00847506122089984] Loss: 22.84280451531887\n",
      "Iteracion: 12617 Gradiente: [0.0004909698895782336,-0.008470555618211506] Loss: 22.842804443270058\n",
      "Iteracion: 12618 Gradiente: [0.0004907088748817993,-0.008466052410844919] Loss: 22.842804371297827\n",
      "Iteracion: 12619 Gradiente: [0.0004904479989590981,-0.008461551597521103] Loss: 22.84280429940212\n",
      "Iteracion: 12620 Gradiente: [0.0004901872617807612,-0.008457053176964753] Loss: 22.842804227582807\n",
      "Iteracion: 12621 Gradiente: [0.0004899266632558389,-0.008452557147904353] Loss: 22.842804155839865\n",
      "Iteracion: 12622 Gradiente: [0.0004896662031512733,-0.00844806350907857] Loss: 22.842804084173164\n",
      "Iteracion: 12623 Gradiente: [0.0004894058815826459,-0.008443572259204283] Loss: 22.842804012582647\n",
      "Iteracion: 12624 Gradiente: [0.0004891456983528997,-0.00843908339702028] Loss: 22.84280394106823\n",
      "Iteracion: 12625 Gradiente: [0.000488885653426981,-0.008434596921254684] Loss: 22.842803869629826\n",
      "Iteracion: 12626 Gradiente: [0.000488625746802048,-0.008430112830633382] Loss: 22.84280379826737\n",
      "Iteracion: 12627 Gradiente: [0.0004883659783672556,-0.008425631123891957] Loss: 22.842803726980762\n",
      "Iteracion: 12628 Gradiente: [0.0004881063480799715,-0.008421151799760433] Loss: 22.84280365576995\n",
      "Iteracion: 12629 Gradiente: [0.00048784685579524495,-0.008416674856977006] Loss: 22.842803584634808\n",
      "Iteracion: 12630 Gradiente: [0.00048758750142212647,-0.00841220029427537] Loss: 22.842803513575287\n",
      "Iteracion: 12631 Gradiente: [0.00048732828488861436,-0.00840772811039135] Loss: 22.84280344259131\n",
      "Iteracion: 12632 Gradiente: [0.00048706920624681517,-0.00840325830405284] Loss: 22.842803371682788\n",
      "Iteracion: 12633 Gradiente: [0.0004868102652636708,-0.008398790874004783] Loss: 22.84280330084963\n",
      "Iteracion: 12634 Gradiente: [0.0004865514620054986,-0.008394325818976256] Loss: 22.84280323009178\n",
      "Iteracion: 12635 Gradiente: [0.0004862927963065052,-0.00838986313770865] Loss: 22.842803159409126\n",
      "Iteracion: 12636 Gradiente: [0.0004860342681865859,-0.008385402828937552] Loss: 22.84280308880162\n",
      "Iteracion: 12637 Gradiente: [0.0004857758774212092,-0.008380944891406727] Loss: 22.842803018269162\n",
      "Iteracion: 12638 Gradiente: [0.0004855176240018485,-0.008376489323854248] Loss: 22.842802947811684\n",
      "Iteracion: 12639 Gradiente: [0.00048525950793324075,-0.008372036125015707] Loss: 22.84280287742909\n",
      "Iteracion: 12640 Gradiente: [0.00048500152909885705,-0.008367585293632256] Loss: 22.842802807121313\n",
      "Iteracion: 12641 Gradiente: [0.0004847436873755366,-0.008363136828451327] Loss: 22.84280273688829\n",
      "Iteracion: 12642 Gradiente: [0.0004844859827490685,-0.008358690728208391] Loss: 22.842802666729895\n",
      "Iteracion: 12643 Gradiente: [0.00048422841515408284,-0.00835424699164733] Loss: 22.84280259664611\n",
      "Iteracion: 12644 Gradiente: [0.0004839709844996302,-0.008349805617512966] Loss: 22.84280252663679\n",
      "Iteracion: 12645 Gradiente: [0.00048371369065970763,-0.008345366604552968] Loss: 22.8428024567019\n",
      "Iteracion: 12646 Gradiente: [0.00048345653359357736,-0.00834092995150802] Loss: 22.842802386841356\n",
      "Iteracion: 12647 Gradiente: [0.0004831995132425012,-0.008336495657123421] Loss: 22.842802317055078\n",
      "Iteracion: 12648 Gradiente: [0.0004829426295752152,-0.00833206372014388] Loss: 22.842802247342963\n",
      "Iteracion: 12649 Gradiente: [0.0004826858824098205,-0.008327634139322034] Loss: 22.842802177704968\n",
      "Iteracion: 12650 Gradiente: [0.00048242927179842354,-0.00832320691339703] Loss: 22.842802108140976\n",
      "Iteracion: 12651 Gradiente: [0.00048217279756386234,-0.008318782041124942] Loss: 22.842802038650937\n",
      "Iteracion: 12652 Gradiente: [0.00048191645977719113,-0.008314359521244678] Loss: 22.84280196923478\n",
      "Iteracion: 12653 Gradiente: [0.00048166025814850856,-0.00830993935251918] Loss: 22.842801899892404\n",
      "Iteracion: 12654 Gradiente: [0.0004814041927801327,-0.00830552153368771] Loss: 22.84280183062373\n",
      "Iteracion: 12655 Gradiente: [0.000481148263573535,-0.008301106063501014] Loss: 22.84280176142869\n",
      "Iteracion: 12656 Gradiente: [0.0004808924703439743,-0.008296692940719552] Loss: 22.842801692307205\n",
      "Iteracion: 12657 Gradiente: [0.00048063681324871744,-0.008292282164081873] Loss: 22.842801623259188\n",
      "Iteracion: 12658 Gradiente: [0.0004803812918614388,-0.008287873732358975] Loss: 22.842801554284566\n",
      "Iteracion: 12659 Gradiente: [0.0004801259064521446,-0.008283467644286209] Loss: 22.842801485383276\n",
      "Iteracion: 12660 Gradiente: [0.0004798706568379885,-0.008279063898624036] Loss: 22.842801416555215\n",
      "Iteracion: 12661 Gradiente: [0.0004796155428759145,-0.008274662494130662] Loss: 22.842801347800325\n",
      "Iteracion: 12662 Gradiente: [0.0004793605645460275,-0.008270263429559558] Loss: 22.842801279118518\n",
      "Iteracion: 12663 Gradiente: [0.0004791057216664285,-0.008265866703672487] Loss: 22.84280121050971\n",
      "Iteracion: 12664 Gradiente: [0.000478851014328067,-0.008261472315215694] Loss: 22.842801141973847\n",
      "Iteracion: 12665 Gradiente: [0.00047859644241820355,-0.008257080262949638] Loss: 22.842801073510827\n",
      "Iteracion: 12666 Gradiente: [0.0004783420058250461,-0.008252690545634897] Loss: 22.842801005120585\n",
      "Iteracion: 12667 Gradiente: [0.0004780877045940694,-0.008248303162022689] Loss: 22.842800936803023\n",
      "Iteracion: 12668 Gradiente: [0.00047783353849411013,-0.00824391811088212] Loss: 22.842800868558104\n",
      "Iteracion: 12669 Gradiente: [0.0004775795075554849,-0.008239535390966541] Loss: 22.84280080038571\n",
      "Iteracion: 12670 Gradiente: [0.000477325611640822,-0.008235155001040913] Loss: 22.842800732285802\n",
      "Iteracion: 12671 Gradiente: [0.0004770718506582246,-0.008230776939868652] Loss: 22.842800664258267\n",
      "Iteracion: 12672 Gradiente: [0.0004768182246162193,-0.008226401206206308] Loss: 22.842800596303046\n",
      "Iteracion: 12673 Gradiente: [0.00047656473338027655,-0.00822202779881896] Loss: 22.842800528420064\n",
      "Iteracion: 12674 Gradiente: [0.0004763113769068165,-0.008217656716470382] Loss: 22.842800460609237\n",
      "Iteracion: 12675 Gradiente: [0.00047605815517215433,-0.008213287957920083] Loss: 22.84280039287048\n",
      "Iteracion: 12676 Gradiente: [0.0004758050680739719,-0.008208921521934916] Loss: 22.842800325203758\n",
      "Iteracion: 12677 Gradiente: [0.0004755521155175302,-0.008204557407281024] Loss: 22.84280025760895\n",
      "Iteracion: 12678 Gradiente: [0.0004752992974308275,-0.008200195612726564] Loss: 22.842800190085992\n",
      "Iteracion: 12679 Gradiente: [0.0004750466136651236,-0.008195836137040283] Loss: 22.84280012263481\n",
      "Iteracion: 12680 Gradiente: [0.0004747940644496869,-0.008191478978972573] Loss: 22.842800055255317\n",
      "Iteracion: 12681 Gradiente: [0.0004745416492634528,-0.008187124137315512] Loss: 22.842799987947462\n",
      "Iteracion: 12682 Gradiente: [0.00047428936837453267,-0.008182771610821505] Loss: 22.842799920711148\n",
      "Iteracion: 12683 Gradiente: [0.0004740372215233416,-0.008178421398268654] Loss: 22.842799853546307\n",
      "Iteracion: 12684 Gradiente: [0.00047378520882640864,-0.008174073498416472] Loss: 22.842799786452858\n",
      "Iteracion: 12685 Gradiente: [0.00047353333006678136,-0.008169727910043651] Loss: 22.842799719430733\n",
      "Iteracion: 12686 Gradiente: [0.0004732815851525629,-0.008165384631922972] Loss: 22.842799652479837\n",
      "Iteracion: 12687 Gradiente: [0.00047302997410933283,-0.008161043662819622] Loss: 22.842799585600112\n",
      "Iteracion: 12688 Gradiente: [0.0004727784969200381,-0.00815670500150342] Loss: 22.842799518791498\n",
      "Iteracion: 12689 Gradiente: [0.00047252715335256804,-0.008152368646756496] Loss: 22.842799452053875\n",
      "Iteracion: 12690 Gradiente: [0.00047227594340218577,-0.00814803459734949] Loss: 22.842799385387206\n",
      "Iteracion: 12691 Gradiente: [0.0004720248669949948,-0.00814370285205636] Loss: 22.8427993187914\n",
      "Iteracion: 12692 Gradiente: [0.00047177392415562733,-0.008139373409648225] Loss: 22.8427992522664\n",
      "Iteracion: 12693 Gradiente: [0.0004715231146358671,-0.008135046268907924] Loss: 22.84279918581209\n",
      "Iteracion: 12694 Gradiente: [0.00047127243853519,-0.008130721428604914] Loss: 22.84279911942843\n",
      "Iteracion: 12695 Gradiente: [0.00047102189559495856,-0.008126398887525118] Loss: 22.842799053115332\n",
      "Iteracion: 12696 Gradiente: [0.0004707714859364387,-0.008122078644436688] Loss: 22.84279898687272\n",
      "Iteracion: 12697 Gradiente: [0.00047052120930383506,-0.008117760698126374] Loss: 22.842798920700538\n",
      "Iteracion: 12698 Gradiente: [0.00047027106581178183,-0.008113445047365294] Loss: 22.842798854598684\n",
      "Iteracion: 12699 Gradiente: [0.0004700210552764853,-0.008109131690937469] Loss: 22.84279878856709\n",
      "Iteracion: 12700 Gradiente: [0.0004697711777045773,-0.008104820627619228] Loss: 22.8427987226057\n",
      "Iteracion: 12701 Gradiente: [0.00046952143290468484,-0.008100511856199333] Loss: 22.84279865671441\n",
      "Iteracion: 12702 Gradiente: [0.00046927182091944055,-0.008096205375452333] Loss: 22.84279859089316\n",
      "Iteracion: 12703 Gradiente: [0.00046902234158115635,-0.008091901184166044] Loss: 22.84279852514189\n",
      "Iteracion: 12704 Gradiente: [0.000468772994874674,-0.008087599281119514] Loss: 22.842798459460504\n",
      "Iteracion: 12705 Gradiente: [0.00046852378081609916,-0.008083299665090849] Loss: 22.842798393848945\n",
      "Iteracion: 12706 Gradiente: [0.00046827469914016245,-0.008079002334877334] Loss: 22.84279832830712\n",
      "Iteracion: 12707 Gradiente: [0.0004680257498970756,-0.008074707289253998] Loss: 22.842798262834958\n",
      "Iteracion: 12708 Gradiente: [0.00046777693306031173,-0.008070414527007823] Loss: 22.84279819743242\n",
      "Iteracion: 12709 Gradiente: [0.00046752824851428916,-0.008066124046924372] Loss: 22.84279813209937\n",
      "Iteracion: 12710 Gradiente: [0.0004672796961585846,-0.008061835847791817] Loss: 22.842798066835794\n",
      "Iteracion: 12711 Gradiente: [0.0004670312759723553,-0.008057549928397141] Loss: 22.842798001641572\n",
      "Iteracion: 12712 Gradiente: [0.0004667829878608624,-0.008053266287527332] Loss: 22.842797936516657\n",
      "Iteracion: 12713 Gradiente: [0.0004665348316488386,-0.0080489849239792] Loss: 22.842797871460967\n",
      "Iteracion: 12714 Gradiente: [0.00046628680738175867,-0.008044705836533694] Loss: 22.842797806474447\n",
      "Iteracion: 12715 Gradiente: [0.0004660389150169901,-0.008040429023979338] Loss: 22.84279774155699\n",
      "Iteracion: 12716 Gradiente: [0.00046579115443326675,-0.008036154485110695] Loss: 22.84279767670854\n",
      "Iteracion: 12717 Gradiente: [0.0004655435254723746,-0.008031882218723752] Loss: 22.84279761192902\n",
      "Iteracion: 12718 Gradiente: [0.0004652960282262105,-0.008027612223599333] Loss: 22.842797547218357\n",
      "Iteracion: 12719 Gradiente: [0.0004650486626587735,-0.008023344498532481] Loss: 22.842797482576497\n",
      "Iteracion: 12720 Gradiente: [0.00046480142849721534,-0.008019079042322493] Loss: 22.842797418003336\n",
      "Iteracion: 12721 Gradiente: [0.000464554325814485,-0.008014815853759434] Loss: 22.842797353498817\n",
      "Iteracion: 12722 Gradiente: [0.00046430735443721006,-0.008010554931639765] Loss: 22.842797289062876\n",
      "Iteracion: 12723 Gradiente: [0.00046406051446486646,-0.00800629627475189] Loss: 22.842797224695403\n",
      "Iteracion: 12724 Gradiente: [0.00046381380562460586,-0.00800203988190257] Loss: 22.84279716039638\n",
      "Iteracion: 12725 Gradiente: [0.0004635672280073777,-0.007997785751878084] Loss: 22.842797096165675\n",
      "Iteracion: 12726 Gradiente: [0.00046332078139054523,-0.007993533883485426] Loss: 22.84279703200326\n",
      "Iteracion: 12727 Gradiente: [0.00046307446587168975,-0.007989284275512413] Loss: 22.84279696790907\n",
      "Iteracion: 12728 Gradiente: [0.00046282828122154267,-0.007985036926764858] Loss: 22.84279690388299\n",
      "Iteracion: 12729 Gradiente: [0.00046258222753294833,-0.007980791836033181] Loss: 22.84279683992496\n",
      "Iteracion: 12730 Gradiente: [0.00046233630454632174,-0.007976549002130066] Loss: 22.842796776034938\n",
      "Iteracion: 12731 Gradiente: [0.0004620905123554545,-0.007972308423843206] Loss: 22.842796712212827\n",
      "Iteracion: 12732 Gradiente: [0.0004618448509252933,-0.007968070099972972] Loss: 22.84279664845854\n",
      "Iteracion: 12733 Gradiente: [0.0004615993200019375,-0.007963834029330622] Loss: 22.842796584772035\n",
      "Iteracion: 12734 Gradiente: [0.00046135391962991433,-0.007959600210712854] Loss: 22.842796521153215\n",
      "Iteracion: 12735 Gradiente: [0.0004611086496709049,-0.007955368642925956] Loss: 22.842796457602034\n",
      "Iteracion: 12736 Gradiente: [0.00046086351020259523,-0.00795113932476473] Loss: 22.84279639411841\n",
      "Iteracion: 12737 Gradiente: [0.000460618501037402,-0.007946912255039724] Loss: 22.84279633070225\n",
      "Iteracion: 12738 Gradiente: [0.00046037362208248094,-0.007942687432556756] Loss: 22.842796267353513\n",
      "Iteracion: 12739 Gradiente: [0.0004601288733150947,-0.007938464856120339] Loss: 22.84279620407211\n",
      "Iteracion: 12740 Gradiente: [0.00045988425474945415,-0.007934244524529533] Loss: 22.842796140857974\n",
      "Iteracion: 12741 Gradiente: [0.0004596397662112395,-0.007930026436597378] Loss: 22.84279607771104\n",
      "Iteracion: 12742 Gradiente: [0.00045939540756118427,-0.007925810591135967] Loss: 22.84279601463122\n",
      "Iteracion: 12743 Gradiente: [0.00045915117889308024,-0.00792159698694377] Loss: 22.842795951618466\n",
      "Iteracion: 12744 Gradiente: [0.0004589070800250283,-0.007917385622834179] Loss: 22.84279588867268\n",
      "Iteracion: 12745 Gradiente: [0.0004586631109115539,-0.007913176497615264] Loss: 22.842795825793807\n",
      "Iteracion: 12746 Gradiente: [0.00045841927154413045,-0.007908969610094492] Loss: 22.84279576298177\n",
      "Iteracion: 12747 Gradiente: [0.00045817556173706936,-0.007904764959089169] Loss: 22.8427957002365\n",
      "Iteracion: 12748 Gradiente: [0.0004579319816144789,-0.007900562543396935] Loss: 22.842795637557938\n",
      "Iteracion: 12749 Gradiente: [0.00045768853084761455,-0.00789636236184658] Loss: 22.842795574945992\n",
      "Iteracion: 12750 Gradiente: [0.0004574452095747953,-0.007892164413238234] Loss: 22.842795512400595\n",
      "Iteracion: 12751 Gradiente: [0.0004572020177183352,-0.007887968696385054] Loss: 22.842795449921706\n",
      "Iteracion: 12752 Gradiente: [0.00045695895502812314,-0.007883775210110381] Loss: 22.842795387509213\n",
      "Iteracion: 12753 Gradiente: [0.00045671602159510865,-0.007879583953219438] Loss: 22.842795325163067\n",
      "Iteracion: 12754 Gradiente: [0.00045647321730465744,-0.007875394924530947] Loss: 22.842795262883193\n",
      "Iteracion: 12755 Gradiente: [0.0004562305421880334,-0.00787120812285238] Loss: 22.842795200669514\n",
      "Iteracion: 12756 Gradiente: [0.0004559879960462846,-0.00786702354700779] Loss: 22.842795138521982\n",
      "Iteracion: 12757 Gradiente: [0.00045574557881972546,-0.007862841195815188] Loss: 22.8427950764405\n",
      "Iteracion: 12758 Gradiente: [0.0004555032904666708,-0.007858661068088206] Loss: 22.84279501442502\n",
      "Iteracion: 12759 Gradiente: [0.0004552611309634358,-0.007854483162643078] Loss: 22.842794952475458\n",
      "Iteracion: 12760 Gradiente: [0.000455019100180228,-0.007850307478301843] Loss: 22.84279489059174\n",
      "Iteracion: 12761 Gradiente: [0.0004547771980270454,-0.007846134013884882] Loss: 22.84279482877381\n",
      "Iteracion: 12762 Gradiente: [0.00045453542451336185,-0.007841962768206655] Loss: 22.842794767021577\n",
      "Iteracion: 12763 Gradiente: [0.0004542937795681231,-0.007837793740089556] Loss: 22.842794705335002\n",
      "Iteracion: 12764 Gradiente: [0.000454052263101327,-0.007833626928355859] Loss: 22.84279464371398\n",
      "Iteracion: 12765 Gradiente: [0.00045381087490265295,-0.007829462331834236] Loss: 22.842794582158476\n",
      "Iteracion: 12766 Gradiente: [0.00045356961514073644,-0.007825299949333933] Loss: 22.84279452066841\n",
      "Iteracion: 12767 Gradiente: [0.00045332848357304556,-0.007821139779687295] Loss: 22.84279445924368\n",
      "Iteracion: 12768 Gradiente: [0.0004530874802460024,-0.007816981821711503] Loss: 22.84279439788428\n",
      "Iteracion: 12769 Gradiente: [0.00045284660501370886,-0.007812826074235819] Loss: 22.84279433659007\n",
      "Iteracion: 12770 Gradiente: [0.0004526058578790071,-0.007808672536080507] Loss: 22.84279427536104\n",
      "Iteracion: 12771 Gradiente: [0.0004523652386884199,-0.007804521206076842] Loss: 22.84279421419708\n",
      "Iteracion: 12772 Gradiente: [0.0004521247474665794,-0.007800372083044493] Loss: 22.842794153098136\n",
      "Iteracion: 12773 Gradiente: [0.0004518843840941145,-0.007796225165813079] Loss: 22.84279409206414\n",
      "Iteracion: 12774 Gradiente: [0.0004516441484696543,-0.007792080453212809] Loss: 22.842794031095025\n",
      "Iteracion: 12775 Gradiente: [0.00045140404056761934,-0.007787937944068801] Loss: 22.842793970190723\n",
      "Iteracion: 12776 Gradiente: [0.0004511640603273766,-0.007783797637208068] Loss: 22.84279390935115\n",
      "Iteracion: 12777 Gradiente: [0.00045092420765418715,-0.007779659531463542] Loss: 22.842793848576264\n",
      "Iteracion: 12778 Gradiente: [0.00045068448247036484,-0.00777552362566342] Loss: 22.842793787865975\n",
      "Iteracion: 12779 Gradiente: [0.00045044488478538367,-0.007771389918634242] Loss: 22.84279372722021\n",
      "Iteracion: 12780 Gradiente: [0.0004502054145746115,-0.007767258409206571] Loss: 22.842793666638915\n",
      "Iteracion: 12781 Gradiente: [0.0004499660716021481,-0.007763129096218198] Loss: 22.84279360612202\n",
      "Iteracion: 12782 Gradiente: [0.0004497268557656753,-0.007759001978507622] Loss: 22.842793545669448\n",
      "Iteracion: 12783 Gradiente: [0.00044948776711066784,-0.007754877054898657] Loss: 22.842793485281142\n",
      "Iteracion: 12784 Gradiente: [0.00044924880567975833,-0.007750754324218197] Loss: 22.84279342495703\n",
      "Iteracion: 12785 Gradiente: [0.00044900997120957223,-0.007746633785312677] Loss: 22.842793364697037\n",
      "Iteracion: 12786 Gradiente: [0.0004487712637038991,-0.007742515437012306] Loss: 22.84279330450109\n",
      "Iteracion: 12787 Gradiente: [0.00044853268312579077,-0.007738399278150728] Loss: 22.842793244369137\n",
      "Iteracion: 12788 Gradiente: [0.000448294229317033,-0.007734285307570469] Loss: 22.842793184301105\n",
      "Iteracion: 12789 Gradiente: [0.0004480559023789965,-0.007730173524096055] Loss: 22.842793124296932\n",
      "Iteracion: 12790 Gradiente: [0.0004478177021060977,-0.0077260639265758135] Loss: 22.84279306435653\n",
      "Iteracion: 12791 Gradiente: [0.0004475796285305478,-0.007721956513838417] Loss: 22.842793004479823\n",
      "Iteracion: 12792 Gradiente: [0.000447341681456237,-0.0077178512847320725] Loss: 22.8427929446668\n",
      "Iteracion: 12793 Gradiente: [0.0004471038608566384,-0.007713748238093861] Loss: 22.842792884917344\n",
      "Iteracion: 12794 Gradiente: [0.00044686616668154023,-0.007709647372761215] Loss: 22.8427928252314\n",
      "Iteracion: 12795 Gradiente: [0.00044662859890157354,-0.0077055486875727535] Loss: 22.8427927656089\n",
      "Iteracion: 12796 Gradiente: [0.00044639115742484137,-0.007701452181370764] Loss: 22.842792706049785\n",
      "Iteracion: 12797 Gradiente: [0.0004461538422143955,-0.0076973578529954045] Loss: 22.84279264655396\n",
      "Iteracion: 12798 Gradiente: [0.0004459166531025479,-0.0076932657012935834] Loss: 22.8427925871214\n",
      "Iteracion: 12799 Gradiente: [0.00044567959009308804,-0.007689175725105457] Loss: 22.842792527752003\n",
      "Iteracion: 12800 Gradiente: [0.00044544265313959384,-0.007685087923273078] Loss: 22.842792468445726\n",
      "Iteracion: 12801 Gradiente: [0.00044520584213595765,-0.007681002294643235] Loss: 22.84279240920247\n",
      "Iteracion: 12802 Gradiente: [0.0004449691570575472,-0.007676918838055731] Loss: 22.842792350022208\n",
      "Iteracion: 12803 Gradiente: [0.0004447325978162553,-0.007672837552358658] Loss: 22.842792290904836\n",
      "Iteracion: 12804 Gradiente: [0.00044449616431450065,-0.0076687584364002245] Loss: 22.84279223185033\n",
      "Iteracion: 12805 Gradiente: [0.00044425985656649424,-0.007664681489020827] Loss: 22.842792172858573\n",
      "Iteracion: 12806 Gradiente: [0.00044402367442728516,-0.00766060670907424] Loss: 22.84279211392954\n",
      "Iteracion: 12807 Gradiente: [0.000443787617798345,-0.007656534095408555] Loss: 22.842792055063132\n",
      "Iteracion: 12808 Gradiente: [0.0004435516866389359,-0.007652463646869971] Loss: 22.84279199625931\n",
      "Iteracion: 12809 Gradiente: [0.0004433158809329522,-0.00764839536230492] Loss: 22.842791937517983\n",
      "Iteracion: 12810 Gradiente: [0.00044308020062165573,-0.00764432924056339] Loss: 22.84279187883911\n",
      "Iteracion: 12811 Gradiente: [0.00044284464562925526,-0.007640265280495603] Loss: 22.842791820222597\n",
      "Iteracion: 12812 Gradiente: [0.0004426092158458535,-0.007636203480955099] Loss: 22.842791761668405\n",
      "Iteracion: 12813 Gradiente: [0.00044237391111228894,-0.007632143840798496] Loss: 22.842791703176452\n",
      "Iteracion: 12814 Gradiente: [0.0004421387316322504,-0.007628086358861452] Loss: 22.842791644746672\n",
      "Iteracion: 12815 Gradiente: [0.000441903677073204,-0.007624031034013612] Loss: 22.842791586379\n",
      "Iteracion: 12816 Gradiente: [0.00044166874751851994,-0.00761997786509833] Loss: 22.842791528073374\n",
      "Iteracion: 12817 Gradiente: [0.0004414339428298793,-0.00761592685097329] Loss: 22.84279146982974\n",
      "Iteracion: 12818 Gradiente: [0.0004411992630499147,-0.007611877990486941] Loss: 22.842791411648005\n",
      "Iteracion: 12819 Gradiente: [0.00044096470792851503,-0.007607831282506202] Loss: 22.842791353528114\n",
      "Iteracion: 12820 Gradiente: [0.00044073027757936717,-0.007603786725873955] Loss: 22.842791295470008\n",
      "Iteracion: 12821 Gradiente: [0.00044049597180162436,-0.007599744319456292] Loss: 22.842791237473598\n",
      "Iteracion: 12822 Gradiente: [0.0004402617907013943,-0.007595704062098108] Loss: 22.842791179538857\n",
      "Iteracion: 12823 Gradiente: [0.00044002773394898515,-0.007591665952674494] Loss: 22.84279112166569\n",
      "Iteracion: 12824 Gradiente: [0.0004397938016789264,-0.00758762999003153] Loss: 22.842791063854055\n",
      "Iteracion: 12825 Gradiente: [0.00043955999385616453,-0.007583596173024769] Loss: 22.842791006103862\n",
      "Iteracion: 12826 Gradiente: [0.00043932631016521857,-0.007579564500529656] Loss: 22.84279094841504\n",
      "Iteracion: 12827 Gradiente: [0.0004390927508306201,-0.0075755349713865884] Loss: 22.842790890787565\n",
      "Iteracion: 12828 Gradiente: [0.0004388593156818388,-0.007571507584465209] Loss: 22.84279083322134\n",
      "Iteracion: 12829 Gradiente: [0.00043862600456539744,-0.007567482338629835] Loss: 22.8427907757163\n",
      "Iteracion: 12830 Gradiente: [0.00043839281753056035,-0.007563459232735071] Loss: 22.842790718272397\n",
      "Iteracion: 12831 Gradiente: [0.00043815975441532375,-0.007559438265649731] Loss: 22.842790660889545\n",
      "Iteracion: 12832 Gradiente: [0.000437926815281268,-0.007555419436230674] Loss: 22.8427906035677\n",
      "Iteracion: 12833 Gradiente: [0.0004376940000118642,-0.007551402743343042] Loss: 22.842790546306787\n",
      "Iteracion: 12834 Gradiente: [0.0004374613084792145,-0.007547388185853047] Loss: 22.842790489106736\n",
      "Iteracion: 12835 Gradiente: [0.00043722874061131735,-0.007543375762628438] Loss: 22.842790431967483\n",
      "Iteracion: 12836 Gradiente: [0.00043699629637406666,-0.007539365472531756] Loss: 22.84279037488898\n",
      "Iteracion: 12837 Gradiente: [0.0004367639756897764,-0.007535357314429921] Loss: 22.842790317871128\n",
      "Iteracion: 12838 Gradiente: [0.0004365317785612888,-0.007531351287185354] Loss: 22.84279026091391\n",
      "Iteracion: 12839 Gradiente: [0.0004362997048663904,-0.0075273473896684115] Loss: 22.84279020401723\n",
      "Iteracion: 12840 Gradiente: [0.00043606775459844964,-0.007523345620744] Loss: 22.842790147181013\n",
      "Iteracion: 12841 Gradiente: [0.0004358359275784096,-0.007519345979284727] Loss: 22.842790090405234\n",
      "Iteracion: 12842 Gradiente: [0.0004356042238309025,-0.007515348464157512] Loss: 22.8427900336898\n",
      "Iteracion: 12843 Gradiente: [0.00043537264326687364,-0.007511353074230816] Loss: 22.842789977034634\n",
      "Iteracion: 12844 Gradiente: [0.0004351411857811627,-0.0075073598083791404] Loss: 22.842789920439717\n",
      "Iteracion: 12845 Gradiente: [0.00043490985135008486,-0.007503368665469879] Loss: 22.84278986390495\n",
      "Iteracion: 12846 Gradiente: [0.00043467863997932455,-0.007499379644369834] Loss: 22.842789807430265\n",
      "Iteracion: 12847 Gradiente: [0.0004344475514272972,-0.007495392743961323] Loss: 22.842789751015633\n",
      "Iteracion: 12848 Gradiente: [0.00043421658575084623,-0.00749140796311103] Loss: 22.84278969466095\n",
      "Iteracion: 12849 Gradiente: [0.0004339857428618643,-0.007487425300692981] Loss: 22.8427896383662\n",
      "Iteracion: 12850 Gradiente: [0.00043375502270445547,-0.007483444755578953] Loss: 22.842789582131264\n",
      "Iteracion: 12851 Gradiente: [0.0004335244252473558,-0.007479466326642381] Loss: 22.842789525956107\n",
      "Iteracion: 12852 Gradiente: [0.0004332939503314037,-0.0074754900127646335] Loss: 22.84278946984068\n",
      "Iteracion: 12853 Gradiente: [0.0004330635980229166,-0.007471515812812986] Loss: 22.84278941378488\n",
      "Iteracion: 12854 Gradiente: [0.00043283336807936243,-0.00746754372567248] Loss: 22.842789357788668\n",
      "Iteracion: 12855 Gradiente: [0.0004326032606580081,-0.007463573750208378] Loss: 22.842789301851994\n",
      "Iteracion: 12856 Gradiente: [0.0004323732754841103,-0.007459605885310338] Loss: 22.842789245974778\n",
      "Iteracion: 12857 Gradiente: [0.0004321434125633535,-0.007455640129851441] Loss: 22.842789190156957\n",
      "Iteracion: 12858 Gradiente: [0.0004319136718910007,-0.007451676482707015] Loss: 22.84278913439846\n",
      "Iteracion: 12859 Gradiente: [0.0004316840532728368,-0.0074477149427643495] Loss: 22.84278907869924\n",
      "Iteracion: 12860 Gradiente: [0.0004314545568405492,-0.007443755508891433] Loss: 22.84278902305922\n",
      "Iteracion: 12861 Gradiente: [0.00043122518233076337,-0.007439798179980172] Loss: 22.84278896747834\n",
      "Iteracion: 12862 Gradiente: [0.00043099592989316685,-0.007435842954899622] Loss: 22.842788911956557\n",
      "Iteracion: 12863 Gradiente: [0.000430766799132698,-0.007431889832547967] Loss: 22.842788856493787\n",
      "Iteracion: 12864 Gradiente: [0.00043053779022746613,-0.0074279388117957975] Loss: 22.84278880108997\n",
      "Iteracion: 12865 Gradiente: [0.0004303089031045223,-0.007423989891525551] Loss: 22.842788745745054\n",
      "Iteracion: 12866 Gradiente: [0.0004300801376615482,-0.00742004307062274] Loss: 22.84278869045895\n",
      "Iteracion: 12867 Gradiente: [0.0004298514938615957,-0.007416098347969916] Loss: 22.84278863523164\n",
      "Iteracion: 12868 Gradiente: [0.00042962297162887355,-0.007412155722451648] Loss: 22.842788580063026\n",
      "Iteracion: 12869 Gradiente: [0.0004293945708970644,-0.0074082151929533305] Loss: 22.842788524953043\n",
      "Iteracion: 12870 Gradiente: [0.0004291662915799558,-0.007404276758361779] Loss: 22.84278846990164\n",
      "Iteracion: 12871 Gradiente: [0.00042893813357522956,-0.007400340417566534] Loss: 22.842788414908764\n",
      "Iteracion: 12872 Gradiente: [0.0004287100968449901,-0.007396406169452874] Loss: 22.842788359974357\n",
      "Iteracion: 12873 Gradiente: [0.00042848218148776595,-0.007392474012896481] Loss: 22.84278830509833\n",
      "Iteracion: 12874 Gradiente: [0.0004282543871672336,-0.007388543946803357] Loss: 22.842788250280627\n",
      "Iteracion: 12875 Gradiente: [0.0004280267140662393,-0.007384615970046819] Loss: 22.84278819552121\n",
      "Iteracion: 12876 Gradiente: [0.000427799161854144,-0.007380690081532867] Loss: 22.842788140819994\n",
      "Iteracion: 12877 Gradiente: [0.0004275717307308469,-0.007376766280133869] Loss: 22.842788086176924\n",
      "Iteracion: 12878 Gradiente: [0.0004273444204983434,-0.0073728445647497885] Loss: 22.842788031591933\n",
      "Iteracion: 12879 Gradiente: [0.0004271172310059986,-0.0073689249342766775] Loss: 22.842787977064965\n",
      "Iteracion: 12880 Gradiente: [0.0004268901624963443,-0.00736500738758572] Loss: 22.84278792259597\n",
      "Iteracion: 12881 Gradiente: [0.0004266632144814745,-0.007361091923597959] Loss: 22.842787868184875\n",
      "Iteracion: 12882 Gradiente: [0.00042643638721339506,-0.0073571785411842205] Loss: 22.84278781383161\n",
      "Iteracion: 12883 Gradiente: [0.00042620968051399665,-0.007353267239246245] Loss: 22.842787759536122\n",
      "Iteracion: 12884 Gradiente: [0.0004259830944183326,-0.007349358016672151] Loss: 22.842787705298353\n",
      "Iteracion: 12885 Gradiente: [0.0004257566287454514,-0.00734545087236332] Loss: 22.84278765111823\n",
      "Iteracion: 12886 Gradiente: [0.00042553028337503446,-0.007341545805217464] Loss: 22.842787596995702\n",
      "Iteracion: 12887 Gradiente: [0.0004253040583724517,-0.0073376428141227025] Loss: 22.84278754293072\n",
      "Iteracion: 12888 Gradiente: [0.00042507795368180724,-0.0073337418979755615] Loss: 22.84278748892318\n",
      "Iteracion: 12889 Gradiente: [0.00042485196925288923,-0.00732984305567174] Loss: 22.842787434973054\n",
      "Iteracion: 12890 Gradiente: [0.00042462610496348435,-0.007325946286112147] Loss: 22.842787381080296\n",
      "Iteracion: 12891 Gradiente: [0.00042440036070748495,-0.007322051588197691] Loss: 22.842787327244817\n",
      "Iteracion: 12892 Gradiente: [0.0004241747364905753,-0.007318158960821227] Loss: 22.84278727346656\n",
      "Iteracion: 12893 Gradiente: [0.0004239492321687521,-0.007314268402889468] Loss: 22.842787219745468\n",
      "Iteracion: 12894 Gradiente: [0.00042372384769085635,-0.0073103799132990584] Loss: 22.842787166081486\n",
      "Iteracion: 12895 Gradiente: [0.00042349858308152004,-0.007306493490947593] Loss: 22.842787112474532\n",
      "Iteracion: 12896 Gradiente: [0.0004232734381910556,-0.007302609134739294] Loss: 22.842787058924582\n",
      "Iteracion: 12897 Gradiente: [0.0004230484130118839,-0.007298726843573533] Loss: 22.84278700543154\n",
      "Iteracion: 12898 Gradiente: [0.0004228235075136884,-0.0072948466163487305] Loss: 22.842786951995357\n",
      "Iteracion: 12899 Gradiente: [0.0004225987215723611,-0.007290968451972901] Loss: 22.842786898615994\n",
      "Iteracion: 12900 Gradiente: [0.0004223740550865311,-0.007287092349351099] Loss: 22.842786845293354\n",
      "Iteracion: 12901 Gradiente: [0.0004221495081613587,-0.0072832183073776] Loss: 22.842786792027404\n",
      "Iteracion: 12902 Gradiente: [0.00042192508048231046,-0.007279346324971551] Loss: 22.842786738818077\n",
      "Iteracion: 12903 Gradiente: [0.00042170077221423223,-0.00727547640102344] Loss: 22.8427866856653\n",
      "Iteracion: 12904 Gradiente: [0.0004214765830700647,-0.007271608534453478] Loss: 22.84278663256902\n",
      "Iteracion: 12905 Gradiente: [0.0004212525132354964,-0.0072677427241527445] Loss: 22.8427865795292\n",
      "Iteracion: 12906 Gradiente: [0.00042102856244715287,-0.007263878969040978] Loss: 22.84278652654576\n",
      "Iteracion: 12907 Gradiente: [0.0004208047307230345,-0.00726001726801897] Loss: 22.84278647361863\n",
      "Iteracion: 12908 Gradiente: [0.00042058101806408864,-0.007256157619991891] Loss: 22.84278642074775\n",
      "Iteracion: 12909 Gradiente: [0.00042035742426567896,-0.007252300023876519] Loss: 22.842786367933083\n",
      "Iteracion: 12910 Gradiente: [0.00042013394937422767,-0.00724844447857483] Loss: 22.842786315174557\n",
      "Iteracion: 12911 Gradiente: [0.000419910593266574,-0.007244590983000047] Loss: 22.842786262472107\n",
      "Iteracion: 12912 Gradiente: [0.00041968735589440105,-0.007240739536063737] Loss: 22.842786209825668\n",
      "Iteracion: 12913 Gradiente: [0.00041946423720370756,-0.00723689013667439] Loss: 22.84278615723522\n",
      "Iteracion: 12914 Gradiente: [0.00041924123713765006,-0.00723304278374215] Loss: 22.842786104700654\n",
      "Iteracion: 12915 Gradiente: [0.0004190183556223322,-0.007229197476183084] Loss: 22.842786052221935\n",
      "Iteracion: 12916 Gradiente: [0.00041879559263785873,-0.007225354212904141] Loss: 22.842785999799002\n",
      "Iteracion: 12917 Gradiente: [0.0004185729480506476,-0.007221512992824112] Loss: 22.8427859474318\n",
      "Iteracion: 12918 Gradiente: [0.00041835042176785464,-0.0072176738148571696] Loss: 22.84278589512025\n",
      "Iteracion: 12919 Gradiente: [0.0004181280138321123,-0.007213836677913458] Loss: 22.84278584286431\n",
      "Iteracion: 12920 Gradiente: [0.00041790572417805074,-0.007210001580906204] Loss: 22.842785790663925\n",
      "Iteracion: 12921 Gradiente: [0.00041768355260577057,-0.007206168522759408] Loss: 22.842785738519012\n",
      "Iteracion: 12922 Gradiente: [0.00041746149922516906,-0.007202337502381558] Loss: 22.842785686429533\n",
      "Iteracion: 12923 Gradiente: [0.0004172395639092959,-0.007198508518689551] Loss: 22.842785634395437\n",
      "Iteracion: 12924 Gradiente: [0.0004170177464933052,-0.007194681570607978] Loss: 22.842785582416642\n",
      "Iteracion: 12925 Gradiente: [0.00041679604703498776,-0.007190856657047107] Loss: 22.8427855304931\n",
      "Iteracion: 12926 Gradiente: [0.00041657446548128976,-0.007187033776925963] Loss: 22.84278547862476\n",
      "Iteracion: 12927 Gradiente: [0.0004163530017175769,-0.007183212929165706] Loss: 22.842785426811552\n",
      "Iteracion: 12928 Gradiente: [0.00041613165574953354,-0.007179394112681337] Loss: 22.842785375053403\n",
      "Iteracion: 12929 Gradiente: [0.00041591042733841733,-0.007175577326404673] Loss: 22.8427853233503\n",
      "Iteracion: 12930 Gradiente: [0.0004156893165590721,-0.007171762569246454] Loss: 22.842785271702134\n",
      "Iteracion: 12931 Gradiente: [0.00041546832339444485,-0.007167949840127719] Loss: 22.842785220108865\n",
      "Iteracion: 12932 Gradiente: [0.00041524744773842787,-0.007164139137970101] Loss: 22.84278516857046\n",
      "Iteracion: 12933 Gradiente: [0.00041502668942238564,-0.007160330461703761] Loss: 22.842785117086827\n",
      "Iteracion: 12934 Gradiente: [0.00041480604851358295,-0.007156523810244645] Loss: 22.842785065657917\n",
      "Iteracion: 12935 Gradiente: [0.0004145855248386473,-0.00715271918252256] Loss: 22.842785014283688\n",
      "Iteracion: 12936 Gradiente: [0.00041436511848758073,-0.007148916577450966] Loss: 22.842784962964068\n",
      "Iteracion: 12937 Gradiente: [0.00041414482926143136,-0.007145115993964761] Loss: 22.842784911698974\n",
      "Iteracion: 12938 Gradiente: [0.0004139246571289353,-0.007141317430986405] Loss: 22.84278486048839\n",
      "Iteracion: 12939 Gradiente: [0.0004137046020531443,-0.0071375208874401375] Loss: 22.84278480933225\n",
      "Iteracion: 12940 Gradiente: [0.00041348466399711016,-0.0071337263622529195] Loss: 22.842784758230486\n",
      "Iteracion: 12941 Gradiente: [0.00041326484281493475,-0.0071299338543523065] Loss: 22.842784707183046\n",
      "Iteracion: 12942 Gradiente: [0.00041304513858335666,-0.007126143362661708] Loss: 22.84278465618985\n",
      "Iteracion: 12943 Gradiente: [0.00041282555116405696,-0.007122354886111992] Loss: 22.84278460525087\n",
      "Iteracion: 12944 Gradiente: [0.00041260608041113753,-0.007118568423637347] Loss: 22.842784554366027\n",
      "Iteracion: 12945 Gradiente: [0.00041238672639565266,-0.007114783974157035] Loss: 22.842784503535288\n",
      "Iteracion: 12946 Gradiente: [0.000412167488968862,-0.0071110015366092705] Loss: 22.842784452758572\n",
      "Iteracion: 12947 Gradiente: [0.00041194836802939486,-0.007107221109924093] Loss: 22.842784402035836\n",
      "Iteracion: 12948 Gradiente: [0.00041172936362083115,-0.007103442693027754] Loss: 22.84278435136701\n",
      "Iteracion: 12949 Gradiente: [0.0004115104756162206,-0.0070996662848565725] Loss: 22.84278430075205\n",
      "Iteracion: 12950 Gradiente: [0.00041129170394829846,-0.007095891884340943] Loss: 22.842784250190885\n",
      "Iteracion: 12951 Gradiente: [0.0004110730487080142,-0.0070921194904049875] Loss: 22.842784199683475\n",
      "Iteracion: 12952 Gradiente: [0.00041085450967652076,-0.0070883491019918905] Loss: 22.842784149229757\n",
      "Iteracion: 12953 Gradiente: [0.0004106360868415019,-0.007084580718031811] Loss: 22.842784098829657\n",
      "Iteracion: 12954 Gradiente: [0.0004104177800419014,-0.007080814337465806] Loss: 22.842784048483146\n",
      "Iteracion: 12955 Gradiente: [0.0004101995894169856,-0.007077049959216097] Loss: 22.84278399819014\n",
      "Iteracion: 12956 Gradiente: [0.000409981514698643,-0.007073287582232032] Loss: 22.842783947950593\n",
      "Iteracion: 12957 Gradiente: [0.00040976355592003223,-0.007069527205440925] Loss: 22.842783897764456\n",
      "Iteracion: 12958 Gradiente: [0.0004095457130527317,-0.0070657688277799245] Loss: 22.84278384763167\n",
      "Iteracion: 12959 Gradiente: [0.00040932798592431633,-0.007062012448192571] Loss: 22.842783797552155\n",
      "Iteracion: 12960 Gradiente: [0.00040911037465415727,-0.007058258065605472] Loss: 22.842783747525896\n",
      "Iteracion: 12961 Gradiente: [0.00040889287899403826,-0.007054505678968089] Loss: 22.842783697552807\n",
      "Iteracion: 12962 Gradiente: [0.00040867549903017184,-0.007050755287210819] Loss: 22.842783647632835\n",
      "Iteracion: 12963 Gradiente: [0.000408458234624239,-0.0070470068892762566] Loss: 22.842783597765923\n",
      "Iteracion: 12964 Gradiente: [0.00040824108571560676,-0.007043260484106995] Loss: 22.84278354795204\n",
      "Iteracion: 12965 Gradiente: [0.0004080240521432188,-0.007039516070647286] Loss: 22.84278349819108\n",
      "Iteracion: 12966 Gradiente: [0.00040780713410697443,-0.007035773647823262] Loss: 22.842783448483036\n",
      "Iteracion: 12967 Gradiente: [0.00040759033124781275,-0.007032033214594809] Loss: 22.842783398827805\n",
      "Iteracion: 12968 Gradiente: [0.00040737364373815885,-0.007028294769889953] Loss: 22.842783349225385\n",
      "Iteracion: 12969 Gradiente: [0.0004071570714112719,-0.007024558312658869] Loss: 22.842783299675677\n",
      "Iteracion: 12970 Gradiente: [0.00040694061423778293,-0.007020823841840477] Loss: 22.84278325017863\n",
      "Iteracion: 12971 Gradiente: [0.0004067242720755833,-0.007017091356386729] Loss: 22.84278320073422\n",
      "Iteracion: 12972 Gradiente: [0.0004065080449834113,-0.007013360855232639] Loss: 22.842783151342356\n",
      "Iteracion: 12973 Gradiente: [0.0004062919328191583,-0.007009632337329327] Loss: 22.842783102003004\n",
      "Iteracion: 12974 Gradiente: [0.0004060759355866139,-0.007005905801617255] Loss: 22.842783052716072\n",
      "Iteracion: 12975 Gradiente: [0.0004058600532185134,-0.007002181247045059] Loss: 22.84278300348155\n",
      "Iteracion: 12976 Gradiente: [0.0004056442855670639,-0.006998458672562554] Loss: 22.842782954299373\n",
      "Iteracion: 12977 Gradiente: [0.00040542863260573846,-0.006994738077115888] Loss: 22.84278290516946\n",
      "Iteracion: 12978 Gradiente: [0.0004052130942663249,-0.006991019459654405] Loss: 22.842782856091784\n",
      "Iteracion: 12979 Gradiente: [0.00040499767053555995,-0.006987302819123068] Loss: 22.84278280706626\n",
      "Iteracion: 12980 Gradiente: [0.00040478236134523137,-0.006983588154470036] Loss: 22.842782758092856\n",
      "Iteracion: 12981 Gradiente: [0.00040456716657123103,-0.006979875464650457] Loss: 22.84278270917152\n",
      "Iteracion: 12982 Gradiente: [0.0004043520862353489,-0.006976164748608819] Loss: 22.842782660302177\n",
      "Iteracion: 12983 Gradiente: [0.00040413712023621427,-0.0069724560052979] Loss: 22.842782611484783\n",
      "Iteracion: 12984 Gradiente: [0.00040392226853782633,-0.006968749233667518] Loss: 22.84278256271927\n",
      "Iteracion: 12985 Gradiente: [0.00040370753102555074,-0.006965044432674124] Loss: 22.84278251400561\n",
      "Iteracion: 12986 Gradiente: [0.0004034929078547596,-0.006961341601254863] Loss: 22.84278246534372\n",
      "Iteracion: 12987 Gradiente: [0.00040327839863039114,-0.006957640738383356] Loss: 22.842782416733566\n",
      "Iteracion: 12988 Gradiente: [0.0004030640034083414,-0.0069539418430064615] Loss: 22.84278236817507\n",
      "Iteracion: 12989 Gradiente: [0.0004028497221743995,-0.006950244914073759] Loss: 22.842782319668206\n",
      "Iteracion: 12990 Gradiente: [0.000402635554888775,-0.0069465499505421725] Loss: 22.842782271212894\n",
      "Iteracion: 12991 Gradiente: [0.0004024215014794663,-0.006942856951364836] Loss: 22.842782222809095\n",
      "Iteracion: 12992 Gradiente: [0.0004022075618517344,-0.006939165915501396] Loss: 22.842782174456747\n",
      "Iteracion: 12993 Gradiente: [0.00040199373598284185,-0.006935476841903329] Loss: 22.84278212615579\n",
      "Iteracion: 12994 Gradiente: [0.00040178002370510056,-0.006931789729535254] Loss: 22.842782077906175\n",
      "Iteracion: 12995 Gradiente: [0.0004015664251397766,-0.006928104577344622] Loss: 22.842782029707863\n",
      "Iteracion: 12996 Gradiente: [0.0004013529400594962,-0.0069244213842970005] Loss: 22.84278198156077\n",
      "Iteracion: 12997 Gradiente: [0.00040113956862342093,-0.0069207401493391295] Loss: 22.842781933464867\n",
      "Iteracion: 12998 Gradiente: [0.0004009263104734373,-0.006917060871447471] Loss: 22.84278188542008\n",
      "Iteracion: 12999 Gradiente: [0.0004007131657942864,-0.006913383549567579] Loss: 22.84278183742638\n",
      "Iteracion: 13000 Gradiente: [0.00040050013440501666,-0.006909708182666682] Loss: 22.842781789483684\n",
      "Iteracion: 13001 Gradiente: [0.00040028721623931084,-0.0069060347697044245] Loss: 22.842781741591953\n",
      "Iteracion: 13002 Gradiente: [0.00040007441121664065,-0.006902363309645428] Loss: 22.842781693751117\n",
      "Iteracion: 13003 Gradiente: [0.00039986171940521823,-0.006898693801441524] Loss: 22.84278164596115\n",
      "Iteracion: 13004 Gradiente: [0.00039964914059756513,-0.006895026244065387] Loss: 22.84278159822198\n",
      "Iteracion: 13005 Gradiente: [0.00039943667483062956,-0.006891360636473583] Loss: 22.842781550533555\n",
      "Iteracion: 13006 Gradiente: [0.000399224322022936,-0.006887696977631682] Loss: 22.84278150289581\n",
      "Iteracion: 13007 Gradiente: [0.00039901208213374655,-0.006884035266500987] Loss: 22.842781455308714\n",
      "Iteracion: 13008 Gradiente: [0.00039879995512800785,-0.006880375502045055] Loss: 22.8427814077722\n",
      "Iteracion: 13009 Gradiente: [0.00039858794083329484,-0.006876717683237269] Loss: 22.84278136028622\n",
      "Iteracion: 13010 Gradiente: [0.00039837603920223806,-0.006873061809040593] Loss: 22.842781312850715\n",
      "Iteracion: 13011 Gradiente: [0.0003981642502481009,-0.006869407878417633] Loss: 22.842781265465636\n",
      "Iteracion: 13012 Gradiente: [0.00039795257391309253,-0.0068657558903345494] Loss: 22.84278121813092\n",
      "Iteracion: 13013 Gradiente: [0.00039774101008826314,-0.0068621058437615306] Loss: 22.842781170846518\n",
      "Iteracion: 13014 Gradiente: [0.00039752955875277016,-0.006858457737664973] Loss: 22.842781123612383\n",
      "Iteracion: 13015 Gradiente: [0.0003973182198440857,-0.006854811571011273] Loss: 22.842781076428455\n",
      "Iteracion: 13016 Gradiente: [0.0003971069933205248,-0.006851167342772158] Loss: 22.84278102929468\n",
      "Iteracion: 13017 Gradiente: [0.00039689587903903126,-0.006847525051918998] Loss: 22.842780982211014\n",
      "Iteracion: 13018 Gradiente: [0.00039668487701002657,-0.006843884697417953] Loss: 22.84278093517739\n",
      "Iteracion: 13019 Gradiente: [0.00039647398722498416,-0.006840246278236843] Loss: 22.842780888193776\n",
      "Iteracion: 13020 Gradiente: [0.00039626320947168853,-0.006836609793355919] Loss: 22.842780841260087\n",
      "Iteracion: 13021 Gradiente: [0.0003960525437283498,-0.00683297524174454] Loss: 22.842780794376303\n",
      "Iteracion: 13022 Gradiente: [0.0003958419900963387,-0.006829342622366852] Loss: 22.842780747542335\n",
      "Iteracion: 13023 Gradiente: [0.00039563154833312334,-0.006825711934203937] Loss: 22.84278070075816\n",
      "Iteracion: 13024 Gradiente: [0.00039542121839512373,-0.006822083176230365] Loss: 22.842780654023734\n",
      "Iteracion: 13025 Gradiente: [0.00039521100031739327,-0.006818456347413241] Loss: 22.84278060733897\n",
      "Iteracion: 13026 Gradiente: [0.00039500089403266735,-0.006814831446727491] Loss: 22.842780560703837\n",
      "Iteracion: 13027 Gradiente: [0.00039479089948410244,-0.006811208473147327] Loss: 22.842780514118253\n",
      "Iteracion: 13028 Gradiente: [0.00039458101652106357,-0.006807587425654068] Loss: 22.842780467582198\n",
      "Iteracion: 13029 Gradiente: [0.0003943712451397611,-0.00680396830322169] Loss: 22.842780421095622\n",
      "Iteracion: 13030 Gradiente: [0.00039416158523882433,-0.006800351104828195] Loss: 22.842780374658467\n",
      "Iteracion: 13031 Gradiente: [0.00039395203690541317,-0.0067967358294415215] Loss: 22.84278032827066\n",
      "Iteracion: 13032 Gradiente: [0.00039374259986099485,-0.006793122476053289] Loss: 22.842780281932168\n",
      "Iteracion: 13033 Gradiente: [0.00039353327421546663,-0.006789511043632146] Loss: 22.84278023564293\n",
      "Iteracion: 13034 Gradiente: [0.0003933240598788264,-0.00678590153115716] Loss: 22.842780189402898\n",
      "Iteracion: 13035 Gradiente: [0.0003931149566795966,-0.00678229393761427] Loss: 22.842780143212\n",
      "Iteracion: 13036 Gradiente: [0.0003929059647636753,-0.006778688261972832] Loss: 22.84278009707023\n",
      "Iteracion: 13037 Gradiente: [0.0003926970837890546,-0.006775084503227665] Loss: 22.8427800509775\n",
      "Iteracion: 13038 Gradiente: [0.00039248831406647847,-0.0067714826603405475] Loss: 22.842780004933754\n",
      "Iteracion: 13039 Gradiente: [0.0003922796552226752,-0.006767882732310326] Loss: 22.842779958938948\n",
      "Iteracion: 13040 Gradiente: [0.0003920711073230147,-0.006764284718110977] Loss: 22.842779912993056\n",
      "Iteracion: 13041 Gradiente: [0.0003918626702822318,-0.006760688616727132] Loss: 22.842779867096\n",
      "Iteracion: 13042 Gradiente: [0.00039165434404064093,-0.006757094427141415] Loss: 22.84277982124772\n",
      "Iteracion: 13043 Gradiente: [0.0003914461285650835,-0.006753502148336565] Loss: 22.842779775448168\n",
      "Iteracion: 13044 Gradiente: [0.0003912380237731365,-0.006749911779298164] Loss: 22.842779729697316\n",
      "Iteracion: 13045 Gradiente: [0.0003910300296610103,-0.006746323319007412] Loss: 22.842779683995094\n",
      "Iteracion: 13046 Gradiente: [0.0003908221460818595,-0.006742736766454982] Loss: 22.842779638341444\n",
      "Iteracion: 13047 Gradiente: [0.0003906143731029488,-0.0067391521206189966] Loss: 22.842779592736335\n",
      "Iteracion: 13048 Gradiente: [0.0003904067104921675,-0.006735569380496642] Loss: 22.84277954717969\n",
      "Iteracion: 13049 Gradiente: [0.00039019915826277917,-0.0067319885450691196] Loss: 22.842779501671483\n",
      "Iteracion: 13050 Gradiente: [0.00038999171639678327,-0.006728409613323076] Loss: 22.84277945621165\n",
      "Iteracion: 13051 Gradiente: [0.0003897843847795457,-0.006724832584248711] Loss: 22.842779410800123\n",
      "Iteracion: 13052 Gradiente: [0.0003895771634546463,-0.006721257456829003] Loss: 22.842779365436876\n",
      "Iteracion: 13053 Gradiente: [0.00038937005230839833,-0.006717684230055928] Loss: 22.842779320121856\n",
      "Iteracion: 13054 Gradiente: [0.0003891630512801688,-0.006714112902919567] Loss: 22.842779274855005\n",
      "Iteracion: 13055 Gradiente: [0.0003889561602382704,-0.0067105434744126075] Loss: 22.842779229636275\n",
      "Iteracion: 13056 Gradiente: [0.0003887493792329148,-0.00670697594352229] Loss: 22.8427791844656\n",
      "Iteracion: 13057 Gradiente: [0.0003885427081314674,-0.006703410309241302] Loss: 22.84277913934296\n",
      "Iteracion: 13058 Gradiente: [0.0003883361468382418,-0.006699846570565763] Loss: 22.842779094268256\n",
      "Iteracion: 13059 Gradiente: [0.00038812969547829347,-0.006696284726475455] Loss: 22.842779049241482\n",
      "Iteracion: 13060 Gradiente: [0.0003879233537797215,-0.006692724775975971] Loss: 22.842779004262564\n",
      "Iteracion: 13061 Gradiente: [0.00038771712179463215,-0.006689166718054433] Loss: 22.84277895933145\n",
      "Iteracion: 13062 Gradiente: [0.0003875109994974461,-0.006685610551703647] Loss: 22.842778914448118\n",
      "Iteracion: 13063 Gradiente: [0.000387304986751739,-0.0066820562759208006] Loss: 22.84277886961249\n",
      "Iteracion: 13064 Gradiente: [0.00038709908355561616,-0.006678503889700001] Loss: 22.84277882482452\n",
      "Iteracion: 13065 Gradiente: [0.00038689328982760194,-0.006674953392034648] Loss: 22.842778780084146\n",
      "Iteracion: 13066 Gradiente: [0.0003866876054691678,-0.006671404781926545] Loss: 22.842778735391335\n",
      "Iteracion: 13067 Gradiente: [0.0003864820304338915,-0.006667858058369091] Loss: 22.842778690746048\n",
      "Iteracion: 13068 Gradiente: [0.0003862765647056676,-0.0066643132203582905] Loss: 22.842778646148197\n",
      "Iteracion: 13069 Gradiente: [0.0003860712082807064,-0.006660770266887776] Loss: 22.84277860159777\n",
      "Iteracion: 13070 Gradiente: [0.0003858659609032126,-0.006657229196967881] Loss: 22.84277855709469\n",
      "Iteracion: 13071 Gradiente: [0.00038566082269729426,-0.0066536900095859625] Loss: 22.84277851263891\n",
      "Iteracion: 13072 Gradiente: [0.0003854557936326349,-0.00665015270373921] Loss: 22.842778468230396\n",
      "Iteracion: 13073 Gradiente: [0.0003852508734572287,-0.006646617278440085] Loss: 22.84277842386909\n",
      "Iteracion: 13074 Gradiente: [0.0003850460622885521,-0.006643083732677723] Loss: 22.842778379554932\n",
      "Iteracion: 13075 Gradiente: [0.0003848413599750226,-0.006639552065457603] Loss: 22.842778335287893\n",
      "Iteracion: 13076 Gradiente: [0.0003846367665346406,-0.006636022275777975] Loss: 22.842778291067876\n",
      "Iteracion: 13077 Gradiente: [0.00038443228187077236,-0.006632494362641595] Loss: 22.84277824689489\n",
      "Iteracion: 13078 Gradiente: [0.0003842279059095214,-0.006628968325053464] Loss: 22.842778202768855\n",
      "Iteracion: 13079 Gradiente: [0.00038402363847277833,-0.006625444162022257] Loss: 22.842778158689725\n",
      "Iteracion: 13080 Gradiente: [0.0003838194797073887,-0.006621921872539834] Loss: 22.84277811465744\n",
      "Iteracion: 13081 Gradiente: [0.000383615429517666,-0.006618401455612144] Loss: 22.84277807067196\n",
      "Iteracion: 13082 Gradiente: [0.00038341148780697646,-0.006614882910246914] Loss: 22.842778026733253\n",
      "Iteracion: 13083 Gradiente: [0.00038320765442089545,-0.006611366235454005] Loss: 22.842777982841245\n",
      "Iteracion: 13084 Gradiente: [0.0003830039294162665,-0.006607851430232377] Loss: 22.84277793899588\n",
      "Iteracion: 13085 Gradiente: [0.0003828003127949842,-0.006604338493583957] Loss: 22.842777895197138\n",
      "Iteracion: 13086 Gradiente: [0.00038259680438083404,-0.006600827424523696] Loss: 22.842777851444946\n",
      "Iteracion: 13087 Gradiente: [0.0003823934042050799,-0.006597318222052806] Loss: 22.84277780773927\n",
      "Iteracion: 13088 Gradiente: [0.0003821901121445611,-0.006593810885181976] Loss: 22.84277776408006\n",
      "Iteracion: 13089 Gradiente: [0.00038198692810548587,-0.006590305412921893] Loss: 22.842777720467232\n",
      "Iteracion: 13090 Gradiente: [0.0003817838521750142,-0.006586801804273416] Loss: 22.842777676900777\n",
      "Iteracion: 13091 Gradiente: [0.00038158088419777415,-0.0065833000582519695] Loss: 22.842777633380635\n",
      "Iteracion: 13092 Gradiente: [0.0003813780240648157,-0.006579800173871083] Loss: 22.842777589906742\n",
      "Iteracion: 13093 Gradiente: [0.00038117527177234934,-0.00657630215013422] Loss: 22.842777546479066\n",
      "Iteracion: 13094 Gradiente: [0.00038097262733269115,-0.006572805986051122] Loss: 22.842777503097572\n",
      "Iteracion: 13095 Gradiente: [0.00038077009064162816,-0.006569311680634963] Loss: 22.84277745976217\n",
      "Iteracion: 13096 Gradiente: [0.00038056766160157924,-0.00656581923290022] Loss: 22.842777416472828\n",
      "Iteracion: 13097 Gradiente: [0.00038036534011117357,-0.006562328641861725] Loss: 22.842777373229524\n",
      "Iteracion: 13098 Gradiente: [0.0003801631261855694,-0.00655883990652922] Loss: 22.842777330032177\n",
      "Iteracion: 13099 Gradiente: [0.00037996101984655675,-0.006555353025910549] Loss: 22.842777286880736\n",
      "Iteracion: 13100 Gradiente: [0.000379759020913184,-0.006551867999025755] Loss: 22.842777243775167\n",
      "Iteracion: 13101 Gradiente: [0.00037955712939492515,-0.006548384824887185] Loss: 22.842777200715442\n",
      "Iteracion: 13102 Gradiente: [0.00037935534505303774,-0.006544903502520446] Loss: 22.842777157701466\n",
      "Iteracion: 13103 Gradiente: [0.0003791536680656312,-0.006541424030926398] Loss: 22.842777114733213\n",
      "Iteracion: 13104 Gradiente: [0.0003789520984611272,-0.006537946409116558] Loss: 22.842777071810648\n",
      "Iteracion: 13105 Gradiente: [0.00037875063588425445,-0.006534470636125889] Loss: 22.84277702893369\n",
      "Iteracion: 13106 Gradiente: [0.00037854928037575066,-0.006530996710964961] Loss: 22.84277698610233\n",
      "Iteracion: 13107 Gradiente: [0.0003783480319763536,-0.006527524632644462] Loss: 22.842776943316483\n",
      "Iteracion: 13108 Gradiente: [0.0003781468904842692,-0.0065240544001933165] Loss: 22.842776900576112\n",
      "Iteracion: 13109 Gradiente: [0.00037794585603213214,-0.006520586012616884] Loss: 22.842776857881194\n",
      "Iteracion: 13110 Gradiente: [0.0003777449284352012,-0.006517119468943259] Loss: 22.842776815231655\n",
      "Iteracion: 13111 Gradiente: [0.0003775441076375804,-0.006513654768192841] Loss: 22.842776772627435\n",
      "Iteracion: 13112 Gradiente: [0.00037734339362126924,-0.006510191909382001] Loss: 22.84277673006852\n",
      "Iteracion: 13113 Gradiente: [0.00037714278625268586,-0.006506730891536942] Loss: 22.842776687554842\n",
      "Iteracion: 13114 Gradiente: [0.0003769422855251984,-0.006503271713675929] Loss: 22.84277664508635\n",
      "Iteracion: 13115 Gradiente: [0.00037674189140564823,-0.006499814374817466] Loss: 22.842776602663005\n",
      "Iteracion: 13116 Gradiente: [0.00037654160388740366,-0.006496358873982899] Loss: 22.84277656028474\n",
      "Iteracion: 13117 Gradiente: [0.00037634142284919867,-0.006492905210199377] Loss: 22.842776517951528\n",
      "Iteracion: 13118 Gradiente: [0.00037614134820766293,-0.006489453382490377] Loss: 22.84277647566332\n",
      "Iteracion: 13119 Gradiente: [0.00037594137985384654,-0.006486003389884468] Loss: 22.842776433420067\n",
      "Iteracion: 13120 Gradiente: [0.0003757415179251211,-0.006482555231393524] Loss: 22.84277639122172\n",
      "Iteracion: 13121 Gradiente: [0.0003755417622291664,-0.0064791089060490725] Loss: 22.84277634906822\n",
      "Iteracion: 13122 Gradiente: [0.0003753421127546138,-0.006475664412877435] Loss: 22.842776306959536\n",
      "Iteracion: 13123 Gradiente: [0.00037514256937735507,-0.006472221750905523] Loss: 22.8427762648956\n",
      "Iteracion: 13124 Gradiente: [0.00037494313200833556,-0.006468780919163563] Loss: 22.8427762228764\n",
      "Iteracion: 13125 Gradiente: [0.00037474380070345127,-0.006465341916670179] Loss: 22.842776180901847\n",
      "Iteracion: 13126 Gradiente: [0.00037454457538975324,-0.006461904742456071] Loss: 22.84277613897191\n",
      "Iteracion: 13127 Gradiente: [0.00037434545602839836,-0.006458469395545545] Loss: 22.842776097086546\n",
      "Iteracion: 13128 Gradiente: [0.0003741464424952786,-0.006455035874974395] Loss: 22.842776055245714\n",
      "Iteracion: 13129 Gradiente: [0.0003739475347553404,-0.006451604179768348] Loss: 22.84277601344936\n",
      "Iteracion: 13130 Gradiente: [0.00037374873276216174,-0.006448174308956211] Loss: 22.842775971697417\n",
      "Iteracion: 13131 Gradiente: [0.0003735500364228983,-0.006444746261571292] Loss: 22.842775929989877\n",
      "Iteracion: 13132 Gradiente: [0.000373351445790604,-0.006441320036636356] Loss: 22.84277588832666\n",
      "Iteracion: 13133 Gradiente: [0.0003731529607354863,-0.0064378956331877886] Loss: 22.842775846707735\n",
      "Iteracion: 13134 Gradiente: [0.0003729545811798592,-0.006434473050258542] Loss: 22.842775805133044\n",
      "Iteracion: 13135 Gradiente: [0.0003727563070337207,-0.0064310522868829875] Loss: 22.842775763602546\n",
      "Iteracion: 13136 Gradiente: [0.00037255813835012454,-0.006427633342086973] Loss: 22.842775722116194\n",
      "Iteracion: 13137 Gradiente: [0.0003723600750362266,-0.006424216214905579] Loss: 22.84277568067395\n",
      "Iteracion: 13138 Gradiente: [0.0003721621169944456,-0.006420800904375549] Loss: 22.842775639275736\n",
      "Iteracion: 13139 Gradiente: [0.0003719642642513084,-0.006417387409526872] Loss: 22.84277559792156\n",
      "Iteracion: 13140 Gradiente: [0.0003717665165472302,-0.00641397572940446] Loss: 22.842775556611322\n",
      "Iteracion: 13141 Gradiente: [0.0003715688740877946,-0.006410565863029423] Loss: 22.842775515344997\n",
      "Iteracion: 13142 Gradiente: [0.00037137133662004846,-0.006407157809448568] Loss: 22.84277547412254\n",
      "Iteracion: 13143 Gradiente: [0.00037117390423115163,-0.006403751567690463] Loss: 22.842775432943906\n",
      "Iteracion: 13144 Gradiente: [0.0003709765767856273,-0.006400347136796351] Loss: 22.842775391809038\n",
      "Iteracion: 13145 Gradiente: [0.00037077935425979073,-0.006396944515800366] Loss: 22.84277535071789\n",
      "Iteracion: 13146 Gradiente: [0.0003705822365513238,-0.006393543703744223] Loss: 22.842775309670433\n",
      "Iteracion: 13147 Gradiente: [0.00037038522370854327,-0.006390144699660757] Loss: 22.842775268666596\n",
      "Iteracion: 13148 Gradiente: [0.00037018831555049776,-0.006386747502594048] Loss: 22.84277522770635\n",
      "Iteracion: 13149 Gradiente: [0.000369991512057292,-0.006383352111583325] Loss: 22.842775186789645\n",
      "Iteracion: 13150 Gradiente: [0.0003697948132109256,-0.006379958525665567] Loss: 22.84277514591644\n",
      "Iteracion: 13151 Gradiente: [0.00036959821891002775,-0.006376566743883435] Loss: 22.842775105086673\n",
      "Iteracion: 13152 Gradiente: [0.0003694017291820728,-0.006373176765273906] Loss: 22.842775064300326\n",
      "Iteracion: 13153 Gradiente: [0.0003692053438385301,-0.006369788588884736] Loss: 22.842775023557298\n",
      "Iteracion: 13154 Gradiente: [0.0003690090628926631,-0.006366402213754914] Loss: 22.842774982857613\n",
      "Iteracion: 13155 Gradiente: [0.0003688128864752116,-0.006363017638916209] Loss: 22.842774942201167\n",
      "Iteracion: 13156 Gradiente: [0.00036861681419111394,-0.006359634863429994] Loss: 22.842774901587962\n",
      "Iteracion: 13157 Gradiente: [0.00036842084614931994,-0.006356253886330639] Loss: 22.842774861017908\n",
      "Iteracion: 13158 Gradiente: [0.0003682249823649878,-0.006352874706659032] Loss: 22.842774820490984\n",
      "Iteracion: 13159 Gradiente: [0.0003680292226808509,-0.0063494973234638746] Loss: 22.84277478000714\n",
      "Iteracion: 13160 Gradiente: [0.0003678335669993279,-0.006346121735793394] Loss: 22.842774739566337\n",
      "Iteracion: 13161 Gradiente: [0.00036763801545779037,-0.006342747942681844] Loss: 22.842774699168526\n",
      "Iteracion: 13162 Gradiente: [0.0003674425678241278,-0.006339375943184796] Loss: 22.84277465881363\n",
      "Iteracion: 13163 Gradiente: [0.0003672472239742319,-0.006336005736353675] Loss: 22.842774618501657\n",
      "Iteracion: 13164 Gradiente: [0.0003670519840738962,-0.006332637321222379] Loss: 22.842774578232518\n",
      "Iteracion: 13165 Gradiente: [0.0003668568479772224,-0.0063292706968435175] Loss: 22.842774538006203\n",
      "Iteracion: 13166 Gradiente: [0.00036666181558283977,-0.00632590586226686] Loss: 22.842774497822617\n",
      "Iteracion: 13167 Gradiente: [0.0003664668869655922,-0.00632254281653483] Loss: 22.84277445768176\n",
      "Iteracion: 13168 Gradiente: [0.000366272061854526,-0.006319181558709038] Loss: 22.84277441758359\n",
      "Iteracion: 13169 Gradiente: [0.0003660773404324876,-0.006315822087823501] Loss: 22.842774377528027\n",
      "Iteracion: 13170 Gradiente: [0.0003658827224948406,-0.006312464402938526] Loss: 22.84277433751504\n",
      "Iteracion: 13171 Gradiente: [0.00036568820798000466,-0.006309108503103645] Loss: 22.842774297544597\n",
      "Iteracion: 13172 Gradiente: [0.00036549379694482317,-0.006305754387364004] Loss: 22.842774257616632\n",
      "Iteracion: 13173 Gradiente: [0.0003652994892073972,-0.0063024020547789664] Loss: 22.84277421773111\n",
      "Iteracion: 13174 Gradiente: [0.00036510528480562245,-0.006299051504394626] Loss: 22.84277417788799\n",
      "Iteracion: 13175 Gradiente: [0.0003649111835888637,-0.00629570273526987] Loss: 22.842774138087233\n",
      "Iteracion: 13176 Gradiente: [0.0003647171856679658,-0.006292355746445821] Loss: 22.84277409832877\n",
      "Iteracion: 13177 Gradiente: [0.00036452329085155574,-0.0062890105369872865] Loss: 22.842774058612566\n",
      "Iteracion: 13178 Gradiente: [0.0003643294990013146,-0.006285667105950902] Loss: 22.84277401893859\n",
      "Iteracion: 13179 Gradiente: [0.0003641358102868253,-0.006282325452377672] Loss: 22.842773979306774\n",
      "Iteracion: 13180 Gradiente: [0.0003639422244418711,-0.006278985575337023] Loss: 22.842773939717095\n",
      "Iteracion: 13181 Gradiente: [0.0003637487416303505,-0.006275647473870668] Loss: 22.842773900169497\n",
      "Iteracion: 13182 Gradiente: [0.0003635553616807859,-0.0062723111470415205] Loss: 22.842773860663932\n",
      "Iteracion: 13183 Gradiente: [0.0003633620845363339,-0.006268976593905388] Loss: 22.842773821200367\n",
      "Iteracion: 13184 Gradiente: [0.00036316890998098946,-0.006265643813530394] Loss: 22.842773781778746\n",
      "Iteracion: 13185 Gradiente: [0.00036297583819949373,-0.006262312804958962] Loss: 22.84277374239903\n",
      "Iteracion: 13186 Gradiente: [0.0003627828691312137,-0.0062589835672500985] Loss: 22.84277370306119\n",
      "Iteracion: 13187 Gradiente: [0.0003625900027050951,-0.006255656099461978] Loss: 22.842773663765136\n",
      "Iteracion: 13188 Gradiente: [0.00036239723869471165,-0.006252330400665684] Loss: 22.842773624510883\n",
      "Iteracion: 13189 Gradiente: [0.0003622045772014341,-0.006249006469910512] Loss: 22.84277358529834\n",
      "Iteracion: 13190 Gradiente: [0.0003620120180850487,-0.006245684306262097] Loss: 22.842773546127468\n",
      "Iteracion: 13191 Gradiente: [0.0003618195613910302,-0.006242363908774943] Loss: 22.842773506998256\n",
      "Iteracion: 13192 Gradiente: [0.0003616272069810596,-0.006239045276514569] Loss: 22.842773467910614\n",
      "Iteracion: 13193 Gradiente: [0.00036143495485608433,-0.006235728408539979] Loss: 22.842773428864543\n",
      "Iteracion: 13194 Gradiente: [0.00036124280499147214,-0.006232413303910415] Loss: 22.84277338985997\n",
      "Iteracion: 13195 Gradiente: [0.0003610507571920607,-0.00622909996169696] Loss: 22.842773350896845\n",
      "Iteracion: 13196 Gradiente: [0.00036085881149290345,-0.0062257883809593295] Loss: 22.842773311975158\n",
      "Iteracion: 13197 Gradiente: [0.0003606669678996847,-0.006222478560755936] Loss: 22.84277327309485\n",
      "Iteracion: 13198 Gradiente: [0.00036047522633471846,-0.006219170500151823] Loss: 22.842773234255848\n",
      "Iteracion: 13199 Gradiente: [0.0003602835865782102,-0.006215864198221273] Loss: 22.84277319545814\n",
      "Iteracion: 13200 Gradiente: [0.0003600920487997428,-0.006212559654017132] Loss: 22.84277315670168\n",
      "Iteracion: 13201 Gradiente: [0.0003599006128003642,-0.006209256866612615] Loss: 22.842773117986404\n",
      "Iteracion: 13202 Gradiente: [0.0003597092785507054,-0.006205955835072648] Loss: 22.84277307931229\n",
      "Iteracion: 13203 Gradiente: [0.0003595180460952937,-0.006202656558458604] Loss: 22.842773040679283\n",
      "Iteracion: 13204 Gradiente: [0.0003593269153194948,-0.006199359035839435] Loss: 22.84277300208735\n",
      "Iteracion: 13205 Gradiente: [0.0003591358860414099,-0.006196063266292029] Loss: 22.842772963536426\n",
      "Iteracion: 13206 Gradiente: [0.00035894495838041017,-0.006192769248873257] Loss: 22.842772925026495\n",
      "Iteracion: 13207 Gradiente: [0.00035875413219533435,-0.0061894769826577566] Loss: 22.842772886557476\n",
      "Iteracion: 13208 Gradiente: [0.00035856340749755117,-0.006186186466707374] Loss: 22.84277284812937\n",
      "Iteracion: 13209 Gradiente: [0.0003583727842169537,-0.006182897700096746] Loss: 22.842772809742105\n",
      "Iteracion: 13210 Gradiente: [0.0003581822622408026,-0.006179610681896719] Loss: 22.842772771395637\n",
      "Iteracion: 13211 Gradiente: [0.0003579918416022565,-0.0061763254111742326] Loss: 22.84277273308994\n",
      "Iteracion: 13212 Gradiente: [0.00035780152217625984,-0.006173041887003095] Loss: 22.842772694824973\n",
      "Iteracion: 13213 Gradiente: [0.00035761130390028485,-0.006169760108456046] Loss: 22.842772656600655\n",
      "Iteracion: 13214 Gradiente: [0.00035742118671369856,-0.006166480074604052] Loss: 22.84277261841699\n",
      "Iteracion: 13215 Gradiente: [0.00035723117070745046,-0.006163201784514172] Loss: 22.842772580273895\n",
      "Iteracion: 13216 Gradiente: [0.0003570412556693251,-0.00615992523726625] Loss: 22.842772542171364\n",
      "Iteracion: 13217 Gradiente: [0.00035685144150742566,-0.006156650431937057] Loss: 22.842772504109323\n",
      "Iteracion: 13218 Gradiente: [0.0003566617283041751,-0.006153377367591754] Loss: 22.842772466087755\n",
      "Iteracion: 13219 Gradiente: [0.00035647211594209695,-0.006150106043309241] Loss: 22.84277242810659\n",
      "Iteracion: 13220 Gradiente: [0.00035628260437950605,-0.006146836458162616] Loss: 22.842772390165802\n",
      "Iteracion: 13221 Gradiente: [0.00035609319369408847,-0.006143568611221421] Loss: 22.842772352265342\n",
      "Iteracion: 13222 Gradiente: [0.00035590388367173396,-0.006140302501568703] Loss: 22.842772314405174\n",
      "Iteracion: 13223 Gradiente: [0.0003557146742167561,-0.006137038128285136] Loss: 22.842772276585247\n",
      "Iteracion: 13224 Gradiente: [0.00035552556534147094,-0.006133775490444293] Loss: 22.842772238805523\n",
      "Iteracion: 13225 Gradiente: [0.0003553365570070355,-0.0061305145871198615] Loss: 22.842772201065948\n",
      "Iteracion: 13226 Gradiente: [0.0003551476491907124,-0.006127255417391334] Loss: 22.84277216336649\n",
      "Iteracion: 13227 Gradiente: [0.000354958841742814,-0.006123997980339032] Loss: 22.842772125707125\n",
      "Iteracion: 13228 Gradiente: [0.0003547701348480814,-0.0061207422750307215] Loss: 22.842772088087777\n",
      "Iteracion: 13229 Gradiente: [0.0003545815281739806,-0.006117488300561291] Loss: 22.84277205050843\n",
      "Iteracion: 13230 Gradiente: [0.0003543930217290381,-0.006114236056006798] Loss: 22.842772012969018\n",
      "Iteracion: 13231 Gradiente: [0.0003542046154469366,-0.006110985540449221] Loss: 22.842771975469496\n",
      "Iteracion: 13232 Gradiente: [0.0003540163094451524,-0.006107736752958696] Loss: 22.842771938009875\n",
      "Iteracion: 13233 Gradiente: [0.00035382810354652367,-0.00610448969262265] Loss: 22.842771900590044\n",
      "Iteracion: 13234 Gradiente: [0.00035363999766010085,-0.006101244358526495] Loss: 22.84277186320998\n",
      "Iteracion: 13235 Gradiente: [0.00035345199173946184,-0.006098000749752094] Loss: 22.842771825869672\n",
      "Iteracion: 13236 Gradiente: [0.00035326408583860787,-0.006094758865375383] Loss: 22.842771788569053\n",
      "Iteracion: 13237 Gradiente: [0.0003530762798230095,-0.006091518704484737] Loss: 22.842771751308074\n",
      "Iteracion: 13238 Gradiente: [0.0003528885736206651,-0.006088280266164267] Loss: 22.84277171408671\n",
      "Iteracion: 13239 Gradiente: [0.00035270096716620477,-0.006085043549499621] Loss: 22.842771676904906\n",
      "Iteracion: 13240 Gradiente: [0.0003525134605913157,-0.006081808553565319] Loss: 22.842771639762642\n",
      "Iteracion: 13241 Gradiente: [0.000352326053549253,-0.006078575277461576] Loss: 22.84277160265984\n",
      "Iteracion: 13242 Gradiente: [0.0003521387462636009,-0.006075343720260662] Loss: 22.842771565596486\n",
      "Iteracion: 13243 Gradiente: [0.00035195153853161777,-0.00607211388105533] Loss: 22.84277152857252\n",
      "Iteracion: 13244 Gradiente: [0.0003517644302093004,-0.006068885758939047] Loss: 22.84277149158792\n",
      "Iteracion: 13245 Gradiente: [0.0003515774214444415,-0.006065659352986922] Loss: 22.842771454642627\n",
      "Iteracion: 13246 Gradiente: [0.00035139051208830094,-0.006062434662292068] Loss: 22.842771417736607\n",
      "Iteracion: 13247 Gradiente: [0.0003512037021626687,-0.0060592116859364605] Loss: 22.84277138086982\n",
      "Iteracion: 13248 Gradiente: [0.00035101699143353927,-0.006055990423021859] Loss: 22.842771344042227\n",
      "Iteracion: 13249 Gradiente: [0.00035083038001365217,-0.006052770872627041] Loss: 22.84277130725377\n",
      "Iteracion: 13250 Gradiente: [0.0003506438678509009,-0.006049553033840027] Loss: 22.842771270504425\n",
      "Iteracion: 13251 Gradiente: [0.00035045745478138696,-0.006046336905758191] Loss: 22.842771233794135\n",
      "Iteracion: 13252 Gradiente: [0.0003502711407984786,-0.006043122487471564] Loss: 22.84277119712288\n",
      "Iteracion: 13253 Gradiente: [0.00035008492597417745,-0.0060399097780612955] Loss: 22.842771160490607\n",
      "Iteracion: 13254 Gradiente: [0.0003498988100773204,-0.006036698776629971] Loss: 22.84277112389726\n",
      "Iteracion: 13255 Gradiente: [0.00034971279308422255,-0.00603348948226774] Loss: 22.842771087342825\n",
      "Iteracion: 13256 Gradiente: [0.0003495268750100422,-0.006030281894063805] Loss: 22.84277105082724\n",
      "Iteracion: 13257 Gradiente: [0.000349341055776146,-0.0060270760111118685] Loss: 22.84277101435047\n",
      "Iteracion: 13258 Gradiente: [0.0003491553354062186,-0.006023871832501371] Loss: 22.842770977912473\n",
      "Iteracion: 13259 Gradiente: [0.0003489697137647833,-0.0060206693573302775] Loss: 22.842770941513205\n",
      "Iteracion: 13260 Gradiente: [0.00034878419071067884,-0.006017468584700107] Loss: 22.84277090515263\n",
      "Iteracion: 13261 Gradiente: [0.00034859876634811825,-0.006014269513695088] Loss: 22.842770868830723\n",
      "Iteracion: 13262 Gradiente: [0.000348413440569099,-0.006011072143413069] Loss: 22.842770832547405\n",
      "Iteracion: 13263 Gradiente: [0.00034822821335183106,-0.006007876472949055] Loss: 22.84277079630266\n",
      "Iteracion: 13264 Gradiente: [0.0003480430844642039,-0.006004682501410248] Loss: 22.84277076009645\n",
      "Iteracion: 13265 Gradiente: [0.0003478580540066408,-0.006001490227885498] Loss: 22.842770723928712\n",
      "Iteracion: 13266 Gradiente: [0.00034767312205967,-0.005998299651461755] Loss: 22.842770687799433\n",
      "Iteracion: 13267 Gradiente: [0.00034748828832486346,-0.005995110771252973] Loss: 22.842770651708552\n",
      "Iteracion: 13268 Gradiente: [0.00034730355284769604,-0.005991923586350841] Loss: 22.84277061565605\n",
      "Iteracion: 13269 Gradiente: [0.00034711891566227375,-0.005988738095848589] Loss: 22.842770579641847\n",
      "Iteracion: 13270 Gradiente: [0.00034693437662459324,-0.005985554298853183] Loss: 22.842770543665935\n",
      "Iteracion: 13271 Gradiente: [0.0003467499356541263,-0.005982372194462115] Loss: 22.842770507728268\n",
      "Iteracion: 13272 Gradiente: [0.00034656559275560996,-0.005979191781775128] Loss: 22.842770471828796\n",
      "Iteracion: 13273 Gradiente: [0.0003463813478617794,-0.005976013059893859] Loss: 22.8427704359675\n",
      "Iteracion: 13274 Gradiente: [0.00034619720091294917,-0.005972836027918523] Loss: 22.842770400144307\n",
      "Iteracion: 13275 Gradiente: [0.0003460131518560653,-0.005969660684950403] Loss: 22.842770364359193\n",
      "Iteracion: 13276 Gradiente: [0.00034582920064470574,-0.005966487030092438] Loss: 22.842770328612133\n",
      "Iteracion: 13277 Gradiente: [0.0003456453472239218,-0.005963315062447686] Loss: 22.842770292903058\n",
      "Iteracion: 13278 Gradiente: [0.0003454615915368701,-0.005960144781118022] Loss: 22.84277025723194\n",
      "Iteracion: 13279 Gradiente: [0.00034527793354849714,-0.005956976185207215] Loss: 22.84277022159876\n",
      "Iteracion: 13280 Gradiente: [0.0003450943731280631,-0.0059538092738232965] Loss: 22.842770186003428\n",
      "Iteracion: 13281 Gradiente: [0.00034491091038830747,-0.005950644046061981] Loss: 22.842770150445958\n",
      "Iteracion: 13282 Gradiente: [0.0003447275451634368,-0.005947480501033553] Loss: 22.84277011492627\n",
      "Iteracion: 13283 Gradiente: [0.0003445442774117661,-0.005944318637842608] Loss: 22.842770079444346\n",
      "Iteracion: 13284 Gradiente: [0.0003443611071475061,-0.00594115845559268] Loss: 22.842770044000122\n",
      "Iteracion: 13285 Gradiente: [0.0003441780341669679,-0.005937999953397602] Loss: 22.8427700085936\n",
      "Iteracion: 13286 Gradiente: [0.00034399505860089143,-0.005934843130354631] Loss: 22.84276997322469\n",
      "Iteracion: 13287 Gradiente: [0.00034381218024085076,-0.005931687985578904] Loss: 22.8427699378934\n",
      "Iteracion: 13288 Gradiente: [0.00034362939909821457,-0.005928534518176439] Loss: 22.84276990259965\n",
      "Iteracion: 13289 Gradiente: [0.0003434467151900359,-0.005925382727249465] Loss: 22.84276986734342\n",
      "Iteracion: 13290 Gradiente: [0.00034326412828325676,-0.005922232611919398] Loss: 22.84276983212467\n",
      "Iteracion: 13291 Gradiente: [0.0003430816386516729,-0.005919084171274965] Loss: 22.84276979694336\n",
      "Iteracion: 13292 Gradiente: [0.00034289924588411697,-0.005915937404445515] Loss: 22.842769761799435\n",
      "Iteracion: 13293 Gradiente: [0.00034271695015680357,-0.005912792310532092] Loss: 22.842769726692875\n",
      "Iteracion: 13294 Gradiente: [0.0003425347513067815,-0.0059096488886472304] Loss: 22.842769691623634\n",
      "Iteracion: 13295 Gradiente: [0.00034235264938236773,-0.005906507137898724] Loss: 22.84276965659166\n",
      "Iteracion: 13296 Gradiente: [0.00034217064415713593,-0.00590336705740917] Loss: 22.842769621596936\n",
      "Iteracion: 13297 Gradiente: [0.00034198873570971954,-0.0059002286462805625] Loss: 22.842769586639406\n",
      "Iteracion: 13298 Gradiente: [0.0003418069240031703,-0.0058970919036253145] Loss: 22.84276955171904\n",
      "Iteracion: 13299 Gradiente: [0.00034162520888306365,-0.005893956828563537] Loss: 22.842769516835787\n",
      "Iteracion: 13300 Gradiente: [0.00034144359046687595,-0.005890823420198406] Loss: 22.842769481989617\n",
      "Iteracion: 13301 Gradiente: [0.0003412620686333412,-0.0058876916776468375] Loss: 22.842769447180494\n",
      "Iteracion: 13302 Gradiente: [0.0003410806432071922,-0.005884561600031783] Loss: 22.842769412408362\n",
      "Iteracion: 13303 Gradiente: [0.0003408993142803259,-0.005881433186459617] Loss: 22.842769377673207\n",
      "Iteracion: 13304 Gradiente: [0.0003407180816642115,-0.005878306436052464] Loss: 22.842769342974943\n",
      "Iteracion: 13305 Gradiente: [0.00034053694547916773,-0.005875181347917765] Loss: 22.842769308313585\n",
      "Iteracion: 13306 Gradiente: [0.0003403559055849807,-0.005872057921176458] Loss: 22.842769273689083\n",
      "Iteracion: 13307 Gradiente: [0.00034017496190396436,-0.005868936154946524] Loss: 22.842769239101372\n",
      "Iteracion: 13308 Gradiente: [0.0003399941144796988,-0.005865816048340022] Loss: 22.842769204550418\n",
      "Iteracion: 13309 Gradiente: [0.00033981336327239356,-0.005862697600472089] Loss: 22.84276917003621\n",
      "Iteracion: 13310 Gradiente: [0.00033963270803667457,-0.005859580810475625] Loss: 22.84276913555869\n",
      "Iteracion: 13311 Gradiente: [0.00033945214886349127,-0.0058564656774580705] Loss: 22.842769101117803\n",
      "Iteracion: 13312 Gradiente: [0.0003392716856713681,-0.00585335220054238] Loss: 22.842769066713526\n",
      "Iteracion: 13313 Gradiente: [0.00033909131846977895,-0.005850240378842386] Loss: 22.842769032345824\n",
      "Iteracion: 13314 Gradiente: [0.000338911047072088,-0.005847130211488031] Loss: 22.84276899801467\n",
      "Iteracion: 13315 Gradiente: [0.00033873087160145586,-0.005844021697588768] Loss: 22.84276896371998\n",
      "Iteracion: 13316 Gradiente: [0.00033855079185608853,-0.005840914836275127] Loss: 22.842768929461784\n",
      "Iteracion: 13317 Gradiente: [0.00033837080785872333,-0.005837809626662842] Loss: 22.842768895239967\n",
      "Iteracion: 13318 Gradiente: [0.000338190919511779,-0.005834706067877349] Loss: 22.84276886105454\n",
      "Iteracion: 13319 Gradiente: [0.0003380111268967312,-0.0058316041590340244] Loss: 22.842768826905456\n",
      "Iteracion: 13320 Gradiente: [0.00033783142980894356,-0.00582850389926269] Loss: 22.842768792792658\n",
      "Iteracion: 13321 Gradiente: [0.00033765182829104866,-0.005825405287682746] Loss: 22.84276875871614\n",
      "Iteracion: 13322 Gradiente: [0.00033747232218956925,-0.005822308323424016] Loss: 22.84276872467583\n",
      "Iteracion: 13323 Gradiente: [0.00033729291157650704,-0.00581921300560424] Loss: 22.842768690671708\n",
      "Iteracion: 13324 Gradiente: [0.0003371135962159618,-0.005816119333356914] Loss: 22.84276865670373\n",
      "Iteracion: 13325 Gradiente: [0.00033693437631256986,-0.005813027305791725] Loss: 22.842768622771867\n",
      "Iteracion: 13326 Gradiente: [0.00033675525162474664,-0.005809936922048564] Loss: 22.842768588876066\n",
      "Iteracion: 13327 Gradiente: [0.0003365762221875457,-0.005806848181246712] Loss: 22.8427685550163\n",
      "Iteracion: 13328 Gradiente: [0.0003363972879431761,-0.005803761082513503] Loss: 22.842768521192518\n",
      "Iteracion: 13329 Gradiente: [0.00033621844877795105,-0.005800675624980063] Loss: 22.842768487404708\n",
      "Iteracion: 13330 Gradiente: [0.0003360397047543984,-0.005797591807765793] Loss: 22.842768453652802\n",
      "Iteracion: 13331 Gradiente: [0.00033586105573135683,-0.005794509630005133] Loss: 22.842768419936764\n",
      "Iteracion: 13332 Gradiente: [0.0003356825017116686,-0.005791429090823285] Loss: 22.842768386256573\n",
      "Iteracion: 13333 Gradiente: [0.00033550404259396295,-0.0057883501893518504] Loss: 22.84276835261219\n",
      "Iteracion: 13334 Gradiente: [0.0003353256783261334,-0.0057852729247202936] Loss: 22.842768319003575\n",
      "Iteracion: 13335 Gradiente: [0.0003351474089489178,-0.00578219729605299] Loss: 22.842768285430672\n",
      "Iteracion: 13336 Gradiente: [0.00033496923430410185,-0.005779123302487695] Loss: 22.84276825189346\n",
      "Iteracion: 13337 Gradiente: [0.0003347911542903148,-0.005776050943156127] Loss: 22.84276821839189\n",
      "Iteracion: 13338 Gradiente: [0.0003346131690401914,-0.0057729802171806455] Loss: 22.842768184925934\n",
      "Iteracion: 13339 Gradiente: [0.00033443527847699294,-0.00576991112369285] Loss: 22.842768151495555\n",
      "Iteracion: 13340 Gradiente: [0.0003342574823382923,-0.0057668436618389055] Loss: 22.842768118100725\n",
      "Iteracion: 13341 Gradiente: [0.00033407978082019933,-0.00576377783073454] Loss: 22.84276808474137\n",
      "Iteracion: 13342 Gradiente: [0.0003339021738336593,-0.005760713629518458] Loss: 22.84276805141748\n",
      "Iteracion: 13343 Gradiente: [0.0003337246610849813,-0.0057576510573345745] Loss: 22.842768018129032\n",
      "Iteracion: 13344 Gradiente: [0.0003335472429341735,-0.005754590113295658] Loss: 22.84276798487595\n",
      "Iteracion: 13345 Gradiente: [0.0003333699189406995,-0.005751530796558058] Loss: 22.842767951658224\n",
      "Iteracion: 13346 Gradiente: [0.0003331926892599313,-0.005748473106244845] Loss: 22.842767918475804\n",
      "Iteracion: 13347 Gradiente: [0.00033301555378291897,-0.005745417041495315] Loss: 22.842767885328655\n",
      "Iteracion: 13348 Gradiente: [0.0003328385125020835,-0.005742362601441068] Loss: 22.84276785221675\n",
      "Iteracion: 13349 Gradiente: [0.00033266156530752745,-0.0057393097852230564] Loss: 22.842767819140025\n",
      "Iteracion: 13350 Gradiente: [0.00033248471217651364,-0.005736258591976196] Loss: 22.842767786098474\n",
      "Iteracion: 13351 Gradiente: [0.00033230795310714714,-0.00573320902083528] Loss: 22.842767753092048\n",
      "Iteracion: 13352 Gradiente: [0.0003321312879696355,-0.0057301610709420935] Loss: 22.842767720120698\n",
      "Iteracion: 13353 Gradiente: [0.0003319547168104009,-0.005727114741429655] Loss: 22.842767687184402\n",
      "Iteracion: 13354 Gradiente: [0.00033177823950817736,-0.005724070031440102] Loss: 22.842767654283115\n",
      "Iteracion: 13355 Gradiente: [0.00033160185603454316,-0.005721026940110718] Loss: 22.84276762141679\n",
      "Iteracion: 13356 Gradiente: [0.00033142556632223357,-0.005717985466583049] Loss: 22.842767588585414\n",
      "Iteracion: 13357 Gradiente: [0.00033124937031156303,-0.005714945609996983] Loss: 22.842767555788928\n",
      "Iteracion: 13358 Gradiente: [0.00033107326796653067,-0.005711907369492645] Loss: 22.8427675230273\n",
      "Iteracion: 13359 Gradiente: [0.0003308972592672414,-0.005708870744207909] Loss: 22.842767490300513\n",
      "Iteracion: 13360 Gradiente: [0.0003307213441265352,-0.005705835733288112] Loss: 22.842767457608524\n",
      "Iteracion: 13361 Gradiente: [0.0003305455225302012,-0.005702802335871837] Loss: 22.842767424951262\n",
      "Iteracion: 13362 Gradiente: [0.0003303697943996061,-0.005699770551103474] Loss: 22.84276739232872\n",
      "Iteracion: 13363 Gradiente: [0.00033019415961348385,-0.005696740378129543] Loss: 22.842767359740854\n",
      "Iteracion: 13364 Gradiente: [0.0003300186183243644,-0.00569371181607939] Loss: 22.84276732718763\n",
      "Iteracion: 13365 Gradiente: [0.00032984317024329355,-0.005690684864114104] Loss: 22.842767294669\n",
      "Iteracion: 13366 Gradiente: [0.0003296678154669053,-0.005687659521368005] Loss: 22.842767262184957\n",
      "Iteracion: 13367 Gradiente: [0.0003294925539904625,-0.005684635786981336] Loss: 22.84276722973544\n",
      "Iteracion: 13368 Gradiente: [0.0003293173856064868,-0.00568161366011104] Loss: 22.842767197320402\n",
      "Iteracion: 13369 Gradiente: [0.0003291423104154016,-0.005678593139891438] Loss: 22.842767164939822\n",
      "Iteracion: 13370 Gradiente: [0.0003289673282267813,-0.005675574225476628] Loss: 22.842767132593668\n",
      "Iteracion: 13371 Gradiente: [0.0003287924390529421,-0.005672556916010052] Loss: 22.842767100281886\n",
      "Iteracion: 13372 Gradiente: [0.00032861764298578084,-0.005669541210631005] Loss: 22.842767068004466\n",
      "Iteracion: 13373 Gradiente: [0.00032844293976476516,-0.005666527108497851] Loss: 22.842767035761362\n",
      "Iteracion: 13374 Gradiente: [0.0003282683294230537,-0.005663514608753909] Loss: 22.842767003552517\n",
      "Iteracion: 13375 Gradiente: [0.00032809381190001353,-0.005660503710547952] Loss: 22.842766971377912\n",
      "Iteracion: 13376 Gradiente: [0.00032791938713690644,-0.005657494413029459] Loss: 22.84276693923751\n",
      "Iteracion: 13377 Gradiente: [0.000327745055072152,-0.005654486715347436] Loss: 22.842766907131285\n",
      "Iteracion: 13378 Gradiente: [0.0003275708157483829,-0.00565148061664497] Loss: 22.842766875059173\n",
      "Iteracion: 13379 Gradiente: [0.0003273966690509648,-0.005648476116079119] Loss: 22.842766843021156\n",
      "Iteracion: 13380 Gradiente: [0.0003272226149041065,-0.005645473212798772] Loss: 22.84276681101719\n",
      "Iteracion: 13381 Gradiente: [0.00032704865330591325,-0.005642471905953172] Loss: 22.842766779047242\n",
      "Iteracion: 13382 Gradiente: [0.00032687478426870104,-0.005639472194689196] Loss: 22.842766747111288\n",
      "Iteracion: 13383 Gradiente: [0.0003267010075480433,-0.00563647407817219] Loss: 22.842766715209276\n",
      "Iteracion: 13384 Gradiente: [0.0003265273232661533,-0.005633477555541214] Loss: 22.842766683341168\n",
      "Iteracion: 13385 Gradiente: [0.0003263537313822932,-0.005630482625951908] Loss: 22.842766651506942\n",
      "Iteracion: 13386 Gradiente: [0.0003261802317221433,-0.005627489288560976] Loss: 22.84276661970656\n",
      "Iteracion: 13387 Gradiente: [0.0003260068243131779,-0.005624497542520146] Loss: 22.842766587939987\n",
      "Iteracion: 13388 Gradiente: [0.00032583350915066,-0.0056215073869792555] Loss: 22.842766556207167\n",
      "Iteracion: 13389 Gradiente: [0.0003256602860574276,-0.005618518821101522] Loss: 22.84276652450809\n",
      "Iteracion: 13390 Gradiente: [0.00032548715500790116,-0.005615531844038676] Loss: 22.842766492842692\n",
      "Iteracion: 13391 Gradiente: [0.00032531411604376596,-0.005612546454943631] Loss: 22.84276646121096\n",
      "Iteracion: 13392 Gradiente: [0.00032514116898028076,-0.005609562652977947] Loss: 22.842766429612873\n",
      "Iteracion: 13393 Gradiente: [0.0003249683140258715,-0.005606580437284] Loss: 22.842766398048344\n",
      "Iteracion: 13394 Gradiente: [0.0003247955508925315,-0.005603599807030572] Loss: 22.842766366517385\n",
      "Iteracion: 13395 Gradiente: [0.0003246228795868925,-0.005600620761373302] Loss: 22.842766335019924\n",
      "Iteracion: 13396 Gradiente: [0.0003244503000814802,-0.00559764329946771] Loss: 22.84276630355597\n",
      "Iteracion: 13397 Gradiente: [0.00032427781230902987,-0.0055946674204728685] Loss: 22.842766272125445\n",
      "Iteracion: 13398 Gradiente: [0.00032410541628659454,-0.005591693123544535] Loss: 22.842766240728338\n",
      "Iteracion: 13399 Gradiente: [0.0003239331118758552,-0.005588720407846044] Loss: 22.842766209364598\n",
      "Iteracion: 13400 Gradiente: [0.0003237608990910227,-0.005585749272532326] Loss: 22.842766178034204\n",
      "Iteracion: 13401 Gradiente: [0.00032358877786199024,-0.005582779716765647] Loss: 22.842766146737095\n",
      "Iteracion: 13402 Gradiente: [0.0003234167482058107,-0.005579811739702004] Loss: 22.84276611547327\n",
      "Iteracion: 13403 Gradiente: [0.00032324480993111137,-0.005576845340510772] Loss: 22.842766084242683\n",
      "Iteracion: 13404 Gradiente: [0.0003230729630246287,-0.005573880518349957] Loss: 22.842766053045278\n",
      "Iteracion: 13405 Gradiente: [0.00032290120752899535,-0.005570917272377448] Loss: 22.84276602188105\n",
      "Iteracion: 13406 Gradiente: [0.00032272954333526134,-0.005567955601756343] Loss: 22.842765990749935\n",
      "Iteracion: 13407 Gradiente: [0.00032255797043016327,-0.005564995505650217] Loss: 22.842765959651913\n",
      "Iteracion: 13408 Gradiente: [0.0003223864887890689,-0.005562036983219087] Loss: 22.84276592858697\n",
      "Iteracion: 13409 Gradiente: [0.0003222150981590251,-0.005559080033636832] Loss: 22.842765897555026\n",
      "Iteracion: 13410 Gradiente: [0.0003220437987617212,-0.00555612465605364] Loss: 22.842765866556082\n",
      "Iteracion: 13411 Gradiente: [0.0003218725903489409,-0.005553170849645994] Loss: 22.842765835590082\n",
      "Iteracion: 13412 Gradiente: [0.0003217014729926859,-0.0055502186135690575] Loss: 22.84276580465699\n",
      "Iteracion: 13413 Gradiente: [0.00032153044662474407,-0.005547267946991733] Loss: 22.8427657737568\n",
      "Iteracion: 13414 Gradiente: [0.0003213595111115334,-0.005544318849082212] Loss: 22.842765742889426\n",
      "Iteracion: 13415 Gradiente: [0.0003211886666074785,-0.005541371318995658] Loss: 22.842765712054884\n",
      "Iteracion: 13416 Gradiente: [0.00032101791286531046,-0.005538425355910685] Loss: 22.842765681253113\n",
      "Iteracion: 13417 Gradiente: [0.0003208472498556603,-0.005535480958992878] Loss: 22.842765650484083\n",
      "Iteracion: 13418 Gradiente: [0.0003206766775870543,-0.005532538127406639] Loss: 22.842765619747773\n",
      "Iteracion: 13419 Gradiente: [0.00032050619600833367,-0.005529596860318975] Loss: 22.842765589044127\n",
      "Iteracion: 13420 Gradiente: [0.0003203358050977082,-0.005526657156897012] Loss: 22.842765558373117\n",
      "Iteracion: 13421 Gradiente: [0.0003201655048239142,-0.005523719016308585] Loss: 22.842765527734702\n",
      "Iteracion: 13422 Gradiente: [0.00031999529503915863,-0.005520782437728992] Loss: 22.84276549712887\n",
      "Iteracion: 13423 Gradiente: [0.00031982517569133505,-0.005517847420326897] Loss: 22.842765466555566\n",
      "Iteracion: 13424 Gradiente: [0.0003196551467908648,-0.005514913963271677] Loss: 22.84276543601475\n",
      "Iteracion: 13425 Gradiente: [0.00031948520829606274,-0.005511982065729389] Loss: 22.842765405506416\n",
      "Iteracion: 13426 Gradiente: [0.0003193153601756649,-0.00550905172687403] Loss: 22.842765375030503\n",
      "Iteracion: 13427 Gradiente: [0.00031914560227619404,-0.0055061229458792365] Loss: 22.842765344586983\n",
      "Iteracion: 13428 Gradiente: [0.00031897593473786403,-0.005503195721908819] Loss: 22.84276531417581\n",
      "Iteracion: 13429 Gradiente: [0.00031880635727266814,-0.005500270054147312] Loss: 22.842765283796993\n",
      "Iteracion: 13430 Gradiente: [0.0003186368700331362,-0.005497345941756274] Loss: 22.84276525345045\n",
      "Iteracion: 13431 Gradiente: [0.0003184674729292662,-0.005494423383912188] Loss: 22.842765223136162\n",
      "Iteracion: 13432 Gradiente: [0.0003182981658900038,-0.00549150237978869] Loss: 22.842765192854117\n",
      "Iteracion: 13433 Gradiente: [0.0003181289488215574,-0.005488582928563209] Loss: 22.842765162604245\n",
      "Iteracion: 13434 Gradiente: [0.00031795982169550524,-0.005485665029408674] Loss: 22.842765132386525\n",
      "Iteracion: 13435 Gradiente: [0.0003177907844180557,-0.005482748681501685] Loss: 22.842765102200932\n",
      "Iteracion: 13436 Gradiente: [0.0003176218370830005,-0.005479833884010787] Loss: 22.84276507204743\n",
      "Iteracion: 13437 Gradiente: [0.0003174529796410752,-0.005476920636110804] Loss: 22.84276504192597\n",
      "Iteracion: 13438 Gradiente: [0.0003172842118336424,-0.005474008936991126] Loss: 22.842765011836534\n",
      "Iteracion: 13439 Gradiente: [0.0003171155338293374,-0.005471098785816034] Loss: 22.84276498177908\n",
      "Iteracion: 13440 Gradiente: [0.0003169469454689988,-0.005468190181767815] Loss: 22.842764951753583\n",
      "Iteracion: 13441 Gradiente: [0.00031677844668441443,-0.0054652831240256695] Loss: 22.842764921759997\n",
      "Iteracion: 13442 Gradiente: [0.0003166100375949554,-0.005462377611758503] Loss: 22.842764891798303\n",
      "Iteracion: 13443 Gradiente: [0.0003164417179997751,-0.0054594736441520315] Loss: 22.842764861868442\n",
      "Iteracion: 13444 Gradiente: [0.0003162734879007682,-0.005456571220383447] Loss: 22.84276483197041\n",
      "Iteracion: 13445 Gradiente: [0.0003161053471814057,-0.005453670339636218] Loss: 22.842764802104135\n",
      "Iteracion: 13446 Gradiente: [0.00031593729585968806,-0.0054507710010852865] Loss: 22.842764772269625\n",
      "Iteracion: 13447 Gradiente: [0.0003157693338759297,-0.0054478732039125784] Loss: 22.84276474246684\n",
      "Iteracion: 13448 Gradiente: [0.0003156014612225514,-0.005444976947295525] Loss: 22.84276471269573\n",
      "Iteracion: 13449 Gradiente: [0.00031543367782376207,-0.005442082230416764] Loss: 22.842764682956258\n",
      "Iteracion: 13450 Gradiente: [0.00031526598355261133,-0.005439189052462723] Loss: 22.842764653248388\n",
      "Iteracion: 13451 Gradiente: [0.00031509837853225994,-0.005436297412604555] Loss: 22.842764623572116\n",
      "Iteracion: 13452 Gradiente: [0.00031493086249933334,-0.005433407310037453] Loss: 22.842764593927377\n",
      "Iteracion: 13453 Gradiente: [0.00031476343556941324,-0.005430518743935527] Loss: 22.842764564314162\n",
      "Iteracion: 13454 Gradiente: [0.00031459609767997185,-0.005427631713481418] Loss: 22.842764534732407\n",
      "Iteracion: 13455 Gradiente: [0.00031442884875237573,-0.00542474621786333] Loss: 22.842764505182114\n",
      "Iteracion: 13456 Gradiente: [0.0003142616887572558,-0.005421862256259402] Loss: 22.84276447566322\n",
      "Iteracion: 13457 Gradiente: [0.0003140946176481899,-0.005418979827858313] Loss: 22.84276444617571\n",
      "Iteracion: 13458 Gradiente: [0.0003139276353020174,-0.005416098931847083] Loss: 22.842764416719547\n",
      "Iteracion: 13459 Gradiente: [0.0003137607416836848,-0.0054132195674124976] Loss: 22.842764387294704\n",
      "Iteracion: 13460 Gradiente: [0.0003135939368746676,-0.005410341733731272] Loss: 22.842764357901114\n",
      "Iteracion: 13461 Gradiente: [0.0003134272207309626,-0.005407465429995995] Loss: 22.84276432853878\n",
      "Iteracion: 13462 Gradiente: [0.00031326059320804236,-0.005404590655392028] Loss: 22.842764299207673\n",
      "Iteracion: 13463 Gradiente: [0.0003130940542263261,-0.005401717409108997] Loss: 22.842764269907725\n",
      "Iteracion: 13464 Gradiente: [0.0003129276037659186,-0.005398845690332029] Loss: 22.84276424063893\n",
      "Iteracion: 13465 Gradiente: [0.000312761241843873,-0.005395975498247078] Loss: 22.842764211401242\n",
      "Iteracion: 13466 Gradiente: [0.0003125949683739767,-0.005393106832043771] Loss: 22.84276418219464\n",
      "Iteracion: 13467 Gradiente: [0.00031242878330886017,-0.005390239690910784] Loss: 22.84276415301907\n",
      "Iteracion: 13468 Gradiente: [0.0003122626865945222,-0.005387374074038457] Loss: 22.842764123874534\n",
      "Iteracion: 13469 Gradiente: [0.00031209667815327674,-0.005384509980617954] Loss: 22.84276409476097\n",
      "Iteracion: 13470 Gradiente: [0.0003119307579974399,-0.005381647409833453] Loss: 22.84276406567834\n",
      "Iteracion: 13471 Gradiente: [0.0003117649260569048,-0.005378786360879436] Loss: 22.842764036626637\n",
      "Iteracion: 13472 Gradiente: [0.00031159918219145764,-0.005375926832953818] Loss: 22.842764007605815\n",
      "Iteracion: 13473 Gradiente: [0.00031143352649204794,-0.005373068825237818] Loss: 22.842763978615842\n",
      "Iteracion: 13474 Gradiente: [0.00031126795886583146,-0.005370212336925443] Loss: 22.842763949656682\n",
      "Iteracion: 13475 Gradiente: [0.000311102479205753,-0.005367357367213188] Loss: 22.842763920728316\n",
      "Iteracion: 13476 Gradiente: [0.00031093708766528986,-0.005364503915281797] Loss: 22.84276389183069\n",
      "Iteracion: 13477 Gradiente: [0.00031077178398675187,-0.0053616519803375935] Loss: 22.842763862963775\n",
      "Iteracion: 13478 Gradiente: [0.00031060656811045343,-0.005358801561572927] Loss: 22.842763834127542\n",
      "Iteracion: 13479 Gradiente: [0.0003104414401472392,-0.005355952658174701] Loss: 22.842763805321976\n",
      "Iteracion: 13480 Gradiente: [0.00031027639993984243,-0.005353105269342843] Loss: 22.842763776547027\n",
      "Iteracion: 13481 Gradiente: [0.00031011144751573737,-0.00535025939426698] Loss: 22.84276374780267\n",
      "Iteracion: 13482 Gradiente: [0.00030994658276218466,-0.005347415032147395] Loss: 22.84276371908885\n",
      "Iteracion: 13483 Gradiente: [0.0003097818055882347,-0.0053445721821827165] Loss: 22.84276369040555\n",
      "Iteracion: 13484 Gradiente: [0.00030961711606967887,-0.005341730843559489] Loss: 22.842763661752763\n",
      "Iteracion: 13485 Gradiente: [0.0003094525141316732,-0.005338891015478472] Loss: 22.842763633130435\n",
      "Iteracion: 13486 Gradiente: [0.00030928799966053094,-0.005336052697140066] Loss: 22.842763604538497\n",
      "Iteracion: 13487 Gradiente: [0.0003091235726818316,-0.0053332158877365055] Loss: 22.842763575976978\n",
      "Iteracion: 13488 Gradiente: [0.0003089592330572562,-0.005330380586471625] Loss: 22.842763547445823\n",
      "Iteracion: 13489 Gradiente: [0.00030879498085122726,-0.005327546792536234] Loss: 22.84276351894498\n",
      "Iteracion: 13490 Gradiente: [0.00030863081597563754,-0.005324714505133107] Loss: 22.84276349047444\n",
      "Iteracion: 13491 Gradiente: [0.000308466738386907,-0.00532188372345909] Loss: 22.84276346203418\n",
      "Iteracion: 13492 Gradiente: [0.0003083027479751384,-0.005319054446719444] Loss: 22.84276343362412\n",
      "Iteracion: 13493 Gradiente: [0.00030813884479149084,-0.005316226674107109] Loss: 22.84276340524429\n",
      "Iteracion: 13494 Gradiente: [0.0003079750287004875,-0.0053134004048276985] Loss: 22.842763376894602\n",
      "Iteracion: 13495 Gradiente: [0.00030781129974381355,-0.00531057563807901] Loss: 22.84276334857506\n",
      "Iteracion: 13496 Gradiente: [0.00030764765778409735,-0.005307752373063579] Loss: 22.84276332028562\n",
      "Iteracion: 13497 Gradiente: [0.0003074841028374446,-0.00530493060898228] Loss: 22.84276329202624\n",
      "Iteracion: 13498 Gradiente: [0.00030732063484511705,-0.005302110345037411] Loss: 22.842763263796922\n",
      "Iteracion: 13499 Gradiente: [0.00030715725375500823,-0.005299291580431979] Loss: 22.842763235597598\n",
      "Iteracion: 13500 Gradiente: [0.0003069939594884848,-0.005296474314369585] Loss: 22.842763207428256\n",
      "Iteracion: 13501 Gradiente: [0.0003068307520900741,-0.005293658546048855] Loss: 22.84276317928885\n",
      "Iteracion: 13502 Gradiente: [0.0003066676314404049,-0.005290844274678245] Loss: 22.84276315117935\n",
      "Iteracion: 13503 Gradiente: [0.00030650459753284545,-0.005288031499460052] Loss: 22.842763123099743\n",
      "Iteracion: 13504 Gradiente: [0.0003063416503039207,-0.005285220219598467] Loss: 22.842763095049985\n",
      "Iteracion: 13505 Gradiente: [0.00030617878967499715,-0.005282410434301828] Loss: 22.842763067030027\n",
      "Iteracion: 13506 Gradiente: [0.0003060160156820757,-0.005279602142769591] Loss: 22.842763039039866\n",
      "Iteracion: 13507 Gradiente: [0.00030585332817546865,-0.005276795344215183] Loss: 22.842763011079455\n",
      "Iteracion: 13508 Gradiente: [0.00030569072709359563,-0.0052739900378438636] Loss: 22.842762983148763\n",
      "Iteracion: 13509 Gradiente: [0.00030552821253593265,-0.0052711862228548515] Loss: 22.842762955247768\n",
      "Iteracion: 13510 Gradiente: [0.0003053657844048985,-0.005268383898458258] Loss: 22.842762927376427\n",
      "Iteracion: 13511 Gradiente: [0.00030520344255270025,-0.005265583063867988] Loss: 22.842762899534712\n",
      "Iteracion: 13512 Gradiente: [0.00030504118705418175,-0.005262783718283378] Loss: 22.842762871722595\n",
      "Iteracion: 13513 Gradiente: [0.00030487901784776263,-0.00525998586091608] Loss: 22.842762843940033\n",
      "Iteracion: 13514 Gradiente: [0.0003047169347837553,-0.005257189490979641] Loss: 22.84276281618701\n",
      "Iteracion: 13515 Gradiente: [0.00030455493786973875,-0.00525439460768086] Loss: 22.842762788463485\n",
      "Iteracion: 13516 Gradiente: [0.00030439302712750307,-0.005251601210224047] Loss: 22.842762760769443\n",
      "Iteracion: 13517 Gradiente: [0.000304231202415887,-0.005248809297825711] Loss: 22.842762733104834\n",
      "Iteracion: 13518 Gradiente: [0.00030406946377562843,-0.005246018869691464] Loss: 22.84276270546962\n",
      "Iteracion: 13519 Gradiente: [0.0003039078110693557,-0.005243229925038051] Loss: 22.842762677863792\n",
      "Iteracion: 13520 Gradiente: [0.00030374624436243873,-0.005240442463069547] Loss: 22.842762650287295\n",
      "Iteracion: 13521 Gradiente: [0.0003035847635241377,-0.00523765648300305] Loss: 22.842762622740118\n",
      "Iteracion: 13522 Gradiente: [0.0003034233685525578,-0.005234871984047847] Loss: 22.84276259522222\n",
      "Iteracion: 13523 Gradiente: [0.00030326205936622347,-0.005232088965418076] Loss: 22.84276256773359\n",
      "Iteracion: 13524 Gradiente: [0.00030310083593766045,-0.0052293074263276415] Loss: 22.842762540274176\n",
      "Iteracion: 13525 Gradiente: [0.00030293969824223644,-0.005226527365986774] Loss: 22.842762512843937\n",
      "Iteracion: 13526 Gradiente: [0.00030277864622121344,-0.005223748783610678] Loss: 22.842762485442858\n",
      "Iteracion: 13527 Gradiente: [0.00030261767985753825,-0.005220971678412786] Loss: 22.842762458070915\n",
      "Iteracion: 13528 Gradiente: [0.0003024567990147868,-0.005218196049611971] Loss: 22.842762430728065\n",
      "Iteracion: 13529 Gradiente: [0.0003022960037166437,-0.005215421896419414] Loss: 22.842762403414277\n",
      "Iteracion: 13530 Gradiente: [0.0003021352938380536,-0.005212649218055295] Loss: 22.842762376129524\n",
      "Iteracion: 13531 Gradiente: [0.00030197466943585977,-0.0052098780137300816] Loss: 22.842762348873766\n",
      "Iteracion: 13532 Gradiente: [0.00030181413041532326,-0.005207108282663834] Loss: 22.842762321646997\n",
      "Iteracion: 13533 Gradiente: [0.00030165367678307574,-0.005204340024067496] Loss: 22.842762294449138\n",
      "Iteracion: 13534 Gradiente: [0.0003014933084491152,-0.00520157323716397] Loss: 22.842762267280218\n",
      "Iteracion: 13535 Gradiente: [0.0003013330253433348,-0.005198807921170238] Loss: 22.842762240140175\n",
      "Iteracion: 13536 Gradiente: [0.0003011728274145753,-0.00519604407530565] Loss: 22.842762213028983\n",
      "Iteracion: 13537 Gradiente: [0.0003010127147689445,-0.005193281698779847] Loss: 22.842762185946615\n",
      "Iteracion: 13538 Gradiente: [0.0003008526871326467,-0.00519052079082473] Loss: 22.842762158893017\n",
      "Iteracion: 13539 Gradiente: [0.0003006927445587356,-0.0051877613506540856] Loss: 22.842762131868177\n",
      "Iteracion: 13540 Gradiente: [0.0003005328871182655,-0.005185003377479329] Loss: 22.84276210487208\n",
      "Iteracion: 13541 Gradiente: [0.0003003731146350219,-0.005182246870529757] Loss: 22.84276207790466\n",
      "Iteracion: 13542 Gradiente: [0.00030021342706826695,-0.005179491829026498] Loss: 22.842762050965927\n",
      "Iteracion: 13543 Gradiente: [0.0003000538244255798,-0.005176738252184758] Loss: 22.842762024055812\n",
      "Iteracion: 13544 Gradiente: [0.000299894306550641,-0.005173986139233833] Loss: 22.842761997174318\n",
      "Iteracion: 13545 Gradiente: [0.00029973487356850607,-0.005171235489386206] Loss: 22.842761970321373\n",
      "Iteracion: 13546 Gradiente: [0.00029957552528401263,-0.005168486301871648] Loss: 22.842761943497\n",
      "Iteracion: 13547 Gradiente: [0.00029941626177200457,-0.005165738575905602] Loss: 22.84276191670113\n",
      "Iteracion: 13548 Gradiente: [0.0002992570828771098,-0.005162992310719024] Loss: 22.842761889933737\n",
      "Iteracion: 13549 Gradiente: [0.0002990979886059601,-0.005160247505530909] Loss: 22.842761863194806\n",
      "Iteracion: 13550 Gradiente: [0.0002989389789377128,-0.005157504159564041] Loss: 22.842761836484286\n",
      "Iteracion: 13551 Gradiente: [0.00029878005380889284,-0.00515476227204393] Loss: 22.842761809802173\n",
      "Iteracion: 13552 Gradiente: [0.0002986212131039186,-0.005152021842199043] Loss: 22.84276178314842\n",
      "Iteracion: 13553 Gradiente: [0.0002984624569450034,-0.005149282869245179] Loss: 22.84276175652299\n",
      "Iteracion: 13554 Gradiente: [0.0002983037851199318,-0.0051465453524177935] Loss: 22.84276172992585\n",
      "Iteracion: 13555 Gradiente: [0.0002981451976182825,-0.00514380929093979] Loss: 22.84276170335702\n",
      "Iteracion: 13556 Gradiente: [0.0002979866945073203,-0.005141074684030873] Loss: 22.8427616768164\n",
      "Iteracion: 13557 Gradiente: [0.0002978282756932534,-0.0051383415309215275] Loss: 22.842761650304013\n",
      "Iteracion: 13558 Gradiente: [0.00029766994106523726,-0.005135609830842351] Loss: 22.84276162381979\n",
      "Iteracion: 13559 Gradiente: [0.0002975116905455858,-0.0051328795830224054] Loss: 22.84276159736373\n",
      "Iteracion: 13560 Gradiente: [0.00029735352414851,-0.00513015078668649] Loss: 22.842761570935796\n",
      "Iteracion: 13561 Gradiente: [0.000297195441823798,-0.005127423441063191] Loss: 22.842761544535954\n",
      "Iteracion: 13562 Gradiente: [0.00029703744366050465,-0.005124697545374938] Loss: 22.842761518164163\n",
      "Iteracion: 13563 Gradiente: [0.0002968795293962027,-0.005121973098860858] Loss: 22.842761491820408\n",
      "Iteracion: 13564 Gradiente: [0.0002967216991379473,-0.005119250100743618] Loss: 22.842761465504665\n",
      "Iteracion: 13565 Gradiente: [0.00029656395278910473,-0.005116528550254173] Loss: 22.84276143921688\n",
      "Iteracion: 13566 Gradiente: [0.0002964062903762018,-0.005113808446622178] Loss: 22.842761412957053\n",
      "Iteracion: 13567 Gradiente: [0.0002962487116822861,-0.005111089789084863] Loss: 22.842761386725137\n",
      "Iteracion: 13568 Gradiente: [0.00029609121670167343,-0.005108372576872711] Loss: 22.8427613605211\n",
      "Iteracion: 13569 Gradiente: [0.0002959338055925779,-0.005105656809207204] Loss: 22.84276133434492\n",
      "Iteracion: 13570 Gradiente: [0.0002957764780622559,-0.005102942485332562] Loss: 22.842761308196565\n",
      "Iteracion: 13571 Gradiente: [0.00029561923422060467,-0.005100229604473109] Loss: 22.842761282076005\n",
      "Iteracion: 13572 Gradiente: [0.0002954620739918331,-0.005097518165862643] Loss: 22.842761255983213\n",
      "Iteracion: 13573 Gradiente: [0.000295304997320045,-0.005094808168737685] Loss: 22.842761229918153\n",
      "Iteracion: 13574 Gradiente: [0.00029514800407639543,-0.005092099612334048] Loss: 22.8427612038808\n",
      "Iteracion: 13575 Gradiente: [0.00029499109439162416,-0.005089392495875463] Loss: 22.842761177871136\n",
      "Iteracion: 13576 Gradiente: [0.0002948342679928828,-0.005086686818611952] Loss: 22.842761151889096\n",
      "Iteracion: 13577 Gradiente: [0.00029467752510091335,-0.005083982579761681] Loss: 22.842761125934697\n",
      "Iteracion: 13578 Gradiente: [0.00029452086547886814,-0.00508127977857112] Loss: 22.842761100007863\n",
      "Iteracion: 13579 Gradiente: [0.00029436428916748507,-0.00507857841427004] Loss: 22.842761074108616\n",
      "Iteracion: 13580 Gradiente: [0.0002942077960843411,-0.005075878486098162] Loss: 22.84276104823687\n",
      "Iteracion: 13581 Gradiente: [0.0002940513861110124,-0.005073179993297335] Loss: 22.842761022392644\n",
      "Iteracion: 13582 Gradiente: [0.0002938950593810811,-0.005070482935092594] Loss: 22.842760996575898\n",
      "Iteracion: 13583 Gradiente: [0.0002937388157420173,-0.005067787310727449] Loss: 22.842760970786586\n",
      "Iteracion: 13584 Gradiente: [0.00029358265519287367,-0.005065093119436289] Loss: 22.842760945024676\n",
      "Iteracion: 13585 Gradiente: [0.00029342657761901594,-0.005062400360463097] Loss: 22.842760919290146\n",
      "Iteracion: 13586 Gradiente: [0.00029327058299865406,-0.005059709033045578] Loss: 22.842760893582994\n",
      "Iteracion: 13587 Gradiente: [0.00029311467137442077,-0.005057019136415993] Loss: 22.842760867903163\n",
      "Iteracion: 13588 Gradiente: [0.0002929588426127339,-0.005054330669819862] Loss: 22.84276084225063\n",
      "Iteracion: 13589 Gradiente: [0.00029280309674959424,-0.005051643632491576] Loss: 22.842760816625358\n",
      "Iteracion: 13590 Gradiente: [0.0002926474337158425,-0.005048958023672393] Loss: 22.84276079102732\n",
      "Iteracion: 13591 Gradiente: [0.0002924918533371586,-0.005046273842611745] Loss: 22.84276076545649\n",
      "Iteracion: 13592 Gradiente: [0.0002923363556514384,-0.00504359108854544] Loss: 22.842760739912872\n",
      "Iteracion: 13593 Gradiente: [0.000292180940707946,-0.005040909760708464] Loss: 22.842760714396384\n",
      "Iteracion: 13594 Gradiente: [0.00029202560828878177,-0.005038229858354507] Loss: 22.842760688907017\n",
      "Iteracion: 13595 Gradiente: [0.0002918703584432099,-0.00503555138071808] Loss: 22.842760663444732\n",
      "Iteracion: 13596 Gradiente: [0.00029171519119586264,-0.005032874327039849] Loss: 22.842760638009548\n",
      "Iteracion: 13597 Gradiente: [0.00029156010650316,-0.005030198696561309] Loss: 22.842760612601367\n",
      "Iteracion: 13598 Gradiente: [0.0002914051042533098,-0.005027524488530114] Loss: 22.842760587220223\n",
      "Iteracion: 13599 Gradiente: [0.0002912501843345202,-0.005024851702193919] Loss: 22.842760561866037\n",
      "Iteracion: 13600 Gradiente: [0.00029109534677426534,-0.0050221803367935065] Loss: 22.842760536538815\n",
      "Iteracion: 13601 Gradiente: [0.0002909405915810718,-0.005019510391570374] Loss: 22.842760511238506\n",
      "Iteracion: 13602 Gradiente: [0.0002907859186090415,-0.005016841865774779] Loss: 22.842760485965105\n",
      "Iteracion: 13603 Gradiente: [0.0002906313278780696,-0.005014174758647864] Loss: 22.842760460718555\n",
      "Iteracion: 13604 Gradiente: [0.00029047681936257655,-0.005011509069436215] Loss: 22.842760435498835\n",
      "Iteracion: 13605 Gradiente: [0.0002903223929896133,-0.005008844797387131] Loss: 22.842760410305928\n",
      "Iteracion: 13606 Gradiente: [0.0002901680486142292,-0.005006181941752885] Loss: 22.84276038513982\n",
      "Iteracion: 13607 Gradiente: [0.0002900137864145336,-0.005003520501765972] Loss: 22.84276036000044\n",
      "Iteracion: 13608 Gradiente: [0.0002898596062029431,-0.005000860476684821] Loss: 22.842760334887803\n",
      "Iteracion: 13609 Gradiente: [0.00028970550797282615,-0.0049982018657538894] Loss: 22.842760309801843\n",
      "Iteracion: 13610 Gradiente: [0.00028955149160386404,-0.004995544668223673] Loss: 22.842760284742557\n",
      "Iteracion: 13611 Gradiente: [0.0002893975571083729,-0.004992888883344903] Loss: 22.8427602597099\n",
      "Iteracion: 13612 Gradiente: [0.00028924370450909007,-0.004990234510357657] Loss: 22.842760234703874\n",
      "Iteracion: 13613 Gradiente: [0.00028908993365917013,-0.004987581548518942] Loss: 22.842760209724407\n",
      "Iteracion: 13614 Gradiente: [0.00028893624450461176,-0.0049849299970788985] Loss: 22.842760184771496\n",
      "Iteracion: 13615 Gradiente: [0.00028878263720457655,-0.004982279855276299] Loss: 22.842760159845106\n",
      "Iteracion: 13616 Gradiente: [0.0002886291115013743,-0.0049796311223753754] Loss: 22.842760134945227\n",
      "Iteracion: 13617 Gradiente: [0.0002884756673533199,-0.004976983797626744] Loss: 22.842760110071804\n",
      "Iteracion: 13618 Gradiente: [0.00028832230488357404,-0.004974337880268583] Loss: 22.84276008522481\n",
      "Iteracion: 13619 Gradiente: [0.00028816902389886917,-0.004971693369566073] Loss: 22.842760060404263\n",
      "Iteracion: 13620 Gradiente: [0.0002880158243802574,-0.004969050264766277] Loss: 22.842760035610066\n",
      "Iteracion: 13621 Gradiente: [0.00028786270635995,-0.004966408565119096] Loss: 22.84276001084224\n",
      "Iteracion: 13622 Gradiente: [0.0002877096697308919,-0.004963768269883199] Loss: 22.84275998610075\n",
      "Iteracion: 13623 Gradiente: [0.00028755671442771323,-0.004961129378309792] Loss: 22.84275996138554\n",
      "Iteracion: 13624 Gradiente: [0.00028740384041062346,-0.004958491889655174] Loss: 22.842759936696613\n",
      "Iteracion: 13625 Gradiente: [0.0002872510476616223,-0.004955855803169248] Loss: 22.842759912033934\n",
      "Iteracion: 13626 Gradiente: [0.0002870983362621852,-0.004953221118100615] Loss: 22.842759887397452\n",
      "Iteracion: 13627 Gradiente: [0.00028694570600578117,-0.004950587833713864] Loss: 22.842759862787172\n",
      "Iteracion: 13628 Gradiente: [0.0002867931567976711,-0.004947955949269082] Loss: 22.842759838203044\n",
      "Iteracion: 13629 Gradiente: [0.0002866406887581737,-0.004945325464010016] Loss: 22.84275981364507\n",
      "Iteracion: 13630 Gradiente: [0.0002864883018020237,-0.004942696377195332] Loss: 22.842759789113178\n",
      "Iteracion: 13631 Gradiente: [0.0002863359958543773,-0.00494006868808512] Loss: 22.842759764607376\n",
      "Iteracion: 13632 Gradiente: [0.0002861837708441802,-0.004937442395936386] Loss: 22.842759740127615\n",
      "Iteracion: 13633 Gradiente: [0.00028603162675058986,-0.004934817500005787] Loss: 22.842759715673886\n",
      "Iteracion: 13634 Gradiente: [0.00028587956349402553,-0.004932193999553173] Loss: 22.84275969124614\n",
      "Iteracion: 13635 Gradiente: [0.0002857275811673314,-0.004929571893829868] Loss: 22.84275966684437\n",
      "Iteracion: 13636 Gradiente: [0.00028557567959429283,-0.0049269511820999885] Loss: 22.84275964246852\n",
      "Iteracion: 13637 Gradiente: [0.0002854238588052264,-0.004924331863619595] Loss: 22.842759618118592\n",
      "Iteracion: 13638 Gradiente: [0.0002852721187868686,-0.004921713937646881] Loss: 22.842759593794565\n",
      "Iteracion: 13639 Gradiente: [0.0002851204593194249,-0.004919097403449039] Loss: 22.84275956949636\n",
      "Iteracion: 13640 Gradiente: [0.00028496888057342556,-0.004916482260277396] Loss: 22.842759545224006\n",
      "Iteracion: 13641 Gradiente: [0.00028481738235086596,-0.004913868507398552] Loss: 22.842759520977438\n",
      "Iteracion: 13642 Gradiente: [0.00028466596468301,-0.004911256144069872] Loss: 22.842759496756663\n",
      "Iteracion: 13643 Gradiente: [0.0002845146275916477,-0.004908645169549785] Loss: 22.842759472561625\n",
      "Iteracion: 13644 Gradiente: [0.0002843633708550897,-0.004906035583109277] Loss: 22.842759448392307\n",
      "Iteracion: 13645 Gradiente: [0.0002842121945330215,-0.004903427384005118] Loss: 22.842759424248676\n",
      "Iteracion: 13646 Gradiente: [0.0002840610986292328,-0.004900820571497396] Loss: 22.842759400130713\n",
      "Iteracion: 13647 Gradiente: [0.0002839100830205628,-0.0048982151448534236] Loss: 22.842759376038384\n",
      "Iteracion: 13648 Gradiente: [0.0002837591476577472,-0.004895611103334118] Loss: 22.84275935197166\n",
      "Iteracion: 13649 Gradiente: [0.00028360829260899815,-0.004893008446200516] Loss: 22.84275932793054\n",
      "Iteracion: 13650 Gradiente: [0.0002834575177047327,-0.0048904071727226515] Loss: 22.842759303914946\n",
      "Iteracion: 13651 Gradiente: [0.0002833068230377952,-0.00488780728215706] Loss: 22.842759279924906\n",
      "Iteracion: 13652 Gradiente: [0.0002831562083902857,-0.004885208773777686] Loss: 22.842759255960345\n",
      "Iteracion: 13653 Gradiente: [0.00028300567385789084,-0.004882611646843552] Loss: 22.84275923202127\n",
      "Iteracion: 13654 Gradiente: [0.0002828552193790301,-0.004880015900620312] Loss: 22.84275920810764\n",
      "Iteracion: 13655 Gradiente: [0.00028270484483906936,-0.004877421534377646] Loss: 22.842759184219425\n",
      "Iteracion: 13656 Gradiente: [0.00028255455028443067,-0.0048748285473773] Loss: 22.842759160356618\n",
      "Iteracion: 13657 Gradiente: [0.0002824043356525863,-0.004872236938887179] Loss: 22.842759136519167\n",
      "Iteracion: 13658 Gradiente: [0.00028225420082132285,-0.004869646708179213] Loss: 22.842759112707054\n",
      "Iteracion: 13659 Gradiente: [0.0002821041458076934,-0.004867057854516924] Loss: 22.84275908892025\n",
      "Iteracion: 13660 Gradiente: [0.0002819541704885372,-0.004864470377174849] Loss: 22.842759065158727\n",
      "Iteracion: 13661 Gradiente: [0.0002818042750827014,-0.004861884275402891] Loss: 22.84275904142247\n",
      "Iteracion: 13662 Gradiente: [0.00028165445929933717,-0.004859299548487428] Loss: 22.84275901771144\n",
      "Iteracion: 13663 Gradiente: [0.0002815047231100228,-0.004856716195694351] Loss: 22.84275899402562\n",
      "Iteracion: 13664 Gradiente: [0.0002813550665554961,-0.0048541342162897886] Loss: 22.84275897036496\n",
      "Iteracion: 13665 Gradiente: [0.00028120548957607144,-0.004851553609543184] Loss: 22.842758946729486\n",
      "Iteracion: 13666 Gradiente: [0.00028105599213669544,-0.004848974374724927] Loss: 22.8427589231191\n",
      "Iteracion: 13667 Gradiente: [0.000280906574164419,-0.004846396511108727] Loss: 22.842758899533838\n",
      "Iteracion: 13668 Gradiente: [0.0002807572355806087,-0.004843820017966039] Loss: 22.842758875973633\n",
      "Iteracion: 13669 Gradiente: [0.0002806079763596851,-0.0048412448945680826] Loss: 22.84275885243848\n",
      "Iteracion: 13670 Gradiente: [0.0002804587965196485,-0.004838671140182763] Loss: 22.842758828928318\n",
      "Iteracion: 13671 Gradiente: [0.00028030969611734233,-0.004836098754076446] Loss: 22.84275880544317\n",
      "Iteracion: 13672 Gradiente: [0.00028016067483633834,-0.00483352773553906] Loss: 22.842758781982997\n",
      "Iteracion: 13673 Gradiente: [0.0002800117328225345,-0.004830958083829747] Loss: 22.84275875854773\n",
      "Iteracion: 13674 Gradiente: [0.000279862869949928,-0.004828389798230148] Loss: 22.8427587351374\n",
      "Iteracion: 13675 Gradiente: [0.0002797140862469405,-0.004825822878008286] Loss: 22.84275871175195\n",
      "Iteracion: 13676 Gradiente: [0.0002795653816406229,-0.004823257322439881] Loss: 22.84275868839135\n",
      "Iteracion: 13677 Gradiente: [0.0002794167560769741,-0.004820693130799588] Loss: 22.84275866505559\n",
      "Iteracion: 13678 Gradiente: [0.0002792682095522044,-0.004818130302362059] Loss: 22.842758641744634\n",
      "Iteracion: 13679 Gradiente: [0.0002791197420587347,-0.0048155688363985165] Loss: 22.842758618458447\n",
      "Iteracion: 13680 Gradiente: [0.0002789713534970891,-0.004813008732188943] Loss: 22.842758595197026\n",
      "Iteracion: 13681 Gradiente: [0.00027882304371757983,-0.004810449989014742] Loss: 22.84275857196032\n",
      "Iteracion: 13682 Gradiente: [0.0002786748128424203,-0.004807892606145122] Loss: 22.84275854874833\n",
      "Iteracion: 13683 Gradiente: [0.0002785266608308727,-0.004805336582854025] Loss: 22.842758525560996\n",
      "Iteracion: 13684 Gradiente: [0.00027837858736650864,-0.004802781918436594] Loss: 22.842758502398315\n",
      "Iteracion: 13685 Gradiente: [0.00027823059287186426,-0.004800228612142495] Loss: 22.842758479260276\n",
      "Iteracion: 13686 Gradiente: [0.00027808267698977335,-0.004797676663267225] Loss: 22.842758456146804\n",
      "Iteracion: 13687 Gradiente: [0.0002779348397448681,-0.004795126071087689] Loss: 22.842758433057917\n",
      "Iteracion: 13688 Gradiente: [0.00027778708095619703,-0.00479257683488908] Loss: 22.842758409993564\n",
      "Iteracion: 13689 Gradiente: [0.0002776394008359754,-0.004790028953935277] Loss: 22.842758386953722\n",
      "Iteracion: 13690 Gradiente: [0.0002774917992212522,-0.004787482427513131] Loss: 22.842758363938383\n",
      "Iteracion: 13691 Gradiente: [0.0002773442760097093,-0.004784937254907125] Loss: 22.842758340947515\n",
      "Iteracion: 13692 Gradiente: [0.0002771968313027173,-0.0047823934353876515] Loss: 22.842758317981072\n",
      "Iteracion: 13693 Gradiente: [0.00027704946500553735,-0.004779850968241443] Loss: 22.84275829503905\n",
      "Iteracion: 13694 Gradiente: [0.00027690217702153555,-0.004777309852748601] Loss: 22.84275827212141\n",
      "Iteracion: 13695 Gradiente: [0.0002767549672843946,-0.004774770088194795] Loss: 22.84275824922814\n",
      "Iteracion: 13696 Gradiente: [0.00027660783583674704,-0.00477223167385669] Loss: 22.842758226359194\n",
      "Iteracion: 13697 Gradiente: [0.00027646078267006636,-0.004769694609012376] Loss: 22.842758203514553\n",
      "Iteracion: 13698 Gradiente: [0.00027631380757687417,-0.004767158892956047] Loss: 22.842758180694204\n",
      "Iteracion: 13699 Gradiente: [0.0002761669106585411,-0.0047646245249625945] Loss: 22.842758157898114\n",
      "Iteracion: 13700 Gradiente: [0.0002760200918155912,-0.004762091504319225] Loss: 22.84275813512626\n",
      "Iteracion: 13701 Gradiente: [0.00027587335108781495,-0.0047595598303022514] Loss: 22.842758112378604\n",
      "Iteracion: 13702 Gradiente: [0.00027572668835773584,-0.004757029502202196] Loss: 22.84275808965513\n",
      "Iteracion: 13703 Gradiente: [0.0002755801036319857,-0.004754500519300227] Loss: 22.842758066955806\n",
      "Iteracion: 13704 Gradiente: [0.00027543359675424504,-0.004751972880887223] Loss: 22.842758044280615\n",
      "Iteracion: 13705 Gradiente: [0.00027528716776904126,-0.004749446586244233] Loss: 22.842758021629535\n",
      "Iteracion: 13706 Gradiente: [0.0002751408167370073,-0.004746921634652187] Loss: 22.842757999002522\n",
      "Iteracion: 13707 Gradiente: [0.00027499454337108394,-0.004744398025408714] Loss: 22.842757976399565\n",
      "Iteracion: 13708 Gradiente: [0.0002748483478152745,-0.004741875757791666] Loss: 22.842757953820634\n",
      "Iteracion: 13709 Gradiente: [0.00027470222998810336,-0.0047393548310886045] Loss: 22.8427579312657\n",
      "Iteracion: 13710 Gradiente: [0.00027455618978725245,-0.004736835244590646] Loss: 22.842757908734743\n",
      "Iteracion: 13711 Gradiente: [0.00027441022735577765,-0.004734316997575287] Loss: 22.84275788622774\n",
      "Iteracion: 13712 Gradiente: [0.00027426434246156836,-0.004731800089340155] Loss: 22.842757863744662\n",
      "Iteracion: 13713 Gradiente: [0.00027411853504872853,-0.004729284519175773] Loss: 22.84275784128548\n",
      "Iteracion: 13714 Gradiente: [0.00027397280530010447,-0.004726770286356204] Loss: 22.842757818850178\n",
      "Iteracion: 13715 Gradiente: [0.0002738271529570587,-0.004724257390183932] Loss: 22.84275779643872\n",
      "Iteracion: 13716 Gradiente: [0.00027368157803664417,-0.0047217458299449785] Loss: 22.842757774051076\n",
      "Iteracion: 13717 Gradiente: [0.00027353608046970144,-0.004719235604931882] Loss: 22.842757751687238\n",
      "Iteracion: 13718 Gradiente: [0.0002733906603784438,-0.004716726714422847] Loss: 22.84275772934717\n",
      "Iteracion: 13719 Gradiente: [0.0002732453174994968,-0.004714219157723202] Loss: 22.842757707030845\n",
      "Iteracion: 13720 Gradiente: [0.0002731000518840195,-0.00471171293411885] Loss: 22.84275768473825\n",
      "Iteracion: 13721 Gradiente: [0.00027295486349506365,-0.004709208042899131] Loss: 22.842757662469353\n",
      "Iteracion: 13722 Gradiente: [0.00027280975226820676,-0.0047067044833577635] Loss: 22.842757640224125\n",
      "Iteracion: 13723 Gradiente: [0.0002726647182290283,-0.0047042022547837325] Loss: 22.842757618002544\n",
      "Iteracion: 13724 Gradiente: [0.00027251976135763317,-0.004701701356467443] Loss: 22.84275759580457\n",
      "Iteracion: 13725 Gradiente: [0.00027237488146264846,-0.004699201787709366] Loss: 22.84275757363021\n",
      "Iteracion: 13726 Gradiente: [0.0002722300786208128,-0.004696703547797654] Loss: 22.84275755147941\n",
      "Iteracion: 13727 Gradiente: [0.0002720853527279132,-0.004694206636029582] Loss: 22.842757529352163\n",
      "Iteracion: 13728 Gradiente: [0.00027194070381616103,-0.00469171105169354] Loss: 22.84275750724844\n",
      "Iteracion: 13729 Gradiente: [0.00027179613183818673,-0.004689216794084554] Loss: 22.842757485168196\n",
      "Iteracion: 13730 Gradiente: [0.00027165163668030345,-0.004686723862501551] Loss: 22.842757463111443\n",
      "Iteracion: 13731 Gradiente: [0.00027150721828661517,-0.0046842322562413346] Loss: 22.842757441078113\n",
      "Iteracion: 13732 Gradiente: [0.0002713628767054388,-0.00468174197459336] Loss: 22.842757419068217\n",
      "Iteracion: 13733 Gradiente: [0.00027121861192445825,-0.004679253016851348] Loss: 22.84275739708172\n",
      "Iteracion: 13734 Gradiente: [0.0002710744237589324,-0.004676765382320743] Loss: 22.84275737511859\n",
      "Iteracion: 13735 Gradiente: [0.00027093031219465046,-0.004674279070294792] Loss: 22.84275735317882\n",
      "Iteracion: 13736 Gradiente: [0.0002707862774229852,-0.004671794080060228] Loss: 22.842757331262355\n",
      "Iteracion: 13737 Gradiente: [0.0002706423191426666,-0.0046693104109271156] Loss: 22.842757309369187\n",
      "Iteracion: 13738 Gradiente: [0.00027049843729874586,-0.004666828062193791] Loss: 22.842757287499307\n",
      "Iteracion: 13739 Gradiente: [0.00027035463207975377,-0.004664347033145333] Loss: 22.84275726565266\n",
      "Iteracion: 13740 Gradiente: [0.0002702109032895805,-0.004661867323090263] Loss: 22.842757243829233\n",
      "Iteracion: 13741 Gradiente: [0.0002700672508694879,-0.0046593889313268026] Loss: 22.84275722202901\n",
      "Iteracion: 13742 Gradiente: [0.0002699236748687402,-0.004656911857149737] Loss: 22.842757200251956\n",
      "Iteracion: 13743 Gradiente: [0.0002697801750647007,-0.004654436099869367] Loss: 22.84275717849805\n",
      "Iteracion: 13744 Gradiente: [0.00026963675167621657,-0.0046519616587697024] Loss: 22.84275715676727\n",
      "Iteracion: 13745 Gradiente: [0.00026949340451665194,-0.004649488533159977] Loss: 22.842757135059593\n",
      "Iteracion: 13746 Gradiente: [0.00026935013355663766,-0.004647016722340898] Loss: 22.842757113374983\n",
      "Iteracion: 13747 Gradiente: [0.00026920693873648814,-0.004644546225613529] Loss: 22.842757091713434\n",
      "Iteracion: 13748 Gradiente: [0.0002690638200609404,-0.004642077042277867] Loss: 22.842757070074907\n",
      "Iteracion: 13749 Gradiente: [0.00026892077744188704,-0.0046396091716366305] Loss: 22.842757048459376\n",
      "Iteracion: 13750 Gradiente: [0.0002687778109238555,-0.004637142612987451] Loss: 22.84275702686682\n",
      "Iteracion: 13751 Gradiente: [0.0002686349204045276,-0.0046346773656373115] Loss: 22.842757005297226\n",
      "Iteracion: 13752 Gradiente: [0.0002684921058374812,-0.0046322134288892865] Loss: 22.842756983750554\n",
      "Iteracion: 13753 Gradiente: [0.00026834936720661063,-0.004629750802045389] Loss: 22.842756962226797\n",
      "Iteracion: 13754 Gradiente: [0.0002682067044446512,-0.00462728948441035] Loss: 22.84275694072591\n",
      "Iteracion: 13755 Gradiente: [0.00026806411752697084,-0.004624829475286892] Loss: 22.84275691924787\n",
      "Iteracion: 13756 Gradiente: [0.0002679216064175686,-0.004622370773979393] Loss: 22.84275689779268\n",
      "Iteracion: 13757 Gradiente: [0.0002677791710169686,-0.004619913379796022] Loss: 22.842756876360266\n",
      "Iteracion: 13758 Gradiente: [0.0002676368114066463,-0.0046174572920352356] Loss: 22.842756854950657\n",
      "Iteracion: 13759 Gradiente: [0.00026749452742365066,-0.004615002510009703] Loss: 22.842756833563797\n",
      "Iteracion: 13760 Gradiente: [0.0002673523191693524,-0.004612549033016222] Loss: 22.842756812199696\n",
      "Iteracion: 13761 Gradiente: [0.00026721018642490436,-0.004610096860370779] Loss: 22.842756790858274\n",
      "Iteracion: 13762 Gradiente: [0.00026706812933525726,-0.004607645991372659] Loss: 22.842756769539545\n",
      "Iteracion: 13763 Gradiente: [0.00026692614768156393,-0.004605196425334649] Loss: 22.842756748243485\n",
      "Iteracion: 13764 Gradiente: [0.00026678424159645904,-0.004602748161557457] Loss: 22.84275672697006\n",
      "Iteracion: 13765 Gradiente: [0.00026664241091699145,-0.004600301199353752] Loss: 22.842756705719243\n",
      "Iteracion: 13766 Gradiente: [0.00026650065563937154,-0.004597855538030634] Loss: 22.84275668449102\n",
      "Iteracion: 13767 Gradiente: [0.0002663589757806525,-0.004595411176892602] Loss: 22.842756663285364\n",
      "Iteracion: 13768 Gradiente: [0.0002662173710821965,-0.004592968115260376] Loss: 22.84275664210224\n",
      "Iteracion: 13769 Gradiente: [0.00026607584182158917,-0.0045905263524264935] Loss: 22.842756620941643\n",
      "Iteracion: 13770 Gradiente: [0.0002659343876786124,-0.004588085887716412] Loss: 22.842756599803536\n",
      "Iteracion: 13771 Gradiente: [0.0002657930088219018,-0.004585646720426458] Loss: 22.8427565786879\n",
      "Iteracion: 13772 Gradiente: [0.00026565170514156005,-0.004583208849871312] Loss: 22.842756557594694\n",
      "Iteracion: 13773 Gradiente: [0.0002655104765409533,-0.004580772275365774] Loss: 22.84275653652393\n",
      "Iteracion: 13774 Gradiente: [0.00026536932302766066,-0.004578336996218842] Loss: 22.842756515475553\n",
      "Iteracion: 13775 Gradiente: [0.00026522824453441746,-0.004575903011741407] Loss: 22.842756494449542\n",
      "Iteracion: 13776 Gradiente: [0.00026508724102995983,-0.004573470321246139] Loss: 22.84275647344591\n",
      "Iteracion: 13777 Gradiente: [0.00026494631247639215,-0.004571038924043691] Loss: 22.842756452464585\n",
      "Iteracion: 13778 Gradiente: [0.00026480545887087225,-0.004568608819445786] Loss: 22.842756431505553\n",
      "Iteracion: 13779 Gradiente: [0.0002646646801698201,-0.004566180006764734] Loss: 22.842756410568814\n",
      "Iteracion: 13780 Gradiente: [0.0002645239762898655,-0.004563752485316286] Loss: 22.84275638965433\n",
      "Iteracion: 13781 Gradiente: [0.0002643833472186922,-0.004561326254412161] Loss: 22.84275636876207\n",
      "Iteracion: 13782 Gradiente: [0.00026424279290608864,-0.004558901313367395] Loss: 22.84275634789203\n",
      "Iteracion: 13783 Gradiente: [0.0002641023132762636,-0.0045564776614976195] Loss: 22.842756327044153\n",
      "Iteracion: 13784 Gradiente: [0.00026396190838416563,-0.004554055298113013] Loss: 22.84275630621847\n",
      "Iteracion: 13785 Gradiente: [0.0002638215781824253,-0.00455163422252743] Loss: 22.842756285414897\n",
      "Iteracion: 13786 Gradiente: [0.0002636813225971461,-0.004549214434059934] Loss: 22.842756264633454\n",
      "Iteracion: 13787 Gradiente: [0.0002635411414833773,-0.004546795932031245] Loss: 22.842756243874085\n",
      "Iteracion: 13788 Gradiente: [0.0002634010349026994,-0.004544378715752491] Loss: 22.842756223136792\n",
      "Iteracion: 13789 Gradiente: [0.00026326100274237283,-0.004541962784542856] Loss: 22.842756202421548\n",
      "Iteracion: 13790 Gradiente: [0.0002631210451350322,-0.004539548137711454] Loss: 22.84275618172831\n",
      "Iteracion: 13791 Gradiente: [0.0002629811619499378,-0.004537134774579244] Loss: 22.842756161057086\n",
      "Iteracion: 13792 Gradiente: [0.0002628413530989822,-0.004534722694465524] Loss: 22.842756140407825\n",
      "Iteracion: 13793 Gradiente: [0.0002627016185717442,-0.004532311896687702] Loss: 22.842756119780525\n",
      "Iteracion: 13794 Gradiente: [0.00026256195827916904,-0.004529902380568629] Loss: 22.84275609917513\n",
      "Iteracion: 13795 Gradiente: [0.00026242237237473397,-0.004527494145414224] Loss: 22.842756078591645\n",
      "Iteracion: 13796 Gradiente: [0.00026228286059316966,-0.004525087190556102] Loss: 22.842756058030048\n",
      "Iteracion: 13797 Gradiente: [0.00026214342296100314,-0.00452268151530942] Loss: 22.84275603749029\n",
      "Iteracion: 13798 Gradiente: [0.00026200405946307605,-0.004520277118994187] Loss: 22.84275601697239\n",
      "Iteracion: 13799 Gradiente: [0.00026186477001980774,-0.004517874000930533] Loss: 22.84275599647628\n",
      "Iteracion: 13800 Gradiente: [0.0002617255546340402,-0.00451547216043977] Loss: 22.84275597600197\n",
      "Iteracion: 13801 Gradiente: [0.0002615864133559853,-0.004513071596834569] Loss: 22.842755955549414\n",
      "Iteracion: 13802 Gradiente: [0.0002614473459402689,-0.004510672309449267] Loss: 22.84275593511861\n",
      "Iteracion: 13803 Gradiente: [0.0002613083525413155,-0.004508274297592744] Loss: 22.842755914709524\n",
      "Iteracion: 13804 Gradiente: [0.00026116943303785927,-0.004505877560593774] Loss: 22.842755894322128\n",
      "Iteracion: 13805 Gradiente: [0.00026103058735505634,-0.004503482097774617] Loss: 22.842755873956385\n",
      "Iteracion: 13806 Gradiente: [0.0002608918154559584,-0.004501087908459904] Loss: 22.842755853612303\n",
      "Iteracion: 13807 Gradiente: [0.0002607531173358287,-0.004498694991969406] Loss: 22.842755833289857\n",
      "Iteracion: 13808 Gradiente: [0.0002606144929795088,-0.004496303347626451] Loss: 22.842755812989008\n",
      "Iteracion: 13809 Gradiente: [0.00026047594236236666,-0.004493912974750695] Loss: 22.842755792709735\n",
      "Iteracion: 13810 Gradiente: [0.00026033746537734714,-0.004491523872672569] Loss: 22.842755772452037\n",
      "Iteracion: 13811 Gradiente: [0.0002601990619505538,-0.004489136040717648] Loss: 22.84275575221584\n",
      "Iteracion: 13812 Gradiente: [0.00026006073220893693,-0.0044867494782013274] Loss: 22.842755732001173\n",
      "Iteracion: 13813 Gradiente: [0.00025992247585785816,-0.004484364184464222] Loss: 22.842755711807985\n",
      "Iteracion: 13814 Gradiente: [0.00025978429315974457,-0.0044819801588127225] Loss: 22.842755691636263\n",
      "Iteracion: 13815 Gradiente: [0.00025964618384459,-0.004479597400587565] Loss: 22.84275567148599\n",
      "Iteracion: 13816 Gradiente: [0.000259508147923763,-0.00447721590911101] Loss: 22.842755651357134\n",
      "Iteracion: 13817 Gradiente: [0.00025937018538400023,-0.0044748356837091075] Loss: 22.84275563124967\n",
      "Iteracion: 13818 Gradiente: [0.00025923229628688205,-0.00447245672370246] Loss: 22.842755611163582\n",
      "Iteracion: 13819 Gradiente: [0.0002590944804533516,-0.00447007902842671] Loss: 22.842755591098854\n",
      "Iteracion: 13820 Gradiente: [0.0002589567378161443,-0.004467702597210751] Loss: 22.84275557105544\n",
      "Iteracion: 13821 Gradiente: [0.0002588190684434721,-0.004465327429376132] Loss: 22.842755551033328\n",
      "Iteracion: 13822 Gradiente: [0.00025868147233722996,-0.004462953524250442] Loss: 22.842755531032523\n",
      "Iteracion: 13823 Gradiente: [0.00025854394937709915,-0.004460580881163523] Loss: 22.842755511052964\n",
      "Iteracion: 13824 Gradiente: [0.00025840649944370854,-0.004458209499453502] Loss: 22.842755491094643\n",
      "Iteracion: 13825 Gradiente: [0.00025826912257305895,-0.004455839378442287] Loss: 22.84275547115753\n",
      "Iteracion: 13826 Gradiente: [0.0002581318188750477,-0.00445347051745119] Loss: 22.84275545124162\n",
      "Iteracion: 13827 Gradiente: [0.0002579945880161934,-0.0044511029158303] Loss: 22.842755431346884\n",
      "Iteracion: 13828 Gradiente: [0.000257857430178395,-0.004448736572893945] Loss: 22.842755411473284\n",
      "Iteracion: 13829 Gradiente: [0.00025772034526217645,-0.004446371487978595] Loss: 22.842755391620827\n",
      "Iteracion: 13830 Gradiente: [0.0002575833332343791,-0.0044440076604148025] Loss: 22.842755371789455\n",
      "Iteracion: 13831 Gradiente: [0.0002574463939149988,-0.004441645089541879] Loss: 22.842755351979175\n",
      "Iteracion: 13832 Gradiente: [0.0002573095275219354,-0.004439283774676876] Loss: 22.842755332189938\n",
      "Iteracion: 13833 Gradiente: [0.0002571727338903429,-0.0044369237151587505] Loss: 22.842755312421744\n",
      "Iteracion: 13834 Gradiente: [0.00025703601297000965,-0.004434564910321252] Loss: 22.842755292674568\n",
      "Iteracion: 13835 Gradiente: [0.00025689936480072597,-0.004432207359493745] Loss: 22.842755272948384\n",
      "Iteracion: 13836 Gradiente: [0.0002567627891901717,-0.004429851062016254] Loss: 22.84275525324315\n",
      "Iteracion: 13837 Gradiente: [0.0002566262861667686,-0.004427496017221107] Loss: 22.842755233558897\n",
      "Iteracion: 13838 Gradiente: [0.00025648985579209695,-0.004425142224434827] Loss: 22.842755213895536\n",
      "Iteracion: 13839 Gradiente: [0.00025635349788994213,-0.004422789683000753] Loss: 22.842755194253098\n",
      "Iteracion: 13840 Gradiente: [0.0002562172124716729,-0.004420438392251095] Loss: 22.842755174631527\n",
      "Iteracion: 13841 Gradiente: [0.0002560809995136045,-0.004418088351518179] Loss: 22.842755155030805\n",
      "Iteracion: 13842 Gradiente: [0.0002559448590678433,-0.004415739560133858] Loss: 22.842755135450926\n",
      "Iteracion: 13843 Gradiente: [0.00025580879093827966,-0.004413392017442774] Loss: 22.842755115891872\n",
      "Iteracion: 13844 Gradiente: [0.00025567279510596563,-0.004411045722779861] Loss: 22.8427550963536\n",
      "Iteracion: 13845 Gradiente: [0.0002555368715889017,-0.0044087006754777985] Loss: 22.84275507683611\n",
      "Iteracion: 13846 Gradiente: [0.00025540102028476973,-0.004406356874877678] Loss: 22.842755057339335\n",
      "Iteracion: 13847 Gradiente: [0.00025526524128072957,-0.00440401432031076] Loss: 22.842755037863306\n",
      "Iteracion: 13848 Gradiente: [0.00025512953440814576,-0.004401673011120503] Loss: 22.842755018407974\n",
      "Iteracion: 13849 Gradiente: [0.00025499389980912686,-0.004399332946634497] Loss: 22.84275499897333\n",
      "Iteracion: 13850 Gradiente: [0.0002548583372155614,-0.004396994126203898] Loss: 22.842754979559338\n",
      "Iteracion: 13851 Gradiente: [0.000254722846691872,-0.004394656549162098] Loss: 22.84275496016598\n",
      "Iteracion: 13852 Gradiente: [0.000254587428273112,-0.0043923202148432] Loss: 22.842754940793245\n",
      "Iteracion: 13853 Gradiente: [0.000254452081785909,-0.004389985122595519] Loss: 22.842754921441088\n",
      "Iteracion: 13854 Gradiente: [0.0002543168071895252,-0.004387651271758959] Loss: 22.84275490210953\n",
      "Iteracion: 13855 Gradiente: [0.00025418160459196316,-0.004385318661663599] Loss: 22.8427548827985\n",
      "Iteracion: 13856 Gradiente: [0.0002540464737876391,-0.004382987291660475] Loss: 22.842754863507995\n",
      "Iteracion: 13857 Gradiente: [0.0002539114148836082,-0.004380657161080848] Loss: 22.842754844238\n",
      "Iteracion: 13858 Gradiente: [0.0002537764278855548,-0.004378328269264387] Loss: 22.842754824988496\n",
      "Iteracion: 13859 Gradiente: [0.00025364151255378905,-0.004376000615565209] Loss: 22.84275480575944\n",
      "Iteracion: 13860 Gradiente: [0.0002535066689641023,-0.004373674199316469] Loss: 22.842754786550838\n",
      "Iteracion: 13861 Gradiente: [0.0002533718970122815,-0.004371349019863639] Loss: 22.84275476736265\n",
      "Iteracion: 13862 Gradiente: [0.00025323719669832675,-0.004369025076549941] Loss: 22.842754748194853\n",
      "Iteracion: 13863 Gradiente: [0.0002531025679900267,-0.0043667023687153994] Loss: 22.84275472904743\n",
      "Iteracion: 13864 Gradiente: [0.00025296801088643407,-0.004364380895702169] Loss: 22.842754709920367\n",
      "Iteracion: 13865 Gradiente: [0.00025283352529849404,-0.004362060656855249] Loss: 22.842754690813628\n",
      "Iteracion: 13866 Gradiente: [0.00025269911134936744,-0.00435974165151111] Loss: 22.842754671727217\n",
      "Iteracion: 13867 Gradiente: [0.00025256476867051937,-0.004357423879031804] Loss: 22.842754652661064\n",
      "Iteracion: 13868 Gradiente: [0.00025243049754332484,-0.004355107338743736] Loss: 22.84275463361521\n",
      "Iteracion: 13869 Gradiente: [0.00025229629769019844,-0.004352792030007535] Loss: 22.842754614589584\n",
      "Iteracion: 13870 Gradiente: [0.0002521621692712491,-0.004350477952153753] Loss: 22.842754595584193\n",
      "Iteracion: 13871 Gradiente: [0.0002520281121159466,-0.0043481651045376895] Loss: 22.842754576599\n",
      "Iteracion: 13872 Gradiente: [0.0002518941262394492,-0.004345853486501975] Loss: 22.842754557633985\n",
      "Iteracion: 13873 Gradiente: [0.0002517602115583865,-0.004343543097395871] Loss: 22.842754538689128\n",
      "Iteracion: 13874 Gradiente: [0.000251626368107812,-0.004341233936559993] Loss: 22.842754519764405\n",
      "Iteracion: 13875 Gradiente: [0.00025149259576930186,-0.004338926003349286] Loss: 22.842754500859808\n",
      "Iteracion: 13876 Gradiente: [0.00025135889454096135,-0.004336619297106737] Loss: 22.842754481975305\n",
      "Iteracion: 13877 Gradiente: [0.0002512252644464752,-0.004334313817176868] Loss: 22.842754463110865\n",
      "Iteracion: 13878 Gradiente: [0.00025109170546121126,-0.004332009562906572] Loss: 22.842754444266486\n",
      "Iteracion: 13879 Gradiente: [0.0002509582174326397,-0.004329706533650916] Loss: 22.842754425442138\n",
      "Iteracion: 13880 Gradiente: [0.00025082480024896846,-0.004327404728763777] Loss: 22.8427544066378\n",
      "Iteracion: 13881 Gradiente: [0.00025069145409872817,-0.004325104147581863] Loss: 22.84275438785345\n",
      "Iteracion: 13882 Gradiente: [0.0002505581789042329,-0.004322804789454319] Loss: 22.84275436908906\n",
      "Iteracion: 13883 Gradiente: [0.0002504249744684254,-0.004320506653741892] Loss: 22.842754350344627\n",
      "Iteracion: 13884 Gradiente: [0.0002502918408320435,-0.004318209739790764] Loss: 22.84275433162012\n",
      "Iteracion: 13885 Gradiente: [0.000250158778031088,-0.004315914046944158] Loss: 22.842754312915513\n",
      "Iteracion: 13886 Gradiente: [0.0002500257859168187,-0.0043136195745625844] Loss: 22.842754294230783\n",
      "Iteracion: 13887 Gradiente: [0.0002498928646152384,-0.004311326321985831] Loss: 22.842754275565916\n",
      "Iteracion: 13888 Gradiente: [0.00024976001385918303,-0.004309034288580567] Loss: 22.842754256920898\n",
      "Iteracion: 13889 Gradiente: [0.0002496272338030773,-0.004306743473686344] Loss: 22.84275423829569\n",
      "Iteracion: 13890 Gradiente: [0.0002494945243294448,-0.0043044538766598825] Loss: 22.84275421969029\n",
      "Iteracion: 13891 Gradiente: [0.000249361885308493,-0.004302165496858616] Loss: 22.842754201104654\n",
      "Iteracion: 13892 Gradiente: [0.00024922931686433003,-0.004299878333627779] Loss: 22.842754182538783\n",
      "Iteracion: 13893 Gradiente: [0.00024909681887474257,-0.004297592386323146] Loss: 22.84275416399264\n",
      "Iteracion: 13894 Gradiente: [0.0002489643913852054,-0.004295307654296347] Loss: 22.842754145466227\n",
      "Iteracion: 13895 Gradiente: [0.00024883203432561157,-0.0042930241369001955] Loss: 22.842754126959495\n",
      "Iteracion: 13896 Gradiente: [0.0002486997475737477,-0.0042907418334957965] Loss: 22.842754108472427\n",
      "Iteracion: 13897 Gradiente: [0.00024856753109456047,-0.004288460743437384] Loss: 22.842754090005023\n",
      "Iteracion: 13898 Gradiente: [0.00024843538503679,-0.0042861808660684154] Loss: 22.84275407155725\n",
      "Iteracion: 13899 Gradiente: [0.000248303309192958,-0.004283902200755089] Loss: 22.84275405312907\n",
      "Iteracion: 13900 Gradiente: [0.0002481713034702201,-0.004281624746856257] Loss: 22.8427540347205\n",
      "Iteracion: 13901 Gradiente: [0.00024803936798510525,-0.004279348503716799] Loss: 22.842754016331483\n",
      "Iteracion: 13902 Gradiente: [0.0002479075026855071,-0.0042770734706956874] Loss: 22.842753997962042\n",
      "Iteracion: 13903 Gradiente: [0.0002477757074255275,-0.00427479964715675] Loss: 22.842753979612088\n",
      "Iteracion: 13904 Gradiente: [0.00024764398219285035,-0.004272527032453866] Loss: 22.842753961281666\n",
      "Iteracion: 13905 Gradiente: [0.00024751232705568784,-0.004270255625938664] Loss: 22.842753942970717\n",
      "Iteracion: 13906 Gradiente: [0.0002473807419117217,-0.004267985426972368] Loss: 22.842753924679247\n",
      "Iteracion: 13907 Gradiente: [0.00024724922674768855,-0.004265716434914187] Loss: 22.842753906407204\n",
      "Iteracion: 13908 Gradiente: [0.00024711778151148187,-0.004263448649119065] Loss: 22.84275388815458\n",
      "Iteracion: 13909 Gradiente: [0.00024698640604867703,-0.004261182068956278] Loss: 22.842753869921378\n",
      "Iteracion: 13910 Gradiente: [0.00024685510051275137,-0.0042589166937717715] Loss: 22.84275385170754\n",
      "Iteracion: 13911 Gradiente: [0.0002467238647322271,-0.00425665252293328] Loss: 22.842753833513065\n",
      "Iteracion: 13912 Gradiente: [0.00024659269880942245,-0.004254389555791012] Loss: 22.842753815337936\n",
      "Iteracion: 13913 Gradiente: [0.0002464616026031763,-0.004252127791713652] Loss: 22.842753797182134\n",
      "Iteracion: 13914 Gradiente: [0.0002463305759600113,-0.004249867230066447] Loss: 22.842753779045616\n",
      "Iteracion: 13915 Gradiente: [0.00024619961909877475,-0.004247607870196883] Loss: 22.84275376092838\n",
      "Iteracion: 13916 Gradiente: [0.00024606873182619893,-0.004245349711473878] Loss: 22.842753742830407\n",
      "Iteracion: 13917 Gradiente: [0.00024593791412333605,-0.0042430927532585375] Loss: 22.842753724751667\n",
      "Iteracion: 13918 Gradiente: [0.0002458071660053444,-0.004240836994907582] Loss: 22.842753706692136\n",
      "Iteracion: 13919 Gradiente: [0.00024567648742011746,-0.004238582435786616] Loss: 22.842753688651825\n",
      "Iteracion: 13920 Gradiente: [0.0002455458782568106,-0.004236329075261717] Loss: 22.842753670630678\n",
      "Iteracion: 13921 Gradiente: [0.00024541533849742336,-0.004234076912693515] Loss: 22.842753652628673\n",
      "Iteracion: 13922 Gradiente: [0.00024528486817795663,-0.004231825947442284] Loss: 22.84275363464583\n",
      "Iteracion: 13923 Gradiente: [0.0002451544671894605,-0.004229576178875168] Loss: 22.8427536166821\n",
      "Iteracion: 13924 Gradiente: [0.0002450241355433036,-0.0042273276063527964] Loss: 22.84275359873745\n",
      "Iteracion: 13925 Gradiente: [0.00024489387314758916,-0.004225080229242787] Loss: 22.84275358081188\n",
      "Iteracion: 13926 Gradiente: [0.00024476368003452836,-0.004222834046906243] Loss: 22.842753562905354\n",
      "Iteracion: 13927 Gradiente: [0.00024463355619748955,-0.004220589058704979] Loss: 22.84275354501788\n",
      "Iteracion: 13928 Gradiente: [0.0002445035014242573,-0.004218345264015255] Loss: 22.842753527149405\n",
      "Iteracion: 13929 Gradiente: [0.00024437351589578306,-0.004216102662189177] Loss: 22.84275350929994\n",
      "Iteracion: 13930 Gradiente: [0.00024424359946237926,-0.004213861252600992] Loss: 22.842753491469445\n",
      "Iteracion: 13931 Gradiente: [0.00024411375202646469,-0.004211621034617608] Loss: 22.842753473657897\n",
      "Iteracion: 13932 Gradiente: [0.00024398397360035536,-0.004209382007605337] Loss: 22.842753455865285\n",
      "Iteracion: 13933 Gradiente: [0.00024385426427689556,-0.004207144170921258] Loss: 22.842753438091602\n",
      "Iteracion: 13934 Gradiente: [0.00024372462386376508,-0.00420490752394258] Loss: 22.84275342033678\n",
      "Iteracion: 13935 Gradiente: [0.00024359505242254424,-0.004202672066031117] Loss: 22.84275340260084\n",
      "Iteracion: 13936 Gradiente: [0.00024346554988596836,-0.004200437796557329] Loss: 22.84275338488377\n",
      "Iteracion: 13937 Gradiente: [0.00024333611608634936,-0.004198204714894397] Loss: 22.842753367185516\n",
      "Iteracion: 13938 Gradiente: [0.00024320675109474149,-0.004195972820407334] Loss: 22.842753349506083\n",
      "Iteracion: 13939 Gradiente: [0.00024307745491114474,-0.004193742112461981] Loss: 22.84275333184544\n",
      "Iteracion: 13940 Gradiente: [0.00024294822751282178,-0.004191512590427137] Loss: 22.842753314203563\n",
      "Iteracion: 13941 Gradiente: [0.00024281906880029662,-0.0041892842536742116] Loss: 22.84275329658046\n",
      "Iteracion: 13942 Gradiente: [0.00024268997867598802,-0.00418705710157982] Loss: 22.842753278976083\n",
      "Iteracion: 13943 Gradiente: [0.00024256095720621336,-0.004184831133505777] Loss: 22.84275326139041\n",
      "Iteracion: 13944 Gradiente: [0.00024243200444686863,-0.00418260634881733] Loss: 22.84275324382343\n",
      "Iteracion: 13945 Gradiente: [0.00024230312011278934,-0.004180382746900927] Loss: 22.842753226275125\n",
      "Iteracion: 13946 Gradiente: [0.00024217430438587447,-0.004178160327113763] Loss: 22.84275320874547\n",
      "Iteracion: 13947 Gradiente: [0.0002420455571353841,-0.004175939088833758] Loss: 22.84275319123447\n",
      "Iteracion: 13948 Gradiente: [0.00024191687821257802,-0.004173719031437528] Loss: 22.842753173742057\n",
      "Iteracion: 13949 Gradiente: [0.00024178826772545866,-0.004171500154292573] Loss: 22.84275315626826\n",
      "Iteracion: 13950 Gradiente: [0.00024165972574318554,-0.004169282456762838] Loss: 22.842753138813006\n",
      "Iteracion: 13951 Gradiente: [0.00024153125197112028,-0.0041670659382357185] Loss: 22.842753121376322\n",
      "Iteracion: 13952 Gradiente: [0.00024140284659589876,-0.004164850598072552] Loss: 22.842753103958184\n",
      "Iteracion: 13953 Gradiente: [0.0002412745094005686,-0.0041626364356580105] Loss: 22.84275308655857\n",
      "Iteracion: 13954 Gradiente: [0.00024114624043723628,-0.00416042345035829] Loss: 22.842753069177423\n",
      "Iteracion: 13955 Gradiente: [0.00024101803968884877,-0.004158211641548703] Loss: 22.842753051814775\n",
      "Iteracion: 13956 Gradiente: [0.00024088990707393047,-0.004156001008607054] Loss: 22.84275303447057\n",
      "Iteracion: 13957 Gradiente: [0.00024076184258679708,-0.004153791550904392] Loss: 22.8427530171448\n",
      "Iteracion: 13958 Gradiente: [0.00024063384617344734,-0.004151583267817926] Loss: 22.84275299983745\n",
      "Iteracion: 13959 Gradiente: [0.00024050591786798728,-0.004149376158718947] Loss: 22.842752982548514\n",
      "Iteracion: 13960 Gradiente: [0.00024037805754915099,-0.00414717022298845] Loss: 22.842752965277914\n",
      "Iteracion: 13961 Gradiente: [0.00024025026523588622,-0.00414496545999891] Loss: 22.842752948025705\n",
      "Iteracion: 13962 Gradiente: [0.00024012254086566525,-0.004142761869128956] Loss: 22.842752930791836\n",
      "Iteracion: 13963 Gradiente: [0.00023999488428311604,-0.004140559449762904] Loss: 22.84275291357629\n",
      "Iteracion: 13964 Gradiente: [0.00023986729566824276,-0.004138358201263988] Loss: 22.842752896379018\n",
      "Iteracion: 13965 Gradiente: [0.00023973977491020075,-0.004136158123014866] Loss: 22.842752879200056\n",
      "Iteracion: 13966 Gradiente: [0.00023961232188014493,-0.004133959214398431] Loss: 22.842752862039333\n",
      "Iteracion: 13967 Gradiente: [0.00023948493652217924,-0.00413176147479426] Loss: 22.842752844896847\n",
      "Iteracion: 13968 Gradiente: [0.00023935761899072835,-0.004129564903569734] Loss: 22.842752827772596\n",
      "Iteracion: 13969 Gradiente: [0.00023923036912757803,-0.004127369500111774] Loss: 22.84275281066656\n",
      "Iteracion: 13970 Gradiente: [0.00023910318697725568,-0.004125175263793561] Loss: 22.842752793578693\n",
      "Iteracion: 13971 Gradiente: [0.00023897607232470364,-0.004122982194004502] Loss: 22.84275277650898\n",
      "Iteracion: 13972 Gradiente: [0.0002388490253783478,-0.004120790290112334] Loss: 22.842752759457426\n",
      "Iteracion: 13973 Gradiente: [0.00023872204585018153,-0.004118599551510253] Loss: 22.84275274242399\n",
      "Iteracion: 13974 Gradiente: [0.00023859513387189206,-0.004116409977570257] Loss: 22.842752725408666\n",
      "Iteracion: 13975 Gradiente: [0.0002384682893638986,-0.0041142215676744105] Loss: 22.842752708411417\n",
      "Iteracion: 13976 Gradiente: [0.00023834151234420158,-0.0041120343212014635] Loss: 22.842752691432246\n",
      "Iteracion: 13977 Gradiente: [0.0002382148027322728,-0.004109848237535492] Loss: 22.84275267447113\n",
      "Iteracion: 13978 Gradiente: [0.00023808816042295196,-0.00410766331606176] Loss: 22.842752657528045\n",
      "Iteracion: 13979 Gradiente: [0.0002379615853053944,-0.004105479556167542] Loss: 22.84275264060295\n",
      "Iteracion: 13980 Gradiente: [0.0002378350776799228,-0.004103296957215245] Loss: 22.842752623695855\n",
      "Iteracion: 13981 Gradiente: [0.00023770863732105834,-0.004101115518599722] Loss: 22.842752606806737\n",
      "Iteracion: 13982 Gradiente: [0.00023758226411890366,-0.004098935239706354] Loss: 22.842752589935568\n",
      "Iteracion: 13983 Gradiente: [0.00023745595809051187,-0.004096756119918391] Loss: 22.842752573082343\n",
      "Iteracion: 13984 Gradiente: [0.0002373297192453568,-0.004094578158613989] Loss: 22.842752556247014\n",
      "Iteracion: 13985 Gradiente: [0.000237203547475436,-0.004092401355183029] Loss: 22.842752539429604\n",
      "Iteracion: 13986 Gradiente: [0.0002370774427580121,-0.004090225709008877] Loss: 22.84275252263005\n",
      "Iteracion: 13987 Gradiente: [0.00023695140506466335,-0.004088051219476678] Loss: 22.842752505848352\n",
      "Iteracion: 13988 Gradiente: [0.0002368254344342328,-0.004085877885965772] Loss: 22.8427524890845\n",
      "Iteracion: 13989 Gradiente: [0.00023669953080798222,-0.004083705707863435] Loss: 22.84275247233847\n",
      "Iteracion: 13990 Gradiente: [0.000236573694014434,-0.004081534684563929] Loss: 22.84275245561024\n",
      "Iteracion: 13991 Gradiente: [0.0002364479242298027,-0.004079364815440792] Loss: 22.84275243889979\n",
      "Iteracion: 13992 Gradiente: [0.00023632222128545284,-0.004077196099887696] Loss: 22.842752422207106\n",
      "Iteracion: 13993 Gradiente: [0.00023619658513590974,-0.0040750285372921505] Loss: 22.842752405532167\n",
      "Iteracion: 13994 Gradiente: [0.0002360710157167508,-0.004072862127043327] Loss: 22.842752388874953\n",
      "Iteracion: 13995 Gradiente: [0.00023594551315966327,-0.004070696868518671] Loss: 22.842752372235438\n",
      "Iteracion: 13996 Gradiente: [0.00023582007735569732,-0.004068532761109841] Loss: 22.842752355613616\n",
      "Iteracion: 13997 Gradiente: [0.00023569470800547757,-0.004066369804221045] Loss: 22.842752339009454\n",
      "Iteracion: 13998 Gradiente: [0.00023556940552017143,-0.004064207997213506] Loss: 22.842752322422946\n",
      "Iteracion: 13999 Gradiente: [0.00023544416958903486,-0.004062047339492026] Loss: 22.842752305854074\n",
      "Iteracion: 14000 Gradiente: [0.0002353190003210178,-0.004059887830437484] Loss: 22.842752289302805\n",
      "Iteracion: 14001 Gradiente: [0.0002351938974773778,-0.004057729469451606] Loss: 22.842752272769136\n",
      "Iteracion: 14002 Gradiente: [0.00023506886118790742,-0.004055572255914441] Loss: 22.84275225625305\n",
      "Iteracion: 14003 Gradiente: [0.00023494389132091935,-0.004053416189220845] Loss: 22.8427522397545\n",
      "Iteracion: 14004 Gradiente: [0.00023481898800904825,-0.004051261268751461] Loss: 22.84275222327351\n",
      "Iteracion: 14005 Gradiente: [0.00023469415102586784,-0.004049107493908366] Loss: 22.842752206810037\n",
      "Iteracion: 14006 Gradiente: [0.00023456938035906205,-0.004046954864080968] Loss: 22.842752190364045\n",
      "Iteracion: 14007 Gradiente: [0.00023444467609200123,-0.004044803378654412] Loss: 22.84275217393556\n",
      "Iteracion: 14008 Gradiente: [0.0002343200381034194,-0.004042653037024616] Loss: 22.842752157524522\n",
      "Iteracion: 14009 Gradiente: [0.00023419546636773702,-0.004040503838581463] Loss: 22.842752141130923\n",
      "Iteracion: 14010 Gradiente: [0.0002340709609067441,-0.004038355782716488] Loss: 22.842752124754767\n",
      "Iteracion: 14011 Gradiente: [0.00023394652159159553,-0.004036208868826444] Loss: 22.842752108396006\n",
      "Iteracion: 14012 Gradiente: [0.00023382214847155562,-0.00403406309629896] Loss: 22.842752092054642\n",
      "Iteracion: 14013 Gradiente: [0.00023369784139883146,-0.004031918464535285] Loss: 22.842752075730647\n",
      "Iteracion: 14014 Gradiente: [0.00023357360048142556,-0.0040297749729183135] Loss: 22.842752059424004\n",
      "Iteracion: 14015 Gradiente: [0.0002334494255488077,-0.0040276326208526095] Loss: 22.842752043134684\n",
      "Iteracion: 14016 Gradiente: [0.0002333253166331891,-0.004025491407726515] Loss: 22.842752026862687\n",
      "Iteracion: 14017 Gradiente: [0.0002332012736995163,-0.004023351332934647] Loss: 22.842752010607985\n",
      "Iteracion: 14018 Gradiente: [0.0002330772968122119,-0.004021212395867479] Loss: 22.84275199437057\n",
      "Iteracion: 14019 Gradiente: [0.0002329533857723239,-0.004019074595926734] Loss: 22.84275197815041\n",
      "Iteracion: 14020 Gradiente: [0.00023282954062532705,-0.0040169379325053715] Loss: 22.8427519619475\n",
      "Iteracion: 14021 Gradiente: [0.00023270576132006227,-0.004014802404999784] Loss: 22.842751945761794\n",
      "Iteracion: 14022 Gradiente: [0.00023258204774000054,-0.004012668012810868] Loss: 22.842751929593295\n",
      "Iteracion: 14023 Gradiente: [0.00023245839993345878,-0.004010534755330752] Loss: 22.84275191344199\n",
      "Iteracion: 14024 Gradiente: [0.00023233481795917517,-0.0040084026319495555] Loss: 22.842751897307856\n",
      "Iteracion: 14025 Gradiente: [0.00023221130158788129,-0.004006271642075869] Loss: 22.842751881190864\n",
      "Iteracion: 14026 Gradiente: [0.0002320878509560013,-0.0040041417850982695] Loss: 22.842751865091014\n",
      "Iteracion: 14027 Gradiente: [0.0002319644659214267,-0.004002013060419666] Loss: 22.842751849008277\n",
      "Iteracion: 14028 Gradiente: [0.00023184114654384302,-0.003999885467432544] Loss: 22.842751832942625\n",
      "Iteracion: 14029 Gradiente: [0.00023171789271524783,-0.003997759005540639] Loss: 22.84275181689407\n",
      "Iteracion: 14030 Gradiente: [0.00023159470436269204,-0.00399563367414378] Loss: 22.842751800862565\n",
      "Iteracion: 14031 Gradiente: [0.0002314715815108078,-0.003993509472637058] Loss: 22.84275178484808\n",
      "Iteracion: 14032 Gradiente: [0.0002313485241198047,-0.003991386400420656] Loss: 22.84275176885064\n",
      "Iteracion: 14033 Gradiente: [0.00023122553210631243,-0.0039892644568977195] Loss: 22.842751752870207\n",
      "Iteracion: 14034 Gradiente: [0.0002311026055072792,-0.003987143641462865] Loss: 22.842751736906752\n",
      "Iteracion: 14035 Gradiente: [0.0002309797442241764,-0.003985023953521723] Loss: 22.842751720960262\n",
      "Iteracion: 14036 Gradiente: [0.00023085694833184788,-0.003982905392467373] Loss: 22.842751705030743\n",
      "Iteracion: 14037 Gradiente: [0.00023073421772892288,-0.003980787957704142] Loss: 22.842751689118142\n",
      "Iteracion: 14038 Gradiente: [0.0002306115522676085,-0.003978671648640978] Loss: 22.84275167322245\n",
      "Iteracion: 14039 Gradiente: [0.0002304889520975924,-0.003976556464667643] Loss: 22.84275165734367\n",
      "Iteracion: 14040 Gradiente: [0.00023036641711087212,-0.003974442405190596] Loss: 22.842751641481765\n",
      "Iteracion: 14041 Gradiente: [0.00023024394727997333,-0.003972329469612035] Loss: 22.842751625636716\n",
      "Iteracion: 14042 Gradiente: [0.00023012154250542002,-0.00397021765733759] Loss: 22.84275160980851\n",
      "Iteracion: 14043 Gradiente: [0.0002299992028251078,-0.0039681069677659055] Loss: 22.842751593997125\n",
      "Iteracion: 14044 Gradiente: [0.00022987692822861542,-0.003965997400299415] Loss: 22.842751578202556\n",
      "Iteracion: 14045 Gradiente: [0.00022975471859751906,-0.003963888954345644] Loss: 22.842751562424777\n",
      "Iteracion: 14046 Gradiente: [0.00022963257393750306,-0.003961781629307737] Loss: 22.84275154666376\n",
      "Iteracion: 14047 Gradiente: [0.00022951049420877705,-0.003959675424586943] Loss: 22.842751530919504\n",
      "Iteracion: 14048 Gradiente: [0.0002293884794189201,-0.003957570339588183] Loss: 22.842751515191978\n",
      "Iteracion: 14049 Gradiente: [0.00022926652944382415,-0.003955466373720521] Loss: 22.842751499481174\n",
      "Iteracion: 14050 Gradiente: [0.00022914464426833093,-0.003953363526386274] Loss: 22.842751483787065\n",
      "Iteracion: 14051 Gradiente: [0.00022902282389907213,-0.003951261796991901] Loss: 22.84275146810965\n",
      "Iteracion: 14052 Gradiente: [0.00022890106837204863,-0.003949161184934388] Loss: 22.842751452448894\n",
      "Iteracion: 14053 Gradiente: [0.00022877937759536356,-0.003947061689627418] Loss: 22.842751436804782\n",
      "Iteracion: 14054 Gradiente: [0.00022865775136248582,-0.003944963310483729] Loss: 22.842751421177297\n",
      "Iteracion: 14055 Gradiente: [0.00022853618989605214,-0.0039428660468978196] Loss: 22.84275140556643\n",
      "Iteracion: 14056 Gradiente: [0.00022841469300374228,-0.003940769898284084] Loss: 22.842751389972157\n",
      "Iteracion: 14057 Gradiente: [0.00022829326079166397,-0.003938674864042587] Loss: 22.84275137439446\n",
      "Iteracion: 14058 Gradiente: [0.00022817189310823474,-0.003936580943586421] Loss: 22.84275135883332\n",
      "Iteracion: 14059 Gradiente: [0.00022805058988145294,-0.003934488136327256] Loss: 22.842751343288715\n",
      "Iteracion: 14060 Gradiente: [0.00022792935116437243,-0.003932396441666933] Loss: 22.84275132776064\n",
      "Iteracion: 14061 Gradiente: [0.00022780817698162537,-0.003930305859010848] Loss: 22.842751312249067\n",
      "Iteracion: 14062 Gradiente: [0.00022768706714562843,-0.003928216387778131] Loss: 22.84275129675399\n",
      "Iteracion: 14063 Gradiente: [0.0002275660217312255,-0.003926128027369084] Loss: 22.84275128127539\n",
      "Iteracion: 14064 Gradiente: [0.00022744504059725538,-0.0039240407772010615] Loss: 22.842751265813224\n",
      "Iteracion: 14065 Gradiente: [0.0002273241239009849,-0.003921954636672354] Loss: 22.84275125036751\n",
      "Iteracion: 14066 Gradiente: [0.00022720327136767083,-0.003919869605207182] Loss: 22.842751234938202\n",
      "Iteracion: 14067 Gradiente: [0.00022708248319437037,-0.003917785682201232] Loss: 22.842751219525304\n",
      "Iteracion: 14068 Gradiente: [0.000226961759100656,-0.003915702867082515] Loss: 22.842751204128785\n",
      "Iteracion: 14069 Gradiente: [0.00022684109928926925,-0.003913621159244821] Loss: 22.842751188748633\n",
      "Iteracion: 14070 Gradiente: [0.0002267205036029433,-0.003911540558108939] Loss: 22.84275117338484\n",
      "Iteracion: 14071 Gradiente: [0.00022659997199336128,-0.003909461063085829] Loss: 22.842751158037363\n",
      "Iteracion: 14072 Gradiente: [0.00022647950456568348,-0.003907382673581475] Loss: 22.842751142706206\n",
      "Iteracion: 14073 Gradiente: [0.00022635910108969408,-0.003905305389017144] Loss: 22.842751127391345\n",
      "Iteracion: 14074 Gradiente: [0.00022623876169518552,-0.003903229208796688] Loss: 22.842751112092763\n",
      "Iteracion: 14075 Gradiente: [0.0002261184861718372,-0.003901154132343502] Loss: 22.842751096810435\n",
      "Iteracion: 14076 Gradiente: [0.00022599827463807288,-0.0038990801590623883] Loss: 22.84275108154437\n",
      "Iteracion: 14077 Gradiente: [0.0002258781270692604,-0.003897007288365728] Loss: 22.84275106629453\n",
      "Iteracion: 14078 Gradiente: [0.00022575804327592172,-0.0038949355196763237] Loss: 22.84275105106089\n",
      "Iteracion: 14079 Gradiente: [0.0002256380232684781,-0.0038928648524069113] Loss: 22.842751035843442\n",
      "Iteracion: 14080 Gradiente: [0.0002255180671947225,-0.0038907952859594513] Loss: 22.842751020642176\n",
      "Iteracion: 14081 Gradiente: [0.00022539817495423147,-0.0038887268197528376] Loss: 22.84275100545706\n",
      "Iteracion: 14082 Gradiente: [0.00022527834630163093,-0.0038866594532155573] Loss: 22.8427509902881\n",
      "Iteracion: 14083 Gradiente: [0.00022515858148229503,-0.0038845931857470835] Loss: 22.84275097513525\n",
      "Iteracion: 14084 Gradiente: [0.00022503888024516527,-0.003882528016774245] Loss: 22.842750959998515\n",
      "Iteracion: 14085 Gradiente: [0.00022491924271056025,-0.0038804639457044486] Loss: 22.84275094487789\n",
      "Iteracion: 14086 Gradiente: [0.00022479966865489586,-0.003878400971965945] Loss: 22.842750929773292\n",
      "Iteracion: 14087 Gradiente: [0.0002246801582742819,-0.003876339094960812] Loss: 22.842750914684775\n",
      "Iteracion: 14088 Gradiente: [0.00022456071145124195,-0.0038742783141123265] Loss: 22.84275089961231\n",
      "Iteracion: 14089 Gradiente: [0.00022444132807966828,-0.003872218628841158] Loss: 22.84275088455585\n",
      "Iteracion: 14090 Gradiente: [0.00022432200814629746,-0.0038701600385632417] Loss: 22.8427508695154\n",
      "Iteracion: 14091 Gradiente: [0.00022420275169091988,-0.0038681025426929713] Loss: 22.84275085449093\n",
      "Iteracion: 14092 Gradiente: [0.00022408355863300737,-0.0038660461406506623] Loss: 22.842750839482445\n",
      "Iteracion: 14093 Gradiente: [0.00022396442896782293,-0.0038639908318529592] Loss: 22.842750824489904\n",
      "Iteracion: 14094 Gradiente: [0.0002238453625807324,-0.0038619366157232565] Loss: 22.84275080951329\n",
      "Iteracion: 14095 Gradiente: [0.00022372635948120963,-0.0038598834916786728] Loss: 22.842750794552614\n",
      "Iteracion: 14096 Gradiente: [0.0002236074196853603,-0.0038578314591362073] Loss: 22.84275077960783\n",
      "Iteracion: 14097 Gradiente: [0.00022348854313918308,-0.003855780517516294] Loss: 22.842750764678936\n",
      "Iteracion: 14098 Gradiente: [0.00022336972972330688,-0.003853730666243275] Loss: 22.842750749765905\n",
      "Iteracion: 14099 Gradiente: [0.00022325097950973334,-0.003851681904732729] Loss: 22.84275073486874\n",
      "Iteracion: 14100 Gradiente: [0.00022313229249940983,-0.0038496342324018922] Loss: 22.8427507199874\n",
      "Iteracion: 14101 Gradiente: [0.0002230136685625439,-0.0038475876486773566] Loss: 22.84275070512188\n",
      "Iteracion: 14102 Gradiente: [0.00022289510761197562,-0.003845542152985123] Loss: 22.84275069027217\n",
      "Iteracion: 14103 Gradiente: [0.00022277660966854758,-0.003843497744741479] Loss: 22.842750675438236\n",
      "Iteracion: 14104 Gradiente: [0.00022265817482320928,-0.003841454423363544] Loss: 22.842750660620062\n",
      "Iteracion: 14105 Gradiente: [0.00022253980297174772,-0.0038394121882751858] Loss: 22.84275064581764\n",
      "Iteracion: 14106 Gradiente: [0.00022242149392752707,-0.0038373710389080886] Loss: 22.842750631030974\n",
      "Iteracion: 14107 Gradiente: [0.00022230324789518363,-0.003835330974673686] Loss: 22.842750616260016\n",
      "Iteracion: 14108 Gradiente: [0.00022218506469850278,-0.0038332919950008204] Loss: 22.84275060150475\n",
      "Iteracion: 14109 Gradiente: [0.00022206694419632338,-0.00383125409932011] Loss: 22.842750586765185\n",
      "Iteracion: 14110 Gradiente: [0.0002219488866226508,-0.003829217287039318] Loss: 22.842750572041272\n",
      "Iteracion: 14111 Gradiente: [0.00022183089176053272,-0.0038271815575941543] Loss: 22.84275055733302\n",
      "Iteracion: 14112 Gradiente: [0.0002217129597075503,-0.0038251469103999605] Loss: 22.842750542640395\n",
      "Iteracion: 14113 Gradiente: [0.00022159509027706766,-0.0038231133448913823] Loss: 22.842750527963396\n",
      "Iteracion: 14114 Gradiente: [0.0002214772835098226,-0.0038210808604892082] Loss: 22.842750513302004\n",
      "Iteracion: 14115 Gradiente: [0.00022135953937739335,-0.0038190494566168335] Loss: 22.84275049865618\n",
      "Iteracion: 14116 Gradiente: [0.0002212418578371474,-0.0038170191327015603] Loss: 22.84275048402593\n",
      "Iteracion: 14117 Gradiente: [0.00022112423885497872,-0.00381498988816927] Loss: 22.84275046941123\n",
      "Iteracion: 14118 Gradiente: [0.00022100668241099204,-0.003812961722445015] Loss: 22.842750454812062\n",
      "Iteracion: 14119 Gradiente: [0.00022088918842465925,-0.0038109346349584666] Loss: 22.842750440228436\n",
      "Iteracion: 14120 Gradiente: [0.00022077175697177154,-0.0038089086251297034] Loss: 22.84275042566029\n",
      "Iteracion: 14121 Gradiente: [0.00022065438800874897,-0.0038068836923869755] Loss: 22.842750411107627\n",
      "Iteracion: 14122 Gradiente: [0.00022053708128642787,-0.003804859836169309] Loss: 22.84275039657044\n",
      "Iteracion: 14123 Gradiente: [0.00022041983705018236,-0.003802837055887191] Loss: 22.842750382048703\n",
      "Iteracion: 14124 Gradiente: [0.00022030265510200782,-0.003800815350982371] Loss: 22.84275036754241\n",
      "Iteracion: 14125 Gradiente: [0.00022018553549780034,-0.0037987947208733886] Loss: 22.84275035305153\n",
      "Iteracion: 14126 Gradiente: [0.00022006847804050266,-0.0037967751650010467] Loss: 22.842750338576057\n",
      "Iteracion: 14127 Gradiente: [0.00021995148286748644,-0.0037947566827831736] Loss: 22.842750324115976\n",
      "Iteracion: 14128 Gradiente: [0.00021983454996927777,-0.0037927392736477827] Loss: 22.842750309671256\n",
      "Iteracion: 14129 Gradiente: [0.00021971767915355636,-0.0037907229370343744] Loss: 22.842750295241885\n",
      "Iteracion: 14130 Gradiente: [0.00021960087052548262,-0.003788707672363619] Loss: 22.842750280827865\n",
      "Iteracion: 14131 Gradiente: [0.0002194841239438953,-0.0037866934790721746] Loss: 22.842750266429167\n",
      "Iteracion: 14132 Gradiente: [0.0002193674394068997,-0.0037846803565885286] Loss: 22.842750252045775\n",
      "Iteracion: 14133 Gradiente: [0.00021925081698744484,-0.003782668304339154] Loss: 22.84275023767766\n",
      "Iteracion: 14134 Gradiente: [0.00021913425649036828,-0.003780657321762722] Loss: 22.84275022332483\n",
      "Iteracion: 14135 Gradiente: [0.00021901775796872395,-0.00377864740828547] Loss: 22.842750208987237\n",
      "Iteracion: 14136 Gradiente: [0.0002189013214234592,-0.0037766385633376605] Loss: 22.842750194664912\n",
      "Iteracion: 14137 Gradiente: [0.00021878494674183458,-0.0037746307863561887] Loss: 22.842750180357804\n",
      "Iteracion: 14138 Gradiente: [0.00021866863390490227,-0.0037726240767721464] Loss: 22.842750166065898\n",
      "Iteracion: 14139 Gradiente: [0.00021855238298561138,-0.003770618434010468] Loss: 22.84275015178918\n",
      "Iteracion: 14140 Gradiente: [0.00021843619378027294,-0.0037686138575144432] Loss: 22.842750137527652\n",
      "Iteracion: 14141 Gradiente: [0.00021832006649636544,-0.0037666103467059267] Loss: 22.842750123281277\n",
      "Iteracion: 14142 Gradiente: [0.0002182040008506192,-0.003764607901031051] Loss: 22.842750109050044\n",
      "Iteracion: 14143 Gradiente: [0.0002180879968979828,-0.0037626065199161713] Loss: 22.84275009483393\n",
      "Iteracion: 14144 Gradiente: [0.00021797205460908722,-0.0037606062027990108] Loss: 22.84275008063294\n",
      "Iteracion: 14145 Gradiente: [0.00021785617399340632,-0.003758606949109596] Loss: 22.842750066447035\n",
      "Iteracion: 14146 Gradiente: [0.00021774035491356852,-0.003756608758288138] Loss: 22.842750052276212\n",
      "Iteracion: 14147 Gradiente: [0.00021762459754484098,-0.0037546116297592154] Loss: 22.842750038120467\n",
      "Iteracion: 14148 Gradiente: [0.00021750890166079748,-0.003752615562967776] Loss: 22.842750023979754\n",
      "Iteracion: 14149 Gradiente: [0.0002173932672481745,-0.00375062055734882] Loss: 22.842750009854075\n",
      "Iteracion: 14150 Gradiente: [0.00021727769430886687,-0.0037486266123373468] Loss: 22.842749995743414\n",
      "Iteracion: 14151 Gradiente: [0.0002171621828267689,-0.0037466337273659887] Loss: 22.842749981647746\n",
      "Iteracion: 14152 Gradiente: [0.00021704673267208818,-0.0037446419018791013] Loss: 22.842749967567062\n",
      "Iteracion: 14153 Gradiente: [0.00021693134400303885,-0.003742651135301737] Loss: 22.842749953501357\n",
      "Iteracion: 14154 Gradiente: [0.00021681601663014287,-0.0037406614270794353] Loss: 22.8427499394506\n",
      "Iteracion: 14155 Gradiente: [0.00021670075052687327,-0.0037386727766488548] Loss: 22.842749925414783\n",
      "Iteracion: 14156 Gradiente: [0.00021658554579081132,-0.0037366851834413238] Loss: 22.842749911393867\n",
      "Iteracion: 14157 Gradiente: [0.00021647040227226928,-0.0037346986469014355] Loss: 22.842749897387858\n",
      "Iteracion: 14158 Gradiente: [0.00021635531993429897,-0.003732713166466439] Loss: 22.842749883396753\n",
      "Iteracion: 14159 Gradiente: [0.00021624029884890206,-0.0037307287415699144] Loss: 22.842749869420512\n",
      "Iteracion: 14160 Gradiente: [0.00021612533887870692,-0.003728745371656572] Loss: 22.842749855459125\n",
      "Iteracion: 14161 Gradiente: [0.00021601043993844845,-0.0037267630561689915] Loss: 22.842749841512582\n",
      "Iteracion: 14162 Gradiente: [0.0002158956021569717,-0.003724781794536251] Loss: 22.84274982758086\n",
      "Iteracion: 14163 Gradiente: [0.00021578082552575021,-0.0037228015861963115] Loss: 22.842749813663954\n",
      "Iteracion: 14164 Gradiente: [0.0002156661098553059,-0.003720822430601108] Loss: 22.842749799761837\n",
      "Iteracion: 14165 Gradiente: [0.00021555145510490092,-0.003718844327188246] Loss: 22.842749785874496\n",
      "Iteracion: 14166 Gradiente: [0.00021543686130390445,-0.0037168672753967514] Loss: 22.842749772001923\n",
      "Iteracion: 14167 Gradiente: [0.00021532232847126426,-0.0037148912746635195] Loss: 22.842749758144087\n",
      "Iteracion: 14168 Gradiente: [0.0002152078565179257,-0.0037129163244336164] Loss: 22.842749744300992\n",
      "Iteracion: 14169 Gradiente: [0.00021509344543820438,-0.003710942424147016] Loss: 22.842749730472597\n",
      "Iteracion: 14170 Gradiente: [0.0002149790951259926,-0.0037089695732506795] Loss: 22.842749716658926\n",
      "Iteracion: 14171 Gradiente: [0.00021486480559360644,-0.003706997771180435] Loss: 22.842749702859912\n",
      "Iteracion: 14172 Gradiente: [0.00021475057693199536,-0.003705027017375902] Loss: 22.842749689075575\n",
      "Iteracion: 14173 Gradiente: [0.0002146364089232596,-0.0037030573112893706] Loss: 22.842749675305893\n",
      "Iteracion: 14174 Gradiente: [0.0002145223016412956,-0.0037010886523576167] Loss: 22.842749661550858\n",
      "Iteracion: 14175 Gradiente: [0.00021440825491841526,-0.0036991210400310354] Loss: 22.842749647810436\n",
      "Iteracion: 14176 Gradiente: [0.00021429426890904324,-0.003697154473742022] Loss: 22.84274963408462\n",
      "Iteracion: 14177 Gradiente: [0.00021418034344454403,-0.0036951889529434584] Loss: 22.84274962037337\n",
      "Iteracion: 14178 Gradiente: [0.0002140664786357623,-0.003693224477071529] Loss: 22.842749606676737\n",
      "Iteracion: 14179 Gradiente: [0.00021395267435764254,-0.003691261045574734] Loss: 22.84274959299464\n",
      "Iteracion: 14180 Gradiente: [0.00021383893053155133,-0.0036892986579013373] Loss: 22.842749579327087\n",
      "Iteracion: 14181 Gradiente: [0.000213725247206753,-0.0036873373134913125] Loss: 22.842749565674065\n",
      "Iteracion: 14182 Gradiente: [0.00021361162427713983,-0.0036853770117950546] Loss: 22.842749552035556\n",
      "Iteracion: 14183 Gradiente: [0.0002134980618118713,-0.0036834177522520645] Loss: 22.842749538411553\n",
      "Iteracion: 14184 Gradiente: [0.0002133845596527332,-0.0036814595343153417] Loss: 22.842749524802016\n",
      "Iteracion: 14185 Gradiente: [0.00021327111790204374,-0.0036795023574242693] Loss: 22.842749511206947\n",
      "Iteracion: 14186 Gradiente: [0.00021315773643569477,-0.003677546221028531] Loss: 22.842749497626343\n",
      "Iteracion: 14187 Gradiente: [0.00021304441530579273,-0.0036755911245730743] Loss: 22.84274948406018\n",
      "Iteracion: 14188 Gradiente: [0.00021293115434370218,-0.0036736370675098345] Loss: 22.84274947050841\n",
      "Iteracion: 14189 Gradiente: [0.00021281795360058217,-0.0036716840492843515] Loss: 22.842749456971067\n",
      "Iteracion: 14190 Gradiente: [0.0002127048129987467,-0.0036697320693446518] Loss: 22.842749443448106\n",
      "Iteracion: 14191 Gradiente: [0.00021259173257135445,-0.0036677811271346172] Loss: 22.842749429939513\n",
      "Iteracion: 14192 Gradiente: [0.00021247871226440414,-0.0036658312221072484] Loss: 22.842749416445297\n",
      "Iteracion: 14193 Gradiente: [0.00021236575212431794,-0.0036638823537037033] Loss: 22.842749402965424\n",
      "Iteracion: 14194 Gradiente: [0.00021225285193414342,-0.0036619345213851537] Loss: 22.842749389499854\n",
      "Iteracion: 14195 Gradiente: [0.00021214001177535617,-0.003659987724593113] Loss: 22.842749376048623\n",
      "Iteracion: 14196 Gradiente: [0.0002120272316470088,-0.003658041962775253] Loss: 22.84274936261168\n",
      "Iteracion: 14197 Gradiente: [0.0002119145115026792,-0.0036560972353820867] Loss: 22.842749349189024\n",
      "Iteracion: 14198 Gradiente: [0.0002118018512353122,-0.0036541535418689835] Loss: 22.842749335780635\n",
      "Iteracion: 14199 Gradiente: [0.00021168925090933043,-0.003652210881681128] Loss: 22.842749322386496\n",
      "Iteracion: 14200 Gradiente: [0.00021157671034188752,-0.0036502692542761395] Loss: 22.842749309006596\n",
      "Iteracion: 14201 Gradiente: [0.0002114642296570916,-0.0036483286590968333] Loss: 22.84274929564092\n",
      "Iteracion: 14202 Gradiente: [0.0002113518088113627,-0.0036463890955959737] Loss: 22.84274928228945\n",
      "Iteracion: 14203 Gradiente: [0.00021123944774406784,-0.0036444505632243105] Loss: 22.842749268952176\n",
      "Iteracion: 14204 Gradiente: [0.0002111271463737315,-0.0036425130614391086] Loss: 22.84274925562908\n",
      "Iteracion: 14205 Gradiente: [0.0002110149046435102,-0.0036405765896925383] Loss: 22.842749242320128\n",
      "Iteracion: 14206 Gradiente: [0.00021090272264909042,-0.0036386411474307323] Loss: 22.84274922902535\n",
      "Iteracion: 14207 Gradiente: [0.00021079060027015355,-0.003636706734111191] Loss: 22.8427492157447\n",
      "Iteracion: 14208 Gradiente: [0.00021067853755596389,-0.0036347733491806384] Loss: 22.842749202478142\n",
      "Iteracion: 14209 Gradiente: [0.00021056653440041374,-0.0036328409920979967] Loss: 22.8427491892257\n",
      "Iteracion: 14210 Gradiente: [0.00021045459072486968,-0.003630909662320055] Loss: 22.842749175987347\n",
      "Iteracion: 14211 Gradiente: [0.0002103427066117547,-0.003628979359293775] Loss: 22.842749162763074\n",
      "Iteracion: 14212 Gradiente: [0.00021023088196159279,-0.0036270500824765388] Loss: 22.84274914955284\n",
      "Iteracion: 14213 Gradiente: [0.00021011911675922573,-0.0036251218313215834] Loss: 22.842749136356677\n",
      "Iteracion: 14214 Gradiente: [0.00021000741098096872,-0.003623194605284515] Loss: 22.842749123174517\n",
      "Iteracion: 14215 Gradiente: [0.0002098957645605045,-0.0036212684038216497] Loss: 22.84274911000637\n",
      "Iteracion: 14216 Gradiente: [0.0002097841775470973,-0.003619343226383028] Loss: 22.842749096852238\n",
      "Iteracion: 14217 Gradiente: [0.00020967264985169247,-0.0036174190724280447] Loss: 22.84274908371206\n",
      "Iteracion: 14218 Gradiente: [0.00020956118141744658,-0.003615495941415503] Loss: 22.842749070585878\n",
      "Iteracion: 14219 Gradiente: [0.00020944977225004398,-0.0036135738327960354] Loss: 22.842749057473636\n",
      "Iteracion: 14220 Gradiente: [0.0002093384223371686,-0.003611652746027971] Loss: 22.84274904437533\n",
      "Iteracion: 14221 Gradiente: [0.00020922713162387178,-0.0036097326805683374] Loss: 22.842749031290968\n",
      "Iteracion: 14222 Gradiente: [0.00020911590002773057,-0.0036078136358772403] Loss: 22.842749018220488\n",
      "Iteracion: 14223 Gradiente: [0.00020900472766148445,-0.0036058956114042454] Loss: 22.84274900516391\n",
      "Iteracion: 14224 Gradiente: [0.00020889361427028536,-0.0036039786066190517] Loss: 22.842748992121212\n",
      "Iteracion: 14225 Gradiente: [0.00020878256005592752,-0.0036020626209658955] Loss: 22.84274897909238\n",
      "Iteracion: 14226 Gradiente: [0.00020867156483177495,-0.0036001476539121078] Loss: 22.842748966077394\n",
      "Iteracion: 14227 Gradiente: [0.00020856062857982732,-0.0035982337049160168] Loss: 22.842748953076242\n",
      "Iteracion: 14228 Gradiente: [0.0002084497514327192,-0.0035963207734260056] Loss: 22.842748940088917\n",
      "Iteracion: 14229 Gradiente: [0.00020833893311381264,-0.0035944088589167458] Loss: 22.84274892711539\n",
      "Iteracion: 14230 Gradiente: [0.00020822817371500454,-0.0035924979608391067] Loss: 22.842748914155646\n",
      "Iteracion: 14231 Gradiente: [0.0002081174731917675,-0.0035905880786542593] Loss: 22.8427489012097\n",
      "Iteracion: 14232 Gradiente: [0.00020800683158483935,-0.0035886792118173362] Loss: 22.84274888827749\n",
      "Iteracion: 14233 Gradiente: [0.00020789624877958583,-0.0035867713597932988] Loss: 22.842748875359046\n",
      "Iteracion: 14234 Gradiente: [0.00020778572471821615,-0.00358486452204545] Loss: 22.842748862454325\n",
      "Iteracion: 14235 Gradiente: [0.00020767525953147015,-0.0035829586980241856] Loss: 22.842748849563325\n",
      "Iteracion: 14236 Gradiente: [0.0002075648530289224,-0.0035810538872005065] Loss: 22.842748836686035\n",
      "Iteracion: 14237 Gradiente: [0.00020745450512341297,-0.0035791500890375973] Loss: 22.842748823822415\n",
      "Iteracion: 14238 Gradiente: [0.00020734421594378698,-0.003577247302989169] Loss: 22.84274881097248\n",
      "Iteracion: 14239 Gradiente: [0.00020723398539435796,-0.0035753455285193543] Loss: 22.842748798136206\n",
      "Iteracion: 14240 Gradiente: [0.00020712381345333597,-0.0035734447650914566] Loss: 22.842748785313574\n",
      "Iteracion: 14241 Gradiente: [0.00020701370010556275,-0.0035715450121667656] Loss: 22.842748772504578\n",
      "Iteracion: 14242 Gradiente: [0.00020690364518619238,-0.0035696462692158095] Loss: 22.842748759709195\n",
      "Iteracion: 14243 Gradiente: [0.00020679364891312463,-0.00356774853568614] Loss: 22.842748746927395\n",
      "Iteracion: 14244 Gradiente: [0.0002066837110940393,-0.0035658518110519566] Loss: 22.84274873415921\n",
      "Iteracion: 14245 Gradiente: [0.0002065738317033568,-0.0035639560947750225] Loss: 22.842748721404586\n",
      "Iteracion: 14246 Gradiente: [0.00020646401072781372,-0.00356206138631876] Loss: 22.842748708663503\n",
      "Iteracion: 14247 Gradiente: [0.00020635424806888145,-0.0035601676851515643] Loss: 22.84274869593598\n",
      "Iteracion: 14248 Gradiente: [0.00020624454383266766,-0.0035582749907309363] Loss: 22.842748683221988\n",
      "Iteracion: 14249 Gradiente: [0.00020613489782779956,-0.0035563833025298902] Loss: 22.842748670521516\n",
      "Iteracion: 14250 Gradiente: [0.00020602531021628087,-0.003554492620001781] Loss: 22.842748657834523\n",
      "Iteracion: 14251 Gradiente: [0.00020591578091284647,-0.003552602942615953] Loss: 22.842748645161027\n",
      "Iteracion: 14252 Gradiente: [0.00020580630971949175,-0.0035507142698480245] Loss: 22.842748632501017\n",
      "Iteracion: 14253 Gradiente: [0.0002056968967413771,-0.0035488266011556155] Loss: 22.842748619854447\n",
      "Iteracion: 14254 Gradiente: [0.00020558754190460604,-0.0035469399360067654] Loss: 22.842748607221324\n",
      "Iteracion: 14255 Gradiente: [0.0002054782452944437,-0.0035450542738614623] Loss: 22.842748594601638\n",
      "Iteracion: 14256 Gradiente: [0.00020536900677825543,-0.0035431696141942598] Loss: 22.842748581995345\n",
      "Iteracion: 14257 Gradiente: [0.00020525982630677694,-0.003541285956471185] Loss: 22.842748569402477\n",
      "Iteracion: 14258 Gradiente: [0.0002051507038923243,-0.0035394033001574362] Loss: 22.84274855682298\n",
      "Iteracion: 14259 Gradiente: [0.00020504163941363154,-0.0035375216447255534] Loss: 22.842748544256853\n",
      "Iteracion: 14260 Gradiente: [0.0002049326329957542,-0.003535640989635051] Loss: 22.842748531704093\n",
      "Iteracion: 14261 Gradiente: [0.00020482368449658376,-0.0035337613343611927] Loss: 22.842748519164676\n",
      "Iteracion: 14262 Gradiente: [0.00020471479400422746,-0.0035318826783630186] Loss: 22.84274850663858\n",
      "Iteracion: 14263 Gradiente: [0.00020460596133204946,-0.003530005021121833] Loss: 22.842748494125797\n",
      "Iteracion: 14264 Gradiente: [0.00020449718650089228,-0.0035281283621016496] Loss: 22.842748481626323\n",
      "Iteracion: 14265 Gradiente: [0.00020438846954201988,-0.0035262527007686134] Loss: 22.84274846914014\n",
      "Iteracion: 14266 Gradiente: [0.00020427981039195705,-0.0035243780365923055] Loss: 22.842748456667216\n",
      "Iteracion: 14267 Gradiente: [0.00020417120896164913,-0.0035225043690482257] Loss: 22.842748444207555\n",
      "Iteracion: 14268 Gradiente: [0.00020406266519993702,-0.0035206316976089153] Loss: 22.84274843176116\n",
      "Iteracion: 14269 Gradiente: [0.00020395417922050758,-0.00351876002173294] Loss: 22.842748419327965\n",
      "Iteracion: 14270 Gradiente: [0.00020384575090588443,-0.0035168893408992355] Loss: 22.842748406908004\n",
      "Iteracion: 14271 Gradiente: [0.00020373738030059484,-0.003515019654572171] Loss: 22.84274839450124\n",
      "Iteracion: 14272 Gradiente: [0.00020362906726253036,-0.0035131509622310375] Loss: 22.842748382107672\n",
      "Iteracion: 14273 Gradiente: [0.00020352081185327127,-0.003511283263340559] Loss: 22.84274836972725\n",
      "Iteracion: 14274 Gradiente: [0.0002034126140065003,-0.0035094165573752887] Loss: 22.842748357360016\n",
      "Iteracion: 14275 Gradiente: [0.0002033044735422133,-0.003507550843816887] Loss: 22.842748345005912\n",
      "Iteracion: 14276 Gradiente: [0.00020319639065367786,-0.003505686122124037] Loss: 22.84274833266495\n",
      "Iteracion: 14277 Gradiente: [0.00020308836519404848,-0.00350382239177686] Loss: 22.842748320337115\n",
      "Iteracion: 14278 Gradiente: [0.00020298039718511519,-0.0035019596522451717] Loss: 22.842748308022365\n",
      "Iteracion: 14279 Gradiente: [0.00020287248656245538,-0.0035000979030026967] Loss: 22.84274829572071\n",
      "Iteracion: 14280 Gradiente: [0.00020276463326164654,-0.003498237143526121] Loss: 22.84274828343214\n",
      "Iteracion: 14281 Gradiente: [0.00020265683736226948,-0.0034963773732832474] Loss: 22.84274827115662\n",
      "Iteracion: 14282 Gradiente: [0.00020254909880842813,-0.003494518591749222] Loss: 22.842748258894165\n",
      "Iteracion: 14283 Gradiente: [0.00020244141746843524,-0.003492660798404401] Loss: 22.842748246644742\n",
      "Iteracion: 14284 Gradiente: [0.00020233379342660858,-0.003490803992715049] Loss: 22.84274823440831\n",
      "Iteracion: 14285 Gradiente: [0.00020222622655315567,-0.0034889481741643634] Loss: 22.842748222184927\n",
      "Iteracion: 14286 Gradiente: [0.00020211871680733868,-0.0034870933422284384] Loss: 22.842748209974513\n",
      "Iteracion: 14287 Gradiente: [0.00020201126435874053,-0.0034852394963666693] Loss: 22.842748197777087\n",
      "Iteracion: 14288 Gradiente: [0.00020190386896767147,-0.0034833866360720832] Loss: 22.84274818559262\n",
      "Iteracion: 14289 Gradiente: [0.00020179653072602833,-0.0034815347608113] Loss: 22.84274817342111\n",
      "Iteracion: 14290 Gradiente: [0.00020168924941591133,-0.0034796838700729654] Loss: 22.84274816126254\n",
      "Iteracion: 14291 Gradiente: [0.00020158202521827206,-0.003477833963318725] Loss: 22.8427481491169\n",
      "Iteracion: 14292 Gradiente: [0.00020147485801468671,-0.003475985040033199] Loss: 22.84274813698414\n",
      "Iteracion: 14293 Gradiente: [0.00020136774786484087,-0.0034741370996878612] Loss: 22.842748124864315\n",
      "Iteracion: 14294 Gradiente: [0.00020126069461336252,-0.003472290141766621] Loss: 22.842748112757352\n",
      "Iteracion: 14295 Gradiente: [0.00020115369824698823,-0.0034704441657455714] Loss: 22.84274810066325\n",
      "Iteracion: 14296 Gradiente: [0.0002010467588178244,-0.003468599171100332] Loss: 22.842748088582038\n",
      "Iteracion: 14297 Gradiente: [0.00020093987618660474,-0.003466755157313628] Loss: 22.842748076513637\n",
      "Iteracion: 14298 Gradiente: [0.00020083305023680016,-0.003464912123868421] Loss: 22.842748064458075\n",
      "Iteracion: 14299 Gradiente: [0.00020072628135873552,-0.0034630700702212637] Loss: 22.842748052415327\n",
      "Iteracion: 14300 Gradiente: [0.00020061956905029394,-0.0034612289958766713] Loss: 22.842748040385377\n",
      "Iteracion: 14301 Gradiente: [0.00020051291363264075,-0.0034593889002955793] Loss: 22.842748028368216\n",
      "Iteracion: 14302 Gradiente: [0.00020040631471260895,-0.003457549782975988] Loss: 22.84274801636383\n",
      "Iteracion: 14303 Gradiente: [0.0002002997725876791,-0.0034557116433826234] Loss: 22.842748004372208\n",
      "Iteracion: 14304 Gradiente: [0.00020019328710532135,-0.0034538744809983275] Loss: 22.842747992393342\n",
      "Iteracion: 14305 Gradiente: [0.0002000868581906919,-0.0034520382953099706] Loss: 22.84274798042718\n",
      "Iteracion: 14306 Gradiente: [0.0001999804858769494,-0.0034502030857917515] Loss: 22.842747968473756\n",
      "Iteracion: 14307 Gradiente: [0.00019987417005419653,-0.003448368851933144] Loss: 22.84274795653305\n",
      "Iteracion: 14308 Gradiente: [0.0001997679108039089,-0.003446535593203374] Loss: 22.842747944605023\n",
      "Iteracion: 14309 Gradiente: [0.0001996617080607166,-0.003444703309091442] Loss: 22.842747932689658\n",
      "Iteracion: 14310 Gradiente: [0.00019955556175735485,-0.0034428719990784156] Loss: 22.842747920786977\n",
      "Iteracion: 14311 Gradiente: [0.00019944947190803458,-0.0034410416626448875] Loss: 22.84274790889696\n",
      "Iteracion: 14312 Gradiente: [0.00019934343849191313,-0.00343921229927228] Loss: 22.842747897019578\n",
      "Iteracion: 14313 Gradiente: [0.0001992374614133041,-0.003437383908446397] Loss: 22.8427478851548\n",
      "Iteracion: 14314 Gradiente: [0.00019913154065325973,-0.003435556489651977] Loss: 22.842747873302653\n",
      "Iteracion: 14315 Gradiente: [0.00019902567623830691,-0.0034337300423670078] Loss: 22.842747861463103\n",
      "Iteracion: 14316 Gradiente: [0.00019891986805949577,-0.0034319045660806085] Loss: 22.842747849636126\n",
      "Iteracion: 14317 Gradiente: [0.00019881411612535278,-0.0034300800602721894] Loss: 22.842747837821733\n",
      "Iteracion: 14318 Gradiente: [0.0001987084204642997,-0.0034282565244258953] Loss: 22.842747826019902\n",
      "Iteracion: 14319 Gradiente: [0.00019860278098633445,-0.003426433958026228] Loss: 22.84274781423061\n",
      "Iteracion: 14320 Gradiente: [0.00019849719764124528,-0.0034246123605619514] Loss: 22.84274780245385\n",
      "Iteracion: 14321 Gradiente: [0.0001983916704460853,-0.0034227917315127125] Loss: 22.8427477906896\n",
      "Iteracion: 14322 Gradiente: [0.00019828619939138054,-0.0034209720703634854] Loss: 22.84274777893786\n",
      "Iteracion: 14323 Gradiente: [0.000198180784304706,-0.0034191533766080085] Loss: 22.84274776719861\n",
      "Iteracion: 14324 Gradiente: [0.000198075425330065,-0.0034173356497216646] Loss: 22.84274775547185\n",
      "Iteracion: 14325 Gradiente: [0.0001979701223168225,-0.003415518889198547] Loss: 22.842747743757545\n",
      "Iteracion: 14326 Gradiente: [0.00019786487535498055,-0.0034137030945162886] Loss: 22.842747732055688\n",
      "Iteracion: 14327 Gradiente: [0.00019775968425885063,-0.00341188826517147] Loss: 22.842747720366287\n",
      "Iteracion: 14328 Gradiente: [0.00019765454920559478,-0.0034100744006389998] Loss: 22.842747708689313\n",
      "Iteracion: 14329 Gradiente: [0.00019754946995836537,-0.0034082615004175903] Loss: 22.84274769702473\n",
      "Iteracion: 14330 Gradiente: [0.0001974444466213754,-0.0034064495639865317] Loss: 22.842747685372554\n",
      "Iteracion: 14331 Gradiente: [0.00019733947907430623,-0.003404638590839326] Loss: 22.842747673732767\n",
      "Iteracion: 14332 Gradiente: [0.0001972345673560009,-0.0034028285804594085] Loss: 22.84274766210536\n",
      "Iteracion: 14333 Gradiente: [0.0001971297114001421,-0.003401019532337675] Loss: 22.842747650490292\n",
      "Iteracion: 14334 Gradiente: [0.00019702491114514942,-0.0033992114459638384] Loss: 22.842747638887584\n",
      "Iteracion: 14335 Gradiente: [0.00019692016667344586,-0.0033974043208205035] Loss: 22.842747627297193\n",
      "Iteracion: 14336 Gradiente: [0.00019681547787513408,-0.0033955981564008177] Loss: 22.842747615719144\n",
      "Iteracion: 14337 Gradiente: [0.0001967108447378981,-0.0033937929521933087] Loss: 22.842747604153406\n",
      "Iteracion: 14338 Gradiente: [0.00019660626719163096,-0.003391988707689701] Loss: 22.842747592599945\n",
      "Iteracion: 14339 Gradiente: [0.00019650174523064834,-0.0033901854223781667] Loss: 22.842747581058777\n",
      "Iteracion: 14340 Gradiente: [0.0001963972788293707,-0.0033883830957488923] Loss: 22.842747569529877\n",
      "Iteracion: 14341 Gradiente: [0.00019629286803800975,-0.0033865817272873263] Loss: 22.842747558013222\n",
      "Iteracion: 14342 Gradiente: [0.00019618851283477549,-0.003384781316483417] Loss: 22.84274754650881\n",
      "Iteracion: 14343 Gradiente: [0.0001960842129487143,-0.003382981862844877] Loss: 22.84274753501664\n",
      "Iteracion: 14344 Gradiente: [0.0001959799685617251,-0.0033811833658463778] Loss: 22.84274752353667\n",
      "Iteracion: 14345 Gradiente: [0.0001958757796169645,-0.0033793858249843824] Loss: 22.84274751206892\n",
      "Iteracion: 14346 Gradiente: [0.00019577164600832476,-0.0033775892397509702] Loss: 22.842747500613335\n",
      "Iteracion: 14347 Gradiente: [0.00019566756780780754,-0.0033757936096369197] Loss: 22.842747489169945\n",
      "Iteracion: 14348 Gradiente: [0.00019556354495762208,-0.003373998934132179] Loss: 22.842747477738722\n",
      "Iteracion: 14349 Gradiente: [0.00019545957738671406,-0.0033722052127347505] Loss: 22.842747466319647\n",
      "Iteracion: 14350 Gradiente: [0.0001953556649956075,-0.003370412444938727] Loss: 22.842747454912697\n",
      "Iteracion: 14351 Gradiente: [0.00019525180796241178,-0.0033686206302271887] Loss: 22.842747443517897\n",
      "Iteracion: 14352 Gradiente: [0.00019514800605596367,-0.003366829768104651] Loss: 22.842747432135184\n",
      "Iteracion: 14353 Gradiente: [0.00019504425942974042,-0.0033650398580538383] Loss: 22.84274742076458\n",
      "Iteracion: 14354 Gradiente: [0.00019494056788668483,-0.0033632508995774892] Loss: 22.84274740940607\n",
      "Iteracion: 14355 Gradiente: [0.0001948369314931142,-0.003361462892165671] Loss: 22.842747398059636\n",
      "Iteracion: 14356 Gradiente: [0.00019473335023765989,-0.0033596758353104644] Loss: 22.84274738672525\n",
      "Iteracion: 14357 Gradiente: [0.0001946298239497916,-0.0033578897285148437] Loss: 22.84274737540291\n",
      "Iteracion: 14358 Gradiente: [0.0001945263527223536,-0.0033561045712684036] Loss: 22.84274736409261\n",
      "Iteracion: 14359 Gradiente: [0.00019442293654208241,-0.0033543203630638156] Loss: 22.842747352794333\n",
      "Iteracion: 14360 Gradiente: [0.0001943195753388712,-0.0033525371033989635] Loss: 22.84274734150808\n",
      "Iteracion: 14361 Gradiente: [0.00019421626910893036,-0.003350754791767822] Loss: 22.84274733023381\n",
      "Iteracion: 14362 Gradiente: [0.00019411301782573293,-0.0033489734276676824] Loss: 22.84274731897153\n",
      "Iteracion: 14363 Gradiente: [0.00019400982137274999,-0.003347193010598678] Loss: 22.842747307721222\n",
      "Iteracion: 14364 Gradiente: [0.0001939066797869297,-0.003345413540053599] Loss: 22.842747296482873\n",
      "Iteracion: 14365 Gradiente: [0.0001938035930853251,-0.003343635015526066] Loss: 22.84274728525645\n",
      "Iteracion: 14366 Gradiente: [0.0001937005610898268,-0.0033418574365233172] Loss: 22.84274727404199\n",
      "Iteracion: 14367 Gradiente: [0.00019359758393401686,-0.00334008080253317] Loss: 22.84274726283943\n",
      "Iteracion: 14368 Gradiente: [0.00019349466148336583,-0.0033383051130567052] Loss: 22.842747251648788\n",
      "Iteracion: 14369 Gradiente: [0.00019339179386577144,-0.0033365303675856476] Loss: 22.842747240470043\n",
      "Iteracion: 14370 Gradiente: [0.00019328898082922782,-0.0033347565656299595] Loss: 22.84274722930318\n",
      "Iteracion: 14371 Gradiente: [0.0001931862224599475,-0.0033329837066801817] Loss: 22.842747218148176\n",
      "Iteracion: 14372 Gradiente: [0.00019308351880245784,-0.0033312117902331313] Loss: 22.84274720700504\n",
      "Iteracion: 14373 Gradiente: [0.00019298086966443862,-0.003329440815795692] Loss: 22.842747195873752\n",
      "Iteracion: 14374 Gradiente: [0.00019287827509041714,-0.0033276707828640895] Loss: 22.842747184754284\n",
      "Iteracion: 14375 Gradiente: [0.0001927757350538665,-0.0033259016909357323] Loss: 22.842747173646636\n",
      "Iteracion: 14376 Gradiente: [0.00019267324952068065,-0.0033241335395102806] Loss: 22.84274716255081\n",
      "Iteracion: 14377 Gradiente: [0.00019257081845580615,-0.003322366328090117] Loss: 22.842747151466778\n",
      "Iteracion: 14378 Gradiente: [0.00019246844198808806,-0.0033206000561648353] Loss: 22.84274714039451\n",
      "Iteracion: 14379 Gradiente: [0.00019236611983994105,-0.0033188347232510295] Loss: 22.842747129334022\n",
      "Iteracion: 14380 Gradiente: [0.00019226385211178846,-0.0033170703288408986] Loss: 22.842747118285292\n",
      "Iteracion: 14381 Gradiente: [0.0001921616387088913,-0.0033153068724397863] Loss: 22.84274710724831\n",
      "Iteracion: 14382 Gradiente: [0.00019205947969756684,-0.0033135443535443915] Loss: 22.842747096223054\n",
      "Iteracion: 14383 Gradiente: [0.00019195737503707733,-0.0033117827716539] Loss: 22.842747085209506\n",
      "Iteracion: 14384 Gradiente: [0.00019185532456162947,-0.003310022126280406] Loss: 22.842747074207686\n",
      "Iteracion: 14385 Gradiente: [0.00019175332845406957,-0.0033082624169135026] Loss: 22.842747063217555\n",
      "Iteracion: 14386 Gradiente: [0.00019165138646902354,-0.0033065036430674154] Loss: 22.8427470522391\n",
      "Iteracion: 14387 Gradiente: [0.00019154949867849306,-0.003304745804240383] Loss: 22.842747041272332\n",
      "Iteracion: 14388 Gradiente: [0.00019144766509574158,-0.0033029888999315917] Loss: 22.842747030317195\n",
      "Iteracion: 14389 Gradiente: [0.00019134588563739878,-0.003301232929648279] Loss: 22.84274701937372\n",
      "Iteracion: 14390 Gradiente: [0.0001912441603546237,-0.003299477892888447] Loss: 22.84274700844188\n",
      "Iteracion: 14391 Gradiente: [0.00019114248907783348,-0.003297723789165256] Loss: 22.84274699752165\n",
      "Iteracion: 14392 Gradiente: [0.0001910408718411342,-0.003295970617977062] Loss: 22.842746986613044\n",
      "Iteracion: 14393 Gradiente: [0.000190939308647368,-0.003294218378826841] Loss: 22.842746975716015\n",
      "Iteracion: 14394 Gradiente: [0.00019083779949179795,-0.003292467071218989] Loss: 22.842746964830578\n",
      "Iteracion: 14395 Gradiente: [0.00019073634420673595,-0.003290716694666429] Loss: 22.842746953956713\n",
      "Iteracion: 14396 Gradiente: [0.00019063494287839452,-0.0032889672486651495] Loss: 22.842746943094415\n",
      "Iteracion: 14397 Gradiente: [0.00019053359546224632,-0.003287218732723692] Loss: 22.842746932243646\n",
      "Iteracion: 14398 Gradiente: [0.00019043230190144791,-0.0032854711463490577] Loss: 22.842746921404416\n",
      "Iteracion: 14399 Gradiente: [0.00019033106231063356,-0.0032837244890361697] Loss: 22.842746910576704\n",
      "Iteracion: 14400 Gradiente: [0.00019022987652306256,-0.003281978760302806] Loss: 22.842746899760506\n",
      "Iteracion: 14401 Gradiente: [0.00019012874445820672,-0.00328023395965585] Loss: 22.84274688895581\n",
      "Iteracion: 14402 Gradiente: [0.0001900276661586986,-0.003278490086597685] Loss: 22.842746878162593\n",
      "Iteracion: 14403 Gradiente: [0.00018992664163211732,-0.003276747140634484] Loss: 22.84274686738086\n",
      "Iteracion: 14404 Gradiente: [0.00018982567083772512,-0.0032750051212730113] Loss: 22.842746856610574\n",
      "Iteracion: 14405 Gradiente: [0.00018972475372341552,-0.003273264028022993] Loss: 22.842746845851746\n",
      "Iteracion: 14406 Gradiente: [0.00018962389022002905,-0.003271523860393799] Loss: 22.842746835104347\n",
      "Iteracion: 14407 Gradiente: [0.00018952308035503999,-0.003269784617889826] Loss: 22.842746824368373\n",
      "Iteracion: 14408 Gradiente: [0.0001894223240166563,-0.003268046300025418] Loss: 22.842746813643807\n",
      "Iteracion: 14409 Gradiente: [0.00018932162130814352,-0.0032663089062995236] Loss: 22.842746802930666\n",
      "Iteracion: 14410 Gradiente: [0.00018922097211486743,-0.0032645724362274347] Loss: 22.842746792228887\n",
      "Iteracion: 14411 Gradiente: [0.00018912037640272198,-0.0032628368893161527] Loss: 22.842746781538487\n",
      "Iteracion: 14412 Gradiente: [0.00018901983419444452,-0.003261102265075048] Loss: 22.842746770859467\n",
      "Iteracion: 14413 Gradiente: [0.00018891934543508644,-0.003259368563012662] Loss: 22.84274676019179\n",
      "Iteracion: 14414 Gradiente: [0.00018881891012370033,-0.003257635782637417] Loss: 22.842746749535433\n",
      "Iteracion: 14415 Gradiente: [0.0001887185282204958,-0.0032559039234618817] Loss: 22.84274673889042\n",
      "Iteracion: 14416 Gradiente: [0.00018861819970273548,-0.0032541729849941228] Loss: 22.842746728256735\n",
      "Iteracion: 14417 Gradiente: [0.00018851792444820603,-0.0032524429667518005] Loss: 22.842746717634327\n",
      "Iteracion: 14418 Gradiente: [0.0001884177025857525,-0.003250713868234338] Loss: 22.84274670702323\n",
      "Iteracion: 14419 Gradiente: [0.00018831753400831984,-0.003248985688959512] Loss: 22.842746696423408\n",
      "Iteracion: 14420 Gradiente: [0.00018821741852642997,-0.0032472584284456945] Loss: 22.84274668583485\n",
      "Iteracion: 14421 Gradiente: [0.0001881173563485087,-0.0032455320861921894] Loss: 22.842746675257544\n",
      "Iteracion: 14422 Gradiente: [0.00018801734742339703,-0.00324380666171263] Loss: 22.842746664691497\n",
      "Iteracion: 14423 Gradiente: [0.00018791739162035508,-0.0032420821545233736] Loss: 22.842746654136658\n",
      "Iteracion: 14424 Gradiente: [0.00018781748890338198,-0.0032403585641394748] Loss: 22.84274664359306\n",
      "Iteracion: 14425 Gradiente: [0.00018771763940132283,-0.0032386358900618954] Loss: 22.84274663306066\n",
      "Iteracion: 14426 Gradiente: [0.00018761784292469958,-0.0032369141318145723] Loss: 22.84274662253947\n",
      "Iteracion: 14427 Gradiente: [0.00018751809958246214,-0.003235193288902494] Loss: 22.842746612029437\n",
      "Iteracion: 14428 Gradiente: [0.0001874184092211332,-0.003233473360845925] Loss: 22.842746601530585\n",
      "Iteracion: 14429 Gradiente: [0.00018731877183976545,-0.003231754347157552] Loss: 22.842746591042904\n",
      "Iteracion: 14430 Gradiente: [0.00018721918737867326,-0.003230036247350772] Loss: 22.842746580566363\n",
      "Iteracion: 14431 Gradiente: [0.0001871196558530149,-0.003228319060939692] Loss: 22.842746570100953\n",
      "Iteracion: 14432 Gradiente: [0.00018702017736605588,-0.0032266027874310764] Loss: 22.84274655964669\n",
      "Iteracion: 14433 Gradiente: [0.00018692075169326471,-0.0032248874263487437] Loss: 22.84274654920352\n",
      "Iteracion: 14434 Gradiente: [0.00018682137887158964,-0.003223172977207748] Loss: 22.84274653877146\n",
      "Iteracion: 14435 Gradiente: [0.00018672205892945233,-0.003221459439516394] Loss: 22.84274652835048\n",
      "Iteracion: 14436 Gradiente: [0.00018662279172095472,-0.003219746812798974] Loss: 22.842746517940597\n",
      "Iteracion: 14437 Gradiente: [0.00018652357729915064,-0.0032180350965648565] Loss: 22.84274650754175\n",
      "Iteracion: 14438 Gradiente: [0.00018642441569719873,-0.003216324290326966] Loss: 22.842746497153957\n",
      "Iteracion: 14439 Gradiente: [0.0001863253067417266,-0.003214614393612199] Loss: 22.84274648677724\n",
      "Iteracion: 14440 Gradiente: [0.00018622625047157726,-0.0032129054059294522] Loss: 22.842746476411538\n",
      "Iteracion: 14441 Gradiente: [0.00018612724695969972,-0.0032111973267914115] Loss: 22.842746466056845\n",
      "Iteracion: 14442 Gradiente: [0.0001860282959446143,-0.003209490155728171] Loss: 22.842746455713158\n",
      "Iteracion: 14443 Gradiente: [0.00018592939768306374,-0.003207783892240338] Loss: 22.842746445380477\n",
      "Iteracion: 14444 Gradiente: [0.00018583055181882932,-0.003206078535866889] Loss: 22.842746435058764\n",
      "Iteracion: 14445 Gradiente: [0.00018573175856981077,-0.003204374086108667] Loss: 22.84274642474804\n",
      "Iteracion: 14446 Gradiente: [0.00018563301787348033,-0.0032026705424864116] Loss: 22.842746414448264\n",
      "Iteracion: 14447 Gradiente: [0.0001855343296938372,-0.0032009679045188477] Loss: 22.84274640415945\n",
      "Iteracion: 14448 Gradiente: [0.00018543569392382627,-0.00319926617172932] Loss: 22.84274639388156\n",
      "Iteracion: 14449 Gradiente: [0.00018533711062787006,-0.003197565343630989] Loss: 22.842746383614596\n",
      "Iteracion: 14450 Gradiente: [0.00018523857975291473,-0.0031958654197447108] Loss: 22.842746373358555\n",
      "Iteracion: 14451 Gradiente: [0.00018514010117485214,-0.0031941663995956066] Loss: 22.842746363113402\n",
      "Iteracion: 14452 Gradiente: [0.0001850416750102113,-0.003192468282693876] Loss: 22.84274635287915\n",
      "Iteracion: 14453 Gradiente: [0.000184943301122568,-0.0031907710685654678] Loss: 22.84274634265577\n",
      "Iteracion: 14454 Gradiente: [0.00018484497960950344,-0.003189074756724608] Loss: 22.84274633244326\n",
      "Iteracion: 14455 Gradiente: [0.00018474671039143687,-0.0031873793466928647] Loss: 22.842746322241606\n",
      "Iteracion: 14456 Gradiente: [0.00018464849343899914,-0.003185684837992279] Loss: 22.842746312050796\n",
      "Iteracion: 14457 Gradiente: [0.0001845503285826074,-0.0031839912301510507] Loss: 22.842746301870832\n",
      "Iteracion: 14458 Gradiente: [0.0001844522159747915,-0.0031822985226806822] Loss: 22.84274629170167\n",
      "Iteracion: 14459 Gradiente: [0.00018435415552839156,-0.0031806067151049907] Loss: 22.842746281543324\n",
      "Iteracion: 14460 Gradiente: [0.0001842561472083541,-0.0031789158069475574] Loss: 22.842746271395775\n",
      "Iteracion: 14461 Gradiente: [0.00018415819092941396,-0.0031772257977318455] Loss: 22.842746261259023\n",
      "Iteracion: 14462 Gradiente: [0.00018406028678157326,-0.0031755366869726724] Loss: 22.84274625113303\n",
      "Iteracion: 14463 Gradiente: [0.000183962434708936,-0.0031738484741976455] Loss: 22.84274624101782\n",
      "Iteracion: 14464 Gradiente: [0.00018386463466886957,-0.003172161158925372] Loss: 22.842746230913338\n",
      "Iteracion: 14465 Gradiente: [0.00018376688662063618,-0.0031704747406823943] Loss: 22.842746220819613\n",
      "Iteracion: 14466 Gradiente: [0.00018366919046949686,-0.0031687892189945425] Loss: 22.842746210736607\n",
      "Iteracion: 14467 Gradiente: [0.0001835715462182937,-0.0031671045933849245] Loss: 22.842746200664333\n",
      "Iteracion: 14468 Gradiente: [0.00018347395397976622,-0.0031654208633681217] Loss: 22.842746190602764\n",
      "Iteracion: 14469 Gradiente: [0.00018337641360801626,-0.0031637380284743463] Loss: 22.84274618055188\n",
      "Iteracion: 14470 Gradiente: [0.00018327892505283218,-0.003162056088229074] Loss: 22.84274617051169\n",
      "Iteracion: 14471 Gradiente: [0.0001831814883369513,-0.0031603750421544657] Loss: 22.842746160482157\n",
      "Iteracion: 14472 Gradiente: [0.00018308410340921454,-0.0031586948897758776] Loss: 22.842746150463295\n",
      "Iteracion: 14473 Gradiente: [0.00018298677026299022,-0.003157015630618195] Loss: 22.842746140455073\n",
      "Iteracion: 14474 Gradiente: [0.0001828894889589113,-0.0031553372642013264] Loss: 22.842746130457517\n",
      "Iteracion: 14475 Gradiente: [0.00018279225931223665,-0.003153659790058564] Loss: 22.842746120470558\n",
      "Iteracion: 14476 Gradiente: [0.00018269508133338756,-0.003151983207713016] Loss: 22.842746110494243\n",
      "Iteracion: 14477 Gradiente: [0.0001825979549996267,-0.003150307516691342] Loss: 22.84274610052851\n",
      "Iteracion: 14478 Gradiente: [0.0001825008803431653,-0.003148632716516294] Loss: 22.842746090573375\n",
      "Iteracion: 14479 Gradiente: [0.00018240385720768397,-0.003146958806721519] Loss: 22.842746080628825\n",
      "Iteracion: 14480 Gradiente: [0.00018230688578739772,-0.00314528578681994] Loss: 22.842746070694844\n",
      "Iteracion: 14481 Gradiente: [0.00018220996583787988,-0.003143613656352902] Loss: 22.842746060771436\n",
      "Iteracion: 14482 Gradiente: [0.00018211309740555256,-0.0031419424148429204] Loss: 22.842746050858544\n",
      "Iteracion: 14483 Gradiente: [0.00018201628043451972,-0.003140272061817247] Loss: 22.842746040956225\n",
      "Iteracion: 14484 Gradiente: [0.00018191951502520473,-0.0031386025967976867] Loss: 22.84274603106441\n",
      "Iteracion: 14485 Gradiente: [0.0001818228010421308,-0.0031369340193172944] Loss: 22.842746021183117\n",
      "Iteracion: 14486 Gradiente: [0.00018172613847108703,-0.0031352663289045068] Loss: 22.84274601131232\n",
      "Iteracion: 14487 Gradiente: [0.00018162952721070268,-0.0031335995250906024] Loss: 22.84274600145203\n",
      "Iteracion: 14488 Gradiente: [0.00018153296744761367,-0.0031319336073947807] Loss: 22.842745991602218\n",
      "Iteracion: 14489 Gradiente: [0.00018143645887297072,-0.003130268575359807] Loss: 22.842745981762867\n",
      "Iteracion: 14490 Gradiente: [0.0001813400016980419,-0.003128604428500381] Loss: 22.84274597193398\n",
      "Iteracion: 14491 Gradiente: [0.00018124359579777167,-0.0031269411663547688] Loss: 22.842745962115544\n",
      "Iteracion: 14492 Gradiente: [0.00018114724120342393,-0.0031252787884454847] Loss: 22.842745952307535\n",
      "Iteracion: 14493 Gradiente: [0.00018105093774920534,-0.003123617294314229] Loss: 22.84274594250996\n",
      "Iteracion: 14494 Gradiente: [0.00018095468546922195,-0.0031219566834842284] Loss: 22.842745932722803\n",
      "Iteracion: 14495 Gradiente: [0.00018085848444210722,-0.003120296955480721] Loss: 22.842745922946044\n",
      "Iteracion: 14496 Gradiente: [0.00018076233457880637,-0.00311863810983913] Loss: 22.84274591317968\n",
      "Iteracion: 14497 Gradiente: [0.00018066623570973662,-0.0031169801460976033] Loss: 22.842745903423694\n",
      "Iteracion: 14498 Gradiente: [0.00018057018796753256,-0.0031153230637780637] Loss: 22.84274589367807\n",
      "Iteracion: 14499 Gradiente: [0.00018047419140145848,-0.003113666862408356] Loss: 22.84274588394283\n",
      "Iteracion: 14500 Gradiente: [0.00018037824571877082,-0.00311201154153468] Loss: 22.842745874217933\n",
      "Iteracion: 14501 Gradiente: [0.00018028235113642192,-0.003110357100675761] Loss: 22.842745864503346\n",
      "Iteracion: 14502 Gradiente: [0.00018018650752935628,-0.0031087035393672596] Loss: 22.8427458547991\n",
      "Iteracion: 14503 Gradiente: [0.00018009071482083527,-0.003107050857147087] Loss: 22.842745845105178\n",
      "Iteracion: 14504 Gradiente: [0.00017999497310370317,-0.0031053990535388227] Loss: 22.84274583542155\n",
      "Iteracion: 14505 Gradiente: [0.0001798992822443779,-0.0031037481280821546] Loss: 22.84274582574822\n",
      "Iteracion: 14506 Gradiente: [0.000179803642273176,-0.0031020980803083613] Loss: 22.842745816085184\n",
      "Iteracion: 14507 Gradiente: [0.00017970805315883353,-0.0031004489097494316] Loss: 22.84274580643239\n",
      "Iteracion: 14508 Gradiente: [0.0001796125148265067,-0.003098800615943039] Loss: 22.84274579678988\n",
      "Iteracion: 14509 Gradiente: [0.0001795170272278786,-0.003097153198422949] Loss: 22.842745787157632\n",
      "Iteracion: 14510 Gradiente: [0.00017942159048895217,-0.003095506656714993] Loss: 22.84274577753559\n",
      "Iteracion: 14511 Gradiente: [0.00017932620445814488,-0.0030938609903609898] Loss: 22.842745767923795\n",
      "Iteracion: 14512 Gradiente: [0.00017923086908808728,-0.0030922161988965985] Loss: 22.842745758322213\n",
      "Iteracion: 14513 Gradiente: [0.00017913558446499186,-0.0030905722818499017] Loss: 22.84274574873084\n",
      "Iteracion: 14514 Gradiente: [0.00017904035056422649,-0.0030889292387565586] Loss: 22.842745739149667\n",
      "Iteracion: 14515 Gradiente: [0.00017894516720578696,-0.003087287069160046] Loss: 22.84274572957867\n",
      "Iteracion: 14516 Gradiente: [0.00017885003452799235,-0.003085645772586195] Loss: 22.842745720017838\n",
      "Iteracion: 14517 Gradiente: [0.00017875495227978415,-0.0030840053485851134] Loss: 22.842745710467188\n",
      "Iteracion: 14518 Gradiente: [0.00017865992068853606,-0.003082365796677422] Loss: 22.842745700926685\n",
      "Iteracion: 14519 Gradiente: [0.0001785649396329821,-0.0030807271164054128] Loss: 22.842745691396328\n",
      "Iteracion: 14520 Gradiente: [0.0001784700089880668,-0.0030790893073096015] Loss: 22.84274568187608\n",
      "Iteracion: 14521 Gradiente: [0.00017837512883526566,-0.0030774523689231616] Loss: 22.84274567236598\n",
      "Iteracion: 14522 Gradiente: [0.0001782802991101562,-0.0030758163007841214] Loss: 22.84274566286596\n",
      "Iteracion: 14523 Gradiente: [0.0001781855198098962,-0.0030741811024279047] Loss: 22.842745653376067\n",
      "Iteracion: 14524 Gradiente: [0.00017809079086153663,-0.003072546773395975] Loss: 22.84274564389624\n",
      "Iteracion: 14525 Gradiente: [0.0001779961124109756,-0.0030709133132161763] Loss: 22.842745634426503\n",
      "Iteracion: 14526 Gradiente: [0.00017790148418346993,-0.003069280721438735] Loss: 22.842745624966827\n",
      "Iteracion: 14527 Gradiente: [0.0001778069063031277,-0.0030676489975959955] Loss: 22.8427456155172\n",
      "Iteracion: 14528 Gradiente: [0.00017771237867710474,-0.0030660181412298943] Loss: 22.84274560607763\n",
      "Iteracion: 14529 Gradiente: [0.00017761790140866652,-0.003064388151869816] Loss: 22.84274559664808\n",
      "Iteracion: 14530 Gradiente: [0.00017752347421643813,-0.0030627590290716713] Loss: 22.84274558722857\n",
      "Iteracion: 14531 Gradiente: [0.00017742909732968806,-0.003061130772358922] Loss: 22.842745577819066\n",
      "Iteracion: 14532 Gradiente: [0.00017733477054851694,-0.003059503381281085] Loss: 22.842745568419556\n",
      "Iteracion: 14533 Gradiente: [0.00017724049390797821,-0.003057876855374649] Loss: 22.84274555903005\n",
      "Iteracion: 14534 Gradiente: [0.00017714626746681005,-0.0030562511941752747] Loss: 22.842745549650513\n",
      "Iteracion: 14535 Gradiente: [0.00017705209106490353,-0.0030546263972299906] Loss: 22.842745540280944\n",
      "Iteracion: 14536 Gradiente: [0.0001769579647932081,-0.0030530024640730365] Loss: 22.842745530921356\n",
      "Iteracion: 14537 Gradiente: [0.00017686388849540434,-0.0030513793942523884] Loss: 22.842745521571704\n",
      "Iteracion: 14538 Gradiente: [0.00017676986223212529,-0.003049757187304773] Loss: 22.842745512231993\n",
      "Iteracion: 14539 Gradiente: [0.000176675885972107,-0.003048135842770705] Loss: 22.8427455029022\n",
      "Iteracion: 14540 Gradiente: [0.0001765819596272422,-0.003046515360195438] Loss: 22.842745493582346\n",
      "Iteracion: 14541 Gradiente: [0.00017648808320037309,-0.003044895739120079] Loss: 22.842745484272374\n",
      "Iteracion: 14542 Gradiente: [0.00017639425668771006,-0.0030432769790844343] Loss: 22.842745474972308\n",
      "Iteracion: 14543 Gradiente: [0.00017630048009872705,-0.003041659079628308] Loss: 22.84274546568212\n",
      "Iteracion: 14544 Gradiente: [0.00017620675336710672,-0.00304004204029719] Loss: 22.842745456401826\n",
      "Iteracion: 14545 Gradiente: [0.00017611307646821692,-0.003038425860635267] Loss: 22.842745447131385\n",
      "Iteracion: 14546 Gradiente: [0.00017601944924573824,-0.0030368105401888577] Loss: 22.8427454378708\n",
      "Iteracion: 14547 Gradiente: [0.00017592587198104562,-0.003035196078487819] Loss: 22.84274542862006\n",
      "Iteracion: 14548 Gradiente: [0.00017583234443255453,-0.003033582475085576] Loss: 22.84274541937914\n",
      "Iteracion: 14549 Gradiente: [0.00017573886654626374,-0.00303196972952738] Loss: 22.84274541014806\n",
      "Iteracion: 14550 Gradiente: [0.0001756454383316471,-0.003030357841355998] Loss: 22.84274540092678\n",
      "Iteracion: 14551 Gradiente: [0.00017555205982849504,-0.0030287468101107607] Loss: 22.842745391715315\n",
      "Iteracion: 14552 Gradiente: [0.00017545873099133284,-0.0030271366353377496] Loss: 22.842745382513645\n",
      "Iteracion: 14553 Gradiente: [0.00017536545170931578,-0.0030255273165876654] Loss: 22.842745373321744\n",
      "Iteracion: 14554 Gradiente: [0.00017527222205823516,-0.0030239188533980627] Loss: 22.842745364139617\n",
      "Iteracion: 14555 Gradiente: [0.00017517904199451095,-0.0030223112453149053] Loss: 22.84274535496725\n",
      "Iteracion: 14556 Gradiente: [0.00017508591146414194,-0.003020704491886169] Loss: 22.84274534580463\n",
      "Iteracion: 14557 Gradiente: [0.00017499283036670477,-0.0030190985926618432] Loss: 22.842745336651753\n",
      "Iteracion: 14558 Gradiente: [0.00017489979877609584,-0.0030174935471810236] Loss: 22.842745327508613\n",
      "Iteracion: 14559 Gradiente: [0.00017480681665063002,-0.0030158893549912117] Loss: 22.84274531837518\n",
      "Iteracion: 14560 Gradiente: [0.0001747138840272555,-0.003014286015634937] Loss: 22.842745309251452\n",
      "Iteracion: 14561 Gradiente: [0.00017462100077523247,-0.0030126835286660974] Loss: 22.842745300137437\n",
      "Iteracion: 14562 Gradiente: [0.00017452816692392997,-0.003011081893626984] Loss: 22.842745291033115\n",
      "Iteracion: 14563 Gradiente: [0.0001744353823369238,-0.0030094811100704294] Loss: 22.842745281938452\n",
      "Iteracion: 14564 Gradiente: [0.0001743426471496908,-0.003007881177536002] Loss: 22.842745272853456\n",
      "Iteracion: 14565 Gradiente: [0.00017424996123622806,-0.0030062820955764143] Loss: 22.84274526377812\n",
      "Iteracion: 14566 Gradiente: [0.00017415732466664243,-0.003004683863732538] Loss: 22.842745254712433\n",
      "Iteracion: 14567 Gradiente: [0.0001740647373073519,-0.0030030864815625336] Loss: 22.842745245656392\n",
      "Iteracion: 14568 Gradiente: [0.0001739721991659356,-0.0030014899486092853] Loss: 22.842745236609968\n",
      "Iteracion: 14569 Gradiente: [0.00017387971022344572,-0.0029998942644205327] Loss: 22.84274522757317\n",
      "Iteracion: 14570 Gradiente: [0.00017378727037566932,-0.0029982994285528974] Loss: 22.842745218545975\n",
      "Iteracion: 14571 Gradiente: [0.00017369487973534586,-0.002996705440543342] Loss: 22.842745209528353\n",
      "Iteracion: 14572 Gradiente: [0.0001736025381546824,-0.002995112299950975] Loss: 22.842745200520344\n",
      "Iteracion: 14573 Gradiente: [0.00017351024575494497,-0.002993520006315483] Loss: 22.842745191521903\n",
      "Iteracion: 14574 Gradiente: [0.00017341800238076152,-0.0029919285591950267] Loss: 22.842745182533026\n",
      "Iteracion: 14575 Gradiente: [0.0001733258080264477,-0.002990337958137227] Loss: 22.842745173553695\n",
      "Iteracion: 14576 Gradiente: [0.00017323366271758308,-0.0029887482026910088] Loss: 22.84274516458392\n",
      "Iteracion: 14577 Gradiente: [0.00017314156635848122,-0.00298715929241015] Loss: 22.84274515562369\n",
      "Iteracion: 14578 Gradiente: [0.00017304951897093208,-0.002985571226840141] Loss: 22.842745146672954\n",
      "Iteracion: 14579 Gradiente: [0.00017295752059662088,-0.0029839840055312077] Loss: 22.842745137731754\n",
      "Iteracion: 14580 Gradiente: [0.00017286557105838558,-0.0029823976280416295] Loss: 22.842745128800054\n",
      "Iteracion: 14581 Gradiente: [0.000172773670457597,-0.0029808120939161854] Loss: 22.842745119877847\n",
      "Iteracion: 14582 Gradiente: [0.00017268181862846178,-0.0029792274027148125] Loss: 22.842745110965122\n",
      "Iteracion: 14583 Gradiente: [0.00017259001569887764,-0.0029776435539788557] Loss: 22.842745102061876\n",
      "Iteracion: 14584 Gradiente: [0.00017249826156084206,-0.0029760605472667124] Loss: 22.842745093168098\n",
      "Iteracion: 14585 Gradiente: [0.00017240655618593336,-0.0029744783821306223] Loss: 22.84274508428377\n",
      "Iteracion: 14586 Gradiente: [0.00017231489957320417,-0.0029728970581217595] Loss: 22.842745075408878\n",
      "Iteracion: 14587 Gradiente: [0.00017222329162128365,-0.0029713165747974556] Loss: 22.84274506654343\n",
      "Iteracion: 14588 Gradiente: [0.0001721317323964892,-0.002969736931703911] Loss: 22.842745057687395\n",
      "Iteracion: 14589 Gradiente: [0.00017204022189882078,-0.002968158128395733] Loss: 22.842745048840793\n",
      "Iteracion: 14590 Gradiente: [0.00017194876001553895,-0.0029665801644288337] Loss: 22.84274504000357\n",
      "Iteracion: 14591 Gradiente: [0.0001718573467760128,-0.0029650030393554515] Loss: 22.842745031175763\n",
      "Iteracion: 14592 Gradiente: [0.00017176598212813586,-0.0029634267527305505] Loss: 22.84274502235733\n",
      "Iteracion: 14593 Gradiente: [0.00017167466607190818,-0.002961851304106015] Loss: 22.842745013548274\n",
      "Iteracion: 14594 Gradiente: [0.00017158339860164537,-0.002960276693036216] Loss: 22.84274500474857\n",
      "Iteracion: 14595 Gradiente: [0.00017149217957618628,-0.002958702919083341] Loss: 22.842744995958224\n",
      "Iteracion: 14596 Gradiente: [0.00017140100903153174,-0.0029571299817969057] Loss: 22.842744987177234\n",
      "Iteracion: 14597 Gradiente: [0.0001713098871003164,-0.002955557880723584] Loss: 22.842744978405573\n",
      "Iteracion: 14598 Gradiente: [0.00017121881345948017,-0.0029539866154370503] Loss: 22.842744969643217\n",
      "Iteracion: 14599 Gradiente: [0.00017112778834776538,-0.0029524161854765176] Loss: 22.842744960890208\n",
      "Iteracion: 14600 Gradiente: [0.00017103681150369235,-0.0029508465904126997] Loss: 22.84274495214648\n",
      "Iteracion: 14601 Gradiente: [0.0001709458830674748,-0.0029492778297893095] Loss: 22.842744943412058\n",
      "Iteracion: 14602 Gradiente: [0.00017085500300690153,-0.0029477099031671133] Loss: 22.842744934686912\n",
      "Iteracion: 14603 Gradiente: [0.00017076417124712863,-0.0029461428101026144] Loss: 22.84274492597104\n",
      "Iteracion: 14604 Gradiente: [0.00017067338780236697,-0.00294457655015196] Loss: 22.842744917264433\n",
      "Iteracion: 14605 Gradiente: [0.0001705826525210341,-0.0029430111228785213] Loss: 22.84274490856708\n",
      "Iteracion: 14606 Gradiente: [0.00017049196563050372,-0.0029414465278250644] Loss: 22.84274489987899\n",
      "Iteracion: 14607 Gradiente: [0.0001704013268314005,-0.0029398827645659748] Loss: 22.842744891200116\n",
      "Iteracion: 14608 Gradiente: [0.0001703107362573064,-0.0029383198326497014] Loss: 22.842744882530475\n",
      "Iteracion: 14609 Gradiente: [0.00017022019383337769,-0.0029367577316354717] Loss: 22.84274487387006\n",
      "Iteracion: 14610 Gradiente: [0.00017012969949519173,-0.0029351964610865385] Loss: 22.842744865218826\n",
      "Iteracion: 14611 Gradiente: [0.00017003925332611896,-0.002933636020551944] Loss: 22.842744856576804\n",
      "Iteracion: 14612 Gradiente: [0.00016994885525131547,-0.002932076409595889] Loss: 22.842744847943965\n",
      "Iteracion: 14613 Gradiente: [0.00016985850527362345,-0.002930517627774639] Loss: 22.8427448393203\n",
      "Iteracion: 14614 Gradiente: [0.0001697682032793561,-0.0029289596746518024] Loss: 22.842744830705804\n",
      "Iteracion: 14615 Gradiente: [0.00016967794926093423,-0.002927402549787672] Loss: 22.842744822100475\n",
      "Iteracion: 14616 Gradiente: [0.00016958774326383263,-0.002925846252735553] Loss: 22.84274481350427\n",
      "Iteracion: 14617 Gradiente: [0.00016949758526152436,-0.0029242907830563307] Loss: 22.842744804917228\n",
      "Iteracion: 14618 Gradiente: [0.00016940747511095348,-0.002922736140317876] Loss: 22.842744796339307\n",
      "Iteracion: 14619 Gradiente: [0.0001693174128386469,-0.002921182324076336] Loss: 22.842744787770485\n",
      "Iteracion: 14620 Gradiente: [0.0001692273985374489,-0.0029196293338858465] Loss: 22.842744779210793\n",
      "Iteracion: 14621 Gradiente: [0.00016913743211072568,-0.002918077169311554] Loss: 22.8427447706602\n",
      "Iteracion: 14622 Gradiente: [0.00016904751349215984,-0.0029165258299158835] Loss: 22.842744762118688\n",
      "Iteracion: 14623 Gradiente: [0.0001689576425803807,-0.002914975315266588] Loss: 22.84274475358626\n",
      "Iteracion: 14624 Gradiente: [0.00016886781950802287,-0.002913425624914486] Loss: 22.842744745062895\n",
      "Iteracion: 14625 Gradiente: [0.0001687780442125586,-0.002911876758423896] Loss: 22.842744736548585\n",
      "Iteracion: 14626 Gradiente: [0.00016868831665703966,-0.0029103287153581904] Loss: 22.84274472804335\n",
      "Iteracion: 14627 Gradiente: [0.00016859863674293743,-0.0029087814952823976] Loss: 22.842744719547138\n",
      "Iteracion: 14628 Gradiente: [0.000168509004487305,-0.002907235097757995] Loss: 22.84274471105995\n",
      "Iteracion: 14629 Gradiente: [0.00016841941989961622,-0.002905689522345038] Loss: 22.842744702581793\n",
      "Iteracion: 14630 Gradiente: [0.00016832988295050199,-0.0029041447686057134] Loss: 22.84274469411265\n",
      "Iteracion: 14631 Gradiente: [0.0001682403936257515,-0.002902600836104341] Loss: 22.84274468565251\n",
      "Iteracion: 14632 Gradiente: [0.00016815095187894258,-0.002901057724405713] Loss: 22.842744677201367\n",
      "Iteracion: 14633 Gradiente: [0.00016806155772239133,-0.0028995154330696475] Loss: 22.84274466875919\n",
      "Iteracion: 14634 Gradiente: [0.00016797221103862132,-0.0028979739616652013] Loss: 22.842744660326012\n",
      "Iteracion: 14635 Gradiente: [0.00016788291184184346,-0.002896433309755035] Loss: 22.842744651901782\n",
      "Iteracion: 14636 Gradiente: [0.00016779366015668985,-0.0028948934769005064] Loss: 22.842744643486505\n",
      "Iteracion: 14637 Gradiente: [0.0001677044559149484,-0.002893354462669251] Loss: 22.84274463508017\n",
      "Iteracion: 14638 Gradiente: [0.00016761529908346044,-0.002891816266625469] Loss: 22.84274462668278\n",
      "Iteracion: 14639 Gradiente: [0.0001675261896612786,-0.0028902788883331237] Loss: 22.84274461829431\n",
      "Iteracion: 14640 Gradiente: [0.00016743712766735067,-0.0028887423273555868] Loss: 22.842744609914767\n",
      "Iteracion: 14641 Gradiente: [0.00016734811287903993,-0.002887206583268546] Loss: 22.84274460154413\n",
      "Iteracion: 14642 Gradiente: [0.00016725914547066623,-0.002885671655627912] Loss: 22.842744593182374\n",
      "Iteracion: 14643 Gradiente: [0.00016717022533991138,-0.002884137544001912] Loss: 22.84274458482952\n",
      "Iteracion: 14644 Gradiente: [0.00016708135250477577,-0.0028826042479572324] Loss: 22.842744576485554\n",
      "Iteracion: 14645 Gradiente: [0.00016699252691410038,-0.00288107176705914] Loss: 22.84274456815043\n",
      "Iteracion: 14646 Gradiente: [0.00016690374854988477,-0.0028795401008742043] Loss: 22.84274455982419\n",
      "Iteracion: 14647 Gradiente: [0.000166815017368549,-0.002878009248970533] Loss: 22.8427445515068\n",
      "Iteracion: 14648 Gradiente: [0.0001667263334487264,-0.0028764792109100767] Loss: 22.842744543198247\n",
      "Iteracion: 14649 Gradiente: [0.00016663769662936072,-0.002874949986267931] Loss: 22.842744534898525\n",
      "Iteracion: 14650 Gradiente: [0.00016654910699571702,-0.002873421574604625] Loss: 22.842744526607625\n",
      "Iteracion: 14651 Gradiente: [0.00016646056435642246,-0.002871893975497149] Loss: 22.842744518325535\n",
      "Iteracion: 14652 Gradiente: [0.00016637206872947748,-0.0028703671885121906] Loss: 22.842744510052242\n",
      "Iteracion: 14653 Gradiente: [0.0001662836202010946,-0.0028688412132113444] Loss: 22.84274450178776\n",
      "Iteracion: 14654 Gradiente: [0.00016619521870590384,-0.0028673160491661537] Loss: 22.842744493532056\n",
      "Iteracion: 14655 Gradiente: [0.0001661068642609583,-0.0028657916959408188] Loss: 22.842744485285117\n",
      "Iteracion: 14656 Gradiente: [0.0001660185567213072,-0.002864268153111264] Loss: 22.842744477046953\n",
      "Iteracion: 14657 Gradiente: [0.0001659302961921109,-0.0028627454202424] Loss: 22.842744468817546\n",
      "Iteracion: 14658 Gradiente: [0.00016584208260610466,-0.0028612234969023357] Loss: 22.842744460596894\n",
      "Iteracion: 14659 Gradiente: [0.0001657539158571808,-0.002859702382667943] Loss: 22.842744452384974\n",
      "Iteracion: 14660 Gradiente: [0.00016566579593776018,-0.0028581820771056716] Loss: 22.84274444418178\n",
      "Iteracion: 14661 Gradiente: [0.00016557772290942314,-0.002856662579782802] Loss: 22.842744435987314\n",
      "Iteracion: 14662 Gradiente: [0.00016548969668690462,-0.0028551438902722974] Loss: 22.842744427801556\n",
      "Iteracion: 14663 Gradiente: [0.0001654017173725227,-0.002853626008137648] Loss: 22.842744419624506\n",
      "Iteracion: 14664 Gradiente: [0.00016531378462900648,-0.002852108932966857] Loss: 22.84274441145613\n",
      "Iteracion: 14665 Gradiente: [0.00016522589881731164,-0.002850592664309796] Loss: 22.84274440329644\n",
      "Iteracion: 14666 Gradiente: [0.00016513805969964324,-0.002849077201747837] Loss: 22.84274439514543\n",
      "Iteracion: 14667 Gradiente: [0.00016505026721726305,-0.0028475625448569036] Loss: 22.842744387003084\n",
      "Iteracion: 14668 Gradiente: [0.0001649625214383832,-0.0028460486932023817] Loss: 22.842744378869387\n",
      "Iteracion: 14669 Gradiente: [0.0001648748222815281,-0.0028445356463596026] Loss: 22.84274437074435\n",
      "Iteracion: 14670 Gradiente: [0.00016478716977038252,-0.002843023403896557] Loss: 22.842744362627936\n",
      "Iteracion: 14671 Gradiente: [0.0001646995639077886,-0.002841511965385853] Loss: 22.84274435452015\n",
      "Iteracion: 14672 Gradiente: [0.0001646120044502671,-0.002840001330410994] Loss: 22.842744346420993\n",
      "Iteracion: 14673 Gradiente: [0.0001645244917161411,-0.0028384914985275364] Loss: 22.84274433833043\n",
      "Iteracion: 14674 Gradiente: [0.00016443702542687789,-0.0028369824693213514] Loss: 22.842744330248486\n",
      "Iteracion: 14675 Gradiente: [0.0001643496057037434,-0.0028354742423573496] Loss: 22.842744322175133\n",
      "Iteracion: 14676 Gradiente: [0.00016426223241505037,-0.0028339668172156014] Loss: 22.842744314110355\n",
      "Iteracion: 14677 Gradiente: [0.00016417490547932328,-0.0028324601934720307] Loss: 22.842744306054126\n",
      "Iteracion: 14678 Gradiente: [0.00016408762513435703,-0.002830954370688114] Loss: 22.842744298006487\n",
      "Iteracion: 14679 Gradiente: [0.0001640003911253037,-0.0028294493484484207] Loss: 22.842744289967396\n",
      "Iteracion: 14680 Gradiente: [0.00016391320347869016,-0.0028279451263253227] Loss: 22.842744281936866\n",
      "Iteracion: 14681 Gradiente: [0.00016382606217746343,-0.0028264417038940336] Loss: 22.842744273914843\n",
      "Iteracion: 14682 Gradiente: [0.0001637389671903596,-0.0028249390807279914] Loss: 22.842744265901374\n",
      "Iteracion: 14683 Gradiente: [0.00016365191848990434,-0.0028234372564038307] Loss: 22.84274425789641\n",
      "Iteracion: 14684 Gradiente: [0.00016356491614146761,-0.002821936230491436] Loss: 22.842744249899955\n",
      "Iteracion: 14685 Gradiente: [0.0001634779600683108,-0.0028204360025696927] Loss: 22.842744241911994\n",
      "Iteracion: 14686 Gradiente: [0.00016339105013495707,-0.0028189365722198546] Loss: 22.842744233932535\n",
      "Iteracion: 14687 Gradiente: [0.00016330418637740726,-0.002817437939015595] Loss: 22.84274422596155\n",
      "Iteracion: 14688 Gradiente: [0.0001632173688809265,-0.00281594010252455] Loss: 22.842744217999048\n",
      "Iteracion: 14689 Gradiente: [0.00016313059750909056,-0.0028144430623308144] Loss: 22.842744210044994\n",
      "Iteracion: 14690 Gradiente: [0.0001630438723187429,-0.0028129468180084177] Loss: 22.842744202099418\n",
      "Iteracion: 14691 Gradiente: [0.00016295719307966768,-0.0028114513691440616] Loss: 22.842744194162268\n",
      "Iteracion: 14692 Gradiente: [0.00016287056011303017,-0.0028099567152944853] Loss: 22.84274418623357\n",
      "Iteracion: 14693 Gradiente: [0.00016278397311756028,-0.0028084628560544425] Loss: 22.842744178313293\n",
      "Iteracion: 14694 Gradiente: [0.0001626974321235745,-0.0028069697909968976] Loss: 22.842744170401417\n",
      "Iteracion: 14695 Gradiente: [0.00016261093718412668,-0.0028054775196963535] Loss: 22.842744162497983\n",
      "Iteracion: 14696 Gradiente: [0.0001625244881817404,-0.0028039860417348925] Loss: 22.842744154602936\n",
      "Iteracion: 14697 Gradiente: [0.00016243808529736725,-0.002802495356680505] Loss: 22.842744146716278\n",
      "Iteracion: 14698 Gradiente: [0.00016235172826384314,-0.00280100546412451] Loss: 22.842744138837997\n",
      "Iteracion: 14699 Gradiente: [0.00016226541710011587,-0.002799516363642359] Loss: 22.842744130968097\n",
      "Iteracion: 14700 Gradiente: [0.0001621791518156594,-0.0027980280548132916] Loss: 22.84274412310657\n",
      "Iteracion: 14701 Gradiente: [0.00016209293232805067,-0.0027965405372191534] Loss: 22.842744115253403\n",
      "Iteracion: 14702 Gradiente: [0.000162006758691291,-0.002795053810434093] Loss: 22.842744107408567\n",
      "Iteracion: 14703 Gradiente: [0.00016192063096696075,-0.002793567874032732] Loss: 22.842744099572087\n",
      "Iteracion: 14704 Gradiente: [0.00016183454900253006,-0.0027920827276030737] Loss: 22.84274409174393\n",
      "Iteracion: 14705 Gradiente: [0.0001617485127961042,-0.002790598370721871] Loss: 22.8427440839241\n",
      "Iteracion: 14706 Gradiente: [0.00016166252230315575,-0.002789114802972274] Loss: 22.842744076112574\n",
      "Iteracion: 14707 Gradiente: [0.0001615765775473695,-0.002787632023931034] Loss: 22.84274406830935\n",
      "Iteracion: 14708 Gradiente: [0.00016149067852969286,-0.0027861500331796425] Loss: 22.842744060514413\n",
      "Iteracion: 14709 Gradiente: [0.0001614048250464369,-0.0027846688303071686] Loss: 22.842744052727774\n",
      "Iteracion: 14710 Gradiente: [0.00016131901731739618,-0.0027831884148803] Loss: 22.842744044949413\n",
      "Iteracion: 14711 Gradiente: [0.00016123325518719868,-0.0027817087864881055] Loss: 22.842744037179315\n",
      "Iteracion: 14712 Gradiente: [0.0001611475386833187,-0.0027802299447102995] Loss: 22.842744029417492\n",
      "Iteracion: 14713 Gradiente: [0.00016106186768922727,-0.002778751889134175] Loss: 22.842744021663894\n",
      "Iteracion: 14714 Gradiente: [0.00016097624230345295,-0.0027772746193339987] Loss: 22.84274401391855\n",
      "Iteracion: 14715 Gradiente: [0.00016089066239146632,-0.0027757981348981295] Loss: 22.84274400618144\n",
      "Iteracion: 14716 Gradiente: [0.00016080512794095132,-0.0027743224354087677] Loss: 22.84274399845255\n",
      "Iteracion: 14717 Gradiente: [0.00016071963903243613,-0.0027728475204432594] Loss: 22.842743990731883\n",
      "Iteracion: 14718 Gradiente: [0.00016063419554844435,-0.002771373389589371] Loss: 22.842743983019428\n",
      "Iteracion: 14719 Gradiente: [0.00016054879748423901,-0.002769900042427764] Loss: 22.842743975315162\n",
      "Iteracion: 14720 Gradiente: [0.00016046344470244852,-0.002768427478550232] Loss: 22.842743967619075\n",
      "Iteracion: 14721 Gradiente: [0.00016037813741149877,-0.002766955697526778] Loss: 22.84274395993119\n",
      "Iteracion: 14722 Gradiente: [0.00016029287545980727,-0.002765484698947418] Loss: 22.842743952251457\n",
      "Iteracion: 14723 Gradiente: [0.00016020765886821665,-0.002764014482394354] Loss: 22.8427439445799\n",
      "Iteracion: 14724 Gradiente: [0.00016012248754861957,-0.0027625450474557073] Loss: 22.8427439369165\n",
      "Iteracion: 14725 Gradiente: [0.00016003736156922817,-0.0027610763937097717] Loss: 22.842743929261232\n",
      "Iteracion: 14726 Gradiente: [0.00015995228075667002,-0.0027596085207509447] Loss: 22.842743921614115\n",
      "Iteracion: 14727 Gradiente: [0.00015986724524547451,-0.0027581414281544407] Loss: 22.84274391397512\n",
      "Iteracion: 14728 Gradiente: [0.00015978225497406128,-0.0027566751155057765] Loss: 22.842743906344253\n",
      "Iteracion: 14729 Gradiente: [0.00015969730972547797,-0.0027552095824044426] Loss: 22.842743898721487\n",
      "Iteracion: 14730 Gradiente: [0.00015961240970720306,-0.0027537448284222182] Loss: 22.842743891106824\n",
      "Iteracion: 14731 Gradiente: [0.0001595275548538666,-0.0027522808531454503] Loss: 22.84274388350028\n",
      "Iteracion: 14732 Gradiente: [0.00015944274511336213,-0.0027508176561624963] Loss: 22.842743875901792\n",
      "Iteracion: 14733 Gradiente: [0.00015935798052263788,-0.002749355237058045] Loss: 22.842743868311395\n",
      "Iteracion: 14734 Gradiente: [0.00015927326088558403,-0.0027478935954254285] Loss: 22.842743860729055\n",
      "Iteracion: 14735 Gradiente: [0.00015918858640778428,-0.002746432730839506] Loss: 22.842743853154783\n",
      "Iteracion: 14736 Gradiente: [0.00015910395690639235,-0.0027449726428966886] Loss: 22.842743845588565\n",
      "Iteracion: 14737 Gradiente: [0.0001590193722724583,-0.0027435133311882963] Loss: 22.84274383803037\n",
      "Iteracion: 14738 Gradiente: [0.0001589348326566172,-0.0027420547952914376] Loss: 22.842743830480234\n",
      "Iteracion: 14739 Gradiente: [0.00015885033803897386,-0.0027405970347956555] Loss: 22.84274382293812\n",
      "Iteracion: 14740 Gradiente: [0.00015876588835226358,-0.002739140049288717] Loss: 22.84274381540401\n",
      "Iteracion: 14741 Gradiente: [0.00015868148342785087,-0.0027376838383671516] Loss: 22.842743807877927\n",
      "Iteracion: 14742 Gradiente: [0.00015859712351584676,-0.0027362284016043976] Loss: 22.842743800359827\n",
      "Iteracion: 14743 Gradiente: [0.0001585128084589845,-0.002734773738595564] Loss: 22.842743792849724\n",
      "Iteracion: 14744 Gradiente: [0.00015842853821842104,-0.002733319848930667] Loss: 22.842743785347604\n",
      "Iteracion: 14745 Gradiente: [0.00015834431272783908,-0.0027318667322011455] Loss: 22.842743777853467\n",
      "Iteracion: 14746 Gradiente: [0.00015826013198629122,-0.0027304143879947657] Loss: 22.842743770367274\n",
      "Iteracion: 14747 Gradiente: [0.00015817599612546473,-0.002728962815891241] Loss: 22.842743762889054\n",
      "Iteracion: 14748 Gradiente: [0.00015809190487914293,-0.002727512015494445] Loss: 22.84274375541879\n",
      "Iteracion: 14749 Gradiente: [0.00015800785838375,-0.0027260619863870516] Loss: 22.842743747956455\n",
      "Iteracion: 14750 Gradiente: [0.0001579238565578104,-0.002724612728159196] Loss: 22.842743740502055\n",
      "Iteracion: 14751 Gradiente: [0.00015783989930658512,-0.0027231642404064614] Loss: 22.84274373305558\n",
      "Iteracion: 14752 Gradiente: [0.00015775598677218264,-0.002721716522710338] Loss: 22.842743725617026\n",
      "Iteracion: 14753 Gradiente: [0.00015767211885986399,-0.0027202695746626187] Loss: 22.842743718186373\n",
      "Iteracion: 14754 Gradiente: [0.00015758829554878654,-0.0027188233958580573] Loss: 22.842743710763624\n",
      "Iteracion: 14755 Gradiente: [0.00015750451676410648,-0.0027173779858874997] Loss: 22.842743703348756\n",
      "Iteracion: 14756 Gradiente: [0.00015742078255698289,-0.0027159333443391867] Loss: 22.842743695941774\n",
      "Iteracion: 14757 Gradiente: [0.0001573370928061498,-0.002714489470810122] Loss: 22.842743688542672\n",
      "Iteracion: 14758 Gradiente: [0.00015725344754476586,-0.0027130463648887826] Loss: 22.842743681151422\n",
      "Iteracion: 14759 Gradiente: [0.00015716984679746323,-0.0027116040261641198] Loss: 22.84274367376804\n",
      "Iteracion: 14760 Gradiente: [0.0001570862905358202,-0.0027101624542280453] Loss: 22.8427436663925\n",
      "Iteracion: 14761 Gradiente: [0.0001570027785722535,-0.0027087216486849047] Loss: 22.842743659024805\n",
      "Iteracion: 14762 Gradiente: [0.00015691931110192552,-0.0027072816091106232] Loss: 22.84274365166494\n",
      "Iteracion: 14763 Gradiente: [0.000156835888031992,-0.0027058423351048098] Loss: 22.8427436443129\n",
      "Iteracion: 14764 Gradiente: [0.0001567525092705561,-0.002704403826262928] Loss: 22.842743636968677\n",
      "Iteracion: 14765 Gradiente: [0.00015666917482993388,-0.002702966082177601] Loss: 22.84274362963224\n",
      "Iteracion: 14766 Gradiente: [0.00015658588470728317,-0.002701529102440503] Loss: 22.842743622303626\n",
      "Iteracion: 14767 Gradiente: [0.00015650263879933845,-0.0027000928866494672] Loss: 22.842743614982794\n",
      "Iteracion: 14768 Gradiente: [0.00015641943716768008,-0.0026986574343939177] Loss: 22.84274360766975\n",
      "Iteracion: 14769 Gradiente: [0.0001563362798331506,-0.0026972227452648195] Loss: 22.842743600364475\n",
      "Iteracion: 14770 Gradiente: [0.00015625316658353464,-0.0026957888188674653] Loss: 22.84274359306695\n",
      "Iteracion: 14771 Gradiente: [0.00015617009763767934,-0.002694355654782517] Loss: 22.842743585777203\n",
      "Iteracion: 14772 Gradiente: [0.00015608707276915841,-0.002692923252617755] Loss: 22.8427435784952\n",
      "Iteracion: 14773 Gradiente: [0.00015600409213523865,-0.0026914916119547883] Loss: 22.842743571220936\n",
      "Iteracion: 14774 Gradiente: [0.0001559211556061276,-0.002690060732396186] Loss: 22.8427435639544\n",
      "Iteracion: 14775 Gradiente: [0.00015583826313729786,-0.0026886306135398995] Loss: 22.842743556695588\n",
      "Iteracion: 14776 Gradiente: [0.00015575541473538123,-0.0026872012549791434] Loss: 22.84274354944451\n",
      "Iteracion: 14777 Gradiente: [0.00015567261036153468,-0.002685772656309974] Loss: 22.842743542201113\n",
      "Iteracion: 14778 Gradiente: [0.00015558985003849557,-0.002684344817125132] Loss: 22.84274353496543\n",
      "Iteracion: 14779 Gradiente: [0.00015550713368478835,-0.002682917737026358] Loss: 22.842743527737433\n",
      "Iteracion: 14780 Gradiente: [0.00015542446132504512,-0.0026814914156055637] Loss: 22.84274352051712\n",
      "Iteracion: 14781 Gradiente: [0.00015534183293273903,-0.0026800658524591607] Loss: 22.84274351330449\n",
      "Iteracion: 14782 Gradiente: [0.0001552592484415527,-0.002678641047188061] Loss: 22.84274350609952\n",
      "Iteracion: 14783 Gradiente: [0.00015517670789790827,-0.0026772169993848865] Loss: 22.842743498902202\n",
      "Iteracion: 14784 Gradiente: [0.00015509421114643374,-0.002675793708653984] Loss: 22.84274349171255\n",
      "Iteracion: 14785 Gradiente: [0.0001550117582856577,-0.0026743711745870276] Loss: 22.842743484530544\n",
      "Iteracion: 14786 Gradiente: [0.00015492934925399974,-0.002672949396783035] Loss: 22.84274347735616\n",
      "Iteracion: 14787 Gradiente: [0.000154846984003143,-0.0026715283748413774] Loss: 22.842743470189394\n",
      "Iteracion: 14788 Gradiente: [0.0001547646626685643,-0.002670108108351125] Loss: 22.842743463030253\n",
      "Iteracion: 14789 Gradiente: [0.00015468238506646987,-0.00266868859691994] Loss: 22.842743455878736\n",
      "Iteracion: 14790 Gradiente: [0.00015460015110875248,-0.002667269840150392] Loss: 22.84274344873479\n",
      "Iteracion: 14791 Gradiente: [0.0001545179609185728,-0.002665851837632142] Loss: 22.842743441598476\n",
      "Iteracion: 14792 Gradiente: [0.00015443581445235093,-0.0026644345889666947] Loss: 22.84274343446972\n",
      "Iteracion: 14793 Gradiente: [0.00015435371167598077,-0.0026630180937518826] Loss: 22.84274342734856\n",
      "Iteracion: 14794 Gradiente: [0.0001542716525477772,-0.0026616023515900387] Loss: 22.842743420234957\n",
      "Iteracion: 14795 Gradiente: [0.0001541896369663694,-0.002660187362084917] Loss: 22.842743413128925\n",
      "Iteracion: 14796 Gradiente: [0.00015410766496870565,-0.0026587731248317445] Loss: 22.842743406030436\n",
      "Iteracion: 14797 Gradiente: [0.0001540257365652072,-0.002657359639430723] Loss: 22.842743398939493\n",
      "Iteracion: 14798 Gradiente: [0.00015394385173597887,-0.0026559469054800405] Loss: 22.8427433918561\n",
      "Iteracion: 14799 Gradiente: [0.00015386201054070625,-0.0026545349225767013] Loss: 22.842743384780224\n",
      "Iteracion: 14800 Gradiente: [0.00015378021278991127,-0.002653123690330735] Loss: 22.842743377711873\n",
      "Iteracion: 14801 Gradiente: [0.00015369845854517432,-0.002651713208337843] Loss: 22.842743370651046\n",
      "Iteracion: 14802 Gradiente: [0.0001536167476084908,-0.002650303476209359] Loss: 22.84274336359771\n",
      "Iteracion: 14803 Gradiente: [0.00015353508028776258,-0.002648894493528194] Loss: 22.842743356551875\n",
      "Iteracion: 14804 Gradiente: [0.00015345345636793203,-0.002647486259905918] Loss: 22.842743349513537\n",
      "Iteracion: 14805 Gradiente: [0.00015337187578173445,-0.0026460787749469955] Loss: 22.842743342482674\n",
      "Iteracion: 14806 Gradiente: [0.00015329033854053856,-0.0026446720382518646] Loss: 22.842743335459286\n",
      "Iteracion: 14807 Gradiente: [0.00015320884463297565,-0.002643266049421792] Loss: 22.84274332844335\n",
      "Iteracion: 14808 Gradiente: [0.00015312739413388953,-0.0026418608080548485] Loss: 22.842743321434888\n",
      "Iteracion: 14809 Gradiente: [0.00015304598692296169,-0.0026404563137570375] Loss: 22.84274331443388\n",
      "Iteracion: 14810 Gradiente: [0.00015296462296798078,-0.0026390525661329415] Loss: 22.84274330744031\n",
      "Iteracion: 14811 Gradiente: [0.0001528833022850525,-0.002637649564783355] Loss: 22.84274330045416\n",
      "Iteracion: 14812 Gradiente: [0.00015280202487133465,-0.0026362473093094253] Loss: 22.842743293475454\n",
      "Iteracion: 14813 Gradiente: [0.00015272079065008863,-0.002634845799319289] Loss: 22.84274328650416\n",
      "Iteracion: 14814 Gradiente: [0.0001526395995928927,-0.002633445034416937] Loss: 22.84274327954029\n",
      "Iteracion: 14815 Gradiente: [0.00015255845166848304,-0.002632045014205531] Loss: 22.842743272583803\n",
      "Iteracion: 14816 Gradiente: [0.00015247734690243913,-0.0026306457382868115] Loss: 22.842743265634713\n",
      "Iteracion: 14817 Gradiente: [0.00015239628523033844,-0.002629247206266901] Loss: 22.84274325869301\n",
      "Iteracion: 14818 Gradiente: [0.00015231526676681522,-0.002627849417743988] Loss: 22.842743251758694\n",
      "Iteracion: 14819 Gradiente: [0.00015223429119638846,-0.002626452372338524] Loss: 22.842743244831745\n",
      "Iteracion: 14820 Gradiente: [0.00015215335882506526,-0.0026250560696355527] Loss: 22.842743237912146\n",
      "Iteracion: 14821 Gradiente: [0.00015207246942547197,-0.0026236605092533932] Loss: 22.84274323099991\n",
      "Iteracion: 14822 Gradiente: [0.00015199162301181938,-0.002622265690796392] Loss: 22.842743224095035\n",
      "Iteracion: 14823 Gradiente: [0.00015191081956610712,-0.0026208716138672373] Loss: 22.84274321719749\n",
      "Iteracion: 14824 Gradiente: [0.00015183005918117942,-0.002619478278066012] Loss: 22.842743210307276\n",
      "Iteracion: 14825 Gradiente: [0.0001517493416713478,-0.0026180856830085495] Loss: 22.842743203424387\n",
      "Iteracion: 14826 Gradiente: [0.00015166866699587445,-0.002616693828301919] Loss: 22.84274319654882\n",
      "Iteracion: 14827 Gradiente: [0.0001515880353385531,-0.0026153027135398096] Loss: 22.842743189680544\n",
      "Iteracion: 14828 Gradiente: [0.00015150744651559004,-0.00261391233833983] Loss: 22.84274318281957\n",
      "Iteracion: 14829 Gradiente: [0.00015142690044266753,-0.0026125227023108266] Loss: 22.842743175965907\n",
      "Iteracion: 14830 Gradiente: [0.0001513463973604227,-0.0026111338050450665] Loss: 22.842743169119537\n",
      "Iteracion: 14831 Gradiente: [0.00015126593690505767,-0.0026097456461705803] Loss: 22.842743162280428\n",
      "Iteracion: 14832 Gradiente: [0.00015118551929352482,-0.0026083582252785695] Loss: 22.842743155448584\n",
      "Iteracion: 14833 Gradiente: [0.00015110514444245383,-0.0026069715419832088] Loss: 22.84274314862402\n",
      "Iteracion: 14834 Gradiente: [0.00015102481230447513,-0.0026055855958913316] Loss: 22.842743141806697\n",
      "Iteracion: 14835 Gradiente: [0.00015094452292790567,-0.002604200386607758] Loss: 22.842743134996617\n",
      "Iteracion: 14836 Gradiente: [0.0001508642761706369,-0.0026028159137476098] Loss: 22.84274312819378\n",
      "Iteracion: 14837 Gradiente: [0.00015078407201845795,-0.002601432176920208] Loss: 22.842743121398165\n",
      "Iteracion: 14838 Gradiente: [0.00015070391057463438,-0.0026000491757244503] Loss: 22.842743114609796\n",
      "Iteracion: 14839 Gradiente: [0.00015062379168284679,-0.002598666909778539] Loss: 22.842743107828642\n",
      "Iteracion: 14840 Gradiente: [0.00015054371548046676,-0.00259728537868232] Loss: 22.84274310105467\n",
      "Iteracion: 14841 Gradiente: [0.0001504636818566496,-0.0025959045820500865] Loss: 22.84274309428791\n",
      "Iteracion: 14842 Gradiente: [0.0001503836907479202,-0.002594524519493054] Loss: 22.84274308752836\n",
      "Iteracion: 14843 Gradiente: [0.0001503037421211199,-0.002593145190623029] Loss: 22.842743080775985\n",
      "Iteracion: 14844 Gradiente: [0.0001502238360510925,-0.002591766595042936] Loss: 22.84274307403079\n",
      "Iteracion: 14845 Gradiente: [0.00015014397248478418,-0.002590388732365175] Loss: 22.84274306729275\n",
      "Iteracion: 14846 Gradiente: [0.00015006415126208594,-0.0025890116022084203] Loss: 22.842743060561876\n",
      "Iteracion: 14847 Gradiente: [0.00014998437258479196,-0.0025876352041690844] Loss: 22.84274305383816\n",
      "Iteracion: 14848 Gradiente: [0.00014990463635153144,-0.002586259537863948] Loss: 22.84274304712161\n",
      "Iteracion: 14849 Gradiente: [0.000149824942500724,-0.0025848846029071855] Loss: 22.842743040412188\n",
      "Iteracion: 14850 Gradiente: [0.00014974529095752588,-0.002583510398909065] Loss: 22.842743033709894\n",
      "Iteracion: 14851 Gradiente: [0.00014966568170393657,-0.0025821369254837617] Loss: 22.842743027014713\n",
      "Iteracion: 14852 Gradiente: [0.00014958611486311688,-0.002580764182234437] Loss: 22.842743020326655\n",
      "Iteracion: 14853 Gradiente: [0.0001495065903014847,-0.002579392168779056] Loss: 22.842743013645727\n",
      "Iteracion: 14854 Gradiente: [0.00014942710804840924,-0.002578020884726347] Loss: 22.842743006971883\n",
      "Iteracion: 14855 Gradiente: [0.00014934766796557142,-0.002576650329694985] Loss: 22.842743000305124\n",
      "Iteracion: 14856 Gradiente: [0.00014926827012497293,-0.00257528050329275] Loss: 22.842742993645466\n",
      "Iteracion: 14857 Gradiente: [0.0001491889144773495,-0.0025739114051321604] Loss: 22.84274298699289\n",
      "Iteracion: 14858 Gradiente: [0.0001491096011108084,-0.0025725430348215875] Loss: 22.842742980347385\n",
      "Iteracion: 14859 Gradiente: [0.00014903032990882064,-0.002571175391978168] Loss: 22.84274297370893\n",
      "Iteracion: 14860 Gradiente: [0.00014895110080033193,-0.002569808476218327] Loss: 22.842742967077534\n",
      "Iteracion: 14861 Gradiente: [0.0001488719138488174,-0.0025684422871513845] Loss: 22.842742960453208\n",
      "Iteracion: 14862 Gradiente: [0.00014879276889037858,-0.0025670768243983844] Loss: 22.8427429538359\n",
      "Iteracion: 14863 Gradiente: [0.00014871366608133486,-0.002565712087563317] Loss: 22.84274294722564\n",
      "Iteracion: 14864 Gradiente: [0.00014863460536010582,-0.0025643480762614244] Loss: 22.842742940622397\n",
      "Iteracion: 14865 Gradiente: [0.00014855558668974329,-0.002562984790109013] Loss: 22.842742934026166\n",
      "Iteracion: 14866 Gradiente: [0.00014847660992340178,-0.0025616222287280738] Loss: 22.84274292743697\n",
      "Iteracion: 14867 Gradiente: [0.00014839767524771713,-0.002560260391720822] Loss: 22.84274292085477\n",
      "Iteracion: 14868 Gradiente: [0.00014831878247415867,-0.002558899278711498] Loss: 22.84274291427956\n",
      "Iteracion: 14869 Gradiente: [0.00014823993168420203,-0.002557538889308001] Loss: 22.842742907711344\n",
      "Iteracion: 14870 Gradiente: [0.0001481611228219511,-0.002556179223128415] Loss: 22.842742901150107\n",
      "Iteracion: 14871 Gradiente: [0.0001480823557756139,-0.002554820279794967] Loss: 22.842742894595848\n",
      "Iteracion: 14872 Gradiente: [0.00014800363068161458,-0.002553462058912004] Loss: 22.84274288804855\n",
      "Iteracion: 14873 Gradiente: [0.00014792494735900165,-0.002552104560106727] Loss: 22.842742881508215\n",
      "Iteracion: 14874 Gradiente: [0.00014784630592335664,-0.0025507477829863254] Loss: 22.84274287497484\n",
      "Iteracion: 14875 Gradiente: [0.000147767706259098,-0.0025493917271705393] Loss: 22.8427428684484\n",
      "Iteracion: 14876 Gradiente: [0.00014768914844201693,-0.0025480363922744925] Loss: 22.842742861928905\n",
      "Iteracion: 14877 Gradiente: [0.00014761063232432054,-0.0025466817779186356] Loss: 22.842742855416336\n",
      "Iteracion: 14878 Gradiente: [0.00014753215795811533,-0.0025453278837175002] Loss: 22.842742848910675\n",
      "Iteracion: 14879 Gradiente: [0.00014745372535192777,-0.0025439747092854976] Loss: 22.842742842411944\n",
      "Iteracion: 14880 Gradiente: [0.00014737533439017624,-0.0025426222542455673] Loss: 22.842742835920138\n",
      "Iteracion: 14881 Gradiente: [0.00014729698521307455,-0.0025412705182048965] Loss: 22.842742829435203\n",
      "Iteracion: 14882 Gradiente: [0.00014721867758661725,-0.0025399195007955438] Loss: 22.842742822957177\n",
      "Iteracion: 14883 Gradiente: [0.00014714041158280604,-0.0025385692016286045] Loss: 22.842742816486023\n",
      "Iteracion: 14884 Gradiente: [0.00014706218722722043,-0.0025372196203197936] Loss: 22.84274281002177\n",
      "Iteracion: 14885 Gradiente: [0.00014698400442796354,-0.0025358707564914575] Loss: 22.84274280356436\n",
      "Iteracion: 14886 Gradiente: [0.00014690586323524714,-0.002534522609758838] Loss: 22.842742797113846\n",
      "Iteracion: 14887 Gradiente: [0.00014682776355812166,-0.0025331751797434523] Loss: 22.842742790670172\n",
      "Iteracion: 14888 Gradiente: [0.00014674970546953622,-0.0025318284660601855] Loss: 22.842742784233344\n",
      "Iteracion: 14889 Gradiente: [0.0001466716887923288,-0.002530482468335412] Loss: 22.842742777803362\n",
      "Iteracion: 14890 Gradiente: [0.00014659371366766057,-0.0025291371861802265] Loss: 22.842742771380227\n",
      "Iteracion: 14891 Gradiente: [0.00014651577998184469,-0.0025277926192182796] Loss: 22.84274276496391\n",
      "Iteracion: 14892 Gradiente: [0.0001464378876723534,-0.0025264487670735745] Loss: 22.842742758554397\n",
      "Iteracion: 14893 Gradiente: [0.0001463600368367679,-0.002525105629357801] Loss: 22.84274275215171\n",
      "Iteracion: 14894 Gradiente: [0.00014628222733582182,-0.0025237632056983963] Loss: 22.84274274575584\n",
      "Iteracion: 14895 Gradiente: [0.00014620445917046254,-0.0025224214957149846] Loss: 22.842742739366756\n",
      "Iteracion: 14896 Gradiente: [0.00014612673237100656,-0.0025210804990226875] Loss: 22.842742732984465\n",
      "Iteracion: 14897 Gradiente: [0.00014604904695640167,-0.0025197402152437338] Loss: 22.842742726608954\n",
      "Iteracion: 14898 Gradiente: [0.00014597140283380364,-0.002518400644000944] Loss: 22.84274272024022\n",
      "Iteracion: 14899 Gradiente: [0.00014589379998805423,-0.00251706178491465] Loss: 22.84274271387828\n",
      "Iteracion: 14900 Gradiente: [0.0001458162383499939,-0.0025157236376100417] Loss: 22.84274270752308\n",
      "Iteracion: 14901 Gradiente: [0.00014573871792530706,-0.002514386201706505] Loss: 22.84274270117464\n",
      "Iteracion: 14902 Gradiente: [0.00014566123871588842,-0.0025130494768250836] Loss: 22.842742694832943\n",
      "Iteracion: 14903 Gradiente: [0.00014558380079373971,-0.0025117134625822026] Loss: 22.842742688498\n",
      "Iteracion: 14904 Gradiente: [0.0001455064039438033,-0.002510378158611104] Loss: 22.84274268216978\n",
      "Iteracion: 14905 Gradiente: [0.00014542904819829042,-0.002509043564530344] Loss: 22.84274267584829\n",
      "Iteracion: 14906 Gradiente: [0.00014535173369741491,-0.0025077096799544546] Loss: 22.842742669533507\n",
      "Iteracion: 14907 Gradiente: [0.00014527446027159385,-0.002506376504513715] Loss: 22.84274266322547\n",
      "Iteracion: 14908 Gradiente: [0.0001451972279189325,-0.002505044037831539] Loss: 22.84274265692412\n",
      "Iteracion: 14909 Gradiente: [0.00014512003652574398,-0.002503712279534535] Loss: 22.84274265062946\n",
      "Iteracion: 14910 Gradiente: [0.0001450428863279285,-0.002502381229231787] Loss: 22.842742644341495\n",
      "Iteracion: 14911 Gradiente: [0.0001449657770270581,-0.0025010508865638786] Loss: 22.842742638060216\n",
      "Iteracion: 14912 Gradiente: [0.00014488870882776913,-0.002499721251140367] Loss: 22.842742631785605\n",
      "Iteracion: 14913 Gradiente: [0.00014481168156142606,-0.0024983923225937823] Loss: 22.842742625517683\n",
      "Iteracion: 14914 Gradiente: [0.00014473469526213496,-0.002497064100545406] Loss: 22.84274261925641\n",
      "Iteracion: 14915 Gradiente: [0.00014465774974799692,-0.0024957365846288346] Loss: 22.842742613001793\n",
      "Iteracion: 14916 Gradiente: [0.00014458084530322897,-0.002494409774451493] Loss: 22.842742606753838\n",
      "Iteracion: 14917 Gradiente: [0.0001445039816303506,-0.002493083669653255] Loss: 22.84274260051251\n",
      "Iteracion: 14918 Gradiente: [0.00014442715891315553,-0.0024917582698484133] Loss: 22.842742594277812\n",
      "Iteracion: 14919 Gradiente: [0.00014435037700574564,-0.002490433574667132] Loss: 22.84274258804974\n",
      "Iteracion: 14920 Gradiente: [0.00014427363595454306,-0.0024891095837332954] Loss: 22.842742581828315\n",
      "Iteracion: 14921 Gradiente: [0.00014419693565722962,-0.002487786296676712] Loss: 22.84274257561348\n",
      "Iteracion: 14922 Gradiente: [0.00014412027609201535,-0.002486463713121978] Loss: 22.842742569405257\n",
      "Iteracion: 14923 Gradiente: [0.00014404365729016414,-0.002485141832693098] Loss: 22.842742563203632\n",
      "Iteracion: 14924 Gradiente: [0.00014396707933599372,-0.0024838206550098127] Loss: 22.84274255700859\n",
      "Iteracion: 14925 Gradiente: [0.00014389054199455132,-0.0024825001797105744] Loss: 22.842742550820166\n",
      "Iteracion: 14926 Gradiente: [0.0001438140454013137,-0.002481180406412875] Loss: 22.842742544638295\n",
      "Iteracion: 14927 Gradiente: [0.0001437375894397519,-0.002479861334749482] Loss: 22.842742538462986\n",
      "Iteracion: 14928 Gradiente: [0.00014366117415439325,-0.0024785429643434517] Loss: 22.842742532294263\n",
      "Iteracion: 14929 Gradiente: [0.00014358479945523564,-0.0024772252948258946] Loss: 22.84274252613209\n",
      "Iteracion: 14930 Gradiente: [0.00014350846540859644,-0.002475908325820105] Loss: 22.84274251997647\n",
      "Iteracion: 14931 Gradiente: [0.0001434321718799462,-0.002474592056957666] Loss: 22.84274251382738\n",
      "Iteracion: 14932 Gradiente: [0.000143355918881601,-0.0024732764878663714] Loss: 22.84274250768483\n",
      "Iteracion: 14933 Gradiente: [0.0001432797064760886,-0.0024719616181689236] Loss: 22.84274250154882\n",
      "Iteracion: 14934 Gradiente: [0.00014320353455919607,-0.0024706474474972613] Loss: 22.842742495419333\n",
      "Iteracion: 14935 Gradiente: [0.00014312740313281817,-0.002469333975480244] Loss: 22.842742489296345\n",
      "Iteracion: 14936 Gradiente: [0.00014305131225569311,-0.0024680212017409294] Loss: 22.84274248317988\n",
      "Iteracion: 14937 Gradiente: [0.000142975261762975,-0.002466709125916111] Loss: 22.842742477069912\n",
      "Iteracion: 14938 Gradiente: [0.000142899251713402,-0.0024653977476300306] Loss: 22.84274247096644\n",
      "Iteracion: 14939 Gradiente: [0.0001428232820719207,-0.0024640870665129694] Loss: 22.842742464869456\n",
      "Iteracion: 14940 Gradiente: [0.00014274735285274193,-0.002462777082193431] Loss: 22.842742458778947\n",
      "Iteracion: 14941 Gradiente: [0.0001426714640198649,-0.0024614677942995655] Loss: 22.842742452694917\n",
      "Iteracion: 14942 Gradiente: [0.0001425956154794979,-0.0024601592024665092] Loss: 22.842742446617358\n",
      "Iteracion: 14943 Gradiente: [0.00014251980725343098,-0.0024588513063224108] Loss: 22.84274244054626\n",
      "Iteracion: 14944 Gradiente: [0.00014244403931797933,-0.0024575441054957756] Loss: 22.842742434481597\n",
      "Iteracion: 14945 Gradiente: [0.00014236831176503984,-0.0024562375996114365] Loss: 22.842742428423396\n",
      "Iteracion: 14946 Gradiente: [0.00014229262437292315,-0.002454931788311754] Loss: 22.842742422371632\n",
      "Iteracion: 14947 Gradiente: [0.00014221697728373784,-0.0024536266712174163] Loss: 22.84274241632631\n",
      "Iteracion: 14948 Gradiente: [0.00014214137031463755,-0.0024523222479686524] Loss: 22.84274241028741\n",
      "Iteracion: 14949 Gradiente: [0.0001420658036522582,-0.002451018518185914] Loss: 22.842742404254917\n",
      "Iteracion: 14950 Gradiente: [0.00014199027716207033,-0.0024497154815045735] Loss: 22.84274239822885\n",
      "Iteracion: 14951 Gradiente: [0.00014191479075407188,-0.002448413137562137] Loss: 22.842742392209168\n",
      "Iteracion: 14952 Gradiente: [0.00014183934443963154,-0.0024471114859882922] Loss: 22.842742386195912\n",
      "Iteracion: 14953 Gradiente: [0.0001417639382670662,-0.002445810526410123] Loss: 22.842742380189044\n",
      "Iteracion: 14954 Gradiente: [0.00014168857220037506,-0.0024445102584607524] Loss: 22.842742374188553\n",
      "Iteracion: 14955 Gradiente: [0.00014161324619124115,-0.002443210681774488] Loss: 22.842742368194425\n",
      "Iteracion: 14956 Gradiente: [0.00014153796028040233,-0.0024419117959798343] Loss: 22.842742362206693\n",
      "Iteracion: 14957 Gradiente: [0.00014146271433996088,-0.0024406136007155983] Loss: 22.842742356225305\n",
      "Iteracion: 14958 Gradiente: [0.00014138750842960235,-0.0024393160956114692] Loss: 22.842742350250305\n",
      "Iteracion: 14959 Gradiente: [0.00014131234251427334,-0.0024380192802991494] Loss: 22.842742344281632\n",
      "Iteracion: 14960 Gradiente: [0.00014123721650491917,-0.0024367231544174454] Loss: 22.842742338319315\n",
      "Iteracion: 14961 Gradiente: [0.00014116213043469846,-0.002435427717596047] Loss: 22.842742332363326\n",
      "Iteracion: 14962 Gradiente: [0.00014108708433487513,-0.0024341329694649973] Loss: 22.842742326413667\n",
      "Iteracion: 14963 Gradiente: [0.00014101207816660615,-0.002432838909660262] Loss: 22.842742320470343\n",
      "Iteracion: 14964 Gradiente: [0.0001409371117839934,-0.0024315455378224253] Loss: 22.842742314533336\n",
      "Iteracion: 14965 Gradiente: [0.0001408621853461985,-0.002430252853575491] Loss: 22.842742308602634\n",
      "Iteracion: 14966 Gradiente: [0.00014078729864384817,-0.0024289608565641885] Loss: 22.842742302678232\n",
      "Iteracion: 14967 Gradiente: [0.00014071245176978665,-0.0024276695464189165] Loss: 22.842742296760147\n",
      "Iteracion: 14968 Gradiente: [0.00014063764471832957,-0.0024263789227713774] Loss: 22.84274229084833\n",
      "Iteracion: 14969 Gradiente: [0.00014056287752832,-0.002425088985252207] Loss: 22.842742284942798\n",
      "Iteracion: 14970 Gradiente: [0.00014048814992975167,-0.002423799733513595] Loss: 22.84274227904356\n",
      "Iteracion: 14971 Gradiente: [0.00014041346207231223,-0.002422511167180493] Loss: 22.842742273150584\n",
      "Iteracion: 14972 Gradiente: [0.0001403388140242138,-0.002421223285881287] Loss: 22.842742267263876\n",
      "Iteracion: 14973 Gradiente: [0.00014026420552871362,-0.0024199360892679304] Loss: 22.842742261383414\n",
      "Iteracion: 14974 Gradiente: [0.0001401896367781319,-0.002418649576963124] Loss: 22.842742255509215\n",
      "Iteracion: 14975 Gradiente: [0.00014011510768341395,-0.002417363748607097] Loss: 22.84274224964127\n",
      "Iteracion: 14976 Gradiente: [0.0001400406181829794,-0.0024160786038379457] Loss: 22.84274224377954\n",
      "Iteracion: 14977 Gradiente: [0.00013996616822756398,-0.00241479414229507] Loss: 22.842742237924035\n",
      "Iteracion: 14978 Gradiente: [0.00013989175800380356,-0.0024135103636022375] Loss: 22.84274223207478\n",
      "Iteracion: 14979 Gradiente: [0.0001398173872701136,-0.002412227267409861] Loss: 22.842742226231735\n",
      "Iteracion: 14980 Gradiente: [0.00013974305608239018,-0.0024109448533501166] Loss: 22.842742220394896\n",
      "Iteracion: 14981 Gradiente: [0.0001396687644055798,-0.002409663121061338] Loss: 22.84274221456428\n",
      "Iteracion: 14982 Gradiente: [0.00013959451218283902,-0.002408382070183753] Loss: 22.842742208739832\n",
      "Iteracion: 14983 Gradiente: [0.00013952029941890486,-0.0024071017003521435] Loss: 22.842742202921574\n",
      "Iteracion: 14984 Gradiente: [0.0001394461261838842,-0.0024058220112005793] Loss: 22.842742197109533\n",
      "Iteracion: 14985 Gradiente: [0.00013937199230250978,-0.0024045430023753294] Loss: 22.84274219130364\n",
      "Iteracion: 14986 Gradiente: [0.0001392978978373094,-0.002403264673511174] Loss: 22.84274218550394\n",
      "Iteracion: 14987 Gradiente: [0.0001392238427627035,-0.002401987024245026] Loss: 22.842742179710402\n",
      "Iteracion: 14988 Gradiente: [0.00013914982712227205,-0.0024007100542132063] Loss: 22.842742173923007\n",
      "Iteracion: 14989 Gradiente: [0.00013907585074074785,-0.002399433763062812] Loss: 22.842742168141765\n",
      "Iteracion: 14990 Gradiente: [0.00013900191379434546,-0.0023981581504222286] Loss: 22.842742162366683\n",
      "Iteracion: 14991 Gradiente: [0.0001389280161077977,-0.002396883215938909] Loss: 22.842742156597733\n",
      "Iteracion: 14992 Gradiente: [0.00013885415770478933,-0.0023956089592497656] Loss: 22.842742150834898\n",
      "Iteracion: 14993 Gradiente: [0.00013878033858816252,-0.0023943353799931325] Loss: 22.84274214507822\n",
      "Iteracion: 14994 Gradiente: [0.00013870655874749597,-0.002393062477807935] Loss: 22.842742139327644\n",
      "Iteracion: 14995 Gradiente: [0.00013863281810415627,-0.0023917902523397316] Loss: 22.84274213358319\n",
      "Iteracion: 14996 Gradiente: [0.0001385591165965631,-0.002390518703229816] Loss: 22.842742127844833\n",
      "Iteracion: 14997 Gradiente: [0.00013848545431661326,-0.0023892478301102455] Loss: 22.84274212211259\n",
      "Iteracion: 14998 Gradiente: [0.00013841183128135981,-0.0023879776326207747] Loss: 22.84274211638642\n",
      "Iteracion: 14999 Gradiente: [0.00013833824723500734,-0.002386708110416909] Loss: 22.842742110666343\n",
      "Iteracion: 15000 Gradiente: [0.0001382647023632444,-0.002385439263126443] Loss: 22.842742104952357\n",
      "Iteracion: 15001 Gradiente: [0.00013819119662154358,-0.002384171090392684] Loss: 22.84274209924443\n",
      "Iteracion: 15002 Gradiente: [0.00013811772988579681,-0.002382903591863439] Loss: 22.842742093542586\n",
      "Iteracion: 15003 Gradiente: [0.00013804430226116438,-0.002381636767170529] Loss: 22.84274208784679\n",
      "Iteracion: 15004 Gradiente: [0.00013797091367943418,-0.0023803706159612878] Loss: 22.842742082157038\n",
      "Iteracion: 15005 Gradiente: [0.00013789756419460748,-0.002379105137872273] Loss: 22.84274207647336\n",
      "Iteracion: 15006 Gradiente: [0.00013782425357741583,-0.0023778403325565023] Loss: 22.842742070795715\n",
      "Iteracion: 15007 Gradiente: [0.00013775098193017735,-0.0023765761996515995] Loss: 22.842742065124103\n",
      "Iteracion: 15008 Gradiente: [0.0001376777493277359,-0.002375312738792819] Loss: 22.842742059458523\n",
      "Iteracion: 15009 Gradiente: [0.00013760455557587647,-0.0023740499496327063] Loss: 22.84274205379897\n",
      "Iteracion: 15010 Gradiente: [0.00013753140079018067,-0.002372787831806633] Loss: 22.84274204814541\n",
      "Iteracion: 15011 Gradiente: [0.00013745828491285768,-0.002371526384961816] Loss: 22.842742042497875\n",
      "Iteracion: 15012 Gradiente: [0.00013738520783211546,-0.0023702656087436933] Loss: 22.842742036856347\n",
      "Iteracion: 15013 Gradiente: [0.00013731216959816567,-0.0023690055027937975] Loss: 22.84274203122082\n",
      "Iteracion: 15014 Gradiente: [0.0001372391702375353,-0.002367746066752948] Loss: 22.842742025591253\n",
      "Iteracion: 15015 Gradiente: [0.00013716620969243346,-0.002366487300266584] Loss: 22.842742019967712\n",
      "Iteracion: 15016 Gradiente: [0.00013709328789749027,-0.002365229202979909] Loss: 22.842742014350115\n",
      "Iteracion: 15017 Gradiente: [0.0001370204049540765,-0.0023639717745320846] Loss: 22.842742008738508\n",
      "Iteracion: 15018 Gradiente: [0.00013694756067460882,-0.0023627150145771954] Loss: 22.842742003132862\n",
      "Iteracion: 15019 Gradiente: [0.00013687475515666847,-0.0023614589227526277] Loss: 22.842741997533174\n",
      "Iteracion: 15020 Gradiente: [0.00013680198838699197,-0.0023602034987015704] Loss: 22.842741991939434\n",
      "Iteracion: 15021 Gradiente: [0.00013672926024052383,-0.0023589487420757394] Loss: 22.842741986351644\n",
      "Iteracion: 15022 Gradiente: [0.00013665657079495002,-0.0023576946525158365] Loss: 22.842741980769794\n",
      "Iteracion: 15023 Gradiente: [0.00013658391997921625,-0.0023564412296691963] Loss: 22.842741975193874\n",
      "Iteracion: 15024 Gradiente: [0.00013651130773837395,-0.002355188473183626] Loss: 22.842741969623894\n",
      "Iteracion: 15025 Gradiente: [0.00013643873407431784,-0.0023539363827026704] Loss: 22.842741964059826\n",
      "Iteracion: 15026 Gradiente: [0.00013636619907705003,-0.002352684957867742] Loss: 22.84274195850167\n",
      "Iteracion: 15027 Gradiente: [0.00013629370261393584,-0.002351434198329135] Loss: 22.842741952949428\n",
      "Iteracion: 15028 Gradiente: [0.00013622124473802917,-0.0023501841037306312] Loss: 22.842741947403095\n",
      "Iteracion: 15029 Gradiente: [0.00013614882535932792,-0.002348934673721814] Loss: 22.842741941862638\n",
      "Iteracion: 15030 Gradiente: [0.00013607644446267386,-0.0023476859079495445] Loss: 22.84274193632808\n",
      "Iteracion: 15031 Gradiente: [0.00013600410201017137,-0.002346437806061156] Loss: 22.84274193079939\n",
      "Iteracion: 15032 Gradiente: [0.00013593179808045382,-0.0023451903676979433] Loss: 22.842741925276602\n",
      "Iteracion: 15033 Gradiente: [0.0001358595325598344,-0.00234394359251399] Loss: 22.84274191975967\n",
      "Iteracion: 15034 Gradiente: [0.00013578730546157658,-0.0023426974801533146] Loss: 22.842741914248606\n",
      "Iteracion: 15035 Gradiente: [0.00013571511678473296,-0.002341452030263961] Loss: 22.842741908743395\n",
      "Iteracion: 15036 Gradiente: [0.00013564296647530227,-0.0023402072424943297] Loss: 22.842741903244043\n",
      "Iteracion: 15037 Gradiente: [0.0001355708544792833,-0.0023389631164936493] Loss: 22.842741897750546\n",
      "Iteracion: 15038 Gradiente: [0.00013549878080046558,-0.002337719651909609] Loss: 22.84274189226287\n",
      "Iteracion: 15039 Gradiente: [0.00013542674546916563,-0.0023364768483882395] Loss: 22.842741886781027\n",
      "Iteracion: 15040 Gradiente: [0.00013535474854317424,-0.0023352347055735595] Loss: 22.84274188130503\n",
      "Iteracion: 15041 Gradiente: [0.00013528278978564384,-0.0023339932231256928] Loss: 22.842741875834843\n",
      "Iteracion: 15042 Gradiente: [0.0001352108692543652,-0.002332752400689723] Loss: 22.84274187037046\n",
      "Iteracion: 15043 Gradiente: [0.00013513898704691957,-0.0023315122379077733] Loss: 22.842741864911886\n",
      "Iteracion: 15044 Gradiente: [0.0001350671429558285,-0.002330272734440797] Loss: 22.842741859459146\n",
      "Iteracion: 15045 Gradiente: [0.00013499533713077957,-0.002329033889927364] Loss: 22.842741854012164\n",
      "Iteracion: 15046 Gradiente: [0.00013492356950545553,-0.0023277957040197823] Loss: 22.842741848570995\n",
      "Iteracion: 15047 Gradiente: [0.00013485183998511728,-0.0023265581763728467] Loss: 22.842741843135606\n",
      "Iteracion: 15048 Gradiente: [0.00013478014865692483,-0.0023253213066304567] Loss: 22.84274183770599\n",
      "Iteracion: 15049 Gradiente: [0.00013470849536929564,-0.0023240850944500125] Loss: 22.84274183228215\n",
      "Iteracion: 15050 Gradiente: [0.00013463688019707358,-0.002322849539476361] Loss: 22.84274182686407\n",
      "Iteracion: 15051 Gradiente: [0.0001345653030862574,-0.0023216146413624016] Loss: 22.842741821451746\n",
      "Iteracion: 15052 Gradiente: [0.00013449376402832058,-0.002320380399758785] Loss: 22.84274181604519\n",
      "Iteracion: 15053 Gradiente: [0.00013442226296263017,-0.0023191468143173447] Loss: 22.842741810644366\n",
      "Iteracion: 15054 Gradiente: [0.00013435080003413682,-0.0023179138846817437] Loss: 22.842741805249293\n",
      "Iteracion: 15055 Gradiente: [0.00013427937498799262,-0.0023166816105145926] Loss: 22.842741799859958\n",
      "Iteracion: 15056 Gradiente: [0.00013420798800609646,-0.002315449991456949] Loss: 22.842741794476332\n",
      "Iteracion: 15057 Gradiente: [0.00013413663887907508,-0.0023142190271726077] Loss: 22.842741789098454\n",
      "Iteracion: 15058 Gradiente: [0.00013406532776703747,-0.002312988717300494] Loss: 22.84274178372627\n",
      "Iteracion: 15059 Gradiente: [0.00013399405452692767,-0.002311759061500614] Loss: 22.842741778359805\n",
      "Iteracion: 15060 Gradiente: [0.00013392281912274484,-0.002310530059427762] Loss: 22.84274177299905\n",
      "Iteracion: 15061 Gradiente: [0.00013385162166154412,-0.0023093017107256014] Loss: 22.84274176764399\n",
      "Iteracion: 15062 Gradiente: [0.0001337804620732186,-0.002308074015051531] Loss: 22.84274176229463\n",
      "Iteracion: 15063 Gradiente: [0.0001337093402393445,-0.0023068469720611755] Loss: 22.842741756950936\n",
      "Iteracion: 15064 Gradiente: [0.00013363825622150216,-0.002305620581404947] Loss: 22.842741751612934\n",
      "Iteracion: 15065 Gradiente: [0.00013356721002916554,-0.0023043948427329043] Loss: 22.842741746280605\n",
      "Iteracion: 15066 Gradiente: [0.0001334962016035964,-0.0023031697557016175] Loss: 22.842741740953947\n",
      "Iteracion: 15067 Gradiente: [0.00013342523092395216,-0.0023019453199643416] Loss: 22.84274173563296\n",
      "Iteracion: 15068 Gradiente: [0.00013335429801770716,-0.002300721535171372] Loss: 22.842741730317606\n",
      "Iteracion: 15069 Gradiente: [0.0001332834027654902,-0.0022994984009841344] Loss: 22.842741725007915\n",
      "Iteracion: 15070 Gradiente: [0.0001332125451919334,-0.0022982759170531606] Loss: 22.842741719703863\n",
      "Iteracion: 15071 Gradiente: [0.00013314172529703682,-0.002297054083032416] Loss: 22.84274171440547\n",
      "Iteracion: 15072 Gradiente: [0.00013307094312816996,-0.0022958328985710117] Loss: 22.84274170911269\n",
      "Iteracion: 15073 Gradiente: [0.00013300019849395994,-0.002294612363333807] Loss: 22.84274170382553\n",
      "Iteracion: 15074 Gradiente: [0.00013292949150904101,-0.0022933924769690845] Loss: 22.842741698544007\n",
      "Iteracion: 15075 Gradiente: [0.00013285882217530797,-0.002292173239129743] Loss: 22.842741693268092\n",
      "Iteracion: 15076 Gradiente: [0.00013278819030423014,-0.00229095464948017] Loss: 22.842741687997783\n",
      "Iteracion: 15077 Gradiente: [0.00013271759601423127,-0.002289736707669358] Loss: 22.84274168273308\n",
      "Iteracion: 15078 Gradiente: [0.0001326470392749949,-0.0022885194133515085] Loss: 22.842741677473956\n",
      "Iteracion: 15079 Gradiente: [0.00013257651998230813,-0.002287302766189233] Loss: 22.84274167222045\n",
      "Iteracion: 15080 Gradiente: [0.0001325060382536473,-0.002286086765829746] Loss: 22.84274166697251\n",
      "Iteracion: 15081 Gradiente: [0.0001324355939270087,-0.0022848714119376723] Loss: 22.84274166173016\n",
      "Iteracion: 15082 Gradiente: [0.00013236518717292257,-0.002283656704156319] Loss: 22.842741656493367\n",
      "Iteracion: 15083 Gradiente: [0.00013229481773275136,-0.0022824426421591917] Loss: 22.842741651262152\n",
      "Iteracion: 15084 Gradiente: [0.0001322244857381823,-0.002281229225592559] Loss: 22.84274164603649\n",
      "Iteracion: 15085 Gradiente: [0.00013215419107837079,-0.0022800164541183204] Loss: 22.842741640816403\n",
      "Iteracion: 15086 Gradiente: [0.00013208393392763658,-0.0022788043273816783] Loss: 22.842741635601843\n",
      "Iteracion: 15087 Gradiente: [0.00013201371401692086,-0.0022775928450567305] Loss: 22.842741630392833\n",
      "Iteracion: 15088 Gradiente: [0.000131943531541386,-0.0022763820067854823] Loss: 22.842741625189365\n",
      "Iteracion: 15089 Gradiente: [0.00013187338636650264,-0.0022751718122339783] Loss: 22.842741619991408\n",
      "Iteracion: 15090 Gradiente: [0.00013180327838805775,-0.002273962261066013] Loss: 22.842741614799\n",
      "Iteracion: 15091 Gradiente: [0.00013173320777089734,-0.002272753352925131] Loss: 22.842741609612094\n",
      "Iteracion: 15092 Gradiente: [0.00013166317434733325,-0.0022715450874821147] Loss: 22.84274160443071\n",
      "Iteracion: 15093 Gradiente: [0.00013159317815526113,-0.0022703374643898637] Loss: 22.842741599254843\n",
      "Iteracion: 15094 Gradiente: [0.00013152321913973236,-0.0022691304833083827] Loss: 22.842741594084462\n",
      "Iteracion: 15095 Gradiente: [0.00013145329735001117,-0.0022679241438936515] Loss: 22.84274158891959\n",
      "Iteracion: 15096 Gradiente: [0.00013138341268946382,-0.0022667184458083985] Loss: 22.842741583760194\n",
      "Iteracion: 15097 Gradiente: [0.0001313135652897775,-0.002265513388703274] Loss: 22.84274157860629\n",
      "Iteracion: 15098 Gradiente: [0.0001312437549652638,-0.00226430897224598] Loss: 22.842741573457882\n",
      "Iteracion: 15099 Gradiente: [0.00013117398172350173,-0.002263105196096878] Loss: 22.84274156831492\n",
      "Iteracion: 15100 Gradiente: [0.00013110424565638824,-0.0022619020599051964] Loss: 22.842741563177427\n",
      "Iteracion: 15101 Gradiente: [0.00013103454660002474,-0.002260699563343612] Loss: 22.842741558045407\n",
      "Iteracion: 15102 Gradiente: [0.00013096488462357078,-0.002259497706063011] Loss: 22.842741552918838\n",
      "Iteracion: 15103 Gradiente: [0.00013089525962470817,-0.0022582964877309783] Loss: 22.84274154779773\n",
      "Iteracion: 15104 Gradiente: [0.00013082567177965151,-0.002257095907994729] Loss: 22.84274154268205\n",
      "Iteracion: 15105 Gradiente: [0.00013075612075965636,-0.00225589596653298] Loss: 22.842741537571793\n",
      "Iteracion: 15106 Gradiente: [0.00013068660675893777,-0.0022546966629957885] Loss: 22.842741532467006\n",
      "Iteracion: 15107 Gradiente: [0.00013061712969223056,-0.0022534979970458835] Loss: 22.84274152736761\n",
      "Iteracion: 15108 Gradiente: [0.00013054768960974646,-0.002252299968340902] Loss: 22.842741522273652\n",
      "Iteracion: 15109 Gradiente: [0.00013047828651338022,-0.0022511025765402574] Loss: 22.84274151718511\n",
      "Iteracion: 15110 Gradiente: [0.00013040892028755026,-0.0022499058213103496] Loss: 22.842741512101973\n",
      "Iteracion: 15111 Gradiente: [0.00013033959082993837,-0.002248709702320895] Loss: 22.842741507024236\n",
      "Iteracion: 15112 Gradiente: [0.000130270298313917,-0.002247514219218516] Loss: 22.842741501951902\n",
      "Iteracion: 15113 Gradiente: [0.00013020104264285237,-0.002246319371672219] Loss: 22.84274149688497\n",
      "Iteracion: 15114 Gradiente: [0.00013013182381674444,-0.002245125159340944] Loss: 22.842741491823404\n",
      "Iteracion: 15115 Gradiente: [0.00013006264179675023,-0.002243931581889195] Loss: 22.842741486767228\n",
      "Iteracion: 15116 Gradiente: [0.00012999349651371023,-0.0022427386389823074] Loss: 22.84274148171643\n",
      "Iteracion: 15117 Gradiente: [0.00012992438798751967,-0.002241546330281589] Loss: 22.84274147667099\n",
      "Iteracion: 15118 Gradiente: [0.0001298553162087046,-0.0022403546554483475] Loss: 22.842741471630926\n",
      "Iteracion: 15119 Gradiente: [0.000129786281104316,-0.002239163614148154] Loss: 22.84274146659622\n",
      "Iteracion: 15120 Gradiente: [0.00012971728266582735,-0.0022379732060443303] Loss: 22.84274146156685\n",
      "Iteracion: 15121 Gradiente: [0.00012964832099839895,-0.002236783430792381] Loss: 22.842741456542836\n",
      "Iteracion: 15122 Gradiente: [0.0001295793959807649,-0.002235594288062496] Loss: 22.842741451524155\n",
      "Iteracion: 15123 Gradiente: [0.00012951050760724077,-0.0022344057775182335] Loss: 22.842741446510807\n",
      "Iteracion: 15124 Gradiente: [0.00012944165581529887,-0.0022332178988242173] Loss: 22.842741441502813\n",
      "Iteracion: 15125 Gradiente: [0.00012937284065894043,-0.0022320306516410444] Loss: 22.842741436500123\n",
      "Iteracion: 15126 Gradiente: [0.000129304062064269,-0.0022308440356370103] Loss: 22.84274143150276\n",
      "Iteracion: 15127 Gradiente: [0.00012923532009475973,-0.0022296580504699887] Loss: 22.842741426510695\n",
      "Iteracion: 15128 Gradiente: [0.00012916661467556878,-0.0022284726958083922] Loss: 22.842741421523947\n",
      "Iteracion: 15129 Gradiente: [0.00012909794566837718,-0.002227287971325135] Loss: 22.842741416542506\n",
      "Iteracion: 15130 Gradiente: [0.00012902931325603124,-0.0022261038766728802] Loss: 22.842741411566344\n",
      "Iteracion: 15131 Gradiente: [0.0001289607173693715,-0.0022249204115183828] Loss: 22.842741406595476\n",
      "Iteracion: 15132 Gradiente: [0.00012889215783313073,-0.0022237375755376357] Loss: 22.842741401629887\n",
      "Iteracion: 15133 Gradiente: [0.00012882363479036485,-0.002222555368385552] Loss: 22.84274139666958\n",
      "Iteracion: 15134 Gradiente: [0.00012875514819275699,-0.002221373789731729] Loss: 22.84274139171456\n",
      "Iteracion: 15135 Gradiente: [0.0001286866980526232,-0.0022201928392380664] Loss: 22.842741386764793\n",
      "Iteracion: 15136 Gradiente: [0.0001286182842098545,-0.0022190125165798473] Loss: 22.842741381820293\n",
      "Iteracion: 15137 Gradiente: [0.00012854990683119165,-0.0022178328214099707] Loss: 22.84274137688105\n",
      "Iteracion: 15138 Gradiente: [0.00012848156573378824,-0.0022166537534072718] Loss: 22.842741371947056\n",
      "Iteracion: 15139 Gradiente: [0.0001284132608920648,-0.0022154753122381504] Loss: 22.8427413670183\n",
      "Iteracion: 15140 Gradiente: [0.0001283449924812885,-0.0022142974975581115] Loss: 22.842741362094795\n",
      "Iteracion: 15141 Gradiente: [0.00012827676045219506,-0.002213120309034622] Loss: 22.84274135717651\n",
      "Iteracion: 15142 Gradiente: [0.00012820856454709428,-0.0022119437463486475] Loss: 22.842741352263463\n",
      "Iteracion: 15143 Gradiente: [0.00012814040487967303,-0.0022107678091613774] Loss: 22.84274134735562\n",
      "Iteracion: 15144 Gradiente: [0.00012807228150866953,-0.0022095924971358965] Loss: 22.842741342453014\n",
      "Iteracion: 15145 Gradiente: [0.00012800419438955638,-0.0022084178099391967] Loss: 22.842741337555626\n",
      "Iteracion: 15146 Gradiente: [0.00012793614344370024,-0.0022072437472443105] Loss: 22.842741332663437\n",
      "Iteracion: 15147 Gradiente: [0.00012786812867394322,-0.0022060703087165715] Loss: 22.842741327776448\n",
      "Iteracion: 15148 Gradiente: [0.0001278001500196524,-0.0022048974940268805] Loss: 22.842741322894636\n",
      "Iteracion: 15149 Gradiente: [0.00012773220756609287,-0.0022037253028382034] Loss: 22.842741318018025\n",
      "Iteracion: 15150 Gradiente: [0.0001276643012857903,-0.0022025537348193086] Loss: 22.8427413131466\n",
      "Iteracion: 15151 Gradiente: [0.00012759643095137108,-0.002201382789651992] Loss: 22.84274130828037\n",
      "Iteracion: 15152 Gradiente: [0.00012752859676936623,-0.0022002124669903366] Loss: 22.842741303419288\n",
      "Iteracion: 15153 Gradiente: [0.00012746079865072108,-0.0021990427665074938] Loss: 22.842741298563386\n",
      "Iteracion: 15154 Gradiente: [0.00012739303660964653,-0.0021978736878716394] Loss: 22.842741293712624\n",
      "Iteracion: 15155 Gradiente: [0.0001273253105485613,-0.00219670523075699] Loss: 22.84274128886705\n",
      "Iteracion: 15156 Gradiente: [0.00012725762048546586,-0.0021955373948299457] Loss: 22.84274128402661\n",
      "Iteracion: 15157 Gradiente: [0.00012718996645257145,-0.0021943701797583278] Loss: 22.842741279191305\n",
      "Iteracion: 15158 Gradiente: [0.00012712234828882174,-0.002193203585219905] Loss: 22.842741274361156\n",
      "Iteracion: 15159 Gradiente: [0.00012705476620453738,-0.00219203761087113] Loss: 22.84274126953614\n",
      "Iteracion: 15160 Gradiente: [0.00012698721996760772,-0.0021908722563941533] Loss: 22.842741264716253\n",
      "Iteracion: 15161 Gradiente: [0.00012691970967277182,-0.0021897075214538358] Loss: 22.842741259901473\n",
      "Iteracion: 15162 Gradiente: [0.00012685223524897538,-0.002188543405723327] Loss: 22.84274125509183\n",
      "Iteracion: 15163 Gradiente: [0.0001267847967265349,-0.0021873799088715155] Loss: 22.84274125028729\n",
      "Iteracion: 15164 Gradiente: [0.00012671739406944956,-0.0021862170305685896] Loss: 22.84274124548786\n",
      "Iteracion: 15165 Gradiente: [0.0001266500272246655,-0.0021850547704890034] Loss: 22.84274124069354\n",
      "Iteracion: 15166 Gradiente: [0.00012658269615712926,-0.0021838931283046036] Loss: 22.842741235904306\n",
      "Iteracion: 15167 Gradiente: [0.00012651540094452685,-0.0021827321036813176] Loss: 22.842741231120165\n",
      "Iteracion: 15168 Gradiente: [0.00012644814139359066,-0.0021815716963014134] Loss: 22.842741226341108\n",
      "Iteracion: 15169 Gradiente: [0.000126380917764853,-0.002180411905820042] Loss: 22.842741221567135\n",
      "Iteracion: 15170 Gradiente: [0.00012631372975325424,-0.002179252731925511] Loss: 22.84274121679824\n",
      "Iteracion: 15171 Gradiente: [0.00012624657753500894,-0.002178094174279366] Loss: 22.842741212034404\n",
      "Iteracion: 15172 Gradiente: [0.00012617946091590207,-0.0021769362325639936] Loss: 22.842741207275644\n",
      "Iteracion: 15173 Gradiente: [0.00012611238005699003,-0.002175778906440821] Loss: 22.842741202521925\n",
      "Iteracion: 15174 Gradiente: [0.00012604533480479554,-0.002174622195591051] Loss: 22.842741197773265\n",
      "Iteracion: 15175 Gradiente: [0.00012597832528058462,-0.002173466099680018] Loss: 22.842741193029667\n",
      "Iteracion: 15176 Gradiente: [0.00012591135132614302,-0.0021723106183871486] Loss: 22.842741188291093\n",
      "Iteracion: 15177 Gradiente: [0.00012584441296041858,-0.0021711557513860668] Loss: 22.842741183557575\n",
      "Iteracion: 15178 Gradiente: [0.00012577751025825515,-0.0021700014983429363] Loss: 22.842741178829083\n",
      "Iteracion: 15179 Gradiente: [0.00012571064307564939,-0.0021688478589389605] Loss: 22.84274117410561\n",
      "Iteracion: 15180 Gradiente: [0.00012564381143344386,-0.002167694832844802] Loss: 22.842741169387146\n",
      "Iteracion: 15181 Gradiente: [0.00012557701533827032,-0.0021665424197340856] Loss: 22.842741164673726\n",
      "Iteracion: 15182 Gradiente: [0.00012551025477497053,-0.0021653906192817375] Loss: 22.84274115996529\n",
      "Iteracion: 15183 Gradiente: [0.00012544352956827726,-0.0021642394311678953] Loss: 22.842741155261876\n",
      "Iteracion: 15184 Gradiente: [0.0001253768399768281,-0.0021630888550532744] Loss: 22.842741150563466\n",
      "Iteracion: 15185 Gradiente: [0.00012531018577040717,-0.0021619388906240528] Loss: 22.84274114587004\n",
      "Iteracion: 15186 Gradiente: [0.00012524356703333221,-0.0021607895375493533] Loss: 22.842741141181605\n",
      "Iteracion: 15187 Gradiente: [0.00012517698371254936,-0.0021596407955058795] Loss: 22.84274113649816\n",
      "Iteracion: 15188 Gradiente: [0.00012511043581469038,-0.0021584926641682028] Loss: 22.84274113181967\n",
      "Iteracion: 15189 Gradiente: [0.0001250439232961753,-0.002157345143210776] Loss: 22.84274112714617\n",
      "Iteracion: 15190 Gradiente: [0.00012497744609826593,-0.0021561982323144474] Loss: 22.84274112247764\n",
      "Iteracion: 15191 Gradiente: [0.00012491100421148835,-0.0021550519311528404] Loss: 22.842741117814064\n",
      "Iteracion: 15192 Gradiente: [0.0001248445976803699,-0.00215390623939804] Loss: 22.84274111315546\n",
      "Iteracion: 15193 Gradiente: [0.00012477822639217113,-0.0021527611567308943] Loss: 22.8427411085018\n",
      "Iteracion: 15194 Gradiente: [0.00012471189050794845,-0.00215161668281875] Loss: 22.842741103853072\n",
      "Iteracion: 15195 Gradiente: [0.00012464558979559116,-0.0021504728173494434] Loss: 22.842741099209306\n",
      "Iteracion: 15196 Gradiente: [0.00012457932437352307,-0.0021493295599928075] Loss: 22.842741094570478\n",
      "Iteracion: 15197 Gradiente: [0.00012451309423132292,-0.002148186910421875] Loss: 22.842741089936567\n",
      "Iteracion: 15198 Gradiente: [0.0001244468992088817,-0.0021470448683255463] Loss: 22.842741085307594\n",
      "Iteracion: 15199 Gradiente: [0.00012438073945020278,-0.0021459034333672616] Loss: 22.84274108068352\n",
      "Iteracion: 15200 Gradiente: [0.0001243146148434941,-0.0021447626052335522] Loss: 22.842741076064392\n",
      "Iteracion: 15201 Gradiente: [0.00012424852539538733,-0.002143622383598161] Loss: 22.842741071450156\n",
      "Iteracion: 15202 Gradiente: [0.00012418247103103872,-0.002142482768143239] Loss: 22.842741066840816\n",
      "Iteracion: 15203 Gradiente: [0.00012411645190392543,-0.0021413437585351856] Loss: 22.84274106223639\n",
      "Iteracion: 15204 Gradiente: [0.0001240504676919348,-0.0021402053544704812] Loss: 22.842741057636854\n",
      "Iteracion: 15205 Gradiente: [0.0001239845187190743,-0.002139067555607236] Loss: 22.842741053042214\n",
      "Iteracion: 15206 Gradiente: [0.00012391860476933895,-0.002137930361634943] Loss: 22.842741048452453\n",
      "Iteracion: 15207 Gradiente: [0.00012385272585883436,-0.0021367937722294767] Loss: 22.84274104386757\n",
      "Iteracion: 15208 Gradiente: [0.00012378688194966496,-0.002135657787071447] Loss: 22.842741039287564\n",
      "Iteracion: 15209 Gradiente: [0.00012372107308541067,-0.0021345224058354263] Loss: 22.84274103471242\n",
      "Iteracion: 15210 Gradiente: [0.00012365529907469862,-0.0021333876282117357] Loss: 22.842741030142147\n",
      "Iteracion: 15211 Gradiente: [0.00012358956009090132,-0.002132253453865947] Loss: 22.842741025576707\n",
      "Iteracion: 15212 Gradiente: [0.00012352385610180742,-0.002131119882479737] Loss: 22.84274102101615\n",
      "Iteracion: 15213 Gradiente: [0.0001234581870941535,-0.002129986913731585] Loss: 22.842741016460433\n",
      "Iteracion: 15214 Gradiente: [0.00012339255292204143,-0.0021288545473088522] Loss: 22.842741011909546\n",
      "Iteracion: 15215 Gradiente: [0.0001233269536574729,-0.0021277227828863468] Loss: 22.842741007363525\n",
      "Iteracion: 15216 Gradiente: [0.0001232613892104458,-0.0021265916201484696] Loss: 22.84274100282231\n",
      "Iteracion: 15217 Gradiente: [0.00012319585965011963,-0.002125461058768726] Loss: 22.842740998285937\n",
      "Iteracion: 15218 Gradiente: [0.0001231303648883871,-0.0021243310984329373] Loss: 22.84274099375438\n",
      "Iteracion: 15219 Gradiente: [0.00012306490510525237,-0.0021232017388091626] Loss: 22.84274098922765\n",
      "Iteracion: 15220 Gradiente: [0.00012299947999470836,-0.002122072979595302] Loss: 22.842740984705706\n",
      "Iteracion: 15221 Gradiente: [0.0001229340897206536,-0.002120944820461427] Loss: 22.84274098018858\n",
      "Iteracion: 15222 Gradiente: [0.00012286873415992734,-0.002119817261094307] Loss: 22.84274097567627\n",
      "Iteracion: 15223 Gradiente: [0.0001228034133892682,-0.0021186903011706447] Loss: 22.842740971168748\n",
      "Iteracion: 15224 Gradiente: [0.00012273812727888375,-0.0021175639403768543] Loss: 22.84274096666601\n",
      "Iteracion: 15225 Gradiente: [0.00012267287598319854,-0.0021164381783835987] Loss: 22.84274096216807\n",
      "Iteracion: 15226 Gradiente: [0.00012260765927294415,-0.002115313014886766] Loss: 22.842740957674895\n",
      "Iteracion: 15227 Gradiente: [0.00012254247721633267,-0.0021141884495632962] Loss: 22.842740953186514\n",
      "Iteracion: 15228 Gradiente: [0.00012247732986736537,-0.002113064482088826] Loss: 22.8427409487029\n",
      "Iteracion: 15229 Gradiente: [0.00012241221722509484,-0.0021119411121464536] Loss: 22.842740944224044\n",
      "Iteracion: 15230 Gradiente: [0.0001223471391796238,-0.0021108183394233037] Loss: 22.84274093974995\n",
      "Iteracion: 15231 Gradiente: [0.0001222820956703193,-0.00210969616360354] Loss: 22.842740935280617\n",
      "Iteracion: 15232 Gradiente: [0.00012221708682223683,-0.0021085745843623252] Loss: 22.842740930816024\n",
      "Iteracion: 15233 Gradiente: [0.00012215211240610793,-0.002107453601394719] Loss: 22.8427409263562\n",
      "Iteracion: 15234 Gradiente: [0.00012208717257825203,-0.002106333214373516] Loss: 22.8427409219011\n",
      "Iteracion: 15235 Gradiente: [0.00012202226729129961,-0.002105213422982762] Loss: 22.84274091745073\n",
      "Iteracion: 15236 Gradiente: [0.00012195739642777426,-0.002104094226913489] Loss: 22.842740913005105\n",
      "Iteracion: 15237 Gradiente: [0.00012189256019325966,-0.0021029756258341098] Loss: 22.842740908564206\n",
      "Iteracion: 15238 Gradiente: [0.00012182775837175087,-0.002101857619440513] Loss: 22.842740904128014\n",
      "Iteracion: 15239 Gradiente: [0.00012176299101724908,-0.0021007402074123623] Loss: 22.84274089969654\n",
      "Iteracion: 15240 Gradiente: [0.00012169825804069963,-0.0020996233894384395] Loss: 22.842740895269785\n",
      "Iteracion: 15241 Gradiente: [0.00012163355948284031,-0.0020985071651987634] Loss: 22.842740890847733\n",
      "Iteracion: 15242 Gradiente: [0.0001215688953929354,-0.0020973915343750114] Loss: 22.842740886430388\n",
      "Iteracion: 15243 Gradiente: [0.00012150426567529849,-0.002096276496652531] Loss: 22.842740882017733\n",
      "Iteracion: 15244 Gradiente: [0.00012143967020582144,-0.002095162051727802] Loss: 22.842740877609764\n",
      "Iteracion: 15245 Gradiente: [0.00012137510918345622,-0.0020940481992685277] Loss: 22.84274087320648\n",
      "Iteracion: 15246 Gradiente: [0.00012131058248977903,-0.0020929349389673985] Loss: 22.84274086880788\n",
      "Iteracion: 15247 Gradiente: [0.00012124609005657779,-0.0020918222705120114] Loss: 22.842740864413955\n",
      "Iteracion: 15248 Gradiente: [0.00012118163188006293,-0.002090710193586768] Loss: 22.842740860024698\n",
      "Iteracion: 15249 Gradiente: [0.00012111720803318349,-0.0020895987078722793] Loss: 22.842740855640116\n",
      "Iteracion: 15250 Gradiente: [0.00012105281839088396,-0.002088487813060406] Loss: 22.84274085126019\n",
      "Iteracion: 15251 Gradiente: [0.00012098846296453303,-0.0020873775088361413] Loss: 22.842740846884915\n",
      "Iteracion: 15252 Gradiente: [0.00012092414176739415,-0.002086267794881754] Loss: 22.842740842514292\n",
      "Iteracion: 15253 Gradiente: [0.00012085985478430909,-0.0020851586708846054] Loss: 22.842740838148327\n",
      "Iteracion: 15254 Gradiente: [0.00012079560200485654,-0.0020840501365295694] Loss: 22.842740833786984\n",
      "Iteracion: 15255 Gradiente: [0.0001207313833522979,-0.002082942191508034] Loss: 22.84274082943028\n",
      "Iteracion: 15256 Gradiente: [0.00012066719885979182,-0.0020818348355015576] Loss: 22.842740825078224\n",
      "Iteracion: 15257 Gradiente: [0.00012060304851502224,-0.002080728068199633] Loss: 22.84274082073077\n",
      "Iteracion: 15258 Gradiente: [0.00012053893219861796,-0.0020796218892927005] Loss: 22.842740816387963\n",
      "Iteracion: 15259 Gradiente: [0.00012047485007542491,-0.0020785162984579366] Loss: 22.84274081204975\n",
      "Iteracion: 15260 Gradiente: [0.00012041080184701513,-0.0020774112954004666] Loss: 22.84274080771616\n",
      "Iteracion: 15261 Gradiente: [0.0001203467878004479,-0.0020763068797885844] Loss: 22.842740803387183\n",
      "Iteracion: 15262 Gradiente: [0.00012028280770361258,-0.0020752030513231526] Loss: 22.8427407990628\n",
      "Iteracion: 15263 Gradiente: [0.00012021886166545907,-0.0020740998096831238] Loss: 22.842740794743\n",
      "Iteracion: 15264 Gradiente: [0.00012015494959219572,-0.0020729971545632017] Loss: 22.842740790427804\n",
      "Iteracion: 15265 Gradiente: [0.00012009107158519327,-0.0020718950856430498] Loss: 22.842740786117197\n",
      "Iteracion: 15266 Gradiente: [0.00012002722756013403,-0.0020707936026137003] Loss: 22.84274078181117\n",
      "Iteracion: 15267 Gradiente: [0.00011996341740333113,-0.0020696927051713962] Loss: 22.84274077750973\n",
      "Iteracion: 15268 Gradiente: [0.00011989964110436328,-0.0020685923930048016] Loss: 22.842740773212842\n",
      "Iteracion: 15269 Gradiente: [0.00011983589879397036,-0.002067492665792159] Loss: 22.842740768920535\n",
      "Iteracion: 15270 Gradiente: [0.00011977219038499243,-0.0020663935232292374] Loss: 22.842740764632797\n",
      "Iteracion: 15271 Gradiente: [0.00011970851576753224,-0.002065294965006359] Loss: 22.842740760349592\n",
      "Iteracion: 15272 Gradiente: [0.00011964487506285574,-0.00206419699080899] Loss: 22.84274075607097\n",
      "Iteracion: 15273 Gradiente: [0.00011958126815253915,-0.0020630996003302943] Loss: 22.84274075179688\n",
      "Iteracion: 15274 Gradiente: [0.00011951769504889854,-0.0020620027932573967] Loss: 22.842740747527333\n",
      "Iteracion: 15275 Gradiente: [0.00011945415576709214,-0.0020609065692807366] Loss: 22.842740743262322\n",
      "Iteracion: 15276 Gradiente: [0.00011939065033648907,-0.0020598109280861366] Loss: 22.84274073900186\n",
      "Iteracion: 15277 Gradiente: [0.00011932717862445467,-0.0020587158693697203] Loss: 22.842740734745913\n",
      "Iteracion: 15278 Gradiente: [0.00011926374063383112,-0.0020576213928213366] Loss: 22.8427407304945\n",
      "Iteracion: 15279 Gradiente: [0.00011920033635419714,-0.0020565274981320177] Loss: 22.842740726247598\n",
      "Iteracion: 15280 Gradiente: [0.00011913696578934226,-0.002055434184990072] Loss: 22.842740722005203\n",
      "Iteracion: 15281 Gradiente: [0.00011907362890800262,-0.002054341453086887] Loss: 22.842740717767338\n",
      "Iteracion: 15282 Gradiente: [0.00011901032565522959,-0.002053249302116219] Loss: 22.842740713533953\n",
      "Iteracion: 15283 Gradiente: [0.00011894705615702605,-0.0020521577317609285] Loss: 22.842740709305083\n",
      "Iteracion: 15284 Gradiente: [0.0001188838202712835,-0.00205106674171797] Loss: 22.842740705080704\n",
      "Iteracion: 15285 Gradiente: [0.00011882061796863279,-0.0020499763316818094] Loss: 22.84274070086082\n",
      "Iteracion: 15286 Gradiente: [0.00011875744923770526,-0.0020488865013440715] Loss: 22.84274069664543\n",
      "Iteracion: 15287 Gradiente: [0.00011869431412776521,-0.002047797250389512] Loss: 22.842740692434507\n",
      "Iteracion: 15288 Gradiente: [0.00011863121261797005,-0.0020467085785133084] Loss: 22.842740688228062\n",
      "Iteracion: 15289 Gradiente: [0.00011856814462494943,-0.0020456204854092165] Loss: 22.842740684026086\n",
      "Iteracion: 15290 Gradiente: [0.00011850511021407328,-0.0020445329707659] Loss: 22.842740679828587\n",
      "Iteracion: 15291 Gradiente: [0.00011844210925365436,-0.002043446034282444] Loss: 22.842740675635543\n",
      "Iteracion: 15292 Gradiente: [0.00011837914175411394,-0.002042359675649645] Loss: 22.842740671446943\n",
      "Iteracion: 15293 Gradiente: [0.00011831620775618983,-0.002041273894557823] Loss: 22.842740667262827\n",
      "Iteracion: 15294 Gradiente: [0.00011825330724188158,-0.0020401886906976567] Loss: 22.84274066308313\n",
      "Iteracion: 15295 Gradiente: [0.00011819044022066312,-0.002039104063763375] Loss: 22.842740658907903\n",
      "Iteracion: 15296 Gradiente: [0.00011812760647368729,-0.0020380200134583267] Loss: 22.84274065473709\n",
      "Iteracion: 15297 Gradiente: [0.00011806480628138161,-0.002036936539457083] Loss: 22.842740650570708\n",
      "Iteracion: 15298 Gradiente: [0.00011800203934910768,-0.002035853641473532] Loss: 22.842740646408778\n",
      "Iteracion: 15299 Gradiente: [0.00011793930589950226,-0.002034771319184377] Loss: 22.842740642251247\n",
      "Iteracion: 15300 Gradiente: [0.0001178766057194025,-0.0020336895722958084] Loss: 22.842740638098142\n",
      "Iteracion: 15301 Gradiente: [0.00011781393883344056,-0.0020326084004987405] Loss: 22.842740633949465\n",
      "Iteracion: 15302 Gradiente: [0.0001177513053429872,-0.002031527803481836] Loss: 22.84274062980519\n",
      "Iteracion: 15303 Gradiente: [0.00011768870511730256,-0.002030447780945958] Loss: 22.842740625665332\n",
      "Iteracion: 15304 Gradiente: [0.0001176261381705975,-0.0020293683325817826] Loss: 22.842740621529845\n",
      "Iteracion: 15305 Gradiente: [0.00011756360453887282,-0.0020282894580841314] Loss: 22.84274061739877\n",
      "Iteracion: 15306 Gradiente: [0.00011750110406865133,-0.0020272111571530377] Loss: 22.84274061327209\n",
      "Iteracion: 15307 Gradiente: [0.00011743863692856849,-0.0020261334294742047] Loss: 22.8427406091498\n",
      "Iteracion: 15308 Gradiente: [0.00011737620296609445,-0.002025056274750033] Loss: 22.842740605031896\n",
      "Iteracion: 15309 Gradiente: [0.00011731380218501878,-0.0020239796926743974] Loss: 22.842740600918358\n",
      "Iteracion: 15310 Gradiente: [0.00011725143454555109,-0.002022903682945317] Loss: 22.842740596809183\n",
      "Iteracion: 15311 Gradiente: [0.00011718910007042874,-0.0020218282452560744] Loss: 22.84274059270439\n",
      "Iteracion: 15312 Gradiente: [0.00011712679877765217,-0.0020207533793007806] Loss: 22.842740588603956\n",
      "Iteracion: 15313 Gradiente: [0.00011706453063974702,-0.0020196790847733107] Loss: 22.84274058450789\n",
      "Iteracion: 15314 Gradiente: [0.00011700229548239349,-0.002018605361381513] Loss: 22.842740580416166\n",
      "Iteracion: 15315 Gradiente: [0.00011694009349696443,-0.0020175322088091955] Loss: 22.84274057632879\n",
      "Iteracion: 15316 Gradiente: [0.00011687792457450996,-0.0020164596267579307] Loss: 22.842740572245763\n",
      "Iteracion: 15317 Gradiente: [0.00011681578858144804,-0.002015387614931067] Loss: 22.842740568167077\n",
      "Iteracion: 15318 Gradiente: [0.00011675368576031057,-0.002014316173011584] Loss: 22.842740564092733\n",
      "Iteracion: 15319 Gradiente: [0.00011669161590551387,-0.00201324530070508] Loss: 22.842740560022715\n",
      "Iteracion: 15320 Gradiente: [0.00011662957899526797,-0.0020121749977105217] Loss: 22.842740555957025\n",
      "Iteracion: 15321 Gradiente: [0.00011656757514420709,-0.002011105263719178] Loss: 22.842740551895652\n",
      "Iteracion: 15322 Gradiente: [0.00011650560431159344,-0.002010036098425753] Loss: 22.842740547838595\n",
      "Iteracion: 15323 Gradiente: [0.00011644366639984582,-0.002008967501535845] Loss: 22.842740543785855\n",
      "Iteracion: 15324 Gradiente: [0.00011638176126022396,-0.002007899472753986] Loss: 22.842740539737424\n",
      "Iteracion: 15325 Gradiente: [0.00011631988916936583,-0.002006832011760314] Loss: 22.842740535693288\n",
      "Iteracion: 15326 Gradiente: [0.00011625804994158292,-0.0020057651182637435] Loss: 22.84274053165347\n",
      "Iteracion: 15327 Gradiente: [0.0001161962436678247,-0.0020046987919557797] Loss: 22.84274052761792\n",
      "Iteracion: 15328 Gradiente: [0.0001161344701500866,-0.0020036330325452194] Loss: 22.84274052358667\n",
      "Iteracion: 15329 Gradiente: [0.00011607272949163417,-0.002002567839725344] Loss: 22.84274051955973\n",
      "Iteracion: 15330 Gradiente: [0.00011601102173888952,-0.0020015032131894374] Loss: 22.842740515537027\n",
      "Iteracion: 15331 Gradiente: [0.00011594934671184849,-0.0020004391526453466] Loss: 22.842740511518627\n",
      "Iteracion: 15332 Gradiente: [0.00011588770447588104,-0.0019993756577890783] Loss: 22.84274050750449\n",
      "Iteracion: 15333 Gradiente: [0.00011582609490024727,-0.001998312728325639] Loss: 22.84274050349463\n",
      "Iteracion: 15334 Gradiente: [0.0001157645182208474,-0.0019972503639413236] Loss: 22.842740499489032\n",
      "Iteracion: 15335 Gradiente: [0.0001157029742633616,-0.001996188564342205] Loss: 22.842740495487686\n",
      "Iteracion: 15336 Gradiente: [0.00011564146298042033,-0.0019951273292306837] Loss: 22.842740491490588\n",
      "Iteracion: 15337 Gradiente: [0.00011557998441749836,-0.0019940666583028845] Loss: 22.84274048749774\n",
      "Iteracion: 15338 Gradiente: [0.00011551853856133221,-0.001993006551259905] Loss: 22.842740483509143\n",
      "Iteracion: 15339 Gradiente: [0.00011545712538539495,-0.001991947007800713] Loss: 22.84274047952478\n",
      "Iteracion: 15340 Gradiente: [0.00011539574482147449,-0.0019908880276300776] Loss: 22.842740475544662\n",
      "Iteracion: 15341 Gradiente: [0.00011533439691883511,-0.0019898296104443602] Loss: 22.842740471568746\n",
      "Iteracion: 15342 Gradiente: [0.00011527308163673903,-0.0019887717559443035] Loss: 22.842740467597086\n",
      "Iteracion: 15343 Gradiente: [0.00011521179887097333,-0.0019877144638381123] Loss: 22.84274046362965\n",
      "Iteracion: 15344 Gradiente: [0.00011515054885270122,-0.001986657733809712] Loss: 22.842740459666423\n",
      "Iteracion: 15345 Gradiente: [0.00011508933124938873,-0.0019856015655803343] Loss: 22.842740455707396\n",
      "Iteracion: 15346 Gradiente: [0.00011502814626282998,-0.00198454595883805] Loss: 22.842740451752597\n",
      "Iteracion: 15347 Gradiente: [0.00011496699370544169,-0.00198349091329438] Loss: 22.842740447801994\n",
      "Iteracion: 15348 Gradiente: [0.00011490587371649023,-0.0019824364286434343] Loss: 22.84274044385559\n",
      "Iteracion: 15349 Gradiente: [0.00011484478625429043,-0.001981382504585838] Loss: 22.842740439913385\n",
      "Iteracion: 15350 Gradiente: [0.00011478373119568156,-0.001980329140830861] Loss: 22.842740435975347\n",
      "Iteracion: 15351 Gradiente: [0.00011472270868466694,-0.0019792763370707197] Loss: 22.84274043204153\n",
      "Iteracion: 15352 Gradiente: [0.00011466171859524366,-0.001978224093015276] Loss: 22.84274042811188\n",
      "Iteracion: 15353 Gradiente: [0.00011460076094541213,-0.001977172408364917] Loss: 22.842740424186402\n",
      "Iteracion: 15354 Gradiente: [0.00011453983560443249,-0.001976121282826663] Loss: 22.84274042026511\n",
      "Iteracion: 15355 Gradiente: [0.00011447894269167591,-0.0019750707160974676] Loss: 22.84274041634797\n",
      "Iteracion: 15356 Gradiente: [0.00011441808218061548,-0.001974020707879494] Loss: 22.842740412435\n",
      "Iteracion: 15357 Gradiente: [0.00011435725405135599,-0.001972971257876329] Loss: 22.84274040852619\n",
      "Iteracion: 15358 Gradiente: [0.00011429645818736845,-0.001971922365797596] Loss: 22.84274040462153\n",
      "Iteracion: 15359 Gradiente: [0.0001142356946597071,-0.001970874031341552] Loss: 22.842740400721027\n",
      "Iteracion: 15360 Gradiente: [0.00011417496345510851,-0.0019698262542111895] Loss: 22.842740396824663\n",
      "Iteracion: 15361 Gradiente: [0.00011411426451199228,-0.0019687790341128184] Loss: 22.842740392932452\n",
      "Iteracion: 15362 Gradiente: [0.00011405359794688744,-0.0019677323707419704] Loss: 22.842740389044376\n",
      "Iteracion: 15363 Gradiente: [0.0001139929635532629,-0.001966686263815139] Loss: 22.842740385160432\n",
      "Iteracion: 15364 Gradiente: [0.000113932361414489,-0.0019656407130286863] Loss: 22.842740381280603\n",
      "Iteracion: 15365 Gradiente: [0.00011387179152109184,-0.0019645957180882095] Loss: 22.842740377404922\n",
      "Iteracion: 15366 Gradiente: [0.000113811253716752,-0.0019635512787054667] Loss: 22.842740373533346\n",
      "Iteracion: 15367 Gradiente: [0.00011375074812557766,-0.0019625073945766995] Loss: 22.842740369665876\n",
      "Iteracion: 15368 Gradiente: [0.00011369027472672618,-0.001961464065407507] Loss: 22.84274036580253\n",
      "Iteracion: 15369 Gradiente: [0.00011362983347472285,-0.0019604212909049086] Loss: 22.8427403619433\n",
      "Iteracion: 15370 Gradiente: [0.00011356942434304074,-0.0019593790707740292] Loss: 22.842740358088157\n",
      "Iteracion: 15371 Gradiente: [0.000113509047395155,-0.001958337404714546] Loss: 22.842740354237126\n",
      "Iteracion: 15372 Gradiente: [0.00011344870246527231,-0.0019572962924412946] Loss: 22.84274035039017\n",
      "Iteracion: 15373 Gradiente: [0.0001133883897002382,-0.0019562557336502808] Loss: 22.842740346547316\n",
      "Iteracion: 15374 Gradiente: [0.00011332810889731111,-0.00195521572805859] Loss: 22.842740342708527\n",
      "Iteracion: 15375 Gradiente: [0.00011326786025070609,-0.001954176275359624] Loss: 22.842740338873853\n",
      "Iteracion: 15376 Gradiente: [0.00011320764359084024,-0.0019531373752682177] Loss: 22.84274033504324\n",
      "Iteracion: 15377 Gradiente: [0.00011314745897834655,-0.0019520990274867719] Loss: 22.842740331216696\n",
      "Iteracion: 15378 Gradiente: [0.00011308730627490604,-0.0019510612317259776] Loss: 22.842740327394214\n",
      "Iteracion: 15379 Gradiente: [0.0001130271856671546,-0.0019500239876831433] Loss: 22.842740323575807\n",
      "Iteracion: 15380 Gradiente: [0.0001129670969334029,-0.0019489872950765393] Loss: 22.842740319761447\n",
      "Iteracion: 15381 Gradiente: [0.00011290704009070396,-0.001947951153610461] Loss: 22.842740315951136\n",
      "Iteracion: 15382 Gradiente: [0.00011284701520916466,-0.0019469155629875464] Loss: 22.8427403121449\n",
      "Iteracion: 15383 Gradiente: [0.00011278702221867813,-0.0019458805229181308] Loss: 22.842740308342687\n",
      "Iteracion: 15384 Gradiente: [0.00011272706125945812,-0.0019448460330984574] Loss: 22.842740304544535\n",
      "Iteracion: 15385 Gradiente: [0.00011266713208992011,-0.0019438120932501117] Loss: 22.842740300750403\n",
      "Iteracion: 15386 Gradiente: [0.00011260723479438184,-0.0019427787030769157] Loss: 22.842740296960322\n",
      "Iteracion: 15387 Gradiente: [0.0001125473692553669,-0.0019417458622899156] Loss: 22.842740293174252\n",
      "Iteracion: 15388 Gradiente: [0.00011248753569266985,-0.0019407135705834595] Loss: 22.842740289392218\n",
      "Iteracion: 15389 Gradiente: [0.00011242773382112622,-0.0019396818276826858] Loss: 22.8427402856142\n",
      "Iteracion: 15390 Gradiente: [0.00011236796385958315,-0.001938650633279811] Loss: 22.84274028184019\n",
      "Iteracion: 15391 Gradiente: [0.00011230822559393043,-0.001937619987096658] Loss: 22.842740278070206\n",
      "Iteracion: 15392 Gradiente: [0.00011224851906111629,-0.0019365898888397718] Loss: 22.842740274304223\n",
      "Iteracion: 15393 Gradiente: [0.00011218884428766766,-0.0019355603382119094] Loss: 22.842740270542244\n",
      "Iteracion: 15394 Gradiente: [0.00011212920125558412,-0.0019345313349247088] Loss: 22.84274026678427\n",
      "Iteracion: 15395 Gradiente: [0.0001120695899326544,-0.0019335028786862552] Loss: 22.842740263030272\n",
      "Iteracion: 15396 Gradiente: [0.00011201001034824762,-0.001932474969206055] Loss: 22.842740259280284\n",
      "Iteracion: 15397 Gradiente: [0.00011195046236120257,-0.0019314476061969307] Loss: 22.84274025553426\n",
      "Iteracion: 15398 Gradiente: [0.00011189094601794143,-0.0019304207893661384] Loss: 22.84274025179224\n",
      "Iteracion: 15399 Gradiente: [0.00011183146132035897,-0.0019293945184230665] Loss: 22.842740248054177\n",
      "Iteracion: 15400 Gradiente: [0.00011177200828171863,-0.0019283687930759185] Loss: 22.84274024432011\n",
      "Iteracion: 15401 Gradiente: [0.00011171258686033525,-0.0019273436130340827] Loss: 22.842740240590008\n",
      "Iteracion: 15402 Gradiente: [0.00011165319692925853,-0.0019263189780158285] Loss: 22.84274023686386\n",
      "Iteracion: 15403 Gradiente: [0.00011159383878881121,-0.001925294887711478] Loss: 22.842740233141676\n",
      "Iteracion: 15404 Gradiente: [0.0001115345120439315,-0.0019242713418548144] Loss: 22.842740229423455\n",
      "Iteracion: 15405 Gradiente: [0.00011147521685472839,-0.001923248340145515] Loss: 22.84274022570918\n",
      "Iteracion: 15406 Gradiente: [0.00011141595330362482,-0.001922225882288823] Loss: 22.84274022199886\n",
      "Iteracion: 15407 Gradiente: [0.00011135672114050976,-0.0019212039680085744] Loss: 22.842740218292473\n",
      "Iteracion: 15408 Gradiente: [0.00011129752046296441,-0.0019201825970107222] Loss: 22.842740214590037\n",
      "Iteracion: 15409 Gradiente: [0.00011123835128898918,-0.0019191617690007472] Loss: 22.84274021089153\n",
      "Iteracion: 15410 Gradiente: [0.00011117921362711059,-0.0019181414836938397] Loss: 22.842740207196968\n",
      "Iteracion: 15411 Gradiente: [0.00011112010731911444,-0.001917121740805546] Loss: 22.84274020350631\n",
      "Iteracion: 15412 Gradiente: [0.00011106103243794981,-0.0019161025400432409] Loss: 22.84274019981958\n",
      "Iteracion: 15413 Gradiente: [0.00011100198905467096,-0.0019150838811147727] Loss: 22.84274019613678\n",
      "Iteracion: 15414 Gradiente: [0.00011094297695611507,-0.0019140657637414904] Loss: 22.842740192457903\n",
      "Iteracion: 15415 Gradiente: [0.00011088399622281032,-0.0019130481876315979] Loss: 22.842740188782905\n",
      "Iteracion: 15416 Gradiente: [0.00011082504688696796,-0.0019120311524948382] Loss: 22.842740185111843\n",
      "Iteracion: 15417 Gradiente: [0.00011076612885858594,-0.0019110146580471128] Loss: 22.842740181444658\n",
      "Iteracion: 15418 Gradiente: [0.00011070724219356029,-0.0019099987039954414] Loss: 22.842740177781398\n",
      "Iteracion: 15419 Gradiente: [0.00011064838682367887,-0.0019089832900585672] Loss: 22.842740174122028\n",
      "Iteracion: 15420 Gradiente: [0.00011058956270630916,-0.00190796841594801] Loss: 22.842740170466538\n",
      "Iteracion: 15421 Gradiente: [0.00011053076998450706,-0.0019069540813678287] Loss: 22.84274016681493\n",
      "Iteracion: 15422 Gradiente: [0.00011047200836647638,-0.001905940286049912] Loss: 22.842740163167218\n",
      "Iteracion: 15423 Gradiente: [0.00011041327805211646,-0.0019049270296916869] Loss: 22.842740159523373\n",
      "Iteracion: 15424 Gradiente: [0.00011035457900258432,-0.0019039143120084627] Loss: 22.84274015588341\n",
      "Iteracion: 15425 Gradiente: [0.00011029591108903484,-0.0019029021327222988] Loss: 22.84274015224729\n",
      "Iteracion: 15426 Gradiente: [0.00011023727445547138,-0.0019018904915357145] Loss: 22.842740148615068\n",
      "Iteracion: 15427 Gradiente: [0.00011017866898441753,-0.00190087938816923] Loss: 22.842740144986678\n",
      "Iteracion: 15428 Gradiente: [0.00011012009453850168,-0.001899868822344904] Loss: 22.842740141362164\n",
      "Iteracion: 15429 Gradiente: [0.00011006155131099149,-0.0018988587937632435] Loss: 22.842740137741508\n",
      "Iteracion: 15430 Gradiente: [0.00011000303935588817,-0.0018978493021339915] Loss: 22.84274013412469\n",
      "Iteracion: 15431 Gradiente: [0.00010994455827812999,-0.0018968403471962604] Loss: 22.842740130511718\n",
      "Iteracion: 15432 Gradiente: [0.00010988610834014404,-0.0018958319286475956] Loss: 22.842740126902576\n",
      "Iteracion: 15433 Gradiente: [0.0001098276894576126,-0.0018948240462047276] Loss: 22.842740123297286\n",
      "Iteracion: 15434 Gradiente: [0.00010976930177074943,-0.0018938166995753865] Loss: 22.84274011969582\n",
      "Iteracion: 15435 Gradiente: [0.00010971094500386394,-0.0018928098884894477] Loss: 22.842740116098184\n",
      "Iteracion: 15436 Gradiente: [0.00010965261934927639,-0.0018918036126502594] Loss: 22.84274011250438\n",
      "Iteracion: 15437 Gradiente: [0.0001095943246364565,-0.0018907978717814208] Loss: 22.84274010891439\n",
      "Iteracion: 15438 Gradiente: [0.00010953606087014123,-0.0018897926655984775] Loss: 22.842740105328215\n",
      "Iteracion: 15439 Gradiente: [0.00010947782815170133,-0.0018887879938098706] Loss: 22.842740101745864\n",
      "Iteracion: 15440 Gradiente: [0.00010941962642808297,-0.001887783856134225] Loss: 22.842740098167305\n",
      "Iteracion: 15441 Gradiente: [0.00010936145554296672,-0.001886780252293955] Loss: 22.842740094592543\n",
      "Iteracion: 15442 Gradiente: [0.00010930331570572586,-0.0018857771819931202] Loss: 22.842740091021593\n",
      "Iteracion: 15443 Gradiente: [0.00010924520670035539,-0.001884774644960885] Loss: 22.84274008745445\n",
      "Iteracion: 15444 Gradiente: [0.00010918712862443651,-0.0018837726409045056] Loss: 22.842740083891087\n",
      "Iteracion: 15445 Gradiente: [0.00010912908130649157,-0.0018827711695524366] Loss: 22.842740080331506\n",
      "Iteracion: 15446 Gradiente: [0.00010907106498336816,-0.0018817702306045921] Loss: 22.84274007677572\n",
      "Iteracion: 15447 Gradiente: [0.00010901307941253435,-0.0018807698237926238] Loss: 22.842740073223705\n",
      "Iteracion: 15448 Gradiente: [0.0001089551247057822,-0.0018797699488242614] Loss: 22.84274006967547\n",
      "Iteracion: 15449 Gradiente: [0.00010889720085932216,-0.0018787706054178936] Loss: 22.84274006613101\n",
      "Iteracion: 15450 Gradiente: [0.00010883930778125735,-0.0018777717932941585] Loss: 22.842740062590313\n",
      "Iteracion: 15451 Gradiente: [0.00010878144549716731,-0.0018767735121683652] Loss: 22.842740059053376\n",
      "Iteracion: 15452 Gradiente: [0.00010872361392083955,-0.0018757757617642312] Loss: 22.842740055520206\n",
      "Iteracion: 15453 Gradiente: [0.00010866581314227612,-0.0018747785417899602] Loss: 22.842740051990777\n",
      "Iteracion: 15454 Gradiente: [0.00010860804299947327,-0.0018737818519750722] Loss: 22.84274004846512\n",
      "Iteracion: 15455 Gradiente: [0.00010855030363453959,-0.0018727856920275343] Loss: 22.8427400449432\n",
      "Iteracion: 15456 Gradiente: [0.00010849259493189341,-0.0018717900616702346] Loss: 22.84274004142503\n",
      "Iteracion: 15457 Gradiente: [0.00010843491698627379,-0.0018707949606168246] Loss: 22.842740037910595\n",
      "Iteracion: 15458 Gradiente: [0.00010837726961767657,-0.001869800388593982] Loss: 22.84274003439989\n",
      "Iteracion: 15459 Gradiente: [0.00010831965297199985,-0.0018688063453122794] Loss: 22.84274003089293\n",
      "Iteracion: 15460 Gradiente: [0.00010826206685408125,-0.001867812830500526] Loss: 22.84274002738969\n",
      "Iteracion: 15461 Gradiente: [0.00010820451148182049,-0.0018668198438638465] Loss: 22.84274002389018\n",
      "Iteracion: 15462 Gradiente: [0.00010814698651984145,-0.001865827385139814] Loss: 22.842740020394384\n",
      "Iteracion: 15463 Gradiente: [0.0001080894923793115,-0.0018648354540243162] Loss: 22.8427400169023\n",
      "Iteracion: 15464 Gradiente: [0.00010803202865664238,-0.001863844050258005] Loss: 22.842740013413938\n",
      "Iteracion: 15465 Gradiente: [0.0001079745955252065,-0.001862853173549676] Loss: 22.84274000992928\n",
      "Iteracion: 15466 Gradiente: [0.0001079171929139496,-0.0018618628236238759] Loss: 22.842740006448324\n",
      "Iteracion: 15467 Gradiente: [0.00010785982081150299,-0.00186087300019769] Loss: 22.84274000297106\n",
      "Iteracion: 15468 Gradiente: [0.00010780247920839277,-0.0018598837029922303] Loss: 22.84273999949751\n",
      "Iteracion: 15469 Gradiente: [0.00010774516816146237,-0.0018588949317233983] Loss: 22.84273999602764\n",
      "Iteracion: 15470 Gradiente: [0.00010768788748028631,-0.001857906686121543] Loss: 22.84273999256147\n",
      "Iteracion: 15471 Gradiente: [0.00010763063731834184,-0.0018569189658985388] Loss: 22.842739989098973\n",
      "Iteracion: 15472 Gradiente: [0.00010757341755246822,-0.0018559317707798801] Loss: 22.84273998564016\n",
      "Iteracion: 15473 Gradiente: [0.00010751622825466711,-0.0018549451004811128] Loss: 22.842739982185023\n",
      "Iteracion: 15474 Gradiente: [0.00010745906927619822,-0.0018539589547315197] Loss: 22.84273997873356\n",
      "Iteracion: 15475 Gradiente: [0.00010740194074401188,-0.0018529733332435683] Loss: 22.842739975285763\n",
      "Iteracion: 15476 Gradiente: [0.00010734484268274022,-0.0018519882357381334] Loss: 22.842739971841635\n",
      "Iteracion: 15477 Gradiente: [0.00010728777473995403,-0.001851003661952788] Loss: 22.84273996840117\n",
      "Iteracion: 15478 Gradiente: [0.00010723073730787292,-0.0018500196115878017] Loss: 22.842739964964363\n",
      "Iteracion: 15479 Gradiente: [0.00010717373007764763,-0.0018490360843806287] Loss: 22.842739961531194\n",
      "Iteracion: 15480 Gradiente: [0.0001071167532927575,-0.001848053080038999] Loss: 22.84273995810169\n",
      "Iteracion: 15481 Gradiente: [0.00010705980674856619,-0.001847070598295512] Loss: 22.84273995467583\n",
      "Iteracion: 15482 Gradiente: [0.0001070028904659163,-0.001846088638870569] Loss: 22.84273995125361\n",
      "Iteracion: 15483 Gradiente: [0.00010694600437375357,-0.0018451072014890712] Loss: 22.84273994783501\n",
      "Iteracion: 15484 Gradiente: [0.000106889148666293,-0.0018441262858618283] Loss: 22.84273994442007\n",
      "Iteracion: 15485 Gradiente: [0.00010683232315310913,-0.0018431458917210837] Loss: 22.842739941008745\n",
      "Iteracion: 15486 Gradiente: [0.00010677552779441158,-0.001842166018790555] Loss: 22.842739937601056\n",
      "Iteracion: 15487 Gradiente: [0.0001067187626129377,-0.0018411866667920644] Loss: 22.842739934196988\n",
      "Iteracion: 15488 Gradiente: [0.00010666202759542405,-0.0018402078354479083] Loss: 22.842739930796522\n",
      "Iteracion: 15489 Gradiente: [0.0001066053227835558,-0.001839229524479317] Loss: 22.842739927399695\n",
      "Iteracion: 15490 Gradiente: [0.00010654864812806864,-0.0018382517336089422] Loss: 22.84273992400646\n",
      "Iteracion: 15491 Gradiente: [0.00010649200363748908,-0.0018372744625603824] Loss: 22.842739920616847\n",
      "Iteracion: 15492 Gradiente: [0.00010643538921234116,-0.0018362977110611448] Loss: 22.842739917230823\n",
      "Iteracion: 15493 Gradiente: [0.00010637880494641649,-0.001835321478830328] Loss: 22.84273991384841\n",
      "Iteracion: 15494 Gradiente: [0.00010632225071371219,-0.0018343457655978076] Loss: 22.84273991046958\n",
      "Iteracion: 15495 Gradiente: [0.00010626572648485914,-0.0018333705710867085] Loss: 22.842739907094355\n",
      "Iteracion: 15496 Gradiente: [0.00010620923241712413,-0.0018323958950128135] Loss: 22.84273990372271\n",
      "Iteracion: 15497 Gradiente: [0.00010615276830492349,-0.0018314217371114456] Loss: 22.842739900354655\n",
      "Iteracion: 15498 Gradiente: [0.00010609633420415321,-0.0018304480971021776] Loss: 22.84273989699017\n",
      "Iteracion: 15499 Gradiente: [0.00010603993019155192,-0.0018294749747056471] Loss: 22.84273989362927\n",
      "Iteracion: 15500 Gradiente: [0.00010598355610606329,-0.0018285023696532694] Loss: 22.842739890271933\n",
      "Iteracion: 15501 Gradiente: [0.0001059272119363186,-0.0018275302816708934] Loss: 22.842739886918178\n",
      "Iteracion: 15502 Gradiente: [0.00010587089788695417,-0.001826558710472407] Loss: 22.842739883567983\n",
      "Iteracion: 15503 Gradiente: [0.00010581461367185815,-0.0018255876557958573] Loss: 22.842739880221345\n",
      "Iteracion: 15504 Gradiente: [0.00010575835928250399,-0.001824617117367211] Loss: 22.842739876878266\n",
      "Iteracion: 15505 Gradiente: [0.00010570213493584409,-0.0018236470948989356] Loss: 22.842739873538736\n",
      "Iteracion: 15506 Gradiente: [0.00010564594038082002,-0.0018226775881304983] Loss: 22.842739870202763\n",
      "Iteracion: 15507 Gradiente: [0.0001055897757026969,-0.0018217085967812351] Loss: 22.842739866870332\n",
      "Iteracion: 15508 Gradiente: [0.00010553364094221252,-0.001820740120573916] Loss: 22.842739863541453\n",
      "Iteracion: 15509 Gradiente: [0.00010547753607947167,-0.0018197721592352186] Loss: 22.842739860216103\n",
      "Iteracion: 15510 Gradiente: [0.0001054214609941558,-0.0018188047124971499] Loss: 22.84273985689429\n",
      "Iteracion: 15511 Gradiente: [0.0001053654157345818,-0.0018178377800831906] Loss: 22.84273985357601\n",
      "Iteracion: 15512 Gradiente: [0.00010530940025338017,-0.0018168713617202552] Loss: 22.84273985026125\n",
      "Iteracion: 15513 Gradiente: [0.00010525341449276008,-0.0018159054571394032] Loss: 22.84273984695002\n",
      "Iteracion: 15514 Gradiente: [0.00010519745860146182,-0.0018149400660557073] Loss: 22.842739843642317\n",
      "Iteracion: 15515 Gradiente: [0.0001051415324724303,-0.0018139751882031874] Loss: 22.842739840338123\n",
      "Iteracion: 15516 Gradiente: [0.00010508563598250475,-0.0018130108233143242] Loss: 22.842739837037424\n",
      "Iteracion: 15517 Gradiente: [0.00010502976924916159,-0.0018120469711095193] Loss: 22.842739833740264\n",
      "Iteracion: 15518 Gradiente: [0.00010497393210660751,-0.0018110836313244504] Loss: 22.842739830446583\n",
      "Iteracion: 15519 Gradiente: [0.00010491812470926712,-0.0018101208036772694] Loss: 22.842739827156418\n",
      "Iteracion: 15520 Gradiente: [0.00010486234705335089,-0.0018091584878948907] Loss: 22.842739823869756\n",
      "Iteracion: 15521 Gradiente: [0.00010480659902043499,-0.0018081966837103873] Loss: 22.84273982058657\n",
      "Iteracion: 15522 Gradiente: [0.00010475088064652028,-0.0018072353908497262] Loss: 22.842739817306878\n",
      "Iteracion: 15523 Gradiente: [0.00010469519183970988,-0.0018062746090447964] Loss: 22.84273981403068\n",
      "Iteracion: 15524 Gradiente: [0.0001046395325905299,-0.0018053143380226306] Loss: 22.84273981075796\n",
      "Iteracion: 15525 Gradiente: [0.00010458390305245757,-0.0018043545775049334] Loss: 22.84273980748872\n",
      "Iteracion: 15526 Gradiente: [0.0001045283030578048,-0.0018033953272261981] Loss: 22.84273980422296\n",
      "Iteracion: 15527 Gradiente: [0.00010447273252793821,-0.0018024365869204453] Loss: 22.84273980096066\n",
      "Iteracion: 15528 Gradiente: [0.00010441719169970534,-0.0018014783563004974] Loss: 22.84273979770185\n",
      "Iteracion: 15529 Gradiente: [0.00010436168026046743,-0.0018005206351128086] Loss: 22.842739794446484\n",
      "Iteracion: 15530 Gradiente: [0.00010430619846601985,-0.0017995634230715042] Loss: 22.84273979119457\n",
      "Iteracion: 15531 Gradiente: [0.00010425074602835594,-0.0017986067199226832] Loss: 22.842739787946133\n",
      "Iteracion: 15532 Gradiente: [0.00010419532311137422,-0.0017976505253841425] Loss: 22.84273978470114\n",
      "Iteracion: 15533 Gradiente: [0.00010413992974539118,-0.0017966948391818485] Loss: 22.842739781459596\n",
      "Iteracion: 15534 Gradiente: [0.0001040845657267179,-0.0017957396610577567] Loss: 22.84273977822149\n",
      "Iteracion: 15535 Gradiente: [0.00010402923125335898,-0.0017947849907297808] Loss: 22.842739774986843\n",
      "Iteracion: 15536 Gradiente: [0.00010397392609320377,-0.001793830827939639] Loss: 22.842739771755628\n",
      "Iteracion: 15537 Gradiente: [0.00010391865033909653,-0.0017928771724121143] Loss: 22.842739768527846\n",
      "Iteracion: 15538 Gradiente: [0.00010386340396545772,-0.0017919240238763714] Loss: 22.842739765303495\n",
      "Iteracion: 15539 Gradiente: [0.00010380818705186811,-0.0017909713820590885] Loss: 22.84273976208257\n",
      "Iteracion: 15540 Gradiente: [0.0001037529993662171,-0.0017900192467034041] Loss: 22.84273975886507\n",
      "Iteracion: 15541 Gradiente: [0.00010369784114061531,-0.0017890676175232065] Loss: 22.842739755650992\n",
      "Iteracion: 15542 Gradiente: [0.00010364271218274249,-0.0017881164942625816] Loss: 22.84273975244033\n",
      "Iteracion: 15543 Gradiente: [0.00010358761246796652,-0.0017871658766532997] Loss: 22.842739749233083\n",
      "Iteracion: 15544 Gradiente: [0.00010353254210239508,-0.0017862157644165913] Loss: 22.84273974602925\n",
      "Iteracion: 15545 Gradiente: [0.00010347750103107955,-0.0017852661572892003] Loss: 22.842739742828815\n",
      "Iteracion: 15546 Gradiente: [0.00010342248921422955,-0.0017843170550018309] Loss: 22.84273973963177\n",
      "Iteracion: 15547 Gradiente: [0.00010336750669447762,-0.0017833684572830558] Loss: 22.842739736438144\n",
      "Iteracion: 15548 Gradiente: [0.00010331255330982003,-0.0017824203638742374] Loss: 22.842739733247903\n",
      "Iteracion: 15549 Gradiente: [0.00010325762916731188,-0.0017814727744988564] Loss: 22.842739730061055\n",
      "Iteracion: 15550 Gradiente: [0.0001032027342555845,-0.0017805256888886827] Loss: 22.842739726877596\n",
      "Iteracion: 15551 Gradiente: [0.00010314786850358359,-0.00177957910678046] Loss: 22.84273972369751\n",
      "Iteracion: 15552 Gradiente: [0.00010309303196152087,-0.0017786330279019325] Loss: 22.842739720520807\n",
      "Iteracion: 15553 Gradiente: [0.00010303822456971071,-0.001777687451988541] Loss: 22.842739717347506\n",
      "Iteracion: 15554 Gradiente: [0.00010298344624194063,-0.0017767423787757271] Loss: 22.84273971417755\n",
      "Iteracion: 15555 Gradiente: [0.00010292869707768659,-0.0017757978079913528] Loss: 22.84273971101097\n",
      "Iteracion: 15556 Gradiente: [0.00010287397697273566,-0.0017748537393712145] Loss: 22.842739707847766\n",
      "Iteracion: 15557 Gradiente: [0.0001028192861383559,-0.001773910172637727] Loss: 22.842739704687915\n",
      "Iteracion: 15558 Gradiente: [0.00010276462419748593,-0.0017729671075433846] Loss: 22.84273970153144\n",
      "Iteracion: 15559 Gradiente: [0.00010270999143623764,-0.0017720245438040886] Loss: 22.84273969837829\n",
      "Iteracion: 15560 Gradiente: [0.00010265538761207911,-0.0017710824811677146] Loss: 22.842739695228506\n",
      "Iteracion: 15561 Gradiente: [0.00010260081285859238,-0.001770140919358217] Loss: 22.842739692082077\n",
      "Iteracion: 15562 Gradiente: [0.00010254626706493278,-0.0017691998581143527] Loss: 22.842739688938984\n",
      "Iteracion: 15563 Gradiente: [0.00010249175037794582,-0.0017682592971604314] Loss: 22.842739685799238\n",
      "Iteracion: 15564 Gradiente: [0.00010243726258541604,-0.0017673192362437362] Loss: 22.84273968266283\n",
      "Iteracion: 15565 Gradiente: [0.00010238280378018771,-0.0017663796750904718] Loss: 22.842739679529757\n",
      "Iteracion: 15566 Gradiente: [0.00010232837392247045,-0.0017654406134377373] Loss: 22.842739676400008\n",
      "Iteracion: 15567 Gradiente: [0.00010227397302363291,-0.001764502051017421] Loss: 22.84273967327357\n",
      "Iteracion: 15568 Gradiente: [0.00010221960100598911,-0.0017635639875686357] Loss: 22.842739670150483\n",
      "Iteracion: 15569 Gradiente: [0.00010216525798227849,-0.0017626264228171114] Loss: 22.84273966703071\n",
      "Iteracion: 15570 Gradiente: [0.00010211094378670774,-0.0017616893565074083] Loss: 22.842739663914244\n",
      "Iteracion: 15571 Gradiente: [0.00010205665851780547,-0.0017607527883690467] Loss: 22.842739660801097\n",
      "Iteracion: 15572 Gradiente: [0.00010200240204388441,-0.0017598167181414937] Loss: 22.842739657691247\n",
      "Iteracion: 15573 Gradiente: [0.00010194817444642013,-0.0017588811455562829] Loss: 22.84273965458471\n",
      "Iteracion: 15574 Gradiente: [0.00010189397571120177,-0.0017579460703483819] Loss: 22.84273965148149\n",
      "Iteracion: 15575 Gradiente: [0.00010183980572264772,-0.0017570114922596267] Loss: 22.842739648381546\n",
      "Iteracion: 15576 Gradiente: [0.00010178566456507573,-0.0017560774110184714] Loss: 22.842739645284926\n",
      "Iteracion: 15577 Gradiente: [0.00010173155217785279,-0.0017551438263645025] Loss: 22.842739642191567\n",
      "Iteracion: 15578 Gradiente: [0.00010167746858750586,-0.001754210738031503] Loss: 22.842739639101524\n",
      "Iteracion: 15579 Gradiente: [0.00010162341363108377,-0.0017532781457635593] Loss: 22.842739636014752\n",
      "Iteracion: 15580 Gradiente: [0.00010156938746490596,-0.0017523460492873493] Loss: 22.84273963293125\n",
      "Iteracion: 15581 Gradiente: [0.0001015153900700246,-0.001751414448339498] Loss: 22.842739629851042\n",
      "Iteracion: 15582 Gradiente: [0.00010146142138959628,-0.0017504833426585265] Loss: 22.842739626774105\n",
      "Iteracion: 15583 Gradiente: [0.00010140748135161933,-0.001749552731984257] Loss: 22.84273962370042\n",
      "Iteracion: 15584 Gradiente: [0.00010135357009157057,-0.0017486226160457364] Loss: 22.842739620630024\n",
      "Iteracion: 15585 Gradiente: [0.00010129968742944584,-0.001747692994590011] Loss: 22.842739617562884\n",
      "Iteracion: 15586 Gradiente: [0.0001012458333690347,-0.001746763867350154] Loss: 22.842739614499017\n",
      "Iteracion: 15587 Gradiente: [0.0001011920080713935,-0.0017458352340565142] Loss: 22.842739611438382\n",
      "Iteracion: 15588 Gradiente: [0.00010113821124946298,-0.00174490709446052] Loss: 22.842739608381017\n",
      "Iteracion: 15589 Gradiente: [0.00010108444314482767,-0.001743979448285297] Loss: 22.84273960532691\n",
      "Iteracion: 15590 Gradiente: [0.0001010307035699043,-0.0017430522952783653] Loss: 22.842739602276037\n",
      "Iteracion: 15591 Gradiente: [0.00010097699245648073,-0.0017421256351809688] Loss: 22.842739599228402\n",
      "Iteracion: 15592 Gradiente: [0.00010092330995329728,-0.0017411994677215621] Loss: 22.842739596184007\n",
      "Iteracion: 15593 Gradiente: [0.00010086965600824745,-0.0017402737926403232] Loss: 22.842739593142863\n",
      "Iteracion: 15594 Gradiente: [0.00010081603068954337,-0.0017393486096692592] Loss: 22.84273959010494\n",
      "Iteracion: 15595 Gradiente: [0.00010076243379065393,-0.0017384239185597985] Loss: 22.84273958707026\n",
      "Iteracion: 15596 Gradiente: [0.00010070886541294992,-0.0017374997190424087] Loss: 22.842739584038792\n",
      "Iteracion: 15597 Gradiente: [0.00010065532542000711,-0.0017365760108634257] Loss: 22.842739581010555\n",
      "Iteracion: 15598 Gradiente: [0.00010060181395582883,-0.0017356527937533173] Loss: 22.842739577985522\n",
      "Iteracion: 15599 Gradiente: [0.00010054833089346478,-0.0017347300674556956] Loss: 22.842739574963726\n",
      "Iteracion: 15600 Gradiente: [0.00010049487630681142,-0.0017338078317052918] Loss: 22.842739571945124\n",
      "Iteracion: 15601 Gradiente: [0.0001004414502091322,-0.001732886086239205] Loss: 22.842739568929737\n",
      "Iteracion: 15602 Gradiente: [0.00010038805243652859,-0.0017319648308050735] Loss: 22.84273956591755\n",
      "Iteracion: 15603 Gradiente: [0.00010033468310363483,-0.0017310440651378655] Loss: 22.842739562908566\n",
      "Iteracion: 15604 Gradiente: [0.00010028134207971107,-0.001730123788978588] Loss: 22.84273955990279\n",
      "Iteracion: 15605 Gradiente: [0.00010022802941212679,-0.0017292040020691958] Loss: 22.8427395569002\n",
      "Iteracion: 15606 Gradiente: [0.00010017474511793504,-0.0017282847041447742] Loss: 22.842739553900806\n",
      "Iteracion: 15607 Gradiente: [0.00010012148912513415,-0.0017273658949478706] Loss: 22.842739550904607\n",
      "Iteracion: 15608 Gradiente: [0.00010006826152562099,-0.0017264475742130969] Loss: 22.842739547911588\n",
      "Iteracion: 15609 Gradiente: [0.00010001506214791789,-0.001725529741691645] Loss: 22.84273954492174\n",
      "Iteracion: 15610 Gradiente: [9.99618910081305e-05,-0.0017246123971201399] Loss: 22.84273954193508\n",
      "Iteracion: 15611 Gradiente: [9.990874829668428e-05,-0.0017236955402263258] Loss: 22.842739538951594\n",
      "Iteracion: 15612 Gradiente: [9.98556336924139e-05,-0.001722779170771934] Loss: 22.84273953597128\n",
      "Iteracion: 15613 Gradiente: [9.98025473393227e-05,-0.0017218632884862472] Loss: 22.842739532994123\n",
      "Iteracion: 15614 Gradiente: [9.974948925541108e-05,-0.0017209478931094443] Loss: 22.842739530020143\n",
      "Iteracion: 15615 Gradiente: [9.969645938099348e-05,-0.0017200329843846637] Loss: 22.84273952704932\n",
      "Iteracion: 15616 Gradiente: [9.964345768480598e-05,-0.0017191185620537416] Loss: 22.842739524081665\n",
      "Iteracion: 15617 Gradiente: [9.959048416874339e-05,-0.0017182046258589878] Loss: 22.84273952111716\n",
      "Iteracion: 15618 Gradiente: [9.953753881670006e-05,-0.00171729117553987] Loss: 22.842739518155803\n",
      "Iteracion: 15619 Gradiente: [9.94846215490952e-05,-0.0017163782108423693] Loss: 22.842739515197582\n",
      "Iteracion: 15620 Gradiente: [9.943173248340523e-05,-0.0017154657315008612] Loss: 22.842739512242524\n",
      "Iteracion: 15621 Gradiente: [9.937887159215583e-05,-0.0017145537372571815] Loss: 22.842739509290595\n",
      "Iteracion: 15622 Gradiente: [9.932603875029145e-05,-0.001713642227861456] Loss: 22.842739506341804\n",
      "Iteracion: 15623 Gradiente: [9.927323398623381e-05,-0.0017127312030541001] Loss: 22.842739503396157\n",
      "Iteracion: 15624 Gradiente: [9.922045721282302e-05,-0.0017118206625784892] Loss: 22.84273950045363\n",
      "Iteracion: 15625 Gradiente: [9.916770855321981e-05,-0.001710910606172078] Loss: 22.84273949751424\n",
      "Iteracion: 15626 Gradiente: [9.911498805195151e-05,-0.0017100010335719654] Loss: 22.84273949457797\n",
      "Iteracion: 15627 Gradiente: [9.9062295468381e-05,-0.0017090919445356197] Loss: 22.84273949164482\n",
      "Iteracion: 15628 Gradiente: [9.900963092851119e-05,-0.0017081833387961136] Loss: 22.8427394887148\n",
      "Iteracion: 15629 Gradiente: [9.895699436223519e-05,-0.0017072752161013227] Loss: 22.84273948578788\n",
      "Iteracion: 15630 Gradiente: [9.890438572881521e-05,-0.0017063675761950965] Loss: 22.842739482864076\n",
      "Iteracion: 15631 Gradiente: [9.885180513814854e-05,-0.0017054604188142975] Loss: 22.84273947994338\n",
      "Iteracion: 15632 Gradiente: [9.879925244623185e-05,-0.0017045537437097617] Loss: 22.842739477025795\n",
      "Iteracion: 15633 Gradiente: [9.874672769475032e-05,-0.0017036475506198912] Loss: 22.8427394741113\n",
      "Iteracion: 15634 Gradiente: [9.869423092917865e-05,-0.0017027418392880614] Loss: 22.842739471199895\n",
      "Iteracion: 15635 Gradiente: [9.86417620310931e-05,-0.001701836609461793] Loss: 22.842739468291608\n",
      "Iteracion: 15636 Gradiente: [9.85893210819692e-05,-0.001700931860880317] Loss: 22.842739465386412\n",
      "Iteracion: 15637 Gradiente: [9.853690793969842e-05,-0.00170002759329518] Loss: 22.842739462484285\n",
      "Iteracion: 15638 Gradiente: [9.8484522697125e-05,-0.001699123806445139] Loss: 22.842739459585243\n",
      "Iteracion: 15639 Gradiente: [9.843216529551077e-05,-0.0016982205000753462] Loss: 22.842739456689305\n",
      "Iteracion: 15640 Gradiente: [9.837983572822396e-05,-0.0016973176739320194] Loss: 22.842739453796423\n",
      "Iteracion: 15641 Gradiente: [9.832753397157983e-05,-0.001696415327759245] Loss: 22.842739450906645\n",
      "Iteracion: 15642 Gradiente: [9.827526012221219e-05,-0.0016955134612945955] Loss: 22.842739448019906\n",
      "Iteracion: 15643 Gradiente: [9.822301396601081e-05,-0.001694612074295776] Loss: 22.84273944513626\n",
      "Iteracion: 15644 Gradiente: [9.817079558918826e-05,-0.0016937111665022541] Loss: 22.842739442255656\n",
      "Iteracion: 15645 Gradiente: [9.811860504006139e-05,-0.0016928107376550372] Loss: 22.84273943937813\n",
      "Iteracion: 15646 Gradiente: [9.806644216799516e-05,-0.0016919107875068562] Loss: 22.842739436503663\n",
      "Iteracion: 15647 Gradiente: [9.801430705541255e-05,-0.001691011315798363] Loss: 22.84273943363224\n",
      "Iteracion: 15648 Gradiente: [9.796219958483714e-05,-0.0016901123222824073] Loss: 22.84273943076388\n",
      "Iteracion: 15649 Gradiente: [9.791011982448102e-05,-0.0016892138066964435] Loss: 22.842739427898568\n",
      "Iteracion: 15650 Gradiente: [9.785806785771456e-05,-0.001688315768783255] Loss: 22.842739425036292\n",
      "Iteracion: 15651 Gradiente: [9.780604347326971e-05,-0.0016874182083001918] Loss: 22.842739422177075\n",
      "Iteracion: 15652 Gradiente: [9.775404677441202e-05,-0.0016865211249854184] Loss: 22.842739419320885\n",
      "Iteracion: 15653 Gradiente: [9.770207771756153e-05,-0.0016856245185884688] Loss: 22.842739416467726\n",
      "Iteracion: 15654 Gradiente: [9.765013630745519e-05,-0.0016847283888542582] Loss: 22.842739413617622\n",
      "Iteracion: 15655 Gradiente: [9.759822254977735e-05,-0.0016838327355266358] Loss: 22.842739410770534\n",
      "Iteracion: 15656 Gradiente: [9.754633630620901e-05,-0.0016829375583616486] Loss: 22.842739407926455\n",
      "Iteracion: 15657 Gradiente: [9.749447765917315e-05,-0.0016820428570999487] Loss: 22.842739405085425\n",
      "Iteracion: 15658 Gradiente: [9.744264663993362e-05,-0.001681148631484793] Loss: 22.84273940224741\n",
      "Iteracion: 15659 Gradiente: [9.739084314427751e-05,-0.001680254881269505] Loss: 22.842739399412405\n",
      "Iteracion: 15660 Gradiente: [9.733906717125743e-05,-0.0016793616061998287] Loss: 22.84273939658042\n",
      "Iteracion: 15661 Gradiente: [9.728731872182076e-05,-0.00167846880602364] Loss: 22.842739393751447\n",
      "Iteracion: 15662 Gradiente: [9.723559781112574e-05,-0.0016775764804853803] Loss: 22.842739390925455\n",
      "Iteracion: 15663 Gradiente: [9.718390439559244e-05,-0.0016766846293342279] Loss: 22.84273938810251\n",
      "Iteracion: 15664 Gradiente: [9.713223842690392e-05,-0.0016757932523209008] Loss: 22.842739385282528\n",
      "Iteracion: 15665 Gradiente: [9.708059989748108e-05,-0.001674902349193393] Loss: 22.84273938246556\n",
      "Iteracion: 15666 Gradiente: [9.702898884048258e-05,-0.0016740119196965017] Loss: 22.842739379651597\n",
      "Iteracion: 15667 Gradiente: [9.697740525401362e-05,-0.001673121963576089] Loss: 22.842739376840605\n",
      "Iteracion: 15668 Gradiente: [9.692584908596777e-05,-0.0016722324805855968] Loss: 22.842739374032615\n",
      "Iteracion: 15669 Gradiente: [9.687432037613538e-05,-0.0016713434704662689] Loss: 22.84273937122761\n",
      "Iteracion: 15670 Gradiente: [9.682281906388349e-05,-0.001670454932973442] Loss: 22.84273936842557\n",
      "Iteracion: 15671 Gradiente: [9.677134506394699e-05,-0.001669566867857005] Loss: 22.842739365626517\n",
      "Iteracion: 15672 Gradiente: [9.671989839811582e-05,-0.0016686792748664913] Loss: 22.84273936283045\n",
      "Iteracion: 15673 Gradiente: [9.666847921702507e-05,-0.0016677921537400664] Loss: 22.84273936003735\n",
      "Iteracion: 15674 Gradiente: [9.661708722603634e-05,-0.0016669055042421851] Loss: 22.842739357247215\n",
      "Iteracion: 15675 Gradiente: [9.656572262978595e-05,-0.001666019326109828] Loss: 22.84273935446004\n",
      "Iteracion: 15676 Gradiente: [9.651438539700999e-05,-0.0016651336190940688] Loss: 22.842739351675828\n",
      "Iteracion: 15677 Gradiente: [9.64630753514939e-05,-0.0016642483829536778] Loss: 22.842739348894597\n",
      "Iteracion: 15678 Gradiente: [9.641179266376791e-05,-0.0016633636174267015] Loss: 22.842739346116307\n",
      "Iteracion: 15679 Gradiente: [9.636053710645834e-05,-0.0016624793222765296] Loss: 22.842739343340973\n",
      "Iteracion: 15680 Gradiente: [9.630930889178065e-05,-0.0016615954972388398] Loss: 22.842739340568585\n",
      "Iteracion: 15681 Gradiente: [9.62581079041532e-05,-0.0016607121420700347] Loss: 22.842739337799152\n",
      "Iteracion: 15682 Gradiente: [9.620693414073382e-05,-0.0016598292565205952] Loss: 22.842739335032643\n",
      "Iteracion: 15683 Gradiente: [9.615578759110121e-05,-0.0016589468403391077] Loss: 22.84273933226909\n",
      "Iteracion: 15684 Gradiente: [9.610466817188505e-05,-0.0016580648932797242] Loss: 22.842739329508483\n",
      "Iteracion: 15685 Gradiente: [9.605357599866693e-05,-0.0016571834150888995] Loss: 22.842739326750788\n",
      "Iteracion: 15686 Gradiente: [9.600251102028778e-05,-0.0016563024055158118] Loss: 22.842739323996053\n",
      "Iteracion: 15687 Gradiente: [9.595147308990211e-05,-0.0016554218643200613] Loss: 22.842739321244213\n",
      "Iteracion: 15688 Gradiente: [9.590046241025145e-05,-0.0016545417912406416] Loss: 22.84273931849533\n",
      "Iteracion: 15689 Gradiente: [9.584947872838256e-05,-0.0016536621860421263] Loss: 22.842739315749345\n",
      "Iteracion: 15690 Gradiente: [9.57985221892462e-05,-0.0016527830484659963] Loss: 22.842739313006284\n",
      "Iteracion: 15691 Gradiente: [9.57475927558941e-05,-0.0016519043782652195] Loss: 22.842739310266158\n",
      "Iteracion: 15692 Gradiente: [9.569669043495803e-05,-0.0016510261751907507] Loss: 22.84273930752892\n",
      "Iteracion: 15693 Gradiente: [9.564581514022544e-05,-0.0016501484389978079] Loss: 22.84273930479461\n",
      "Iteracion: 15694 Gradiente: [9.55949669465402e-05,-0.0016492711694343853] Loss: 22.8427393020632\n",
      "Iteracion: 15695 Gradiente: [9.554414567768769e-05,-0.0016483943662591353] Loss: 22.842739299334692\n",
      "Iteracion: 15696 Gradiente: [9.549335145872343e-05,-0.0016475180292176836] Loss: 22.84273929660909\n",
      "Iteracion: 15697 Gradiente: [9.544258427543658e-05,-0.001646642158062761] Loss: 22.842739293886364\n",
      "Iteracion: 15698 Gradiente: [9.539184405393068e-05,-0.0016457667525488753] Loss: 22.842739291166556\n",
      "Iteracion: 15699 Gradiente: [9.534113087189174e-05,-0.0016448918124231918] Loss: 22.842739288449632\n",
      "Iteracion: 15700 Gradiente: [9.529044463837029e-05,-0.001644017337444481] Loss: 22.842739285735586\n",
      "Iteracion: 15701 Gradiente: [9.523978529178597e-05,-0.0016431433273650005] Loss: 22.84273928302444\n",
      "Iteracion: 15702 Gradiente: [9.518915286529742e-05,-0.0016422697819383103] Loss: 22.84273928031617\n",
      "Iteracion: 15703 Gradiente: [9.513854733900947e-05,-0.00164139670091726] Loss: 22.84273927761077\n",
      "Iteracion: 15704 Gradiente: [9.508796882187198e-05,-0.0016405240840453436] Loss: 22.842739274908258\n",
      "Iteracion: 15705 Gradiente: [9.503741717177642e-05,-0.0016396519310852397] Loss: 22.84273927220861\n",
      "Iteracion: 15706 Gradiente: [9.498689226745682e-05,-0.001638780241794772] Loss: 22.842739269511842\n",
      "Iteracion: 15707 Gradiente: [9.493639430218082e-05,-0.0016379090159169608] Loss: 22.842739266817937\n",
      "Iteracion: 15708 Gradiente: [9.488592322289453e-05,-0.001637038253207379] Loss: 22.842739264126887\n",
      "Iteracion: 15709 Gradiente: [9.483547899833412e-05,-0.0016361679534220741] Loss: 22.842739261438705\n",
      "Iteracion: 15710 Gradiente: [9.478506154797136e-05,-0.0016352981163166192] Loss: 22.842739258753397\n",
      "Iteracion: 15711 Gradiente: [9.473467086138499e-05,-0.0016344287416461137] Loss: 22.84273925607091\n",
      "Iteracion: 15712 Gradiente: [9.468430703615619e-05,-0.0016335598291561837] Loss: 22.842739253391304\n",
      "Iteracion: 15713 Gradiente: [9.46339699434399e-05,-0.0016326913786091524] Loss: 22.842739250714512\n",
      "Iteracion: 15714 Gradiente: [9.458365962871085e-05,-0.001631823389756922] Loss: 22.842739248040598\n",
      "Iteracion: 15715 Gradiente: [9.453337605312603e-05,-0.0016309558623541184] Loss: 22.842739245369504\n",
      "Iteracion: 15716 Gradiente: [9.448311924226497e-05,-0.0016300887961537096] Loss: 22.842739242701274\n",
      "Iteracion: 15717 Gradiente: [9.443288904644002e-05,-0.001629222190918019] Loss: 22.842739240035865\n",
      "Iteracion: 15718 Gradiente: [9.43826856295497e-05,-0.0016283560463932645] Loss: 22.84273923737329\n",
      "Iteracion: 15719 Gradiente: [9.433250883906415e-05,-0.0016274903623403484] Loss: 22.842739234713534\n",
      "Iteracion: 15720 Gradiente: [9.42823587915124e-05,-0.0016266251385076203] Loss: 22.842739232056616\n",
      "Iteracion: 15721 Gradiente: [9.42322354632097e-05,-0.0016257603746514823] Loss: 22.842739229402543\n",
      "Iteracion: 15722 Gradiente: [9.418213865236188e-05,-0.001624896070537692] Loss: 22.842739226751256\n",
      "Iteracion: 15723 Gradiente: [9.41320685114988e-05,-0.0016240322259123492] Loss: 22.842739224102807\n",
      "Iteracion: 15724 Gradiente: [9.408202501504093e-05,-0.0016231688405312639] Loss: 22.84273922145717\n",
      "Iteracion: 15725 Gradiente: [9.40320081194083e-05,-0.0016223059141539172] Loss: 22.842739218814344\n",
      "Iteracion: 15726 Gradiente: [9.398201784733828e-05,-0.0016214434465315008] Loss: 22.842739216174337\n",
      "Iteracion: 15727 Gradiente: [9.393205414104007e-05,-0.0016205814374243251] Loss: 22.842739213537122\n",
      "Iteracion: 15728 Gradiente: [9.388211700240845e-05,-0.0016197198865866606] Loss: 22.842739210902717\n",
      "Iteracion: 15729 Gradiente: [9.383220642765385e-05,-0.00161885879377562] Loss: 22.842739208271105\n",
      "Iteracion: 15730 Gradiente: [9.378232235898546e-05,-0.0016179981587491454] Loss: 22.842739205642296\n",
      "Iteracion: 15731 Gradiente: [9.373246476892897e-05,-0.001617137981265415] Loss: 22.84273920301627\n",
      "Iteracion: 15732 Gradiente: [9.368263373990734e-05,-0.0016162782610758578] Loss: 22.842739200393055\n",
      "Iteracion: 15733 Gradiente: [9.363282921034018e-05,-0.0016154189979391256] Loss: 22.84273919777262\n",
      "Iteracion: 15734 Gradiente: [9.358305109117283e-05,-0.001614560191616358] Loss: 22.842739195154966\n",
      "Iteracion: 15735 Gradiente: [9.353329946672299e-05,-0.0016137018418594569] Loss: 22.842739192540105\n",
      "Iteracion: 15736 Gradiente: [9.348357425930469e-05,-0.0016128439484300353] Loss: 22.842739189928015\n",
      "Iteracion: 15737 Gradiente: [9.343387555418303e-05,-0.001611986511079048] Loss: 22.842739187318703\n",
      "Iteracion: 15738 Gradiente: [9.33842032168286e-05,-0.0016111295295718974] Loss: 22.842739184712176\n",
      "Iteracion: 15739 Gradiente: [9.333455732398003e-05,-0.0016102730036593016] Loss: 22.84273918210841\n",
      "Iteracion: 15740 Gradiente: [9.328493771268617e-05,-0.0016094169331092683] Loss: 22.842739179507394\n",
      "Iteracion: 15741 Gradiente: [9.32353446273737e-05,-0.0016085613176644623] Loss: 22.84273917690917\n",
      "Iteracion: 15742 Gradiente: [9.31857778975124e-05,-0.001607706157090405] Loss: 22.842739174313692\n",
      "Iteracion: 15743 Gradiente: [9.313623736867764e-05,-0.0016068514511560514] Loss: 22.842739171720986\n",
      "Iteracion: 15744 Gradiente: [9.308672339898294e-05,-0.001605997199595303] Loss: 22.84273916913103\n",
      "Iteracion: 15745 Gradiente: [9.303723560094568e-05,-0.0016051434021900235] Loss: 22.842739166543815\n",
      "Iteracion: 15746 Gradiente: [9.298777410435833e-05,-0.001604290058691286] Loss: 22.84273916395937\n",
      "Iteracion: 15747 Gradiente: [9.293833896322213e-05,-0.0016034371688520585] Loss: 22.842739161377665\n",
      "Iteracion: 15748 Gradiente: [9.288893010079846e-05,-0.0016025847324338354] Loss: 22.842739158798693\n",
      "Iteracion: 15749 Gradiente: [9.283954748961302e-05,-0.0016017327492002427] Loss: 22.842739156222486\n",
      "Iteracion: 15750 Gradiente: [9.279019123956308e-05,-0.001600881218899867] Loss: 22.842739153648992\n",
      "Iteracion: 15751 Gradiente: [9.274086109390585e-05,-0.0016000301413071109] Loss: 22.842739151078245\n",
      "Iteracion: 15752 Gradiente: [9.269155723264551e-05,-0.0015991795161696132] Loss: 22.84273914851023\n",
      "Iteracion: 15753 Gradiente: [9.264227949946265e-05,-0.0015983293432552633] Loss: 22.842739145944947\n",
      "Iteracion: 15754 Gradiente: [9.259302804025538e-05,-0.001597479622314779] Loss: 22.842739143382385\n",
      "Iteracion: 15755 Gradiente: [9.254380272049427e-05,-0.0015966303531155764] Loss: 22.84273914082256\n",
      "Iteracion: 15756 Gradiente: [9.249460361786533e-05,-0.001595781535409439] Loss: 22.842739138265436\n",
      "Iteracion: 15757 Gradiente: [9.244543064331386e-05,-0.0015949331689635452] Loss: 22.842739135711046\n",
      "Iteracion: 15758 Gradiente: [9.23962838233668e-05,-0.0015940852535346532] Loss: 22.84273913315936\n",
      "Iteracion: 15759 Gradiente: [9.23471631343394e-05,-0.001593237788883665] Loss: 22.842739130610408\n",
      "Iteracion: 15760 Gradiente: [9.229806858002121e-05,-0.0015923907747687593] Loss: 22.842739128064142\n",
      "Iteracion: 15761 Gradiente: [9.224900007609449e-05,-0.0015915442109564045] Loss: 22.8427391255206\n",
      "Iteracion: 15762 Gradiente: [9.219995766898136e-05,-0.0015906980972027658] Loss: 22.84273912297974\n",
      "Iteracion: 15763 Gradiente: [9.215094134162882e-05,-0.0015898524332677985] Loss: 22.842739120441593\n",
      "Iteracion: 15764 Gradiente: [9.210195104382516e-05,-0.001589007218915602] Loss: 22.842739117906145\n",
      "Iteracion: 15765 Gradiente: [9.205298686462507e-05,-0.0015881624539016315] Loss: 22.842739115373394\n",
      "Iteracion: 15766 Gradiente: [9.200404863823527e-05,-0.0015873181379950788] Loss: 22.842739112843336\n",
      "Iteracion: 15767 Gradiente: [9.19551365039221e-05,-0.0015864742709489122] Loss: 22.84273911031596\n",
      "Iteracion: 15768 Gradiente: [9.19062503328405e-05,-0.0015856308525287707] Loss: 22.842739107791278\n",
      "Iteracion: 15769 Gradiente: [9.185739012688525e-05,-0.0015847878824983988] Loss: 22.842739105269278\n",
      "Iteracion: 15770 Gradiente: [9.180855589268807e-05,-0.0015839453606159755] Loss: 22.842739102749963\n",
      "Iteracion: 15771 Gradiente: [9.175974764161766e-05,-0.001583103286644061] Loss: 22.84273910023332\n",
      "Iteracion: 15772 Gradiente: [9.171096535756837e-05,-0.0015822616603435571] Loss: 22.842739097719353\n",
      "Iteracion: 15773 Gradiente: [9.166220896569635e-05,-0.001581420481479867] Loss: 22.842739095208046\n",
      "Iteracion: 15774 Gradiente: [9.161347854747722e-05,-0.0015805797498088007] Loss: 22.842739092699425\n",
      "Iteracion: 15775 Gradiente: [9.156477400627713e-05,-0.001579739465097892] Loss: 22.842739090193483\n",
      "Iteracion: 15776 Gradiente: [9.151609533451696e-05,-0.001578899627110412] Loss: 22.842739087690177\n",
      "Iteracion: 15777 Gradiente: [9.146744258904012e-05,-0.0015780602356037102] Loss: 22.842739085189535\n",
      "Iteracion: 15778 Gradiente: [9.141881571016105e-05,-0.0015772212903427155] Loss: 22.84273908269157\n",
      "Iteracion: 15779 Gradiente: [9.137021468461626e-05,-0.0015763827910905803] Loss: 22.84273908019624\n",
      "Iteracion: 15780 Gradiente: [9.132163944419366e-05,-0.0015755447376136545] Loss: 22.842739077703573\n",
      "Iteracion: 15781 Gradiente: [9.127309007889532e-05,-0.0015747071296682217] Loss: 22.842739075213554\n",
      "Iteracion: 15782 Gradiente: [9.122456651671958e-05,-0.001573869967022882] Loss: 22.84273907272618\n",
      "Iteracion: 15783 Gradiente: [9.117606871124432e-05,-0.001573033249440551] Loss: 22.84273907024145\n",
      "Iteracion: 15784 Gradiente: [9.112759670794427e-05,-0.0015721969766808285] Loss: 22.84273906775936\n",
      "Iteracion: 15785 Gradiente: [9.107915044050211e-05,-0.001571361148511959] Loss: 22.842739065279908\n",
      "Iteracion: 15786 Gradiente: [9.103072995912953e-05,-0.0015705257646935421] Loss: 22.842739062803112\n",
      "Iteracion: 15787 Gradiente: [9.098233520508833e-05,-0.0015696908249915728] Loss: 22.842739060328924\n",
      "Iteracion: 15788 Gradiente: [9.093396620869498e-05,-0.0015688563291671898] Loss: 22.84273905785738\n",
      "Iteracion: 15789 Gradiente: [9.08856229519491e-05,-0.0015680222769854405] Loss: 22.842739055388453\n",
      "Iteracion: 15790 Gradiente: [9.083730535053292e-05,-0.0015671886682135038] Loss: 22.842739052922163\n",
      "Iteracion: 15791 Gradiente: [9.078901340728863e-05,-0.001566355502615835] Loss: 22.842739050458484\n",
      "Iteracion: 15792 Gradiente: [9.074074720748134e-05,-0.0015655227799501385] Loss: 22.842739047997426\n",
      "Iteracion: 15793 Gradiente: [9.069250666395116e-05,-0.001564690499985962] Loss: 22.84273904553898\n",
      "Iteracion: 15794 Gradiente: [9.064429175964506e-05,-0.0015638586624874052] Loss: 22.842739043083146\n",
      "Iteracion: 15795 Gradiente: [9.059610243961439e-05,-0.0015630272672220022] Loss: 22.84273904062993\n",
      "Iteracion: 15796 Gradiente: [9.054793875217608e-05,-0.0015621963139513658] Loss: 22.842739038179317\n",
      "Iteracion: 15797 Gradiente: [9.04998007551209e-05,-0.00156136580243628] Loss: 22.842739035731324\n",
      "Iteracion: 15798 Gradiente: [9.045168825897084e-05,-0.0015605357324507926] Loss: 22.842739033285913\n",
      "Iteracion: 15799 Gradiente: [9.04036013376223e-05,-0.0015597061037569897] Loss: 22.842739030843102\n",
      "Iteracion: 15800 Gradiente: [9.035554001854962e-05,-0.0015588769161168396] Loss: 22.84273902840291\n",
      "Iteracion: 15801 Gradiente: [9.03075042515411e-05,-0.0015580481692987055] Loss: 22.84273902596529\n",
      "Iteracion: 15802 Gradiente: [9.025949406691325e-05,-0.0015572198630645555] Loss: 22.842739023530264\n",
      "Iteracion: 15803 Gradiente: [9.02115092846619e-05,-0.0015563919971911607] Loss: 22.84273902109782\n",
      "Iteracion: 15804 Gradiente: [9.0163550026053e-05,-0.0015555645714362262] Loss: 22.842739018667977\n",
      "Iteracion: 15805 Gradiente: [9.011561639435209e-05,-0.0015547375855579303] Loss: 22.842739016240714\n",
      "Iteracion: 15806 Gradiente: [9.006770817071204e-05,-0.0015539110393341105] Loss: 22.842739013816026\n",
      "Iteracion: 15807 Gradiente: [9.00198254072393e-05,-0.001553084932529695] Loss: 22.842739011393927\n",
      "Iteracion: 15808 Gradiente: [8.997196808403866e-05,-0.001552259264909613] Loss: 22.8427390089744\n",
      "Iteracion: 15809 Gradiente: [8.992413624563748e-05,-0.0015514340362367798] Loss: 22.842739006557437\n",
      "Iteracion: 15810 Gradiente: [8.987632982287626e-05,-0.001550609246282401] Loss: 22.84273900414304\n",
      "Iteracion: 15811 Gradiente: [8.982854881954457e-05,-0.0015497848948106943] Loss: 22.84273900173122\n",
      "Iteracion: 15812 Gradiente: [8.978079316743029e-05,-0.001548960981592747] Loss: 22.84273899932195\n",
      "Iteracion: 15813 Gradiente: [8.973306291485036e-05,-0.0015481375063923035] Loss: 22.84273899691526\n",
      "Iteracion: 15814 Gradiente: [8.968535803527781e-05,-0.0015473144689752397] Loss: 22.8427389945111\n",
      "Iteracion: 15815 Gradiente: [8.963767858650347e-05,-0.0015464918691081428] Loss: 22.842738992109535\n",
      "Iteracion: 15816 Gradiente: [8.959002445484051e-05,-0.0015456697065597306] Loss: 22.84273898971049\n",
      "Iteracion: 15817 Gradiente: [8.954239560618285e-05,-0.001544847981102393] Loss: 22.84273898731401\n",
      "Iteracion: 15818 Gradiente: [8.949479219779732e-05,-0.0015440266924932426] Loss: 22.84273898492007\n",
      "Iteracion: 15819 Gradiente: [8.944721401083673e-05,-0.0015432058405095244] Loss: 22.842738982528672\n",
      "Iteracion: 15820 Gradiente: [8.939966110972364e-05,-0.001542385424917588] Loss: 22.842738980139835\n",
      "Iteracion: 15821 Gradiente: [8.935213351151106e-05,-0.0015415654454820071] Loss: 22.842738977753516\n",
      "Iteracion: 15822 Gradiente: [8.930463119819858e-05,-0.001540745901971737] Loss: 22.84273897536975\n",
      "Iteracion: 15823 Gradiente: [8.925715409683714e-05,-0.0015399267941582195] Loss: 22.8427389729885\n",
      "Iteracion: 15824 Gradiente: [8.920970225290148e-05,-0.0015391081218068573] Loss: 22.842738970609794\n",
      "Iteracion: 15825 Gradiente: [8.916227559533733e-05,-0.0015382898846899213] Loss: 22.842738968233608\n",
      "Iteracion: 15826 Gradiente: [8.911487422267328e-05,-0.0015374720825671298] Loss: 22.84273896585996\n",
      "Iteracion: 15827 Gradiente: [8.906749804585465e-05,-0.0015366547152146617] Loss: 22.84273896348881\n",
      "Iteracion: 15828 Gradiente: [8.902014701088017e-05,-0.0015358377824023013] Loss: 22.84273896112022\n",
      "Iteracion: 15829 Gradiente: [8.897282112248679e-05,-0.0015350212838975827] Loss: 22.842738958754115\n",
      "Iteracion: 15830 Gradiente: [8.892552047541357e-05,-0.00153420521946425] Loss: 22.842738956390537\n",
      "Iteracion: 15831 Gradiente: [8.887824491239371e-05,-0.0015333895888790749] Loss: 22.842738954029482\n",
      "Iteracion: 15832 Gradiente: [8.883099453858753e-05,-0.0015325743919046176] Loss: 22.84273895167092\n",
      "Iteracion: 15833 Gradiente: [8.878376925546642e-05,-0.001531759628315399] Loss: 22.842738949314874\n",
      "Iteracion: 15834 Gradiente: [8.873656906208302e-05,-0.0015309452978814401] Loss: 22.842738946961333\n",
      "Iteracion: 15835 Gradiente: [8.86893940361233e-05,-0.0015301314003656568] Loss: 22.84273894461029\n",
      "Iteracion: 15836 Gradiente: [8.864224402411006e-05,-0.001529317935546004] Loss: 22.84273894226175\n",
      "Iteracion: 15837 Gradiente: [8.859511908478149e-05,-0.0015285049031893057] Loss: 22.842738939915698\n",
      "Iteracion: 15838 Gradiente: [8.854801923424323e-05,-0.0015276923030639248] Loss: 22.842738937572143\n",
      "Iteracion: 15839 Gradiente: [8.850094440617795e-05,-0.001526880134942014] Loss: 22.84273893523108\n",
      "Iteracion: 15840 Gradiente: [8.845389458353262e-05,-0.0015260683985947783] Loss: 22.842738932892505\n",
      "Iteracion: 15841 Gradiente: [8.840686974546468e-05,-0.0015252570937943706] Loss: 22.842738930556422\n",
      "Iteracion: 15842 Gradiente: [8.83598698512363e-05,-0.0015244462203104566] Loss: 22.84273892822281\n",
      "Iteracion: 15843 Gradiente: [8.831289506758822e-05,-0.0015236357779048859] Loss: 22.842738925891684\n",
      "Iteracion: 15844 Gradiente: [8.82659452685175e-05,-0.0015228257663554956] Loss: 22.842738923563047\n",
      "Iteracion: 15845 Gradiente: [8.821902036686425e-05,-0.001522016185436925] Loss: 22.842738921236883\n",
      "Iteracion: 15846 Gradiente: [8.817212045263053e-05,-0.0015212070349128008] Loss: 22.842738918913174\n",
      "Iteracion: 15847 Gradiente: [8.812524537233912e-05,-0.0015203983145643937] Loss: 22.84273891659195\n",
      "Iteracion: 15848 Gradiente: [8.807839532778416e-05,-0.0015195900241503561] Loss: 22.84273891427318\n",
      "Iteracion: 15849 Gradiente: [8.803157013517193e-05,-0.001518782163451012] Loss: 22.842738911956886\n",
      "Iteracion: 15850 Gradiente: [8.798476991292622e-05,-0.001517974732230698] Loss: 22.842738909643064\n",
      "Iteracion: 15851 Gradiente: [8.793799448483241e-05,-0.0015171677302702117] Loss: 22.842738907331682\n",
      "Iteracion: 15852 Gradiente: [8.789124394468218e-05,-0.0015163611573356661] Loss: 22.842738905022774\n",
      "Iteracion: 15853 Gradiente: [8.784451827921202e-05,-0.0015155550131992139] Loss: 22.842738902716302\n",
      "Iteracion: 15854 Gradiente: [8.779781741547291e-05,-0.001514749297635139] Loss: 22.842738900412296\n",
      "Iteracion: 15855 Gradiente: [8.775114139893958e-05,-0.0015139440104140552] Loss: 22.842738898110728\n",
      "Iteracion: 15856 Gradiente: [8.770449015192602e-05,-0.0015131391513116673] Loss: 22.842738895811614\n",
      "Iteracion: 15857 Gradiente: [8.765786375874995e-05,-0.0015123347200918383] Loss: 22.842738893514944\n",
      "Iteracion: 15858 Gradiente: [8.761126210098761e-05,-0.001511530716537024] Loss: 22.842738891220723\n",
      "Iteracion: 15859 Gradiente: [8.75646853245371e-05,-0.0015107271404091922] Loss: 22.842738888928917\n",
      "Iteracion: 15860 Gradiente: [8.751813324465729e-05,-0.0015099239914910356] Loss: 22.84273888663957\n",
      "Iteracion: 15861 Gradiente: [8.747160597692983e-05,-0.0015091212695466539] Loss: 22.842738884352634\n",
      "Iteracion: 15862 Gradiente: [8.742510334892964e-05,-0.0015083189743596867] Loss: 22.842738882068154\n",
      "Iteracion: 15863 Gradiente: [8.737862543739539e-05,-0.0015075171056980234] Loss: 22.842738879786094\n",
      "Iteracion: 15864 Gradiente: [8.73321722660118e-05,-0.0015067156633329878] Loss: 22.84273887750646\n",
      "Iteracion: 15865 Gradiente: [8.728574384046321e-05,-0.001505914647036377] Loss: 22.84273887522924\n",
      "Iteracion: 15866 Gradiente: [8.723934012664359e-05,-0.0015051140565834232] Loss: 22.842738872954456\n",
      "Iteracion: 15867 Gradiente: [8.719296091991661e-05,-0.0015043138917570549] Loss: 22.842738870682073\n",
      "Iteracion: 15868 Gradiente: [8.714660650449938e-05,-0.001503514152315925] Loss: 22.842738868412116\n",
      "Iteracion: 15869 Gradiente: [8.710027661985956e-05,-0.0015027148380466334] Loss: 22.842738866144565\n",
      "Iteracion: 15870 Gradiente: [8.705397151989777e-05,-0.0015019159487088985] Loss: 22.84273886387943\n",
      "Iteracion: 15871 Gradiente: [8.700769090429125e-05,-0.0015011174840932283] Loss: 22.842738861616713\n",
      "Iteracion: 15872 Gradiente: [8.696143503262495e-05,-0.0015003194439564993] Loss: 22.84273885935638\n",
      "Iteracion: 15873 Gradiente: [8.691520363489266e-05,-0.0014995218280895747] Loss: 22.842738857098464\n",
      "Iteracion: 15874 Gradiente: [8.686899681435989e-05,-0.0014987246362608176] Loss: 22.842738854842928\n",
      "Iteracion: 15875 Gradiente: [8.682281453123627e-05,-0.001497927868244157] Loss: 22.84273885258982\n",
      "Iteracion: 15876 Gradiente: [8.677665693615685e-05,-0.001497131523806061] Loss: 22.842738850339092\n",
      "Iteracion: 15877 Gradiente: [8.67305238169062e-05,-0.0014963356027327753] Loss: 22.842738848090768\n",
      "Iteracion: 15878 Gradiente: [8.668441515074695e-05,-0.0014955401047994125] Loss: 22.842738845844814\n",
      "Iteracion: 15879 Gradiente: [8.663833101536511e-05,-0.00149474502977777] Loss: 22.84273884360125\n",
      "Iteracion: 15880 Gradiente: [8.659227152350014e-05,-0.0014939503774325403] Loss: 22.842738841360067\n",
      "Iteracion: 15881 Gradiente: [8.65462363729345e-05,-0.0014931561475588495] Loss: 22.84273883912128\n",
      "Iteracion: 15882 Gradiente: [8.6500225720935e-05,-0.0014923623399194952] Loss: 22.842738836884873\n",
      "Iteracion: 15883 Gradiente: [8.645423961392376e-05,-0.0014915689542873403] Loss: 22.842738834650838\n",
      "Iteracion: 15884 Gradiente: [8.64082778851601e-05,-0.0014907759904481566] Loss: 22.84273883241919\n",
      "Iteracion: 15885 Gradiente: [8.63623405450653e-05,-0.0014899834481752806] Loss: 22.84273883018989\n",
      "Iteracion: 15886 Gradiente: [8.631642763911411e-05,-0.0014891913272418122] Loss: 22.842738827962982\n",
      "Iteracion: 15887 Gradiente: [8.627053921278124e-05,-0.0014883996274202597] Loss: 22.842738825738433\n",
      "Iteracion: 15888 Gradiente: [8.622467517606461e-05,-0.0014876083484903547] Loss: 22.84273882351624\n",
      "Iteracion: 15889 Gradiente: [8.61788354645417e-05,-0.0014868174902320655] Loss: 22.842738821296425\n",
      "Iteracion: 15890 Gradiente: [8.613302018147805e-05,-0.0014860270524145846] Loss: 22.842738819078953\n",
      "Iteracion: 15891 Gradiente: [8.608722917150166e-05,-0.001485237034824157] Loss: 22.842738816863857\n",
      "Iteracion: 15892 Gradiente: [8.604146262598533e-05,-0.001484447437222395] Loss: 22.84273881465109\n",
      "Iteracion: 15893 Gradiente: [8.599572033176627e-05,-0.0014836582594008737] Loss: 22.84273881244071\n",
      "Iteracion: 15894 Gradiente: [8.595000237221484e-05,-0.0014828695011296134] Loss: 22.842738810232643\n",
      "Iteracion: 15895 Gradiente: [8.590430868196108e-05,-0.0014820811621876354] Loss: 22.842738808026944\n",
      "Iteracion: 15896 Gradiente: [8.585863925437328e-05,-0.001481293242353369] Loss: 22.84273880582359\n",
      "Iteracion: 15897 Gradiente: [8.58129941785061e-05,-0.0014805057413969536] Loss: 22.84273880362257\n",
      "Iteracion: 15898 Gradiente: [8.576737336625228e-05,-0.0014797186591003707] Loss: 22.842738801423895\n",
      "Iteracion: 15899 Gradiente: [8.572177675413665e-05,-0.0014789319952445367] Loss: 22.842738799227558\n",
      "Iteracion: 15900 Gradiente: [8.567620445205648e-05,-0.0014781457495988803] Loss: 22.84273879703355\n",
      "Iteracion: 15901 Gradiente: [8.563065640695791e-05,-0.0014773599219436069] Loss: 22.842738794841885\n",
      "Iteracion: 15902 Gradiente: [8.55851325477867e-05,-0.0014765745120608167] Loss: 22.842738792652536\n",
      "Iteracion: 15903 Gradiente: [8.553963285085804e-05,-0.001475789519727755] Loss: 22.842738790465532\n",
      "Iteracion: 15904 Gradiente: [8.549415732090892e-05,-0.0014750049447226133] Loss: 22.84273878828083\n",
      "Iteracion: 15905 Gradiente: [8.544870599204538e-05,-0.0014742207868186105] Loss: 22.84273878609846\n",
      "Iteracion: 15906 Gradiente: [8.540327888037305e-05,-0.0014734370457958335] Loss: 22.84273878391841\n",
      "Iteracion: 15907 Gradiente: [8.535787588073162e-05,-0.0014726537214360272] Loss: 22.842738781740692\n",
      "Iteracion: 15908 Gradiente: [8.531249696564676e-05,-0.0014718708135165552] Loss: 22.842738779565266\n",
      "Iteracion: 15909 Gradiente: [8.526714221754142e-05,-0.0014710883218143067] Loss: 22.842738777392167\n",
      "Iteracion: 15910 Gradiente: [8.522181158241438e-05,-0.0014703062461078294] Loss: 22.842738775221374\n",
      "Iteracion: 15911 Gradiente: [8.517650506216039e-05,-0.001469524586175434] Loss: 22.84273877305288\n",
      "Iteracion: 15912 Gradiente: [8.513122264635816e-05,-0.001468743341797681] Loss: 22.8427387708867\n",
      "Iteracion: 15913 Gradiente: [8.508596427153255e-05,-0.0014679625127551313] Loss: 22.842738768722825\n",
      "Iteracion: 15914 Gradiente: [8.504072994999964e-05,-0.001467182098824793] Loss: 22.842738766561244\n",
      "Iteracion: 15915 Gradiente: [8.499551963533728e-05,-0.0014664020997897135] Loss: 22.84273876440196\n",
      "Iteracion: 15916 Gradiente: [8.495033342417932e-05,-0.001465622515421572] Loss: 22.842738762244974\n",
      "Iteracion: 15917 Gradiente: [8.490517125778752e-05,-0.0014648433455031788] Loss: 22.842738760090285\n",
      "Iteracion: 15918 Gradiente: [8.486003299973769e-05,-0.0014640645898211346] Loss: 22.842738757937873\n",
      "Iteracion: 15919 Gradiente: [8.481491885750833e-05,-0.0014632862481424999] Loss: 22.842738755787764\n",
      "Iteracion: 15920 Gradiente: [8.476982863877916e-05,-0.0014625083202590853] Loss: 22.842738753639928\n",
      "Iteracion: 15921 Gradiente: [8.472476238902497e-05,-0.0014617308059447017] Loss: 22.842738751494398\n",
      "Iteracion: 15922 Gradiente: [8.467972012340396e-05,-0.0014609537049788438] Loss: 22.84273874935112\n",
      "Iteracion: 15923 Gradiente: [8.4634701800231e-05,-0.00146017701714527] Loss: 22.842738747210138\n",
      "Iteracion: 15924 Gradiente: [8.458970743371688e-05,-0.0014594007422213432] Loss: 22.842738745071426\n",
      "Iteracion: 15925 Gradiente: [8.454473692059613e-05,-0.0014586248799912957] Loss: 22.842738742934998\n",
      "Iteracion: 15926 Gradiente: [8.449979031108038e-05,-0.001457849430235214] Loss: 22.842738740800808\n",
      "Iteracion: 15927 Gradiente: [8.445486765917091e-05,-0.0014570743927283304] Loss: 22.84273873866892\n",
      "Iteracion: 15928 Gradiente: [8.440996885402303e-05,-0.0014562997672564156] Loss: 22.842738736539296\n",
      "Iteracion: 15929 Gradiente: [8.436509391079502e-05,-0.001455525553600386] Loss: 22.842738734411924\n",
      "Iteracion: 15930 Gradiente: [8.432024290148851e-05,-0.0014547517515344074] Loss: 22.842738732286815\n",
      "Iteracion: 15931 Gradiente: [8.427541560346678e-05,-0.0014539783608540802] Loss: 22.842738730163962\n",
      "Iteracion: 15932 Gradiente: [8.423061228389391e-05,-0.001453205381323149] Loss: 22.84273872804338\n",
      "Iteracion: 15933 Gradiente: [8.418583271539622e-05,-0.0014524328127356748] Loss: 22.84273872592504\n",
      "Iteracion: 15934 Gradiente: [8.41410768600781e-05,-0.0014516606548752975] Loss: 22.84273872380896\n",
      "Iteracion: 15935 Gradiente: [8.409634492068108e-05,-0.0014508889075113274] Loss: 22.842738721695117\n",
      "Iteracion: 15936 Gradiente: [8.40516366774106e-05,-0.0014501175704351018] Loss: 22.84273871958353\n",
      "Iteracion: 15937 Gradiente: [8.40069522145844e-05,-0.0014493466434252866] Loss: 22.842738717474184\n",
      "Iteracion: 15938 Gradiente: [8.396229153030769e-05,-0.00144857612626339] Loss: 22.842738715367087\n",
      "Iteracion: 15939 Gradiente: [8.391765459805355e-05,-0.0014478060187310385] Loss: 22.842738713262218\n",
      "Iteracion: 15940 Gradiente: [8.387304143298024e-05,-0.0014470363206089112] Loss: 22.842738711159587\n",
      "Iteracion: 15941 Gradiente: [8.382845194698045e-05,-0.001446267031684556] Loss: 22.842738709059198\n",
      "Iteracion: 15942 Gradiente: [8.378388620068714e-05,-0.0014454981517364018] Loss: 22.842738706961043\n",
      "Iteracion: 15943 Gradiente: [8.373934406715003e-05,-0.0014447296805524701] Loss: 22.842738704865113\n",
      "Iteracion: 15944 Gradiente: [8.369482565531902e-05,-0.0014439616179083477] Loss: 22.842738702771417\n",
      "Iteracion: 15945 Gradiente: [8.365033088466589e-05,-0.00144319396359253] Loss: 22.842738700679945\n",
      "Iteracion: 15946 Gradiente: [8.360585983287668e-05,-0.0014424267173801298] Loss: 22.84273869859069\n",
      "Iteracion: 15947 Gradiente: [8.356141244121318e-05,-0.001441659879060353] Loss: 22.842738696503663\n",
      "Iteracion: 15948 Gradiente: [8.351698860830463e-05,-0.0014408934484183788] Loss: 22.842738694418856\n",
      "Iteracion: 15949 Gradiente: [8.347258842794266e-05,-0.001440127425231689] Loss: 22.842738692336248\n",
      "Iteracion: 15950 Gradiente: [8.34282118309678e-05,-0.0014393618092887787] Loss: 22.842738690255878\n",
      "Iteracion: 15951 Gradiente: [8.338385880127438e-05,-0.0014385966003729322] Loss: 22.842738688177715\n",
      "Iteracion: 15952 Gradiente: [8.333952942223277e-05,-0.0014378317982597366] Loss: 22.842738686101747\n",
      "Iteracion: 15953 Gradiente: [8.329522354131314e-05,-0.0014370674027434896] Loss: 22.842738684027985\n",
      "Iteracion: 15954 Gradiente: [8.325094120304281e-05,-0.0014363034136051075] Loss: 22.84273868195644\n",
      "Iteracion: 15955 Gradiente: [8.320668248700258e-05,-0.0014355398306206506] Loss: 22.8427386798871\n",
      "Iteracion: 15956 Gradiente: [8.316244732308557e-05,-0.0014347766535792062] Loss: 22.842738677819945\n",
      "Iteracion: 15957 Gradiente: [8.311823551991893e-05,-0.0014340138822748353] Loss: 22.84273867575499\n",
      "Iteracion: 15958 Gradiente: [8.307404731908719e-05,-0.0014332515164794538] Loss: 22.84273867369223\n",
      "Iteracion: 15959 Gradiente: [8.302988256142877e-05,-0.0014324895559833333] Loss: 22.84273867163167\n",
      "Iteracion: 15960 Gradiente: [8.298574139663136e-05,-0.001431728000561113] Loss: 22.842738669573297\n",
      "Iteracion: 15961 Gradiente: [8.294162361532169e-05,-0.001430966850009933] Loss: 22.842738667517107\n",
      "Iteracion: 15962 Gradiente: [8.289752936434525e-05,-0.0014302061041056172] Loss: 22.842738665463113\n",
      "Iteracion: 15963 Gradiente: [8.285345856506865e-05,-0.0014294457626366607] Loss: 22.8427386634113\n",
      "Iteracion: 15964 Gradiente: [8.28094110706464e-05,-0.0014286858253957035] Loss: 22.84273866136166\n",
      "Iteracion: 15965 Gradiente: [8.276538700044967e-05,-0.0014279262921604642] Loss: 22.8427386593142\n",
      "Iteracion: 15966 Gradiente: [8.272138641511144e-05,-0.0014271671627119767] Loss: 22.842738657268924\n",
      "Iteracion: 15967 Gradiente: [8.267740917631273e-05,-0.0014264084368414605] Loss: 22.842738655225823\n",
      "Iteracion: 15968 Gradiente: [8.263345532194914e-05,-0.0014256501143336208] Loss: 22.842738653184885\n",
      "Iteracion: 15969 Gradiente: [8.258952476675555e-05,-0.0014248921949770712] Loss: 22.84273865114612\n",
      "Iteracion: 15970 Gradiente: [8.254561769168352e-05,-0.001424134678546925] Loss: 22.842738649109535\n",
      "Iteracion: 15971 Gradiente: [8.250173392620279e-05,-0.0014233775648373618] Loss: 22.842738647075098\n",
      "Iteracion: 15972 Gradiente: [8.245787346841857e-05,-0.0014226208536327314] Loss: 22.842738645042825\n",
      "Iteracion: 15973 Gradiente: [8.241403636854254e-05,-0.001421864544719161] Loss: 22.842738643012712\n",
      "Iteracion: 15974 Gradiente: [8.237022253752003e-05,-0.0014211086378830137] Loss: 22.842738640984756\n",
      "Iteracion: 15975 Gradiente: [8.232643193461324e-05,-0.0014203531329134952] Loss: 22.84273863895896\n",
      "Iteracion: 15976 Gradiente: [8.228266473035244e-05,-0.0014195980295884426] Loss: 22.84273863693532\n",
      "Iteracion: 15977 Gradiente: [8.22389207797869e-05,-0.0014188433276989562] Loss: 22.84273863491382\n",
      "Iteracion: 15978 Gradiente: [8.219519999859888e-05,-0.001418089027037913] Loss: 22.8427386328945\n",
      "Iteracion: 15979 Gradiente: [8.215150252037044e-05,-0.0014173351273823207] Loss: 22.8427386308773\n",
      "Iteracion: 15980 Gradiente: [8.210782823709905e-05,-0.0014165816285235167] Loss: 22.84273862886224\n",
      "Iteracion: 15981 Gradiente: [8.206417720468077e-05,-0.001415828530248812] Loss: 22.842738626849332\n",
      "Iteracion: 15982 Gradiente: [8.202054932458699e-05,-0.001415075832346228] Loss: 22.842738624838553\n",
      "Iteracion: 15983 Gradiente: [8.197694473039973e-05,-0.0014143235345945488] Loss: 22.842738622829923\n",
      "Iteracion: 15984 Gradiente: [8.19333632297988e-05,-0.0014135716367938756] Loss: 22.84273862082343\n",
      "Iteracion: 15985 Gradiente: [8.188980494973445e-05,-0.0014128201387225185] Loss: 22.842738618819073\n",
      "Iteracion: 15986 Gradiente: [8.184626983620547e-05,-0.0014120690401696833] Loss: 22.842738616816835\n",
      "Iteracion: 15987 Gradiente: [8.180275785889535e-05,-0.0014113183409238653] Loss: 22.84273861481673\n",
      "Iteracion: 15988 Gradiente: [8.175926894485505e-05,-0.0014105680407774676] Loss: 22.842738612818735\n",
      "Iteracion: 15989 Gradiente: [8.171580324661439e-05,-0.0014098181395085637] Loss: 22.842738610822902\n",
      "Iteracion: 15990 Gradiente: [8.167236060406443e-05,-0.0014090686369113332] Loss: 22.842738608829155\n",
      "Iteracion: 15991 Gradiente: [8.162894109394377e-05,-0.0014083195327714293] Loss: 22.842738606837546\n",
      "Iteracion: 15992 Gradiente: [8.158554456561736e-05,-0.0014075708268843337] Loss: 22.84273860484804\n",
      "Iteracion: 15993 Gradiente: [8.154217123603757e-05,-0.0014068225190258705] Loss: 22.842738602860667\n",
      "Iteracion: 15994 Gradiente: [8.149882093372677e-05,-0.0014060746089910481] Loss: 22.84273860087539\n",
      "Iteracion: 15995 Gradiente: [8.145549361036805e-05,-0.0014053270965717958] Loss: 22.84273859889223\n",
      "Iteracion: 15996 Gradiente: [8.141218938059561e-05,-0.0014045799815505689] Loss: 22.842738596911186\n",
      "Iteracion: 15997 Gradiente: [8.136890814209134e-05,-0.001403833263720126] Loss: 22.842738594932232\n",
      "Iteracion: 15998 Gradiente: [8.13256499734886e-05,-0.0014030869428650543] Loss: 22.84273859295538\n",
      "Iteracion: 15999 Gradiente: [8.128241473267886e-05,-0.0014023410187796515] Loss: 22.842738590980648\n",
      "Iteracion: 16000 Gradiente: [8.1239202511559e-05,-0.0014015954912500442] Loss: 22.842738589007986\n",
      "Iteracion: 16001 Gradiente: [8.119601328928638e-05,-0.0014008503600624778] Loss: 22.842738587037438\n",
      "Iteracion: 16002 Gradiente: [8.115284703554455e-05,-0.0014001056250104208] Loss: 22.842738585069004\n",
      "Iteracion: 16003 Gradiente: [8.110970360159323e-05,-0.0013993612858901846] Loss: 22.84273858310265\n",
      "Iteracion: 16004 Gradiente: [8.106658321859565e-05,-0.0013986173424753426] Loss: 22.84273858113837\n",
      "Iteracion: 16005 Gradiente: [8.102348574539065e-05,-0.00139787379456493] Loss: 22.842738579176192\n",
      "Iteracion: 16006 Gradiente: [8.09804112340847e-05,-0.0013971306419443626] Loss: 22.84273857721609\n",
      "Iteracion: 16007 Gradiente: [8.093735951035797e-05,-0.0013963878844125569] Loss: 22.842738575258085\n",
      "Iteracion: 16008 Gradiente: [8.089433074379334e-05,-0.001395645521752087] Loss: 22.84273857330216\n",
      "Iteracion: 16009 Gradiente: [8.085132478659791e-05,-0.0013949035537566582] Loss: 22.84273857134831\n",
      "Iteracion: 16010 Gradiente: [8.080834179982807e-05,-0.0013941619802082528] Loss: 22.842738569396534\n",
      "Iteracion: 16011 Gradiente: [8.076538159874265e-05,-0.0013934208009054316] Loss: 22.84273856744684\n",
      "Iteracion: 16012 Gradiente: [8.072244425818553e-05,-0.0013926800156365716] Loss: 22.84273856549921\n",
      "Iteracion: 16013 Gradiente: [8.067952972794501e-05,-0.0013919396241918254] Loss: 22.842738563553652\n",
      "Iteracion: 16014 Gradiente: [8.063663802412671e-05,-0.0013911996263618201] Loss: 22.842738561610155\n",
      "Iteracion: 16015 Gradiente: [8.059376905672858e-05,-0.001390460021940735] Loss: 22.842738559668728\n",
      "Iteracion: 16016 Gradiente: [8.055092301333389e-05,-0.0013897208107084206] Loss: 22.84273855772938\n",
      "Iteracion: 16017 Gradiente: [8.050809964667375e-05,-0.0013889819924694771] Loss: 22.842738555792096\n",
      "Iteracion: 16018 Gradiente: [8.046529900790726e-05,-0.0013882435670110975] Loss: 22.842738553856844\n",
      "Iteracion: 16019 Gradiente: [8.042252113871958e-05,-0.0013875055341222501] Loss: 22.842738551923663\n",
      "Iteracion: 16020 Gradiente: [8.037976610637542e-05,-0.001386767893587996] Loss: 22.842738549992536\n",
      "Iteracion: 16021 Gradiente: [8.033703372708108e-05,-0.0013860306452113966] Loss: 22.842738548063473\n",
      "Iteracion: 16022 Gradiente: [8.029432415905073e-05,-0.001385293788772657] Loss: 22.84273854613645\n",
      "Iteracion: 16023 Gradiente: [8.025163724596495e-05,-0.0013845573240717595] Loss: 22.842738544211468\n",
      "Iteracion: 16024 Gradiente: [8.020897292340123e-05,-0.0013838212509051336] Loss: 22.84273854228854\n",
      "Iteracion: 16025 Gradiente: [8.016633138178501e-05,-0.0013830855690510901] Loss: 22.842738540367655\n",
      "Iteracion: 16026 Gradiente: [8.012371257753633e-05,-0.0013823502783044006] Loss: 22.842738538448824\n",
      "Iteracion: 16027 Gradiente: [8.008111633065104e-05,-0.0013816153784664683] Loss: 22.842738536532018\n",
      "Iteracion: 16028 Gradiente: [8.00385427415525e-05,-0.0013808808693236566] Loss: 22.84273853461726\n",
      "Iteracion: 16029 Gradiente: [7.999599178845073e-05,-0.0013801467506682505] Loss: 22.842738532704512\n",
      "Iteracion: 16030 Gradiente: [7.995346344008188e-05,-0.0013794130222928894] Loss: 22.842738530793817\n",
      "Iteracion: 16031 Gradiente: [7.991095778834279e-05,-0.001378679683985832] Loss: 22.842738528885146\n",
      "Iteracion: 16032 Gradiente: [7.986847460775456e-05,-0.0013779467355515608] Loss: 22.842738526978515\n",
      "Iteracion: 16033 Gradiente: [7.982601420432426e-05,-0.0013772141767629384] Loss: 22.8427385250739\n",
      "Iteracion: 16034 Gradiente: [7.978357615267366e-05,-0.001376482007438066] Loss: 22.842738523171324\n",
      "Iteracion: 16035 Gradiente: [7.974116073701983e-05,-0.0013757502273538335] Loss: 22.842738521270764\n",
      "Iteracion: 16036 Gradiente: [7.969876786262375e-05,-0.0013750188363065518] Loss: 22.84273851937222\n",
      "Iteracion: 16037 Gradiente: [7.965639763748792e-05,-0.0013742878340815186] Loss: 22.8427385174757\n",
      "Iteracion: 16038 Gradiente: [7.961404976034222e-05,-0.001373557220491269] Loss: 22.842738515581186\n",
      "Iteracion: 16039 Gradiente: [7.9571724469929e-05,-0.0013728269953132846] Loss: 22.84273851368869\n",
      "Iteracion: 16040 Gradiente: [7.952942167814096e-05,-0.0013720971583451795] Loss: 22.84273851179821\n",
      "Iteracion: 16041 Gradiente: [7.94871414152946e-05,-0.0013713677093797117] Loss: 22.842738509909736\n",
      "Iteracion: 16042 Gradiente: [7.944488366717906e-05,-0.001370638648207271] Loss: 22.842738508023267\n",
      "Iteracion: 16043 Gradiente: [7.94026483684244e-05,-0.0013699099746284323] Loss: 22.842738506138815\n",
      "Iteracion: 16044 Gradiente: [7.93604353958699e-05,-0.00136918168844223] Loss: 22.842738504256342\n",
      "Iteracion: 16045 Gradiente: [7.931824502899568e-05,-0.0013684537894254353] Loss: 22.842738502375898\n",
      "Iteracion: 16046 Gradiente: [7.927607698926901e-05,-0.0013677262773880963] Loss: 22.842738500497443\n",
      "Iteracion: 16047 Gradiente: [7.923393130605897e-05,-0.0013669991521222612] Loss: 22.84273849862098\n",
      "Iteracion: 16048 Gradiente: [7.919180810915805e-05,-0.001366272413413346] Loss: 22.842738496746506\n",
      "Iteracion: 16049 Gradiente: [7.914970740519796e-05,-0.001365546061056359] Loss: 22.842738494874038\n",
      "Iteracion: 16050 Gradiente: [7.910762896396288e-05,-0.0013648200948575588] Loss: 22.84273849300355\n",
      "Iteracion: 16051 Gradiente: [7.906557295882522e-05,-0.0013640945145998748] Loss: 22.84273849113505\n",
      "Iteracion: 16052 Gradiente: [7.902353925430816e-05,-0.00136336932008696] Loss: 22.842738489268545\n",
      "Iteracion: 16053 Gradiente: [7.898152791578165e-05,-0.001362644511108139] Loss: 22.84273848740403\n",
      "Iteracion: 16054 Gradiente: [7.893953885134882e-05,-0.0013619200874630385] Loss: 22.842738485541474\n",
      "Iteracion: 16055 Gradiente: [7.889757218890737e-05,-0.0013611960489405088] Loss: 22.84273848368092\n",
      "Iteracion: 16056 Gradiente: [7.885562777119049e-05,-0.0013604723953418347] Loss: 22.84273848182233\n",
      "Iteracion: 16057 Gradiente: [7.8813705733675e-05,-0.0013597491264540906] Loss: 22.84273847996572\n",
      "Iteracion: 16058 Gradiente: [7.877180596267408e-05,-0.0013590262420811665] Loss: 22.84273847811108\n",
      "Iteracion: 16059 Gradiente: [7.872992849797811e-05,-0.0013583037420132154] Loss: 22.842738476258425\n",
      "Iteracion: 16060 Gradiente: [7.868807317947812e-05,-0.00135758162605543] Loss: 22.842738474407735\n",
      "Iteracion: 16061 Gradiente: [7.864624020802088e-05,-0.0013568598939906215] Loss: 22.842738472558988\n",
      "Iteracion: 16062 Gradiente: [7.860442950592035e-05,-0.001356138545619956] Loss: 22.84273847071224\n",
      "Iteracion: 16063 Gradiente: [7.856264102485966e-05,-0.0013554175807401] Loss: 22.842738468867427\n",
      "Iteracion: 16064 Gradiente: [7.852087466915236e-05,-0.0013546969991523384] Loss: 22.842738467024592\n",
      "Iteracion: 16065 Gradiente: [7.847913053732706e-05,-0.0013539768006464688] Loss: 22.842738465183718\n",
      "Iteracion: 16066 Gradiente: [7.843740866538458e-05,-0.0013532569850170262] Loss: 22.84273846334479\n",
      "Iteracion: 16067 Gradiente: [7.83957089784811e-05,-0.0013525375520622163] Loss: 22.842738461507814\n",
      "Iteracion: 16068 Gradiente: [7.835403135914021e-05,-0.0013518185015879425] Loss: 22.8427384596728\n",
      "Iteracion: 16069 Gradiente: [7.831237601957734e-05,-0.001351099833374055] Loss: 22.842738457839744\n",
      "Iteracion: 16070 Gradiente: [7.827074272294491e-05,-0.0013503815472329706] Loss: 22.84273845600862\n",
      "Iteracion: 16071 Gradiente: [7.82291315412446e-05,-0.0013496636429556712] Loss: 22.84273845417946\n",
      "Iteracion: 16072 Gradiente: [7.818754250668766e-05,-0.0013489461203375205] Loss: 22.84273845235223\n",
      "Iteracion: 16073 Gradiente: [7.814597566853838e-05,-0.001348228979170211] Loss: 22.84273845052695\n",
      "Iteracion: 16074 Gradiente: [7.810443085816132e-05,-0.0013475122192618963] Loss: 22.842738448703606\n",
      "Iteracion: 16075 Gradiente: [7.806290811439946e-05,-0.001346795840405927] Loss: 22.8427384468822\n",
      "Iteracion: 16076 Gradiente: [7.802140745904277e-05,-0.0013460798423988508] Loss: 22.842738445062732\n",
      "Iteracion: 16077 Gradiente: [7.797992888451215e-05,-0.0013453642250368602] Loss: 22.842738443245192\n",
      "Iteracion: 16078 Gradiente: [7.793847233585893e-05,-0.0013446489881198194] Loss: 22.842738441429585\n",
      "Iteracion: 16079 Gradiente: [7.789703789455871e-05,-0.0013439341314417893] Loss: 22.842738439615925\n",
      "Iteracion: 16080 Gradiente: [7.785562544313507e-05,-0.0013432196548061861] Loss: 22.84273843780418\n",
      "Iteracion: 16081 Gradiente: [7.781423497685106e-05,-0.0013425055580084924] Loss: 22.842738435994363\n",
      "Iteracion: 16082 Gradiente: [7.77728664758115e-05,-0.0013417918408507035] Loss: 22.84273843418646\n",
      "Iteracion: 16083 Gradiente: [7.773152010675707e-05,-0.0013410785031175247] Loss: 22.842738432380482\n",
      "Iteracion: 16084 Gradiente: [7.769019566410407e-05,-0.0013403655446201886] Loss: 22.842738430576432\n",
      "Iteracion: 16085 Gradiente: [7.764889320280114e-05,-0.0013396529651524011] Loss: 22.84273842877429\n",
      "Iteracion: 16086 Gradiente: [7.760761267358399e-05,-0.0013389407645161575] Loss: 22.842738426974073\n",
      "Iteracion: 16087 Gradiente: [7.756635400824052e-05,-0.0013382289425115583] Loss: 22.842738425175774\n",
      "Iteracion: 16088 Gradiente: [7.75251174151966e-05,-0.0013375174989250856] Loss: 22.842738423379355\n",
      "Iteracion: 16089 Gradiente: [7.748390267655243e-05,-0.001336806433567972] Loss: 22.84273842158488\n",
      "Iteracion: 16090 Gradiente: [7.744270987852057e-05,-0.001336095746233923] Loss: 22.842738419792305\n",
      "Iteracion: 16091 Gradiente: [7.74015389528889e-05,-0.0013353854367238681] Loss: 22.842738418001638\n",
      "Iteracion: 16092 Gradiente: [7.736038991576303e-05,-0.0013346755048370795] Loss: 22.842738416212867\n",
      "Iteracion: 16093 Gradiente: [7.731926275861648e-05,-0.0013339659503711705] Loss: 22.842738414426\n",
      "Iteracion: 16094 Gradiente: [7.72781574283954e-05,-0.001333256773128255] Loss: 22.84273841264103\n",
      "Iteracion: 16095 Gradiente: [7.723707399520663e-05,-0.001332547972903934] Loss: 22.842738410857944\n",
      "Iteracion: 16096 Gradiente: [7.719601234820553e-05,-0.001331839549502097] Loss: 22.842738409076787\n",
      "Iteracion: 16097 Gradiente: [7.71549726446589e-05,-0.001331131502713608] Loss: 22.842738407297496\n",
      "Iteracion: 16098 Gradiente: [7.711395463445569e-05,-0.0013304238323513574] Loss: 22.84273840552011\n",
      "Iteracion: 16099 Gradiente: [7.707295852128482e-05,-0.0013297165382036033] Loss: 22.842738403744608\n",
      "Iteracion: 16100 Gradiente: [7.703198423882895e-05,-0.001329009620073407] Loss: 22.842738401970998\n",
      "Iteracion: 16101 Gradiente: [7.699103161182089e-05,-0.0013283030777690404] Loss: 22.842738400199263\n",
      "Iteracion: 16102 Gradiente: [7.695010082689654e-05,-0.0013275969110800646] Loss: 22.84273839842941\n",
      "Iteracion: 16103 Gradiente: [7.690919181110681e-05,-0.0013268911198106063] Loss: 22.84273839666145\n",
      "Iteracion: 16104 Gradiente: [7.686830443560666e-05,-0.001326185703767635] Loss: 22.84273839489536\n",
      "Iteracion: 16105 Gradiente: [7.682743896282318e-05,-0.0013254806627362114] Loss: 22.84273839313115\n",
      "Iteracion: 16106 Gradiente: [7.678659515875097e-05,-0.0013247759965299365] Loss: 22.842738391368826\n",
      "Iteracion: 16107 Gradiente: [7.674577306033825e-05,-0.0013240717049457136] Loss: 22.842738389608364\n",
      "Iteracion: 16108 Gradiente: [7.670497263916332e-05,-0.0013233677877854196] Loss: 22.84273838784977\n",
      "Iteracion: 16109 Gradiente: [7.66641939302796e-05,-0.0013226642448489181] Loss: 22.84273838609306\n",
      "Iteracion: 16110 Gradiente: [7.662343685600111e-05,-0.001321961075939034] Loss: 22.842738384338208\n",
      "Iteracion: 16111 Gradiente: [7.658270143527565e-05,-0.0013212582808575255] Loss: 22.842738382585228\n",
      "Iteracion: 16112 Gradiente: [7.654198771073576e-05,-0.0013205558593988088] Loss: 22.8427383808341\n",
      "Iteracion: 16113 Gradiente: [7.65012957382775e-05,-0.0013198538113646425] Loss: 22.842738379084835\n",
      "Iteracion: 16114 Gradiente: [7.646062529431674e-05,-0.0013191521365659043] Loss: 22.84273837733743\n",
      "Iteracion: 16115 Gradiente: [7.641997641011737e-05,-0.001318450834801747] Loss: 22.842738375591885\n",
      "Iteracion: 16116 Gradiente: [7.637934926757831e-05,-0.0013177499058662316] Loss: 22.842738373848196\n",
      "Iteracion: 16117 Gradiente: [7.633874360711465e-05,-0.0013170493495699986] Loss: 22.842738372106364\n",
      "Iteracion: 16118 Gradiente: [7.629815962294136e-05,-0.001316349165708175] Loss: 22.84273837036638\n",
      "Iteracion: 16119 Gradiente: [7.625759721747727e-05,-0.0013156493540838218] Loss: 22.842738368628247\n",
      "Iteracion: 16120 Gradiente: [7.621705636040588e-05,-0.0013149499145023687] Loss: 22.842738366891947\n",
      "Iteracion: 16121 Gradiente: [7.617653696456728e-05,-0.0013142508467700745] Loss: 22.842738365157512\n",
      "Iteracion: 16122 Gradiente: [7.613603920712346e-05,-0.0013135521506778029] Loss: 22.842738363424925\n",
      "Iteracion: 16123 Gradiente: [7.609556291754416e-05,-0.0013128538260366678] Loss: 22.842738361694163\n",
      "Iteracion: 16124 Gradiente: [7.605510819720014e-05,-0.0013121558726438092] Loss: 22.84273835996525\n",
      "Iteracion: 16125 Gradiente: [7.601467487745594e-05,-0.0013114582903116438] Loss: 22.842738358238176\n",
      "Iteracion: 16126 Gradiente: [7.597426322263346e-05,-0.001310761078825588] Loss: 22.84273835651292\n",
      "Iteracion: 16127 Gradiente: [7.593387290493562e-05,-0.001310064238006466] Loss: 22.842738354789514\n",
      "Iteracion: 16128 Gradiente: [7.589350411763007e-05,-0.0013093677676458526] Loss: 22.842738353067936\n",
      "Iteracion: 16129 Gradiente: [7.58531568218738e-05,-0.001308671667549414] Loss: 22.8427383513482\n",
      "Iteracion: 16130 Gradiente: [7.581283095419167e-05,-0.0013079759375206853] Loss: 22.842738349630274\n",
      "Iteracion: 16131 Gradiente: [7.57725265695323e-05,-0.0013072805773623722] Loss: 22.842738347914178\n",
      "Iteracion: 16132 Gradiente: [7.573224359589404e-05,-0.0013065855868787206] Loss: 22.84273834619991\n",
      "Iteracion: 16133 Gradiente: [7.569198200296038e-05,-0.001305890965876344] Loss: 22.842738344487454\n",
      "Iteracion: 16134 Gradiente: [7.565174178788917e-05,-0.0013051967141572381] Loss: 22.842738342776848\n",
      "Iteracion: 16135 Gradiente: [7.56115229809969e-05,-0.0013045028315232798] Loss: 22.84273834106802\n",
      "Iteracion: 16136 Gradiente: [7.557132553775621e-05,-0.00130380931777907] Loss: 22.84273833936104\n",
      "Iteracion: 16137 Gradiente: [7.553114957090657e-05,-0.0013031161727206826] Loss: 22.84273833765587\n",
      "Iteracion: 16138 Gradiente: [7.549099483791602e-05,-0.001302423396167877] Loss: 22.842738335952497\n",
      "Iteracion: 16139 Gradiente: [7.54508615642635e-05,-0.0013017309879092664] Loss: 22.842738334250956\n",
      "Iteracion: 16140 Gradiente: [7.541074959741915e-05,-0.0013010389477586888] Loss: 22.842738332551203\n",
      "Iteracion: 16141 Gradiente: [7.537065889948736e-05,-0.0013003472755203896] Loss: 22.842738330853265\n",
      "Iteracion: 16142 Gradiente: [7.533058955099629e-05,-0.0012996559709950617] Loss: 22.842738329157136\n",
      "Iteracion: 16143 Gradiente: [7.529054145625954e-05,-0.0012989650339909767] Loss: 22.842738327462804\n",
      "Iteracion: 16144 Gradiente: [7.525051474791175e-05,-0.0012982744643040907] Loss: 22.842738325770274\n",
      "Iteracion: 16145 Gradiente: [7.521050928289696e-05,-0.0012975842617480046] Loss: 22.842738324079544\n",
      "Iteracion: 16146 Gradiente: [7.517052503089872e-05,-0.0012968944261272004] Loss: 22.84273832239062\n",
      "Iteracion: 16147 Gradiente: [7.513056203644434e-05,-0.0012962049572447399] Loss: 22.842738320703482\n",
      "Iteracion: 16148 Gradiente: [7.509062031184991e-05,-0.001295515854903447] Loss: 22.84273831901813\n",
      "Iteracion: 16149 Gradiente: [7.50506998400624e-05,-0.0012948271189095807] Loss: 22.842738317334586\n",
      "Iteracion: 16150 Gradiente: [7.501080056613319e-05,-0.0012941387490693994] Loss: 22.84273831565282\n",
      "Iteracion: 16151 Gradiente: [7.497092264732904e-05,-0.001293450745180517] Loss: 22.842738313972855\n",
      "Iteracion: 16152 Gradiente: [7.493106578901157e-05,-0.0012927631070622188] Loss: 22.84273831229466\n",
      "Iteracion: 16153 Gradiente: [7.489123010297286e-05,-0.001292075834514724] Loss: 22.84273831061825\n",
      "Iteracion: 16154 Gradiente: [7.4851415618582e-05,-0.0012913889273419225] Loss: 22.84273830894363\n",
      "Iteracion: 16155 Gradiente: [7.481162232068073e-05,-0.0012907023853491258] Loss: 22.842738307270782\n",
      "Iteracion: 16156 Gradiente: [7.477185022632208e-05,-0.0012900162083391584] Loss: 22.842738305599717\n",
      "Iteracion: 16157 Gradiente: [7.473209914034366e-05,-0.001289330396130713] Loss: 22.84273830393044\n",
      "Iteracion: 16158 Gradiente: [7.469236923138093e-05,-0.0012886449485191539] Loss: 22.842738302262926\n",
      "Iteracion: 16159 Gradiente: [7.465266050227607e-05,-0.0012879598653076604] Loss: 22.84273830059717\n",
      "Iteracion: 16160 Gradiente: [7.461297285923743e-05,-0.0012872751463101886] Loss: 22.842738298933195\n",
      "Iteracion: 16161 Gradiente: [7.457330633258153e-05,-0.0012865907913282607] Loss: 22.84273829727101\n",
      "Iteracion: 16162 Gradiente: [7.453366077735761e-05,-0.0012859068001772537] Loss: 22.842738295610566\n",
      "Iteracion: 16163 Gradiente: [7.449403640862329e-05,-0.0012852231726511103] Loss: 22.84273829395189\n",
      "Iteracion: 16164 Gradiente: [7.445443308142785e-05,-0.0012845399085634312] Loss: 22.84273829229498\n",
      "Iteracion: 16165 Gradiente: [7.441485084219342e-05,-0.001283857007718107] Loss: 22.842738290639836\n",
      "Iteracion: 16166 Gradiente: [7.437528964070831e-05,-0.0012831744699241198] Loss: 22.84273828898646\n",
      "Iteracion: 16167 Gradiente: [7.433574942107649e-05,-0.0012824922949908077] Loss: 22.842738287334825\n",
      "Iteracion: 16168 Gradiente: [7.429623032161696e-05,-0.0012818104827170866] Loss: 22.842738285684945\n",
      "Iteracion: 16169 Gradiente: [7.425673218695769e-05,-0.001281129032917505] Loss: 22.84273828403682\n",
      "Iteracion: 16170 Gradiente: [7.421725508436339e-05,-0.0012804479453971376] Loss: 22.842738282390464\n",
      "Iteracion: 16171 Gradiente: [7.417779888119942e-05,-0.001279767219968638] Loss: 22.84273828074585\n",
      "Iteracion: 16172 Gradiente: [7.413836370820567e-05,-0.0012790868564310405] Loss: 22.842738279102985\n",
      "Iteracion: 16173 Gradiente: [7.409894945359004e-05,-0.0012784068545981834] Loss: 22.84273827746185\n",
      "Iteracion: 16174 Gradiente: [7.405955616945903e-05,-0.0012777272142746672] Loss: 22.84273827582247\n",
      "Iteracion: 16175 Gradiente: [7.40201837705475e-05,-0.0012770479352712508] Loss: 22.842738274184835\n",
      "Iteracion: 16176 Gradiente: [7.398083238664791e-05,-0.0012763690173887455] Loss: 22.842738272548946\n",
      "Iteracion: 16177 Gradiente: [7.394150194670602e-05,-0.0012756904604389755] Loss: 22.842738270914786\n",
      "Iteracion: 16178 Gradiente: [7.39021923228241e-05,-0.0012750122642368447] Loss: 22.84273826928237\n",
      "Iteracion: 16179 Gradiente: [7.386290361258338e-05,-0.001274334428582217] Loss: 22.84273826765168\n",
      "Iteracion: 16180 Gradiente: [7.382363587661681e-05,-0.0012736569532819431] Loss: 22.84273826602273\n",
      "Iteracion: 16181 Gradiente: [7.378438897471066e-05,-0.0012729798381504527] Loss: 22.842738264395507\n",
      "Iteracion: 16182 Gradiente: [7.37451629476027e-05,-0.0012723030829930573] Loss: 22.84273826277002\n",
      "Iteracion: 16183 Gradiente: [7.370595774887078e-05,-0.001271626687621108] Loss: 22.84273826114626\n",
      "Iteracion: 16184 Gradiente: [7.366677340125231e-05,-0.0012709506518420474] Loss: 22.84273825952423\n",
      "Iteracion: 16185 Gradiente: [7.362760987916772e-05,-0.0012702749754639104] Loss: 22.842738257903914\n",
      "Iteracion: 16186 Gradiente: [7.358846719114353e-05,-0.0012695996582963904] Loss: 22.842738256285323\n",
      "Iteracion: 16187 Gradiente: [7.354934536844364e-05,-0.0012689247001437328] Loss: 22.84273825466847\n",
      "Iteracion: 16188 Gradiente: [7.351024430590769e-05,-0.001268250100822262] Loss: 22.842738253053312\n",
      "Iteracion: 16189 Gradiente: [7.347116390974407e-05,-0.001267575860144987] Loss: 22.842738251439865\n",
      "Iteracion: 16190 Gradiente: [7.343210438648384e-05,-0.001266901977909877] Loss: 22.84273824982815\n",
      "Iteracion: 16191 Gradiente: [7.339306559875544e-05,-0.0012662284539334933] Loss: 22.842738248218147\n",
      "Iteracion: 16192 Gradiente: [7.335404757213837e-05,-0.0012655552880219763] Loss: 22.842738246609862\n",
      "Iteracion: 16193 Gradiente: [7.331505034358088e-05,-0.0012648824799839531] Loss: 22.842738245003265\n",
      "Iteracion: 16194 Gradiente: [7.327607380508046e-05,-0.0012642100296327877] Loss: 22.842738243398397\n",
      "Iteracion: 16195 Gradiente: [7.323711796421624e-05,-0.0012635379367785286] Loss: 22.842738241795228\n",
      "Iteracion: 16196 Gradiente: [7.319818289677945e-05,-0.0012628662012247106] Loss: 22.842738240193757\n",
      "Iteracion: 16197 Gradiente: [7.31592685023467e-05,-0.001262194822788724] Loss: 22.842738238593984\n",
      "Iteracion: 16198 Gradiente: [7.312037485576183e-05,-0.0012615238012742224] Loss: 22.84273823699594\n",
      "Iteracion: 16199 Gradiente: [7.308150176186246e-05,-0.0012608531365033333] Loss: 22.842738235399565\n",
      "Iteracion: 16200 Gradiente: [7.304264937412578e-05,-0.0012601828282748546] Loss: 22.842738233804894\n",
      "Iteracion: 16201 Gradiente: [7.300381758644411e-05,-0.0012595128764072427] Loss: 22.84273823221192\n",
      "Iteracion: 16202 Gradiente: [7.296500651155687e-05,-0.0012588432807021377] Loss: 22.842738230620657\n",
      "Iteracion: 16203 Gradiente: [7.292621615420102e-05,-0.0012581740409694693] Loss: 22.842738229031056\n",
      "Iteracion: 16204 Gradiente: [7.288744629173986e-05,-0.0012575051570326678] Loss: 22.842738227443157\n",
      "Iteracion: 16205 Gradiente: [7.284869715059964e-05,-0.0012568366286887548] Loss: 22.84273822585696\n",
      "Iteracion: 16206 Gradiente: [7.280996854698666e-05,-0.001256168455757726] Loss: 22.84273822427243\n",
      "Iteracion: 16207 Gradiente: [7.277126058700863e-05,-0.0012555006380461956] Loss: 22.842738222689594\n",
      "Iteracion: 16208 Gradiente: [7.273257312382006e-05,-0.0012548331753701328] Loss: 22.84273822110843\n",
      "Iteracion: 16209 Gradiente: [7.269390628247644e-05,-0.0012541660675356781] Loss: 22.84273821952896\n",
      "Iteracion: 16210 Gradiente: [7.265525992086927e-05,-0.0012534993143609322] Loss: 22.842738217951158\n",
      "Iteracion: 16211 Gradiente: [7.261663414510622e-05,-0.0012528329156499042] Loss: 22.842738216375043\n",
      "Iteracion: 16212 Gradiente: [7.257802896466122e-05,-0.001252166871212997] Loss: 22.842738214800598\n",
      "Iteracion: 16213 Gradiente: [7.253944423458354e-05,-0.0012515011808706808] Loss: 22.842738213227822\n",
      "Iteracion: 16214 Gradiente: [7.250088002687486e-05,-0.0012508358444300427] Loss: 22.842738211656712\n",
      "Iteracion: 16215 Gradiente: [7.246233630742911e-05,-0.0012501708617026707] Loss: 22.842738210087298\n",
      "Iteracion: 16216 Gradiente: [7.242381306582502e-05,-0.0012495062325015738] Loss: 22.84273820851954\n",
      "Iteracion: 16217 Gradiente: [7.238531031437864e-05,-0.0012488419566360895] Loss: 22.842738206953435\n",
      "Iteracion: 16218 Gradiente: [7.234682803887911e-05,-0.001248178033921358] Loss: 22.842738205389008\n",
      "Iteracion: 16219 Gradiente: [7.230836625732688e-05,-0.0012475144641655334] Loss: 22.84273820382624\n",
      "Iteracion: 16220 Gradiente: [7.226992492519458e-05,-0.0012468512471835189] Loss: 22.84273820226514\n",
      "Iteracion: 16221 Gradiente: [7.223150404816655e-05,-0.0012461883827865469] Loss: 22.842738200705686\n",
      "Iteracion: 16222 Gradiente: [7.219310355139896e-05,-0.0012455258707916527] Loss: 22.842738199147906\n",
      "Iteracion: 16223 Gradiente: [7.215472353436781e-05,-0.0012448637110034375] Loss: 22.84273819759177\n",
      "Iteracion: 16224 Gradiente: [7.211636385970147e-05,-0.0012442019032459228] Loss: 22.84273819603728\n",
      "Iteracion: 16225 Gradiente: [7.207802460129642e-05,-0.0012435404473232362] Loss: 22.842738194484472\n",
      "Iteracion: 16226 Gradiente: [7.203970568241403e-05,-0.0012428793430524128] Loss: 22.842738192933286\n",
      "Iteracion: 16227 Gradiente: [7.200140711726514e-05,-0.00124221859024658] Loss: 22.842738191383756\n",
      "Iteracion: 16228 Gradiente: [7.196312902237877e-05,-0.0012415581887101012] Loss: 22.84273818983588\n",
      "Iteracion: 16229 Gradiente: [7.192487126701508e-05,-0.0012408981382641571] Loss: 22.842738188289655\n",
      "Iteracion: 16230 Gradiente: [7.188663381327843e-05,-0.0012402384387241246] Loss: 22.84273818674506\n",
      "Iteracion: 16231 Gradiente: [7.184841664506317e-05,-0.0012395790899032495] Loss: 22.84273818520211\n",
      "Iteracion: 16232 Gradiente: [7.1810219795528e-05,-0.0012389200916115802] Loss: 22.842738183660792\n",
      "Iteracion: 16233 Gradiente: [7.17720433234111e-05,-0.001238261443660349] Loss: 22.84273818212114\n",
      "Iteracion: 16234 Gradiente: [7.173388700702314e-05,-0.0012376031458742887] Loss: 22.8427381805831\n",
      "Iteracion: 16235 Gradiente: [7.169575104057913e-05,-0.0012369451980561053] Loss: 22.84273817904669\n",
      "Iteracion: 16236 Gradiente: [7.165763542976341e-05,-0.0012362876000175048] Loss: 22.84273817751194\n",
      "Iteracion: 16237 Gradiente: [7.161954003152005e-05,-0.001235630351581681] Loss: 22.842738175978806\n",
      "Iteracion: 16238 Gradiente: [7.158146487142857e-05,-0.0012349734525601028] Loss: 22.842738174447284\n",
      "Iteracion: 16239 Gradiente: [7.154340993338337e-05,-0.0012343169027674368] Loss: 22.842738172917414\n",
      "Iteracion: 16240 Gradiente: [7.150537533865039e-05,-0.0012336607020094685] Loss: 22.842738171389158\n",
      "Iteracion: 16241 Gradiente: [7.146736084943466e-05,-0.0012330048501150751] Loss: 22.84273816986254\n",
      "Iteracion: 16242 Gradiente: [7.142936658700212e-05,-0.0012323493468913446] Loss: 22.84273816833754\n",
      "Iteracion: 16243 Gradiente: [7.13913925229311e-05,-0.0012316941921530619] Loss: 22.842738166814158\n",
      "Iteracion: 16244 Gradiente: [7.135343864964247e-05,-0.001231039385715486] Loss: 22.84273816529238\n",
      "Iteracion: 16245 Gradiente: [7.131550498987357e-05,-0.001230384927390915] Loss: 22.842738163772243\n",
      "Iteracion: 16246 Gradiente: [7.127759147067536e-05,-0.0012297308169972136] Loss: 22.842738162253717\n",
      "Iteracion: 16247 Gradiente: [7.123969816973386e-05,-0.0012290770543449032] Loss: 22.842738160736793\n",
      "Iteracion: 16248 Gradiente: [7.120182496767787e-05,-0.0012284236392568224] Loss: 22.842738159221497\n",
      "Iteracion: 16249 Gradiente: [7.116397194787775e-05,-0.0012277705715400581] Loss: 22.842738157707814\n",
      "Iteracion: 16250 Gradiente: [7.112613893601368e-05,-0.001227117851021712] Loss: 22.842738156195733\n",
      "Iteracion: 16251 Gradiente: [7.108832607703638e-05,-0.0012264654775075692] Loss: 22.84273815468525\n",
      "Iteracion: 16252 Gradiente: [7.105053329799678e-05,-0.0012258134508166781] Loss: 22.842738153176374\n",
      "Iteracion: 16253 Gradiente: [7.101276067373874e-05,-0.0012251617707587314] Loss: 22.84273815166911\n",
      "Iteracion: 16254 Gradiente: [7.097500813889231e-05,-0.00122451043715337] Loss: 22.84273815016345\n",
      "Iteracion: 16255 Gradiente: [7.093727567640448e-05,-0.0012238594498167998] Loss: 22.84273814865939\n",
      "Iteracion: 16256 Gradiente: [7.089956325406395e-05,-0.0012232088085668855] Loss: 22.842738147156922\n",
      "Iteracion: 16257 Gradiente: [7.086187083681731e-05,-0.0012225585132197144] Loss: 22.842738145656046\n",
      "Iteracion: 16258 Gradiente: [7.082419849003448e-05,-0.0012219085635882957] Loss: 22.842738144156787\n",
      "Iteracion: 16259 Gradiente: [7.078654616445116e-05,-0.0012212589594904936] Loss: 22.84273814265911\n",
      "Iteracion: 16260 Gradiente: [7.074891387048865e-05,-0.0012206097007412117] Loss: 22.84273814116302\n",
      "Iteracion: 16261 Gradiente: [7.071130157877784e-05,-0.001219960787157485] Loss: 22.842738139668516\n",
      "Iteracion: 16262 Gradiente: [7.06737093035296e-05,-0.0012193122185555202] Loss: 22.842738138175612\n",
      "Iteracion: 16263 Gradiente: [7.063613700211135e-05,-0.0012186639947531811] Loss: 22.842738136684307\n",
      "Iteracion: 16264 Gradiente: [7.0598584645154e-05,-0.0012180161155673848] Loss: 22.842738135194555\n",
      "Iteracion: 16265 Gradiente: [7.056105226013187e-05,-0.0012173685808143375] Loss: 22.842738133706412\n",
      "Iteracion: 16266 Gradiente: [7.052353979872806e-05,-0.0012167213903117849] Loss: 22.842738132219836\n",
      "Iteracion: 16267 Gradiente: [7.048604730168033e-05,-0.0012160745438753414] Loss: 22.842738130734844\n",
      "Iteracion: 16268 Gradiente: [7.044857481351604e-05,-0.0012154280413171866] Loss: 22.842738129251433\n",
      "Iteracion: 16269 Gradiente: [7.041112222054835e-05,-0.0012147818824615798] Loss: 22.8427381277696\n",
      "Iteracion: 16270 Gradiente: [7.037368945645994e-05,-0.0012141360671279245] Loss: 22.842738126289337\n",
      "Iteracion: 16271 Gradiente: [7.033627660556855e-05,-0.001213490595129585] Loss: 22.84273812481066\n",
      "Iteracion: 16272 Gradiente: [7.029888370955936e-05,-0.0012128454662794516] Loss: 22.842738123333547\n",
      "Iteracion: 16273 Gradiente: [7.026151071916804e-05,-0.0012122006803992964] Loss: 22.842738121858005\n",
      "Iteracion: 16274 Gradiente: [7.022415760597293e-05,-0.0012115562373042601] Loss: 22.842738120384016\n",
      "Iteracion: 16275 Gradiente: [7.018682425912933e-05,-0.0012109121368226283] Loss: 22.842738118911612\n",
      "Iteracion: 16276 Gradiente: [7.014951067863724e-05,-0.0012102683787681202] Loss: 22.842738117440767\n",
      "Iteracion: 16277 Gradiente: [7.01122170880808e-05,-0.0012096249629475863] Loss: 22.842738115971493\n",
      "Iteracion: 16278 Gradiente: [7.007494328092889e-05,-0.0012089818891878915] Loss: 22.842738114503774\n",
      "Iteracion: 16279 Gradiente: [7.003768932160408e-05,-0.0012083391573044129] Loss: 22.842738113037612\n",
      "Iteracion: 16280 Gradiente: [7.000045506989257e-05,-0.0012076967671225938] Loss: 22.84273811157301\n",
      "Iteracion: 16281 Gradiente: [6.996324079485323e-05,-0.0012070547184439562] Loss: 22.84273811010998\n",
      "Iteracion: 16282 Gradiente: [6.992604614500428e-05,-0.0012064130111075618] Loss: 22.842738108648483\n",
      "Iteracion: 16283 Gradiente: [6.98888713012972e-05,-0.0012057716449215643] Loss: 22.842738107188556\n",
      "Iteracion: 16284 Gradiente: [6.985171620878342e-05,-0.001205130619705841] Loss: 22.842738105730177\n",
      "Iteracion: 16285 Gradiente: [6.98145808788316e-05,-0.0012044899352786113] Loss: 22.84273810427334\n",
      "Iteracion: 16286 Gradiente: [6.977746534838994e-05,-0.0012038495914548975] Loss: 22.84273810281805\n",
      "Iteracion: 16287 Gradiente: [6.97403694687182e-05,-0.0012032095880621558] Loss: 22.84273810136432\n",
      "Iteracion: 16288 Gradiente: [6.970329338855663e-05,-0.0012025699249107902] Loss: 22.842738099912125\n",
      "Iteracion: 16289 Gradiente: [6.966623694305932e-05,-0.0012019306018280199] Loss: 22.84273809846149\n",
      "Iteracion: 16290 Gradiente: [6.96292002184388e-05,-0.0012012916186288673] Loss: 22.842738097012372\n",
      "Iteracion: 16291 Gradiente: [6.959218323932722e-05,-0.001200652975127999] Loss: 22.84273809556481\n",
      "Iteracion: 16292 Gradiente: [6.955518583424691e-05,-0.0012000146711564242] Loss: 22.84273809411879\n",
      "Iteracion: 16293 Gradiente: [6.951820815099078e-05,-0.001199376706524428] Loss: 22.8427380926743\n",
      "Iteracion: 16294 Gradiente: [6.948125021324358e-05,-0.0011987390810480985] Loss: 22.842738091231347\n",
      "Iteracion: 16295 Gradiente: [6.94443117784734e-05,-0.001198101794560813] Loss: 22.842738089789925\n",
      "Iteracion: 16296 Gradiente: [6.940739301342091e-05,-0.0011974648468747518] Loss: 22.84273808835003\n",
      "Iteracion: 16297 Gradiente: [6.937049401187779e-05,-0.001196828237800555] Loss: 22.842738086911677\n",
      "Iteracion: 16298 Gradiente: [6.933361452278556e-05,-0.0011961919671730215] Loss: 22.842738085474846\n",
      "Iteracion: 16299 Gradiente: [6.929675457930291e-05,-0.001195556034810134] Loss: 22.842738084039556\n",
      "Iteracion: 16300 Gradiente: [6.925991432922274e-05,-0.0011949204405225327] Loss: 22.842738082605766\n",
      "Iteracion: 16301 Gradiente: [6.922309368254295e-05,-0.0011942851841361345] Loss: 22.842738081173522\n",
      "Iteracion: 16302 Gradiente: [6.918629253884016e-05,-0.0011936502654753174] Loss: 22.842738079742794\n",
      "Iteracion: 16303 Gradiente: [6.914951103453859e-05,-0.0011930156843526162] Loss: 22.842738078313584\n",
      "Iteracion: 16304 Gradiente: [6.911274900573971e-05,-0.001192381440597501] Loss: 22.842738076885897\n",
      "Iteracion: 16305 Gradiente: [6.907600648844436e-05,-0.0011917475340271248] Loss: 22.842738075459724\n",
      "Iteracion: 16306 Gradiente: [6.903928358876025e-05,-0.0011911139644557997] Loss: 22.842738074035076\n",
      "Iteracion: 16307 Gradiente: [6.900258015605232e-05,-0.0011904807317126398] Loss: 22.842738072611937\n",
      "Iteracion: 16308 Gradiente: [6.896589627558569e-05,-0.0011898478356146805] Loss: 22.84273807119031\n",
      "Iteracion: 16309 Gradiente: [6.892923193599168e-05,-0.0011892152759799047] Loss: 22.84273806977019\n",
      "Iteracion: 16310 Gradiente: [6.889258706621604e-05,-0.0011885830526360054] Loss: 22.842738068351576\n",
      "Iteracion: 16311 Gradiente: [6.885596164257398e-05,-0.001187951165401794] Loss: 22.842738066934483\n",
      "Iteracion: 16312 Gradiente: [6.881935570011896e-05,-0.0011873196140982146] Loss: 22.84273806551889\n",
      "Iteracion: 16313 Gradiente: [6.878276914979627e-05,-0.0011866883985514201] Loss: 22.842738064104804\n",
      "Iteracion: 16314 Gradiente: [6.87462021024506e-05,-0.0011860575185750123] Loss: 22.84273806269222\n",
      "Iteracion: 16315 Gradiente: [6.870965455808194e-05,-0.0011854269739889863] Loss: 22.842738061281132\n",
      "Iteracion: 16316 Gradiente: [6.86731263688974e-05,-0.001184796764624115] Loss: 22.842738059871547\n",
      "Iteracion: 16317 Gradiente: [6.863661765900512e-05,-0.001184166890295657] Loss: 22.84273805846346\n",
      "Iteracion: 16318 Gradiente: [6.86001283194552e-05,-0.0011835373508300032] Loss: 22.842738057056877\n",
      "Iteracion: 16319 Gradiente: [6.856365840330151e-05,-0.0011829081460440704] Loss: 22.842738055651786\n",
      "Iteracion: 16320 Gradiente: [6.852720791338622e-05,-0.001182279275760815] Loss: 22.842738054248173\n",
      "Iteracion: 16321 Gradiente: [6.849077671517989e-05,-0.0011816507398104174] Loss: 22.84273805284608\n",
      "Iteracion: 16322 Gradiente: [6.845436496973888e-05,-0.001181022538003636] Loss: 22.84273805144545\n",
      "Iteracion: 16323 Gradiente: [6.841797246768996e-05,-0.001180394670174323] Loss: 22.842738050046332\n",
      "Iteracion: 16324 Gradiente: [6.838159936914204e-05,-0.0011797671361350125] Loss: 22.842738048648677\n",
      "Iteracion: 16325 Gradiente: [6.834524561346218e-05,-0.0011791399357126882] Loss: 22.842738047252542\n",
      "Iteracion: 16326 Gradiente: [6.830891118643952e-05,-0.0011785130687306615] Loss: 22.842738045857867\n",
      "Iteracion: 16327 Gradiente: [6.827259602933584e-05,-0.0011778865350109413] Loss: 22.84273804446467\n",
      "Iteracion: 16328 Gradiente: [6.823630025868019e-05,-0.0011772603343723393] Loss: 22.842738043072963\n",
      "Iteracion: 16329 Gradiente: [6.820002377120697e-05,-0.0011766344666416018] Loss: 22.842738041682743\n",
      "Iteracion: 16330 Gradiente: [6.816376647596674e-05,-0.001176008931646777] Loss: 22.842738040293987\n",
      "Iteracion: 16331 Gradiente: [6.81275285311737e-05,-0.0011753837292009925] Loss: 22.84273803890672\n",
      "Iteracion: 16332 Gradiente: [6.809130988472134e-05,-0.001174758859130165] Loss: 22.842738037520917\n",
      "Iteracion: 16333 Gradiente: [6.805511042671242e-05,-0.0011741343212632908] Loss: 22.842738036136588\n",
      "Iteracion: 16334 Gradiente: [6.801893019030558e-05,-0.0011735101154208393] Loss: 22.842738034753737\n",
      "Iteracion: 16335 Gradiente: [6.798276927402943e-05,-0.0011728862414211486] Loss: 22.842738033372353\n",
      "Iteracion: 16336 Gradiente: [6.794662761251402e-05,-0.0011722626990909647] Loss: 22.84273803199243\n",
      "Iteracion: 16337 Gradiente: [6.791050503049215e-05,-0.0011716394882608228] Loss: 22.842738030613987\n",
      "Iteracion: 16338 Gradiente: [6.787440173733709e-05,-0.001171016608745153] Loss: 22.842738029236994\n",
      "Iteracion: 16339 Gradiente: [6.783831761936199e-05,-0.0011703940603723594] Loss: 22.84273802786148\n",
      "Iteracion: 16340 Gradiente: [6.780225271730463e-05,-0.00116977184296303] Loss: 22.842738026487407\n",
      "Iteracion: 16341 Gradiente: [6.77662069837955e-05,-0.0011691499563433183] Loss: 22.842738025114823\n",
      "Iteracion: 16342 Gradiente: [6.773018029662125e-05,-0.0011685284003455365] Loss: 22.842738023743678\n",
      "Iteracion: 16343 Gradiente: [6.769417287936601e-05,-0.0011679071747783118] Loss: 22.842738022373997\n",
      "Iteracion: 16344 Gradiente: [6.765818457665773e-05,-0.0011672862794753769] Loss: 22.842738021005783\n",
      "Iteracion: 16345 Gradiente: [6.762221542354989e-05,-0.0011666657142585042] Loss: 22.842738019638993\n",
      "Iteracion: 16346 Gradiente: [6.758626537930467e-05,-0.001166045478954203] Loss: 22.842738018273685\n",
      "Iteracion: 16347 Gradiente: [6.755033442402691e-05,-0.001165425573386732] Loss: 22.842738016909813\n",
      "Iteracion: 16348 Gradiente: [6.751442260792828e-05,-0.0011648059973789298] Loss: 22.8427380155474\n",
      "Iteracion: 16349 Gradiente: [6.747852992342966e-05,-0.001164186750754226] Loss: 22.842738014186427\n",
      "Iteracion: 16350 Gradiente: [6.744265625400204e-05,-0.0011635678333441035] Loss: 22.842738012826903\n",
      "Iteracion: 16351 Gradiente: [6.740680164701492e-05,-0.0011629492449685586] Loss: 22.84273801146883\n",
      "Iteracion: 16352 Gradiente: [6.737096610341572e-05,-0.0011623309854550475] Loss: 22.842738010112196\n",
      "Iteracion: 16353 Gradiente: [6.733514958625619e-05,-0.0011617130546283032] Loss: 22.842738008757\n",
      "Iteracion: 16354 Gradiente: [6.7299352150485e-05,-0.001161095452310216] Loss: 22.84273800740325\n",
      "Iteracion: 16355 Gradiente: [6.726357379894428e-05,-0.0011604781783256376] Loss: 22.842738006050936\n",
      "Iteracion: 16356 Gradiente: [6.722781440752594e-05,-0.0011598612325059321] Loss: 22.842738004700063\n",
      "Iteracion: 16357 Gradiente: [6.719207401507295e-05,-0.001159244614674293] Loss: 22.84273800335062\n",
      "Iteracion: 16358 Gradiente: [6.715635273243e-05,-0.0011586283246490582] Loss: 22.84273800200263\n",
      "Iteracion: 16359 Gradiente: [6.712065033601297e-05,-0.0011580123622697632] Loss: 22.842738000656052\n",
      "Iteracion: 16360 Gradiente: [6.708496690540262e-05,-0.0011573967273548647] Loss: 22.84273799931091\n",
      "Iteracion: 16361 Gradiente: [6.704930247754722e-05,-0.0011567814197281478] Loss: 22.8427379979672\n",
      "Iteracion: 16362 Gradiente: [6.701365696623421e-05,-0.0011561664392221616] Loss: 22.84273799662491\n",
      "Iteracion: 16363 Gradiente: [6.697803043399138e-05,-0.0011555517856555988] Loss: 22.842737995284054\n",
      "Iteracion: 16364 Gradiente: [6.694242285902874e-05,-0.001154937458856864] Loss: 22.842737993944635\n",
      "Iteracion: 16365 Gradiente: [6.69068342205037e-05,-0.001154323458651992] Loss: 22.842737992606626\n",
      "Iteracion: 16366 Gradiente: [6.68712645241006e-05,-0.0011537097848677291] Loss: 22.842737991270038\n",
      "Iteracion: 16367 Gradiente: [6.683571368739649e-05,-0.0011530964373340188] Loss: 22.842737989934864\n",
      "Iteracion: 16368 Gradiente: [6.680018183260472e-05,-0.0011524834158701462] Loss: 22.842737988601122\n",
      "Iteracion: 16369 Gradiente: [6.676466879298459e-05,-0.0011518707203125682] Loss: 22.84273798726881\n",
      "Iteracion: 16370 Gradiente: [6.672917465569602e-05,-0.00115125835048057] Loss: 22.84273798593789\n",
      "Iteracion: 16371 Gradiente: [6.6693699420739e-05,-0.0011506463062014897] Loss: 22.8427379846084\n",
      "Iteracion: 16372 Gradiente: [6.665824291000415e-05,-0.001150034587312021] Loss: 22.842737983280312\n",
      "Iteracion: 16373 Gradiente: [6.662280536886555e-05,-0.0011494231936248175] Loss: 22.84273798195365\n",
      "Iteracion: 16374 Gradiente: [6.658738667416249e-05,-0.0011488121249718355] Loss: 22.84273798062838\n",
      "Iteracion: 16375 Gradiente: [6.65519867235768e-05,-0.0011482013811870454] Loss: 22.84273797930454\n",
      "Iteracion: 16376 Gradiente: [6.651660559574187e-05,-0.0011475909620916268] Loss: 22.842737977982097\n",
      "Iteracion: 16377 Gradiente: [6.648124341002888e-05,-0.0011469808675059312] Loss: 22.842737976661056\n",
      "Iteracion: 16378 Gradiente: [6.644589992580071e-05,-0.0011463710972720994] Loss: 22.842737975341418\n",
      "Iteracion: 16379 Gradiente: [6.641057529748196e-05,-0.0011457616512055087] Loss: 22.84273797402319\n",
      "Iteracion: 16380 Gradiente: [6.63752693517002e-05,-0.001145152529145103] Loss: 22.842737972706356\n",
      "Iteracion: 16381 Gradiente: [6.6339982269407e-05,-0.0011445437309074435] Loss: 22.842737971390928\n",
      "Iteracion: 16382 Gradiente: [6.630471391512552e-05,-0.00114393525632804] Loss: 22.842737970076897\n",
      "Iteracion: 16383 Gradiente: [6.626946424432844e-05,-0.0011433271052354143] Loss: 22.84273796876426\n",
      "Iteracion: 16384 Gradiente: [6.623423342470384e-05,-0.0011427192774489706] Loss: 22.842737967453022\n",
      "Iteracion: 16385 Gradiente: [6.619902125256279e-05,-0.0011421117728070602] Loss: 22.84273796614317\n",
      "Iteracion: 16386 Gradiente: [6.616382780274913e-05,-0.0011415045911322844] Loss: 22.842737964834726\n",
      "Iteracion: 16387 Gradiente: [6.612865306105201e-05,-0.0011408977322557709] Loss: 22.842737963527664\n",
      "Iteracion: 16388 Gradiente: [6.609349702273448e-05,-0.0011402911960029626] Loss: 22.84273796222199\n",
      "Iteracion: 16389 Gradiente: [6.605835970390217e-05,-0.0011396849822001324] Loss: 22.842737960917706\n",
      "Iteracion: 16390 Gradiente: [6.602324108560728e-05,-0.001139079090679592] Loss: 22.842737959614805\n",
      "Iteracion: 16391 Gradiente: [6.59881410778477e-05,-0.0011384735212729424] Loss: 22.842737958313283\n",
      "Iteracion: 16392 Gradiente: [6.595305971283476e-05,-0.0011378682738058643] Loss: 22.842737957013153\n",
      "Iteracion: 16393 Gradiente: [6.59179970976235e-05,-0.0011372633481015507] Loss: 22.84273795571441\n",
      "Iteracion: 16394 Gradiente: [6.58829530531572e-05,-0.0011366587439983534] Loss: 22.842737954417036\n",
      "Iteracion: 16395 Gradiente: [6.584792762396319e-05,-0.0011360544613223075] Loss: 22.842737953121055\n",
      "Iteracion: 16396 Gradiente: [6.581292087920095e-05,-0.0011354504998976722] Loss: 22.842737951826436\n",
      "Iteracion: 16397 Gradiente: [6.577793270518365e-05,-0.0011348468595597202] Loss: 22.842737950533206\n",
      "Iteracion: 16398 Gradiente: [6.574296316728124e-05,-0.001134243540134013] Loss: 22.842737949241343\n",
      "Iteracion: 16399 Gradiente: [6.57080122048607e-05,-0.0011336405414516784] Loss: 22.84273794795085\n",
      "Iteracion: 16400 Gradiente: [6.567307978855297e-05,-0.001133037863344318] Loss: 22.842737946661746\n",
      "Iteracion: 16401 Gradiente: [6.563816594393757e-05,-0.0011324355056382036] Loss: 22.842737945374\n",
      "Iteracion: 16402 Gradiente: [6.560327062080281e-05,-0.001131833468168253] Loss: 22.842737944087627\n",
      "Iteracion: 16403 Gradiente: [6.556839388357123e-05,-0.0011312317507563563] Loss: 22.842737942802614\n",
      "Iteracion: 16404 Gradiente: [6.553353576066456e-05,-0.001130630353232694] Loss: 22.842737941518976\n",
      "Iteracion: 16405 Gradiente: [6.549869616113331e-05,-0.0011300292754301703] Loss: 22.842737940236685\n",
      "Iteracion: 16406 Gradiente: [6.546387499876498e-05,-0.0011294285171840575] Loss: 22.842737938955768\n",
      "Iteracion: 16407 Gradiente: [6.542907240050985e-05,-0.0011288280783157726] Loss: 22.842737937676226\n",
      "Iteracion: 16408 Gradiente: [6.53942883095245e-05,-0.0011282279586603513] Loss: 22.842737936398024\n",
      "Iteracion: 16409 Gradiente: [6.535952277033629e-05,-0.0011276281580428816] Loss: 22.842737935121182\n",
      "Iteracion: 16410 Gradiente: [6.532477560767802e-05,-0.0011270286763032545] Loss: 22.84273793384572\n",
      "Iteracion: 16411 Gradiente: [6.529004696744777e-05,-0.0011264295132638344] Loss: 22.842737932571588\n",
      "Iteracion: 16412 Gradiente: [6.525533675869611e-05,-0.0011258306687601306] Loss: 22.842737931298817\n",
      "Iteracion: 16413 Gradiente: [6.522064506384596e-05,-0.0011252321426165205] Loss: 22.842737930027408\n",
      "Iteracion: 16414 Gradiente: [6.518597169057709e-05,-0.0011246339346753823] Loss: 22.842737928757344\n",
      "Iteracion: 16415 Gradiente: [6.515131681036715e-05,-0.0011240360447563565] Loss: 22.84273792748863\n",
      "Iteracion: 16416 Gradiente: [6.511668037489926e-05,-0.0011234384726930577] Loss: 22.842737926221268\n",
      "Iteracion: 16417 Gradiente: [6.508206241543728e-05,-0.0011228412183129422] Loss: 22.842737924955248\n",
      "Iteracion: 16418 Gradiente: [6.504746278703047e-05,-0.0011222442814556644] Loss: 22.842737923690574\n",
      "Iteracion: 16419 Gradiente: [6.501288155315402e-05,-0.001121647661949036] Loss: 22.842737922427258\n",
      "Iteracion: 16420 Gradiente: [6.497831870054446e-05,-0.0011210513596246585] Loss: 22.84273792116527\n",
      "Iteracion: 16421 Gradiente: [6.49437742159383e-05,-0.0011204553743108174] Loss: 22.842737919904614\n",
      "Iteracion: 16422 Gradiente: [6.490924814954724e-05,-0.0011198597058400613] Loss: 22.842737918645312\n",
      "Iteracion: 16423 Gradiente: [6.487474034126232e-05,-0.0011192643540500313] Loss: 22.842737917387346\n",
      "Iteracion: 16424 Gradiente: [6.484025097961422e-05,-0.0011186693187614338] Loss: 22.842737916130716\n",
      "Iteracion: 16425 Gradiente: [6.480577991775741e-05,-0.001118074599813923] Loss: 22.842737914875432\n",
      "Iteracion: 16426 Gradiente: [6.477132717653452e-05,-0.0011174801970375607] Loss: 22.842737913621477\n",
      "Iteracion: 16427 Gradiente: [6.47368927275238e-05,-0.0011168861102656062] Loss: 22.842737912368857\n",
      "Iteracion: 16428 Gradiente: [6.470247661146307e-05,-0.0011162923393259897] Loss: 22.84273791111756\n",
      "Iteracion: 16429 Gradiente: [6.466807871087591e-05,-0.0011156988840595498] Loss: 22.842737909867598\n",
      "Iteracion: 16430 Gradiente: [6.463369920766127e-05,-0.0011151057442837953] Loss: 22.84273790861895\n",
      "Iteracion: 16431 Gradiente: [6.459933791234108e-05,-0.001114512919844183] Loss: 22.84273790737165\n",
      "Iteracion: 16432 Gradiente: [6.456499499165602e-05,-0.001113920410562841] Loss: 22.84273790612567\n",
      "Iteracion: 16433 Gradiente: [6.453067025139111e-05,-0.0011133282162815543] Loss: 22.842737904881023\n",
      "Iteracion: 16434 Gradiente: [6.449636379291708e-05,-0.0011127363368270693] Loss: 22.84273790363768\n",
      "Iteracion: 16435 Gradiente: [6.446207551675797e-05,-0.0011121447720371454] Loss: 22.842737902395676\n",
      "Iteracion: 16436 Gradiente: [6.442780556407494e-05,-0.0011115535217359233] Loss: 22.842737901154983\n",
      "Iteracion: 16437 Gradiente: [6.439355371412603e-05,-0.0011109625857679125] Loss: 22.84273789991562\n",
      "Iteracion: 16438 Gradiente: [6.435932012228325e-05,-0.0011103719639553589] Loss: 22.84273789867755\n",
      "Iteracion: 16439 Gradiente: [6.432510471465018e-05,-0.0011097816561393377] Loss: 22.842737897440827\n",
      "Iteracion: 16440 Gradiente: [6.429090744006772e-05,-0.001109191662149911] Loss: 22.842737896205385\n",
      "Iteracion: 16441 Gradiente: [6.42567284709609e-05,-0.0011086019818128772] Loss: 22.842737894971275\n",
      "Iteracion: 16442 Gradiente: [6.422256765858947e-05,-0.001108012614968601] Loss: 22.84273789373848\n",
      "Iteracion: 16443 Gradiente: [6.41884249347413e-05,-0.0011074235614549602] Loss: 22.842737892506996\n",
      "Iteracion: 16444 Gradiente: [6.415430038373416e-05,-0.001106834821098346] Loss: 22.842737891276816\n",
      "Iteracion: 16445 Gradiente: [6.412019403683189e-05,-0.0011062463937308328] Loss: 22.842737890047932\n",
      "Iteracion: 16446 Gradiente: [6.408610573487295e-05,-0.0011056582791934963] Loss: 22.842737888820366\n",
      "Iteracion: 16447 Gradiente: [6.405203562091326e-05,-0.0011050704773129638] Loss: 22.842737887594115\n",
      "Iteracion: 16448 Gradiente: [6.401798358695032e-05,-0.0011044829879275871] Loss: 22.84273788636915\n",
      "Iteracion: 16449 Gradiente: [6.39839496974067e-05,-0.0011038958108667174] Loss: 22.842737885145485\n",
      "Iteracion: 16450 Gradiente: [6.394993380827903e-05,-0.0011033089459734431] Loss: 22.842737883923135\n",
      "Iteracion: 16451 Gradiente: [6.391593611567714e-05,-0.001102722393068234] Loss: 22.84273788270209\n",
      "Iteracion: 16452 Gradiente: [6.388195645380771e-05,-0.001102136151995481] Loss: 22.842737881482336\n",
      "Iteracion: 16453 Gradiente: [6.384799479614382e-05,-0.0011015502225905748] Loss: 22.842737880263872\n",
      "Iteracion: 16454 Gradiente: [6.3814051207108e-05,-0.001100964604682512] Loss: 22.842737879046695\n",
      "Iteracion: 16455 Gradiente: [6.378012573691195e-05,-0.0011003792981031306] Loss: 22.84273787783083\n",
      "Iteracion: 16456 Gradiente: [6.374621824628927e-05,-0.0010997943026926768] Loss: 22.842737876616255\n",
      "Iteracion: 16457 Gradiente: [6.37123287532404e-05,-0.0010992096182865414] Loss: 22.842737875402975\n",
      "Iteracion: 16458 Gradiente: [6.367845733734612e-05,-0.001098625244713484] Loss: 22.842737874190956\n",
      "Iteracion: 16459 Gradiente: [6.364460389818305e-05,-0.001098041181812448] Loss: 22.84273787298025\n",
      "Iteracion: 16460 Gradiente: [6.361076854754326e-05,-0.0010974574294120743] Loss: 22.84273787177083\n",
      "Iteracion: 16461 Gradiente: [6.357695105426349e-05,-0.0010968739873600697] Loss: 22.842737870562697\n",
      "Iteracion: 16462 Gradiente: [6.354315163150658e-05,-0.0010962908554774961] Loss: 22.842737869355837\n",
      "Iteracion: 16463 Gradiente: [6.350937011158445e-05,-0.001095708033610876] Loss: 22.84273786815027\n",
      "Iteracion: 16464 Gradiente: [6.347560657597266e-05,-0.0010951255215881398] Loss: 22.84273786694597\n",
      "Iteracion: 16465 Gradiente: [6.344186098203864e-05,-0.0010945433192466917] Loss: 22.84273786574298\n",
      "Iteracion: 16466 Gradiente: [6.340813347852267e-05,-0.001093961426413396] Loss: 22.842737864541242\n",
      "Iteracion: 16467 Gradiente: [6.337442374994376e-05,-0.001093379842941052] Loss: 22.842737863340783\n",
      "Iteracion: 16468 Gradiente: [6.334073197725351e-05,-0.0010927985686539187] Loss: 22.8427378621416\n",
      "Iteracion: 16469 Gradiente: [6.33070580590811e-05,-0.0010922176033941373] Loss: 22.842737860943704\n",
      "Iteracion: 16470 Gradiente: [6.327340204469086e-05,-0.0010916369469936645] Loss: 22.84273785974706\n",
      "Iteracion: 16471 Gradiente: [6.323976398618925e-05,-0.0010910565992843383] Loss: 22.842737858551725\n",
      "Iteracion: 16472 Gradiente: [6.320614374999423e-05,-0.0010904765601080632] Loss: 22.842737857357637\n",
      "Iteracion: 16473 Gradiente: [6.317254147821434e-05,-0.0010898968292932427] Loss: 22.842737856164817\n",
      "Iteracion: 16474 Gradiente: [6.313895699747719e-05,-0.0010893174066862817] Loss: 22.842737854973276\n",
      "Iteracion: 16475 Gradiente: [6.310539038073178e-05,-0.0010887382921166496] Loss: 22.842737853782978\n",
      "Iteracion: 16476 Gradiente: [6.307184160239862e-05,-0.001088159485423527] Loss: 22.842737852593974\n",
      "Iteracion: 16477 Gradiente: [6.303831072595282e-05,-0.0010875809864363836] Loss: 22.84273785140621\n",
      "Iteracion: 16478 Gradiente: [6.30047975960224e-05,-0.0010870027950024526] Loss: 22.842737850219716\n",
      "Iteracion: 16479 Gradiente: [6.2971302306399e-05,-0.0010864249109520329] Loss: 22.842737849034496\n",
      "Iteracion: 16480 Gradiente: [6.293782480118656e-05,-0.001085847334123476] Loss: 22.842737847850525\n",
      "Iteracion: 16481 Gradiente: [6.29043651580711e-05,-0.001085270064348857] Loss: 22.842737846667816\n",
      "Iteracion: 16482 Gradiente: [6.28709232842084e-05,-0.0010846931014693694] Loss: 22.84273784548636\n",
      "Iteracion: 16483 Gradiente: [6.28374991142285e-05,-0.0010841164453259703] Loss: 22.842737844306157\n",
      "Iteracion: 16484 Gradiente: [6.28040927352913e-05,-0.001083540095749195] Loss: 22.842737843127217\n",
      "Iteracion: 16485 Gradiente: [6.27707041303438e-05,-0.0010829640525765664] Loss: 22.84273784194952\n",
      "Iteracion: 16486 Gradiente: [6.273733335338723e-05,-0.0010823883156421724] Loss: 22.842737840773097\n",
      "Iteracion: 16487 Gradiente: [6.270398026989218e-05,-0.0010818128847887465] Loss: 22.842737839597905\n",
      "Iteracion: 16488 Gradiente: [6.267064489880643e-05,-0.0010812377598533375] Loss: 22.842737838423965\n",
      "Iteracion: 16489 Gradiente: [6.263732731402646e-05,-0.001080662940669678] Loss: 22.842737837251274\n",
      "Iteracion: 16490 Gradiente: [6.260402738291759e-05,-0.0010800884270793175] Loss: 22.842737836079834\n",
      "Iteracion: 16491 Gradiente: [6.257074517274456e-05,-0.001079514218917647] Loss: 22.84273783490962\n",
      "Iteracion: 16492 Gradiente: [6.253748061434787e-05,-0.0010789403160255044] Loss: 22.842737833740685\n",
      "Iteracion: 16493 Gradiente: [6.250423380436133e-05,-0.0010783667182330703] Loss: 22.842737832572958\n",
      "Iteracion: 16494 Gradiente: [6.247100465088807e-05,-0.0010777934253852095] Loss: 22.842737831406488\n",
      "Iteracion: 16495 Gradiente: [6.24377931330855e-05,-0.001077220437317076] Loss: 22.842737830241255\n",
      "Iteracion: 16496 Gradiente: [6.240459927179624e-05,-0.0010766477538690347] Loss: 22.84273782907726\n",
      "Iteracion: 16497 Gradiente: [6.237142306607287e-05,-0.001076075374875174] Loss: 22.8427378279145\n",
      "Iteracion: 16498 Gradiente: [6.233826446191414e-05,-0.001075503300178937] Loss: 22.842737826752984\n",
      "Iteracion: 16499 Gradiente: [6.230512353700609e-05,-0.0010749315296113338] Loss: 22.842737825592717\n",
      "Iteracion: 16500 Gradiente: [6.227200025819002e-05,-0.0010743600630130838] Loss: 22.842737824433655\n",
      "Iteracion: 16501 Gradiente: [6.223889454967471e-05,-0.0010737889002265652] Loss: 22.842737823275833\n",
      "Iteracion: 16502 Gradiente: [6.220580640861803e-05,-0.0010732180410894187] Loss: 22.84273782211925\n",
      "Iteracion: 16503 Gradiente: [6.217273588333682e-05,-0.00107264748543822] Loss: 22.842737820963883\n",
      "Iteracion: 16504 Gradiente: [6.213968297856809e-05,-0.0010720772331084779] Loss: 22.84273781980976\n",
      "Iteracion: 16505 Gradiente: [6.210664757683541e-05,-0.0010715072839465971] Loss: 22.84273781865685\n",
      "Iteracion: 16506 Gradiente: [6.2073629814563e-05,-0.0010709376377824023] Loss: 22.842737817505174\n",
      "Iteracion: 16507 Gradiente: [6.204062955437925e-05,-0.0010703682944631272] Loss: 22.84273781635471\n",
      "Iteracion: 16508 Gradiente: [6.200764689670754e-05,-0.0010697992538198993] Loss: 22.84273781520549\n",
      "Iteracion: 16509 Gradiente: [6.197468174112449e-05,-0.0010692305156987678] Loss: 22.84273781405748\n",
      "Iteracion: 16510 Gradiente: [6.194173401657584e-05,-0.001068662079940926] Loss: 22.84273781291068\n",
      "Iteracion: 16511 Gradiente: [6.190880394190875e-05,-0.0010680939463748966] Loss: 22.84273781176511\n",
      "Iteracion: 16512 Gradiente: [6.187589132101342e-05,-0.0010675261148481497] Loss: 22.84273781062075\n",
      "Iteracion: 16513 Gradiente: [6.184299623347063e-05,-0.0010669585851957209] Loss: 22.84273780947763\n",
      "Iteracion: 16514 Gradiente: [6.181011856085661e-05,-0.001066391357263304] Loss: 22.842737808335706\n",
      "Iteracion: 16515 Gradiente: [6.177725837422562e-05,-0.0010658244308872375] Loss: 22.842737807194982\n",
      "Iteracion: 16516 Gradiente: [6.174441567263027e-05,-0.0010652578059046883] Loss: 22.842737806055496\n",
      "Iteracion: 16517 Gradiente: [6.171159050628224e-05,-0.0010646914821543637] Loss: 22.842737804917213\n",
      "Iteracion: 16518 Gradiente: [6.16787827027565e-05,-0.0010641254594834966] Loss: 22.842737803780135\n",
      "Iteracion: 16519 Gradiente: [6.164599237100296e-05,-0.0010635597377261754] Loss: 22.842737802644287\n",
      "Iteracion: 16520 Gradiente: [6.161321944091469e-05,-0.0010629943167250152] Loss: 22.84273780150961\n",
      "Iteracion: 16521 Gradiente: [6.15804640024938e-05,-0.0010624291963163537] Loss: 22.84273780037616\n",
      "Iteracion: 16522 Gradiente: [6.15477259174213e-05,-0.001061864376344346] Loss: 22.84273779924392\n",
      "Iteracion: 16523 Gradiente: [6.151500521980324e-05,-0.0010612998566511323] Loss: 22.84273779811288\n",
      "Iteracion: 16524 Gradiente: [6.148230192953482e-05,-0.0010607356370723408] Loss: 22.842737796983048\n",
      "Iteracion: 16525 Gradiente: [6.144961611198596e-05,-0.001060171717445139] Loss: 22.842737795854404\n",
      "Iteracion: 16526 Gradiente: [6.141694758431034e-05,-0.0010596080976201941] Loss: 22.842737794726965\n",
      "Iteracion: 16527 Gradiente: [6.138429648293217e-05,-0.0010590447774294631] Loss: 22.842737793600712\n",
      "Iteracion: 16528 Gradiente: [6.13516626723746e-05,-0.0010584817567201791] Loss: 22.842737792475663\n",
      "Iteracion: 16529 Gradiente: [6.131904628337755e-05,-0.0010579190353269042] Loss: 22.842737791351826\n",
      "Iteracion: 16530 Gradiente: [6.128644720983326e-05,-0.0010573566130939109] Loss: 22.84273779022916\n",
      "Iteracion: 16531 Gradiente: [6.125386540910919e-05,-0.0010567944898649984] Loss: 22.842737789107705\n",
      "Iteracion: 16532 Gradiente: [6.122130107636774e-05,-0.001056232665470939] Loss: 22.842737787987446\n",
      "Iteracion: 16533 Gradiente: [6.11887538686536e-05,-0.001055671139769861] Loss: 22.84273778686835\n",
      "Iteracion: 16534 Gradiente: [6.115622406070998e-05,-0.001055109912587208] Loss: 22.842737785750476\n",
      "Iteracion: 16535 Gradiente: [6.112371149527007e-05,-0.0010545489837734104] Loss: 22.842737784633762\n",
      "Iteracion: 16536 Gradiente: [6.109121630686332e-05,-0.0010539883531626752] Loss: 22.842737783518263\n",
      "Iteracion: 16537 Gradiente: [6.105873831453815e-05,-0.0010534280206040117] Loss: 22.842737782403933\n",
      "Iteracion: 16538 Gradiente: [6.102627760450711e-05,-0.0010528679859337585] Loss: 22.84273778129079\n",
      "Iteracion: 16539 Gradiente: [6.099383420234972e-05,-0.0010523082489923988] Loss: 22.842737780178823\n",
      "Iteracion: 16540 Gradiente: [6.096140802280085e-05,-0.0010517488096255078] Loss: 22.842737779068045\n",
      "Iteracion: 16541 Gradiente: [6.09289990563866e-05,-0.0010511896676762926] Loss: 22.842737777958455\n",
      "Iteracion: 16542 Gradiente: [6.089660734289737e-05,-0.001050630822982749] Loss: 22.84273777685004\n",
      "Iteracion: 16543 Gradiente: [6.08642328482271e-05,-0.0010500722753878477] Loss: 22.842737775742798\n",
      "Iteracion: 16544 Gradiente: [6.083187559606055e-05,-0.0010495140247315977] Loss: 22.842737774636735\n",
      "Iteracion: 16545 Gradiente: [6.0799535441447004e-05,-0.0010489560708654957] Loss: 22.842737773531848\n",
      "Iteracion: 16546 Gradiente: [6.076721255017977e-05,-0.001048398413620551] Loss: 22.84273777242813\n",
      "Iteracion: 16547 Gradiente: [6.0734906807624614e-05,-0.001047841052844826] Loss: 22.842737771325606\n",
      "Iteracion: 16548 Gradiente: [6.070261817683331e-05,-0.0010472839883822378] Loss: 22.84273777022423\n",
      "Iteracion: 16549 Gradiente: [6.067034683212569e-05,-0.0010467272200673486] Loss: 22.842737769124035\n",
      "Iteracion: 16550 Gradiente: [6.063809268160488e-05,-0.001046170747744076] Loss: 22.842737768025007\n",
      "Iteracion: 16551 Gradiente: [6.0605855535792824e-05,-0.0010456145712665213] Loss: 22.84273776692714\n",
      "Iteracion: 16552 Gradiente: [6.057363567795922e-05,-0.0010450586904610759] Loss: 22.842737765830453\n",
      "Iteracion: 16553 Gradiente: [6.054143287220389e-05,-0.0010445031051843283] Loss: 22.84273776473493\n",
      "Iteracion: 16554 Gradiente: [6.050924713557985e-05,-0.0010439478152759326] Loss: 22.84273776364058\n",
      "Iteracion: 16555 Gradiente: [6.047707854008877e-05,-0.0010433928205748324] Loss: 22.842737762547376\n",
      "Iteracion: 16556 Gradiente: [6.044492713688972e-05,-0.0010428381209209193] Loss: 22.842737761455343\n",
      "Iteracion: 16557 Gradiente: [6.0412792792400674e-05,-0.0010422837161633207] Loss: 22.842737760364464\n",
      "Iteracion: 16558 Gradiente: [6.0380675464936454e-05,-0.0010417296061498622] Loss: 22.842737759274755\n",
      "Iteracion: 16559 Gradiente: [6.034857518765572e-05,-0.0010411757907187772] Loss: 22.84273775818618\n",
      "Iteracion: 16560 Gradiente: [6.0316492025928406e-05,-0.0010406222697103118] Loss: 22.842737757098796\n",
      "Iteracion: 16561 Gradiente: [6.028442591438458e-05,-0.0010400690429711072] Loss: 22.842737756012536\n",
      "Iteracion: 16562 Gradiente: [6.0252376888077684e-05,-0.0010395161103416465] Loss: 22.842737754927466\n",
      "Iteracion: 16563 Gradiente: [6.0220344920480784e-05,-0.00103896347166715] Loss: 22.84273775384352\n",
      "Iteracion: 16564 Gradiente: [6.018832992348659e-05,-0.0010384111267957982] Loss: 22.842737752760737\n",
      "Iteracion: 16565 Gradiente: [6.015633195583329e-05,-0.001037859075567127] Loss: 22.842737751679103\n",
      "Iteracion: 16566 Gradiente: [6.012435104025826e-05,-0.0010373073178244615] Loss: 22.842737750598623\n",
      "Iteracion: 16567 Gradiente: [6.00923870592851e-05,-0.0010367558534162195] Loss: 22.84273774951928\n",
      "Iteracion: 16568 Gradiente: [6.0060440056493765e-05,-0.0010362046821843051] Loss: 22.8427377484411\n",
      "Iteracion: 16569 Gradiente: [6.002851015030804e-05,-0.0010356538039645832] Loss: 22.84273774736405\n",
      "Iteracion: 16570 Gradiente: [5.9996597174934624e-05,-0.0010351032186123395] Loss: 22.84273774628817\n",
      "Iteracion: 16571 Gradiente: [5.996470111521527e-05,-0.0010345529259700706] Loss: 22.84273774521341\n",
      "Iteracion: 16572 Gradiente: [5.993282193893871e-05,-0.0010340029258851283] Loss: 22.842737744139786\n",
      "Iteracion: 16573 Gradiente: [5.9900959805266514e-05,-0.001033453218192193] Loss: 22.84273774306732\n",
      "Iteracion: 16574 Gradiente: [5.986911471040912e-05,-0.001032903802734353] Loss: 22.842737741996007\n",
      "Iteracion: 16575 Gradiente: [5.9837286429835027e-05,-0.0010323546793686708] Loss: 22.842737740925806\n",
      "Iteracion: 16576 Gradiente: [5.980547510281061e-05,-0.0010318058479323138] Loss: 22.842737739856773\n",
      "Iteracion: 16577 Gradiente: [5.977368070849328e-05,-0.0010312573082706205] Loss: 22.84273773878884\n",
      "Iteracion: 16578 Gradiente: [5.974190320330308e-05,-0.0010307090602301135] Loss: 22.842737737722068\n",
      "Iteracion: 16579 Gradiente: [5.971014251334358e-05,-0.0010301611036599213] Loss: 22.842737736656417\n",
      "Iteracion: 16580 Gradiente: [5.967839873998552e-05,-0.001029613438399224] Loss: 22.84273773559191\n",
      "Iteracion: 16581 Gradiente: [5.9646671936282776e-05,-0.0010290660642880312] Loss: 22.84273773452852\n",
      "Iteracion: 16582 Gradiente: [5.961496187959862e-05,-0.001028518981184471] Loss: 22.842737733466272\n",
      "Iteracion: 16583 Gradiente: [5.9583268746147645e-05,-0.0010279721889228691] Loss: 22.842737732405137\n",
      "Iteracion: 16584 Gradiente: [5.9551592482876006e-05,-0.0010274256873519978] Loss: 22.84273773134515\n",
      "Iteracion: 16585 Gradiente: [5.951993300925551e-05,-0.0010268794763207485] Loss: 22.84273773028628\n",
      "Iteracion: 16586 Gradiente: [5.9488290403919564e-05,-0.0010263335556705518] Loss: 22.842737729228546\n",
      "Iteracion: 16587 Gradiente: [5.945666459581389e-05,-0.001025787925250299] Loss: 22.842737728171922\n",
      "Iteracion: 16588 Gradiente: [5.942505557167503e-05,-0.0010252425849044992] Loss: 22.842737727116422\n",
      "Iteracion: 16589 Gradiente: [5.939346340824159e-05,-0.0010246975344765966] Loss: 22.842737726062055\n",
      "Iteracion: 16590 Gradiente: [5.936188798424761e-05,-0.0010241527738159562] Loss: 22.842737725008806\n",
      "Iteracion: 16591 Gradiente: [5.93303293584313e-05,-0.0010236083027672057] Loss: 22.84273772395666\n",
      "Iteracion: 16592 Gradiente: [5.9298787569635654e-05,-0.0010230641211728417] Loss: 22.84273772290565\n",
      "Iteracion: 16593 Gradiente: [5.926726247669952e-05,-0.001022520228885308] Loss: 22.842737721855755\n",
      "Iteracion: 16594 Gradiente: [5.923575419520451e-05,-0.0010219766257446139] Loss: 22.842737720806962\n",
      "Iteracion: 16595 Gradiente: [5.9204262679675894e-05,-0.0010214333115991772] Loss: 22.842737719759313\n",
      "Iteracion: 16596 Gradiente: [5.9172787892218064e-05,-0.001020890286297534] Loss: 22.84273771871276\n",
      "Iteracion: 16597 Gradiente: [5.914132978072454e-05,-0.0010203475496881017] Loss: 22.84273771766731\n",
      "Iteracion: 16598 Gradiente: [5.9109888467408686e-05,-0.0010198051016089948] Loss: 22.842737716622985\n",
      "Iteracion: 16599 Gradiente: [5.907846380637238e-05,-0.0010192629419164471] Loss: 22.842737715579766\n",
      "Iteracion: 16600 Gradiente: [5.904705586298557e-05,-0.0010187210704515337] Loss: 22.842737714537652\n",
      "Iteracion: 16601 Gradiente: [5.90156646315639e-05,-0.0010181794870608959] Loss: 22.84273771349665\n",
      "Iteracion: 16602 Gradiente: [5.8984290168003405e-05,-0.0010176381915889246] Loss: 22.84273771245676\n",
      "Iteracion: 16603 Gradiente: [5.895293234061683e-05,-0.0010170971838875905] Loss: 22.842737711417964\n",
      "Iteracion: 16604 Gradiente: [5.892159114466722e-05,-0.0010165564638055476] Loss: 22.842737710380273\n",
      "Iteracion: 16605 Gradiente: [5.889026660478673e-05,-0.0010160160311873056] Loss: 22.842737709343698\n",
      "Iteracion: 16606 Gradiente: [5.885895870771189e-05,-0.0010154758858808084] Loss: 22.84273770830822\n",
      "Iteracion: 16607 Gradiente: [5.8827667436389676e-05,-0.0010149360277324603] Loss: 22.84273770727384\n",
      "Iteracion: 16608 Gradiente: [5.879639291208605e-05,-0.0010143964565815604] Loss: 22.842737706240545\n",
      "Iteracion: 16609 Gradiente: [5.876513493869121e-05,-0.001013857172289434] Loss: 22.842737705208382\n",
      "Iteracion: 16610 Gradiente: [5.87338936014703e-05,-0.001013318174695499] Loss: 22.842737704177285\n",
      "Iteracion: 16611 Gradiente: [5.870266888621245e-05,-0.0010127794636495934] Loss: 22.842737703147304\n",
      "Iteracion: 16612 Gradiente: [5.867146069249429e-05,-0.0010122410390021486] Loss: 22.842737702118384\n",
      "Iteracion: 16613 Gradiente: [5.864026908568576e-05,-0.0010117029005981474] Loss: 22.84273770109059\n",
      "Iteracion: 16614 Gradiente: [5.860909415768371e-05,-0.001011165048279968] Loss: 22.84273770006389\n",
      "Iteracion: 16615 Gradiente: [5.857793574458962e-05,-0.0010106274819038437] Loss: 22.842737699038274\n",
      "Iteracion: 16616 Gradiente: [5.8546793948721645e-05,-0.0010100902013115605] Loss: 22.842737698013742\n",
      "Iteracion: 16617 Gradiente: [5.851566869239377e-05,-0.0010095532063542597] Loss: 22.842737696990305\n",
      "Iteracion: 16618 Gradiente: [5.8484559923499545e-05,-0.0010090164968852141] Loss: 22.842737695967955\n",
      "Iteracion: 16619 Gradiente: [5.8453467817306165e-05,-0.0010084800727394594] Loss: 22.84273769494669\n",
      "Iteracion: 16620 Gradiente: [5.842239208959654e-05,-0.0010079439337817556] Loss: 22.842737693926516\n",
      "Iteracion: 16621 Gradiente: [5.8391333038798623e-05,-0.0010074080798422832] Loss: 22.84273769290742\n",
      "Iteracion: 16622 Gradiente: [5.83602903418523e-05,-0.0010068725107887624] Loss: 22.84273769188941\n",
      "Iteracion: 16623 Gradiente: [5.832926423844735e-05,-0.0010063372264549268] Loss: 22.84273769087249\n",
      "Iteracion: 16624 Gradiente: [5.829825459121215e-05,-0.0010058022266974831] Loss: 22.842737689856655\n",
      "Iteracion: 16625 Gradiente: [5.8267261442779274e-05,-0.0010052675113611778] Loss: 22.84273768884187\n",
      "Iteracion: 16626 Gradiente: [5.823628477704309e-05,-0.0010047330802954945] Loss: 22.84273768782818\n",
      "Iteracion: 16627 Gradiente: [5.820532470105869e-05,-0.0010041989333432848] Loss: 22.84273768681558\n",
      "Iteracion: 16628 Gradiente: [5.817438086334429e-05,-0.0010036650703710848] Loss: 22.842737685804035\n",
      "Iteracion: 16629 Gradiente: [5.814345360780256e-05,-0.0010031314912114435] Loss: 22.842737684793576\n",
      "Iteracion: 16630 Gradiente: [5.811254272695502e-05,-0.0010025981957207128] Loss: 22.842737683784204\n",
      "Iteracion: 16631 Gradiente: [5.8081648295645515e-05,-0.0010020651837457707] Loss: 22.842737682775887\n",
      "Iteracion: 16632 Gradiente: [5.805077032050576e-05,-0.00100153245513539] Loss: 22.842737681768646\n",
      "Iteracion: 16633 Gradiente: [5.801990874469235e-05,-0.0010010000097409489] Loss: 22.84273768076248\n",
      "Iteracion: 16634 Gradiente: [5.7989063617469586e-05,-0.0010004678474084963] Loss: 22.842737679757377\n",
      "Iteracion: 16635 Gradiente: [5.7958234889573154e-05,-0.0009999359679890554] Loss: 22.842737678753338\n",
      "Iteracion: 16636 Gradiente: [5.7927422495633134e-05,-0.0009994043713361358] Loss: 22.842737677750375\n",
      "Iteracion: 16637 Gradiente: [5.789662651523031e-05,-0.0009988730572960236] Loss: 22.842737676748484\n",
      "Iteracion: 16638 Gradiente: [5.78658468195196e-05,-0.0009983420257224651] Loss: 22.84273767574765\n",
      "Iteracion: 16639 Gradiente: [5.7835083574294306e-05,-0.000997811276456891] Loss: 22.84273767474787\n",
      "Iteracion: 16640 Gradiente: [5.780433670755277e-05,-0.0009972808093518637] Loss: 22.84273767374917\n",
      "Iteracion: 16641 Gradiente: [5.777360609613424e-05,-0.0009967506242661037] Loss: 22.84273767275152\n",
      "Iteracion: 16642 Gradiente: [5.774289194278026e-05,-0.0009962207210345278] Loss: 22.84273767175494\n",
      "Iteracion: 16643 Gradiente: [5.77121940248541e-05,-0.0009956910995213045] Loss: 22.842737670759416\n",
      "Iteracion: 16644 Gradiente: [5.768151237835658e-05,-0.0009951617595741406] Loss: 22.84273766976495\n",
      "Iteracion: 16645 Gradiente: [5.7650847110342815e-05,-0.0009946327010374272] Loss: 22.842737668771544\n",
      "Iteracion: 16646 Gradiente: [5.762019811091553e-05,-0.0009941039237655028] Loss: 22.842737667779183\n",
      "Iteracion: 16647 Gradiente: [5.7589565504182856e-05,-0.0009935754276021668] Loss: 22.842737666787887\n",
      "Iteracion: 16648 Gradiente: [5.7558949135720167e-05,-0.0009930472124066133] Loss: 22.842737665797635\n",
      "Iteracion: 16649 Gradiente: [5.752834901973832e-05,-0.0009925192780277333] Loss: 22.84273766480845\n",
      "Iteracion: 16650 Gradiente: [5.749776520076466e-05,-0.0009919916243134707] Loss: 22.842737663820298\n",
      "Iteracion: 16651 Gradiente: [5.746719764943009e-05,-0.0009914642511144934] Loss: 22.84273766283322\n",
      "Iteracion: 16652 Gradiente: [5.74366462880486e-05,-0.0009909371582878634] Loss: 22.842737661847174\n",
      "Iteracion: 16653 Gradiente: [5.74061112142014e-05,-0.0009904103456785643] Loss: 22.842737660862177\n",
      "Iteracion: 16654 Gradiente: [5.737559236915028e-05,-0.0009898838131370269] Loss: 22.842737659878242\n",
      "Iteracion: 16655 Gradiente: [5.734508976521132e-05,-0.000989357560517353] Loss: 22.842737658895324\n",
      "Iteracion: 16656 Gradiente: [5.731460336354151e-05,-0.0009888315876694993] Loss: 22.842737657913478\n",
      "Iteracion: 16657 Gradiente: [5.728413317835172e-05,-0.000988305894444134] Loss: 22.84273765693267\n",
      "Iteracion: 16658 Gradiente: [5.7253679127218975e-05,-0.0009877804806978456] Loss: 22.842737655952895\n",
      "Iteracion: 16659 Gradiente: [5.722324133804098e-05,-0.000987255346273012] Loss: 22.84273765497418\n",
      "Iteracion: 16660 Gradiente: [5.719281971039436e-05,-0.0009867304910272877] Loss: 22.842737653996476\n",
      "Iteracion: 16661 Gradiente: [5.716241420543611e-05,-0.0009862059148139461] Loss: 22.84273765301983\n",
      "Iteracion: 16662 Gradiente: [5.7132024968116944e-05,-0.000985681617474062] Loss: 22.84273765204421\n",
      "Iteracion: 16663 Gradiente: [5.710165187906568e-05,-0.0009851575988675402] Loss: 22.84273765106965\n",
      "Iteracion: 16664 Gradiente: [5.707129486059633e-05,-0.0009846338588517984] Loss: 22.842737650096115\n",
      "Iteracion: 16665 Gradiente: [5.704095400555313e-05,-0.0009841103972693332] Loss: 22.842737649123613\n",
      "Iteracion: 16666 Gradiente: [5.701062928172481e-05,-0.0009835872139751936] Loss: 22.84273764815215\n",
      "Iteracion: 16667 Gradiente: [5.698032062374144e-05,-0.0009830643088245476] Loss: 22.842737647181703\n",
      "Iteracion: 16668 Gradiente: [5.695002811686815e-05,-0.0009825416816643914] Loss: 22.842737646212292\n",
      "Iteracion: 16669 Gradiente: [5.69197517942636e-05,-0.0009820193323444451] Loss: 22.84273764524392\n",
      "Iteracion: 16670 Gradiente: [5.688949147402885e-05,-0.0009814972607263901] Loss: 22.84273764427658\n",
      "Iteracion: 16671 Gradiente: [5.68592472149021e-05,-0.0009809754666591174] Loss: 22.84273764331027\n",
      "Iteracion: 16672 Gradiente: [5.6829019073726764e-05,-0.0009804539499917552] Loss: 22.842737642344975\n",
      "Iteracion: 16673 Gradiente: [5.6798807055239804e-05,-0.000979932710576037] Loss: 22.842737641380715\n",
      "Iteracion: 16674 Gradiente: [5.676861110259779e-05,-0.0009794117482651169] Loss: 22.84273764041747\n",
      "Iteracion: 16675 Gradiente: [5.6738431147588624e-05,-0.0009788910629188999] Loss: 22.84273763945526\n",
      "Iteracion: 16676 Gradiente: [5.670826732379434e-05,-0.0009783706543788166] Loss: 22.842737638494064\n",
      "Iteracion: 16677 Gradiente: [5.667811938584085e-05,-0.0009778505225134164] Loss: 22.842737637533894\n",
      "Iteracion: 16678 Gradiente: [5.6647987616997855e-05,-0.000977330667157498] Loss: 22.842737636574746\n",
      "Iteracion: 16679 Gradiente: [5.661787180410253e-05,-0.0009768110881754667] Loss: 22.84273763561662\n",
      "Iteracion: 16680 Gradiente: [5.6587772079789525e-05,-0.0009762917854136077] Loss: 22.84273763465951\n",
      "Iteracion: 16681 Gradiente: [5.655768823942253e-05,-0.000975772758735971] Loss: 22.842737633703415\n",
      "Iteracion: 16682 Gradiente: [5.6527620355950606e-05,-0.0009752540079901451] Loss: 22.842737632748335\n",
      "Iteracion: 16683 Gradiente: [5.649756856674533e-05,-0.0009747355330240737] Loss: 22.842737631794286\n",
      "Iteracion: 16684 Gradiente: [5.646753272685601e-05,-0.00097421733369553] Loss: 22.842737630841224\n",
      "Iteracion: 16685 Gradiente: [5.643751290828429e-05,-0.0009736994098536419] Loss: 22.842737629889196\n",
      "Iteracion: 16686 Gradiente: [5.640750897650075e-05,-0.0009731817613612747] Loss: 22.842737628938163\n",
      "Iteracion: 16687 Gradiente: [5.637752103382354e-05,-0.0009726643880649514] Loss: 22.842737627988146\n",
      "Iteracion: 16688 Gradiente: [5.634754897319756e-05,-0.0009721472898226816] Loss: 22.84273762703916\n",
      "Iteracion: 16689 Gradiente: [5.6317592838202774e-05,-0.0009716304664859621] Loss: 22.84273762609116\n",
      "Iteracion: 16690 Gradiente: [5.6287652740631225e-05,-0.0009711139179016707] Loss: 22.842737625144178\n",
      "Iteracion: 16691 Gradiente: [5.625772845595141e-05,-0.0009705976439368176] Loss: 22.842737624198197\n",
      "Iteracion: 16692 Gradiente: [5.622782010163974e-05,-0.0009700816444394651] Loss: 22.842737623253228\n",
      "Iteracion: 16693 Gradiente: [5.6197927666327514e-05,-0.0009695659192595703] Loss: 22.84273762230925\n",
      "Iteracion: 16694 Gradiente: [5.6168051164225595e-05,-0.0009690504682541956] Loss: 22.842737621366293\n",
      "Iteracion: 16695 Gradiente: [5.6138190541332736e-05,-0.0009685352912782719] Loss: 22.842737620424316\n",
      "Iteracion: 16696 Gradiente: [5.61083457228051e-05,-0.0009680203881902826] Loss: 22.842737619483355\n",
      "Iteracion: 16697 Gradiente: [5.6078516791065644e-05,-0.0009675057588407772] Loss: 22.842737618543392\n",
      "Iteracion: 16698 Gradiente: [5.604870378685216e-05,-0.0009669914030773441] Loss: 22.842737617604442\n",
      "Iteracion: 16699 Gradiente: [5.601890656331913e-05,-0.000966477320767467] Loss: 22.84273761666647\n",
      "Iteracion: 16700 Gradiente: [5.5989125197205185e-05,-0.000965963511757432] Loss: 22.842737615729508\n",
      "Iteracion: 16701 Gradiente: [5.595935963640386e-05,-0.0009654499759063147] Loss: 22.842737614793542\n",
      "Iteracion: 16702 Gradiente: [5.5929609945337686e-05,-0.0009649367130633616] Loss: 22.842737613858564\n",
      "Iteracion: 16703 Gradiente: [5.589987599231942e-05,-0.0009644237230912011] Loss: 22.842737612924566\n",
      "Iteracion: 16704 Gradiente: [5.587015789387806e-05,-0.0009639110058381324] Loss: 22.84273761199159\n",
      "Iteracion: 16705 Gradiente: [5.5840455629171024e-05,-0.000963398561158139] Loss: 22.842737611059594\n",
      "Iteracion: 16706 Gradiente: [5.581076916219748e-05,-0.0009628863889104149] Loss: 22.842737610128584\n",
      "Iteracion: 16707 Gradiente: [5.578109844558791e-05,-0.0009623744889498909] Loss: 22.842737609198576\n",
      "Iteracion: 16708 Gradiente: [5.5751443532396176e-05,-0.0009618628611294847] Loss: 22.842737608269537\n",
      "Iteracion: 16709 Gradiente: [5.572180432030412e-05,-0.0009613515053105222] Loss: 22.842737607341498\n",
      "Iteracion: 16710 Gradiente: [5.5692180903103385e-05,-0.0009608404213408288] Loss: 22.84273760641445\n",
      "Iteracion: 16711 Gradiente: [5.566257320879231e-05,-0.0009603296090820852] Loss: 22.84273760548837\n",
      "Iteracion: 16712 Gradiente: [5.563298129421431e-05,-0.0009598190683841305] Loss: 22.842737604563283\n",
      "Iteracion: 16713 Gradiente: [5.5603405072209475e-05,-0.0009593087991063953] Loss: 22.84273760363918\n",
      "Iteracion: 16714 Gradiente: [5.557384463846423e-05,-0.0009587988011008501] Loss: 22.842737602716063\n",
      "Iteracion: 16715 Gradiente: [5.5544299826237875e-05,-0.0009582890742317811] Loss: 22.842737601793928\n",
      "Iteracion: 16716 Gradiente: [5.5514770789007643e-05,-0.0009577796183439346] Loss: 22.84273760087277\n",
      "Iteracion: 16717 Gradiente: [5.548525744529798e-05,-0.000957270433298163] Loss: 22.84273759995258\n",
      "Iteracion: 16718 Gradiente: [5.545575974489717e-05,-0.0009567615189549628] Loss: 22.842737599033388\n",
      "Iteracion: 16719 Gradiente: [5.542627776738603e-05,-0.0009562528751629884] Loss: 22.84273759811516\n",
      "Iteracion: 16720 Gradiente: [5.5396811426552024e-05,-0.0009557445017828551] Loss: 22.842737597197914\n",
      "Iteracion: 16721 Gradiente: [5.536736087018805e-05,-0.0009552363986652305] Loss: 22.842737596281648\n",
      "Iteracion: 16722 Gradiente: [5.533792582828786e-05,-0.0009547285656783088] Loss: 22.84273759536634\n",
      "Iteracion: 16723 Gradiente: [5.5308506509277323e-05,-0.0009542210026656524] Loss: 22.842737594452025\n",
      "Iteracion: 16724 Gradiente: [5.527910280231178e-05,-0.0009537137094911922] Loss: 22.842737593538654\n",
      "Iteracion: 16725 Gradiente: [5.5249714715917735e-05,-0.0009532066860105696] Loss: 22.842737592626282\n",
      "Iteracion: 16726 Gradiente: [5.5220342242516075e-05,-0.0009526999320804919] Loss: 22.842737591714876\n",
      "Iteracion: 16727 Gradiente: [5.519098535842204e-05,-0.0009521934475573109] Loss: 22.842737590804425\n",
      "Iteracion: 16728 Gradiente: [5.5161644164059e-05,-0.0009516872322912207] Loss: 22.84273758989496\n",
      "Iteracion: 16729 Gradiente: [5.5132318577951386e-05,-0.0009511812861442574] Loss: 22.84273758898643\n",
      "Iteracion: 16730 Gradiente: [5.510300860578354e-05,-0.0009506756089726546] Loss: 22.842737588078883\n",
      "Iteracion: 16731 Gradiente: [5.5073714128184294e-05,-0.0009501702006398697] Loss: 22.842737587172312\n",
      "Iteracion: 16732 Gradiente: [5.504443529010435e-05,-0.0009496650609937281] Loss: 22.842737586266697\n",
      "Iteracion: 16733 Gradiente: [5.5015171997752074e-05,-0.0009491601898967398] Loss: 22.84273758536205\n",
      "Iteracion: 16734 Gradiente: [5.498592422175837e-05,-0.0009486555872058489] Loss: 22.84273758445836\n",
      "Iteracion: 16735 Gradiente: [5.495669200191363e-05,-0.0009481512527779993] Loss: 22.84273758355562\n",
      "Iteracion: 16736 Gradiente: [5.4927475293690504e-05,-0.0009476471864713194] Loss: 22.842737582653857\n",
      "Iteracion: 16737 Gradiente: [5.489827417666978e-05,-0.0009471433881382533] Loss: 22.84273758175304\n",
      "Iteracion: 16738 Gradiente: [5.486908854379635e-05,-0.0009466398576420213] Loss: 22.842737580853186\n",
      "Iteracion: 16739 Gradiente: [5.4839918436755394e-05,-0.0009461365948377913] Loss: 22.842737579954285\n",
      "Iteracion: 16740 Gradiente: [5.481076387828428e-05,-0.0009456335995812044] Loss: 22.842737579056344\n",
      "Iteracion: 16741 Gradiente: [5.478162479638134e-05,-0.0009451308717338236] Loss: 22.842737578159362\n",
      "Iteracion: 16742 Gradiente: [5.475250117778311e-05,-0.0009446284111529479] Loss: 22.842737577263318\n",
      "Iteracion: 16743 Gradiente: [5.4723393056595644e-05,-0.0009441262176945742] Loss: 22.84273757636823\n",
      "Iteracion: 16744 Gradiente: [5.469430041860808e-05,-0.0009436242912182517] Loss: 22.842737575474107\n",
      "Iteracion: 16745 Gradiente: [5.466522329034736e-05,-0.0009431226315786745] Loss: 22.842737574580926\n",
      "Iteracion: 16746 Gradiente: [5.463616160265398e-05,-0.0009426212386363394] Loss: 22.84273757368868\n",
      "Iteracion: 16747 Gradiente: [5.4607115303421475e-05,-0.0009421201122550589] Loss: 22.842737572797414\n",
      "Iteracion: 16748 Gradiente: [5.457808448833627e-05,-0.0009416192522857376] Loss: 22.842737571907083\n",
      "Iteracion: 16749 Gradiente: [5.45490690948706e-05,-0.0009411186585882803] Loss: 22.84273757101768\n",
      "Iteracion: 16750 Gradiente: [5.4520069146709225e-05,-0.0009406183310216439] Loss: 22.842737570129227\n",
      "Iteracion: 16751 Gradiente: [5.449108466090517e-05,-0.0009401182694413516] Loss: 22.84273756924173\n",
      "Iteracion: 16752 Gradiente: [5.4462115465033396e-05,-0.0009396184737170188] Loss: 22.842737568355194\n",
      "Iteracion: 16753 Gradiente: [5.44331617362559e-05,-0.0009391189436941024] Loss: 22.842737567469573\n",
      "Iteracion: 16754 Gradiente: [5.440422335899105e-05,-0.0009386196792402046] Loss: 22.8427375665849\n",
      "Iteracion: 16755 Gradiente: [5.437530051134824e-05,-0.0009381206802012561] Loss: 22.84273756570117\n",
      "Iteracion: 16756 Gradiente: [5.434639287310953e-05,-0.000937621946455991] Loss: 22.842737564818382\n",
      "Iteracion: 16757 Gradiente: [5.4317500643226896e-05,-0.0009371234778499845] Loss: 22.842737563936513\n",
      "Iteracion: 16758 Gradiente: [5.428862372601391e-05,-0.000936625274248352] Loss: 22.842737563055593\n",
      "Iteracion: 16759 Gradiente: [5.4259762318527764e-05,-0.0009361273354967873] Loss: 22.84273756217562\n",
      "Iteracion: 16760 Gradiente: [5.423091615644656e-05,-0.0009356296614718929] Loss: 22.842737561296584\n",
      "Iteracion: 16761 Gradiente: [5.420208530987717e-05,-0.0009351322520272968] Loss: 22.842737560418467\n",
      "Iteracion: 16762 Gradiente: [5.417326983282085e-05,-0.0009346351070169827] Loss: 22.842737559541284\n",
      "Iteracion: 16763 Gradiente: [5.4144469656118116e-05,-0.0009341382263059472] Loss: 22.842737558665043\n",
      "Iteracion: 16764 Gradiente: [5.411568487545537e-05,-0.0009336416097466345] Loss: 22.84273755778973\n",
      "Iteracion: 16765 Gradiente: [5.408691530230196e-05,-0.000933145257209489] Loss: 22.84273755691535\n",
      "Iteracion: 16766 Gradiente: [5.405816099255389e-05,-0.0009326491685507439] Loss: 22.84273755604191\n",
      "Iteracion: 16767 Gradiente: [5.4029421968001164e-05,-0.000932153343628291] Loss: 22.842737555169382\n",
      "Iteracion: 16768 Gradiente: [5.400069822201203e-05,-0.0009316577823021532] Loss: 22.84273755429778\n",
      "Iteracion: 16769 Gradiente: [5.39719898521677e-05,-0.0009311624844238271] Loss: 22.84273755342711\n",
      "Iteracion: 16770 Gradiente: [5.3943296753307855e-05,-0.0009306674498625729] Loss: 22.84273755255736\n",
      "Iteracion: 16771 Gradiente: [5.391461883827257e-05,-0.0009301726784791242] Loss: 22.842737551688547\n",
      "Iteracion: 16772 Gradiente: [5.388595619137959e-05,-0.0009296781701308987] Loss: 22.842737550820644\n",
      "Iteracion: 16773 Gradiente: [5.385730880694458e-05,-0.0009291839246762615] Loss: 22.842737549953664\n",
      "Iteracion: 16774 Gradiente: [5.382867652296378e-05,-0.0009286899419857756] Loss: 22.84273754908762\n",
      "Iteracion: 16775 Gradiente: [5.3800059493861836e-05,-0.0009281962219090426] Loss: 22.84273754822249\n",
      "Iteracion: 16776 Gradiente: [5.377145776985041e-05,-0.0009277027643051384] Loss: 22.842737547358272\n",
      "Iteracion: 16777 Gradiente: [5.374287123819007e-05,-0.0009272095690377568] Loss: 22.842737546494973\n",
      "Iteracion: 16778 Gradiente: [5.37142998505639e-05,-0.000926716635971303] Loss: 22.8427375456326\n",
      "Iteracion: 16779 Gradiente: [5.3685743630656664e-05,-0.0009262239649650894] Loss: 22.842737544771136\n",
      "Iteracion: 16780 Gradiente: [5.3657202675102177e-05,-0.0009257315558730995] Loss: 22.84273754391059\n",
      "Iteracion: 16781 Gradiente: [5.362867688347705e-05,-0.0009252394085624615] Loss: 22.842737543050966\n",
      "Iteracion: 16782 Gradiente: [5.360016623399133e-05,-0.0009247475228924884] Loss: 22.84273754219225\n",
      "Iteracion: 16783 Gradiente: [5.357167072285544e-05,-0.0009242558987258083] Loss: 22.842737541334454\n",
      "Iteracion: 16784 Gradiente: [5.3543190387964995e-05,-0.000923764535919247] Loss: 22.84273754047754\n",
      "Iteracion: 16785 Gradiente: [5.35147251895296e-05,-0.0009232734343376829] Loss: 22.842737539621574\n",
      "Iteracion: 16786 Gradiente: [5.348627519007702e-05,-0.0009227825938362837] Loss: 22.84273753876651\n",
      "Iteracion: 16787 Gradiente: [5.3457840196339626e-05,-0.0009222920142870332] Loss: 22.842737537912345\n",
      "Iteracion: 16788 Gradiente: [5.342942033716251e-05,-0.0009218016955452175] Loss: 22.84273753705909\n",
      "Iteracion: 16789 Gradiente: [5.340101559170307e-05,-0.0009213116374702679] Loss: 22.842737536206744\n",
      "Iteracion: 16790 Gradiente: [5.337262597606696e-05,-0.0009208218399250494] Loss: 22.842737535355305\n",
      "Iteracion: 16791 Gradiente: [5.3344251477938086e-05,-0.0009203323027688744] Loss: 22.842737534504767\n",
      "Iteracion: 16792 Gradiente: [5.3315892066052585e-05,-0.0009198430258655558] Loss: 22.84273753365513\n",
      "Iteracion: 16793 Gradiente: [5.328754767219834e-05,-0.0009193540090797352] Loss: 22.842737532806407\n",
      "Iteracion: 16794 Gradiente: [5.325921837785093e-05,-0.0009188652522681195] Loss: 22.84273753195858\n",
      "Iteracion: 16795 Gradiente: [5.323090415269386e-05,-0.0009183767552961797] Loss: 22.84273753111166\n",
      "Iteracion: 16796 Gradiente: [5.3202604982516276e-05,-0.0009178885180220438] Loss: 22.842737530265627\n",
      "Iteracion: 16797 Gradiente: [5.317432079152695e-05,-0.0009174005403147352] Loss: 22.842737529420504\n",
      "Iteracion: 16798 Gradiente: [5.314605171236053e-05,-0.0009169128220275269] Loss: 22.842737528576276\n",
      "Iteracion: 16799 Gradiente: [5.3117797618066714e-05,-0.0009164253630288499] Loss: 22.842737527732954\n",
      "Iteracion: 16800 Gradiente: [5.3089558515277226e-05,-0.0009159381631795564] Loss: 22.84273752689052\n",
      "Iteracion: 16801 Gradiente: [5.306133448452025e-05,-0.0009154512223375378] Loss: 22.84273752604897\n",
      "Iteracion: 16802 Gradiente: [5.303312548979496e-05,-0.0009149645403655408] Loss: 22.842737525208328\n",
      "Iteracion: 16803 Gradiente: [5.300493144962578e-05,-0.0009144781171314047] Loss: 22.84273752436858\n",
      "Iteracion: 16804 Gradiente: [5.297675234980185e-05,-0.0009139919524983498] Loss: 22.842737523529724\n",
      "Iteracion: 16805 Gradiente: [5.294858836180083e-05,-0.000913506046317399] Loss: 22.84273752269177\n",
      "Iteracion: 16806 Gradiente: [5.292043923930123e-05,-0.0009130203984651549] Loss: 22.842737521854694\n",
      "Iteracion: 16807 Gradiente: [5.289230516041244e-05,-0.0009125350087927586] Loss: 22.842737521018513\n",
      "Iteracion: 16808 Gradiente: [5.286418599913152e-05,-0.0009120498771712467] Loss: 22.842737520183224\n",
      "Iteracion: 16809 Gradiente: [5.2836081750721556e-05,-0.0009115650034626554] Loss: 22.84273751934881\n",
      "Iteracion: 16810 Gradiente: [5.280799247013116e-05,-0.0009110803875262974] Loss: 22.842737518515282\n",
      "Iteracion: 16811 Gradiente: [5.277991816872903e-05,-0.000910596029224564] Loss: 22.842737517682643\n",
      "Iteracion: 16812 Gradiente: [5.2751858687353587e-05,-0.0009101119284281367] Loss: 22.842737516850892\n",
      "Iteracion: 16813 Gradiente: [5.2723814208851155e-05,-0.0009096280849898146] Loss: 22.84273751602004\n",
      "Iteracion: 16814 Gradiente: [5.269578456648105e-05,-0.000909144498780871] Loss: 22.842737515190038\n",
      "Iteracion: 16815 Gradiente: [5.266776985592969e-05,-0.0009086611696605] Loss: 22.842737514360945\n",
      "Iteracion: 16816 Gradiente: [5.26397700355119e-05,-0.0009081780974927511] Loss: 22.842737513532732\n",
      "Iteracion: 16817 Gradiente: [5.2611785145018074e-05,-0.0009076952821384765] Loss: 22.8427375127054\n",
      "Iteracion: 16818 Gradiente: [5.258381511244655e-05,-0.0009072127234636203] Loss: 22.84273751187893\n",
      "Iteracion: 16819 Gradiente: [5.2555859925481246e-05,-0.000906730421334719] Loss: 22.84273751105336\n",
      "Iteracion: 16820 Gradiente: [5.252791964664994e-05,-0.0009062483756096639] Loss: 22.84273751022865\n",
      "Iteracion: 16821 Gradiente: [5.249999420489833e-05,-0.0009057665861552285] Loss: 22.842737509404813\n",
      "Iteracion: 16822 Gradiente: [5.24720836239112e-05,-0.0009052850528349884] Loss: 22.84273750858188\n",
      "Iteracion: 16823 Gradiente: [5.244418783452905e-05,-0.0009048037755150062] Loss: 22.8427375077598\n",
      "Iteracion: 16824 Gradiente: [5.241630686327881e-05,-0.0009043227540570816] Loss: 22.842737506938597\n",
      "Iteracion: 16825 Gradiente: [5.238844075279303e-05,-0.0009038419883226586] Loss: 22.842737506118272\n",
      "Iteracion: 16826 Gradiente: [5.236058945475482e-05,-0.0009033614781785104] Loss: 22.842737505298814\n",
      "Iteracion: 16827 Gradiente: [5.2332752962532446e-05,-0.0009028812234883314] Loss: 22.842737504480233\n",
      "Iteracion: 16828 Gradiente: [5.230493125338853e-05,-0.000902401224116763] Loss: 22.842737503662516\n",
      "Iteracion: 16829 Gradiente: [5.227712431974396e-05,-0.0009019214799279733] Loss: 22.842737502845658\n",
      "Iteracion: 16830 Gradiente: [5.224933216823047e-05,-0.0009014419907860116] Loss: 22.842737502029685\n",
      "Iteracion: 16831 Gradiente: [5.222155487369188e-05,-0.0009009627565517302] Loss: 22.842737501214582\n",
      "Iteracion: 16832 Gradiente: [5.219379229117749e-05,-0.0009004837770950994] Loss: 22.842737500400332\n",
      "Iteracion: 16833 Gradiente: [5.216604439889731e-05,-0.0009000050522833665] Loss: 22.842737499586942\n",
      "Iteracion: 16834 Gradiente: [5.2138311361697257e-05,-0.0008995265819701596] Loss: 22.84273749877443\n",
      "Iteracion: 16835 Gradiente: [5.211059300904708e-05,-0.0008990483660308967] Loss: 22.842737497962784\n",
      "Iteracion: 16836 Gradiente: [5.2082889498213564e-05,-0.0008985704043187326] Loss: 22.842737497151994\n",
      "Iteracion: 16837 Gradiente: [5.2055200564874817e-05,-0.0008980926967147695] Loss: 22.842737496342068\n",
      "Iteracion: 16838 Gradiente: [5.202752646577361e-05,-0.0008976152430675437] Loss: 22.84273749553301\n",
      "Iteracion: 16839 Gradiente: [5.1999866979220615e-05,-0.0008971380432562625] Loss: 22.8427374947248\n",
      "Iteracion: 16840 Gradiente: [5.197222224164004e-05,-0.0008966610971345545] Loss: 22.84273749391746\n",
      "Iteracion: 16841 Gradiente: [5.194459224829492e-05,-0.0008961844045709692] Loss: 22.84273749311097\n",
      "Iteracion: 16842 Gradiente: [5.1916976990658746e-05,-0.0008957079654284901] Loss: 22.84273749230534\n",
      "Iteracion: 16843 Gradiente: [5.1889376304832995e-05,-0.0008952317795820619] Loss: 22.842737491500564\n",
      "Iteracion: 16844 Gradiente: [5.1861790330084055e-05,-0.0008947558468880364] Loss: 22.842737490696646\n",
      "Iteracion: 16845 Gradiente: [5.183421893093509e-05,-0.0008942801672206476] Loss: 22.842737489893583\n",
      "Iteracion: 16846 Gradiente: [5.180666228265333e-05,-0.0008938047404332868] Loss: 22.842737489091373\n",
      "Iteracion: 16847 Gradiente: [5.1779120284815386e-05,-0.0008933295663980564] Loss: 22.842737488290023\n",
      "Iteracion: 16848 Gradiente: [5.175159292510519e-05,-0.0008928546449785321] Loss: 22.8427374874895\n",
      "Iteracion: 16849 Gradiente: [5.172408017320625e-05,-0.0008923799760443292] Loss: 22.842737486689852\n",
      "Iteracion: 16850 Gradiente: [5.169658209164633e-05,-0.0008919055594566553] Loss: 22.842737485891046\n",
      "Iteracion: 16851 Gradiente: [5.166909860179203e-05,-0.0008914313950837046] Loss: 22.84273748509309\n",
      "Iteracion: 16852 Gradiente: [5.164162974153896e-05,-0.0008909574827914213] Loss: 22.842737484295984\n",
      "Iteracion: 16853 Gradiente: [5.161417550236062e-05,-0.0008904838224442102] Loss: 22.842737483499732\n",
      "Iteracion: 16854 Gradiente: [5.158673571562152e-05,-0.000890010413917608] Loss: 22.842737482704308\n",
      "Iteracion: 16855 Gradiente: [5.1559310628590536e-05,-0.0008895372570625189] Loss: 22.842737481909737\n",
      "Iteracion: 16856 Gradiente: [5.1531900137054736e-05,-0.0008890643517518744] Loss: 22.842737481116014\n",
      "Iteracion: 16857 Gradiente: [5.150450422680327e-05,-0.0008885916978516188] Loss: 22.842737480323137\n",
      "Iteracion: 16858 Gradiente: [5.147712284572966e-05,-0.0008881192952301831] Loss: 22.842737479531102\n",
      "Iteracion: 16859 Gradiente: [5.144975603741386e-05,-0.000887647143750551] Loss: 22.842737478739892\n",
      "Iteracion: 16860 Gradiente: [5.1422403816066736e-05,-0.0008871752432804432] Loss: 22.842737477949544\n",
      "Iteracion: 16861 Gradiente: [5.139506603389539e-05,-0.0008867035936929095] Loss: 22.842737477160036\n",
      "Iteracion: 16862 Gradiente: [5.136774287279877e-05,-0.0008862321948432358] Loss: 22.842737476371358\n",
      "Iteracion: 16863 Gradiente: [5.1340434168878346e-05,-0.0008857610466080246] Loss: 22.84273747558352\n",
      "Iteracion: 16864 Gradiente: [5.131314004718964e-05,-0.0008852901488451674] Loss: 22.842737474796518\n",
      "Iteracion: 16865 Gradiente: [5.1285860457520964e-05,-0.0008848195014252269] Loss: 22.842737474010356\n",
      "Iteracion: 16866 Gradiente: [5.1258595273869405e-05,-0.000884349104221845] Loss: 22.84273747322503\n",
      "Iteracion: 16867 Gradiente: [5.1231344573920976e-05,-0.0008838789570962291] Loss: 22.842737472440536\n",
      "Iteracion: 16868 Gradiente: [5.120410848557337e-05,-0.0008834090599085206] Loss: 22.842737471656893\n",
      "Iteracion: 16869 Gradiente: [5.117688678240029e-05,-0.0008829394125378087] Loss: 22.84273747087406\n",
      "Iteracion: 16870 Gradiente: [5.114967957240424e-05,-0.000882470014844472] Loss: 22.842737470092057\n",
      "Iteracion: 16871 Gradiente: [5.1122486854637825e-05,-0.0008820008666963493] Loss: 22.8427374693109\n",
      "Iteracion: 16872 Gradiente: [5.1095308561836344e-05,-0.0008815319679635299] Loss: 22.842737468530576\n",
      "Iteracion: 16873 Gradiente: [5.106814464852505e-05,-0.0008810633185156291] Loss: 22.842737467751064\n",
      "Iteracion: 16874 Gradiente: [5.1040995297550275e-05,-0.0008805949182089989] Loss: 22.842737466972398\n",
      "Iteracion: 16875 Gradiente: [5.101386040280431e-05,-0.0008801267669175179] Loss: 22.84273746619455\n",
      "Iteracion: 16876 Gradiente: [5.098673980607297e-05,-0.0008796588645168413] Loss: 22.842737465417525\n",
      "Iteracion: 16877 Gradiente: [5.0959633743256445e-05,-0.0008791912108598865] Loss: 22.84273746464134\n",
      "Iteracion: 16878 Gradiente: [5.0932541952875e-05,-0.0008787238058311904] Loss: 22.842737463865966\n",
      "Iteracion: 16879 Gradiente: [5.090546471440878e-05,-0.0008782566492798812] Loss: 22.842737463091428\n",
      "Iteracion: 16880 Gradiente: [5.0878401761641116e-05,-0.0008777897410891929] Loss: 22.842737462317704\n",
      "Iteracion: 16881 Gradiente: [5.085135331436656e-05,-0.0008773230811151223] Loss: 22.842737461544807\n",
      "Iteracion: 16882 Gradiente: [5.0824319153737936e-05,-0.0008768566692368769] Loss: 22.84273746077273\n",
      "Iteracion: 16883 Gradiente: [5.0797299328072164e-05,-0.0008763905053198092] Loss: 22.84273746000147\n",
      "Iteracion: 16884 Gradiente: [5.0770293998425584e-05,-0.000875924589222521] Loss: 22.842737459231035\n",
      "Iteracion: 16885 Gradiente: [5.0743302971530586e-05,-0.0008754589208236278] Loss: 22.842737458461425\n",
      "Iteracion: 16886 Gradiente: [5.071632629759885e-05,-0.0008749934999869424] Loss: 22.842737457692625\n",
      "Iteracion: 16887 Gradiente: [5.0689363915997396e-05,-0.0008745283265865803] Loss: 22.842737456924638\n",
      "Iteracion: 16888 Gradiente: [5.0662415949886964e-05,-0.0008740634004814988] Loss: 22.842737456157472\n",
      "Iteracion: 16889 Gradiente: [5.063548218136778e-05,-0.0008735987215526819] Loss: 22.842737455391124\n",
      "Iteracion: 16890 Gradiente: [5.0608562772443597e-05,-0.000873134289658258] Loss: 22.84273745462559\n",
      "Iteracion: 16891 Gradiente: [5.0581657787536946e-05,-0.000872670104664645] Loss: 22.84273745386087\n",
      "Iteracion: 16892 Gradiente: [5.055476698885286e-05,-0.0008722061664518795] Loss: 22.842737453096966\n",
      "Iteracion: 16893 Gradiente: [5.0527890501446866e-05,-0.0008717424748838927] Loss: 22.84273745233387\n",
      "Iteracion: 16894 Gradiente: [5.050102832721374e-05,-0.0008712790298252078] Loss: 22.842737451571587\n",
      "Iteracion: 16895 Gradiente: [5.047418038278314e-05,-0.0008708158311526641] Loss: 22.842737450810105\n",
      "Iteracion: 16896 Gradiente: [5.04473468007897e-05,-0.000870352878723916] Loss: 22.842737450049434\n",
      "Iteracion: 16897 Gradiente: [5.0420527485547004e-05,-0.0008698901724150924] Loss: 22.842737449289586\n",
      "Iteracion: 16898 Gradiente: [5.039372235936905e-05,-0.0008694277121005456] Loss: 22.842737448530535\n",
      "Iteracion: 16899 Gradiente: [5.036693154636396e-05,-0.0008689654976395881] Loss: 22.8427374477723\n",
      "Iteracion: 16900 Gradiente: [5.0340154931897514e-05,-0.0008685035289102435] Loss: 22.84273744701486\n",
      "Iteracion: 16901 Gradiente: [5.0313392455336726e-05,-0.0008680418057810613] Loss: 22.842737446258234\n",
      "Iteracion: 16902 Gradiente: [5.0286644281527515e-05,-0.000867580328114433] Loss: 22.8427374455024\n",
      "Iteracion: 16903 Gradiente: [5.0259910422785956e-05,-0.000867119095777961] Loss: 22.84273744474738\n",
      "Iteracion: 16904 Gradiente: [5.0233190737003494e-05,-0.0008666581086488397] Loss: 22.842737443993148\n",
      "Iteracion: 16905 Gradiente: [5.0206485244075336e-05,-0.0008661973665960924] Loss: 22.84273744323973\n",
      "Iteracion: 16906 Gradiente: [5.0179793941159305e-05,-0.0008657368694882687] Loss: 22.842737442487113\n",
      "Iteracion: 16907 Gradiente: [5.01531167894124e-05,-0.0008652766171969972] Loss: 22.8427374417353\n",
      "Iteracion: 16908 Gradiente: [5.012645384378326e-05,-0.0008648166095882223] Loss: 22.842737440984266\n",
      "Iteracion: 16909 Gradiente: [5.009980507679757e-05,-0.0008643568465335723] Loss: 22.842737440234046\n",
      "Iteracion: 16910 Gradiente: [5.007317051308746e-05,-0.0008638973279015971] Loss: 22.84273743948463\n",
      "Iteracion: 16911 Gradiente: [5.0046550072124774e-05,-0.000863438053564991] Loss: 22.84273743873599\n",
      "Iteracion: 16912 Gradiente: [5.001994382306899e-05,-0.0008629790233904089] Loss: 22.84273743798817\n",
      "Iteracion: 16913 Gradiente: [4.999335170339236e-05,-0.0008625202372518477] Loss: 22.842737437241134\n",
      "Iteracion: 16914 Gradiente: [4.996677366383059e-05,-0.0008620616950212915] Loss: 22.842737436494893\n",
      "Iteracion: 16915 Gradiente: [4.994020980670181e-05,-0.0008616033965632634] Loss: 22.842737435749438\n",
      "Iteracion: 16916 Gradiente: [4.991366007516263e-05,-0.0008611453417502209] Loss: 22.842737435004782\n",
      "Iteracion: 16917 Gradiente: [4.9887124412369606e-05,-0.0008606875304554507] Loss: 22.842737434260922\n",
      "Iteracion: 16918 Gradiente: [4.986060283821795e-05,-0.0008602299625483312] Loss: 22.842737433517847\n",
      "Iteracion: 16919 Gradiente: [4.983409545597321e-05,-0.0008597726378942146] Loss: 22.84273743277557\n",
      "Iteracion: 16920 Gradiente: [4.980760205057777e-05,-0.0008593155563730193] Loss: 22.84273743203407\n",
      "Iteracion: 16921 Gradiente: [4.9781122811509704e-05,-0.000858858717846663] Loss: 22.84273743129336\n",
      "Iteracion: 16922 Gradiente: [4.9754657663925175e-05,-0.0008584021221883139] Loss: 22.84273743055346\n",
      "Iteracion: 16923 Gradiente: [4.972820654624381e-05,-0.0008579457692727981] Loss: 22.842737429814314\n",
      "Iteracion: 16924 Gradiente: [4.970176958352113e-05,-0.000857489658961678] Loss: 22.84273742907596\n",
      "Iteracion: 16925 Gradiente: [4.967534657775256e-05,-0.0008570337911397274] Loss: 22.842737428338406\n",
      "Iteracion: 16926 Gradiente: [4.9648937670099257e-05,-0.0008565781656680353] Loss: 22.84273742760162\n",
      "Iteracion: 16927 Gradiente: [4.9622542741190044e-05,-0.0008561227824248618] Loss: 22.842737426865632\n",
      "Iteracion: 16928 Gradiente: [4.9596161837447046e-05,-0.0008556676412779277] Loss: 22.842737426130416\n",
      "Iteracion: 16929 Gradiente: [4.956979501192412e-05,-0.0008552127420942431] Loss: 22.842737425395974\n",
      "Iteracion: 16930 Gradiente: [4.9543442179356136e-05,-0.0008547580847505287] Loss: 22.842737424662335\n",
      "Iteracion: 16931 Gradiente: [4.951710334448004e-05,-0.0008543036691171106] Loss: 22.842737423929464\n",
      "Iteracion: 16932 Gradiente: [4.949077855277058e-05,-0.0008538494950624198] Loss: 22.842737423197377\n",
      "Iteracion: 16933 Gradiente: [4.946446770096221e-05,-0.0008533955624653089] Loss: 22.842737422466044\n",
      "Iteracion: 16934 Gradiente: [4.9438170907478704e-05,-0.000852941871187222] Loss: 22.842737421735517\n",
      "Iteracion: 16935 Gradiente: [4.941188806715975e-05,-0.0008524884211072485] Loss: 22.84273742100576\n",
      "Iteracion: 16936 Gradiente: [4.9385619187584474e-05,-0.0008520352120957142] Loss: 22.84273742027677\n",
      "Iteracion: 16937 Gradiente: [4.9359364282016335e-05,-0.0008515822440235373] Loss: 22.842737419548563\n",
      "Iteracion: 16938 Gradiente: [4.933312328413801e-05,-0.0008511295167654253] Loss: 22.84273741882113\n",
      "Iteracion: 16939 Gradiente: [4.930689629058331e-05,-0.0008506770301882701] Loss: 22.842737418094465\n",
      "Iteracion: 16940 Gradiente: [4.9280683211350154e-05,-0.0008502247841687923] Loss: 22.84273741736858\n",
      "Iteracion: 16941 Gradiente: [4.9254484073912864e-05,-0.0008497727785766074] Loss: 22.842737416643455\n",
      "Iteracion: 16942 Gradiente: [4.9228298910482714e-05,-0.0008493210132820413] Loss: 22.84273741591911\n",
      "Iteracion: 16943 Gradiente: [4.9202127630110223e-05,-0.0008488694881619333] Loss: 22.842737415195543\n",
      "Iteracion: 16944 Gradiente: [4.917597023753236e-05,-0.0008484182030873199] Loss: 22.842737414472722\n",
      "Iteracion: 16945 Gradiente: [4.9149826871068096e-05,-0.0008479671579235533] Loss: 22.84273741375069\n",
      "Iteracion: 16946 Gradiente: [4.9123697288185515e-05,-0.0008475163525557623] Loss: 22.84273741302943\n",
      "Iteracion: 16947 Gradiente: [4.909758159025538e-05,-0.0008470657868504835] Loss: 22.842737412308907\n",
      "Iteracion: 16948 Gradiente: [4.90714797962255e-05,-0.0008466154606787531] Loss: 22.842737411589177\n",
      "Iteracion: 16949 Gradiente: [4.904539191746456e-05,-0.0008461653739123184] Loss: 22.842737410870207\n",
      "Iteracion: 16950 Gradiente: [4.9019317918919116e-05,-0.0008457155264232815] Loss: 22.842737410151987\n",
      "Iteracion: 16951 Gradiente: [4.8993257659428004e-05,-0.0008452659180962978] Loss: 22.84273740943455\n",
      "Iteracion: 16952 Gradiente: [4.896721138531272e-05,-0.0008448165487861796] Loss: 22.842737408717866\n",
      "Iteracion: 16953 Gradiente: [4.89411789250956e-05,-0.00084436741837699] Loss: 22.842737408001938\n",
      "Iteracion: 16954 Gradiente: [4.891516029204013e-05,-0.000843918526739292] Loss: 22.842737407286773\n",
      "Iteracion: 16955 Gradiente: [4.888915546435631e-05,-0.0008434698737487405] Loss: 22.842737406572372\n",
      "Iteracion: 16956 Gradiente: [4.886316451309843e-05,-0.0008430214592724638] Loss: 22.842737405858728\n",
      "Iteracion: 16957 Gradiente: [4.8837187394686527e-05,-0.0008425732831864725] Loss: 22.84273740514585\n",
      "Iteracion: 16958 Gradiente: [4.881122405511936e-05,-0.0008421253453653558] Loss: 22.842737404433716\n",
      "Iteracion: 16959 Gradiente: [4.878527452755558e-05,-0.0008416776456818079] Loss: 22.842737403722346\n",
      "Iteracion: 16960 Gradiente: [4.875933875988873e-05,-0.000841230184011484] Loss: 22.842737403011736\n",
      "Iteracion: 16961 Gradiente: [4.873341677390878e-05,-0.0008407829602256574] Loss: 22.842737402301882\n",
      "Iteracion: 16962 Gradiente: [4.8707508522246217e-05,-0.0008403359742014042] Loss: 22.842737401592785\n",
      "Iteracion: 16963 Gradiente: [4.8681614190589545e-05,-0.0008398892257985106] Loss: 22.842737400884435\n",
      "Iteracion: 16964 Gradiente: [4.8655733487142545e-05,-0.0008394427149088557] Loss: 22.84273740017683\n",
      "Iteracion: 16965 Gradiente: [4.862986665254236e-05,-0.0008389964413928179] Loss: 22.842737399469986\n",
      "Iteracion: 16966 Gradiente: [4.860401356647041e-05,-0.0008385504051292496] Loss: 22.8427373987639\n",
      "Iteracion: 16967 Gradiente: [4.8578174239347995e-05,-0.0008381046059903715] Loss: 22.84273739805856\n",
      "Iteracion: 16968 Gradiente: [4.8552348541382644e-05,-0.0008376590438592994] Loss: 22.842737397353968\n",
      "Iteracion: 16969 Gradiente: [4.85265366251042e-05,-0.0008372137186002012] Loss: 22.842737396650126\n",
      "Iteracion: 16970 Gradiente: [4.8500738419458385e-05,-0.0008367686300889687] Loss: 22.84273739594703\n",
      "Iteracion: 16971 Gradiente: [4.847495396139342e-05,-0.0008363237781997176] Loss: 22.84273739524469\n",
      "Iteracion: 16972 Gradiente: [4.84491831040638e-05,-0.0008358791628136686] Loss: 22.84273739454307\n",
      "Iteracion: 16973 Gradiente: [4.84234260369476e-05,-0.0008354347837927397] Loss: 22.842737393842224\n",
      "Iteracion: 16974 Gradiente: [4.839768274583397e-05,-0.0008349906410128228] Loss: 22.84273739314212\n",
      "Iteracion: 16975 Gradiente: [4.83719529948227e-05,-0.0008345467343614151] Loss: 22.84273739244275\n",
      "Iteracion: 16976 Gradiente: [4.8346236977181436e-05,-0.000834103063702211] Loss: 22.842737391744127\n",
      "Iteracion: 16977 Gradiente: [4.8320534636066746e-05,-0.0008336596289113392] Loss: 22.842737391046253\n",
      "Iteracion: 16978 Gradiente: [4.829484592031955e-05,-0.0008332164298650469] Loss: 22.84273739034911\n",
      "Iteracion: 16979 Gradiente: [4.826917086973026e-05,-0.0008327734664381599] Loss: 22.84273738965271\n",
      "Iteracion: 16980 Gradiente: [4.8243509495667544e-05,-0.0008323307385013597] Loss: 22.842737388957058\n",
      "Iteracion: 16981 Gradiente: [4.8217861813289646e-05,-0.0008318882459304196] Loss: 22.84273738826213\n",
      "Iteracion: 16982 Gradiente: [4.819222776480577e-05,-0.0008314459886018237] Loss: 22.84273738756796\n",
      "Iteracion: 16983 Gradiente: [4.816660728010902e-05,-0.0008310039663944243] Loss: 22.84273738687453\n",
      "Iteracion: 16984 Gradiente: [4.81410003885685e-05,-0.0008305621791804423] Loss: 22.842737386181827\n",
      "Iteracion: 16985 Gradiente: [4.8115407161238484e-05,-0.0008301206268319798] Loss: 22.84273738548984\n",
      "Iteracion: 16986 Gradiente: [4.808982752232775e-05,-0.0008296793092271789] Loss: 22.84273738479862\n",
      "Iteracion: 16987 Gradiente: [4.806426148604714e-05,-0.000829238226240155] Loss: 22.842737384108116\n",
      "Iteracion: 16988 Gradiente: [4.8038709089344896e-05,-0.0008287973777432474] Loss: 22.842737383418363\n",
      "Iteracion: 16989 Gradiente: [4.8013170269693245e-05,-0.000828356763616019] Loss: 22.842737382729332\n",
      "Iteracion: 16990 Gradiente: [4.798764494656401e-05,-0.0008279163837380328] Loss: 22.84273738204103\n",
      "Iteracion: 16991 Gradiente: [4.796213319101146e-05,-0.0008274762379784306] Loss: 22.84273738135347\n",
      "Iteracion: 16992 Gradiente: [4.793663511009072e-05,-0.0008270363262058803] Loss: 22.842737380666627\n",
      "Iteracion: 16993 Gradiente: [4.7911150503902414e-05,-0.0008265966483117874] Loss: 22.842737379980527\n",
      "Iteracion: 16994 Gradiente: [4.788567944634299e-05,-0.0008261572041619777] Loss: 22.842737379295155\n",
      "Iteracion: 16995 Gradiente: [4.786022199804544e-05,-0.000825717993629856] Loss: 22.84273737861052\n",
      "Iteracion: 16996 Gradiente: [4.78347779979534e-05,-0.0008252790166011437] Loss: 22.842737377926582\n",
      "Iteracion: 16997 Gradiente: [4.7809347604281055e-05,-0.0008248402729428506] Loss: 22.842737377243402\n",
      "Iteracion: 16998 Gradiente: [4.778393067776202e-05,-0.000824401762535724] Loss: 22.84273737656093\n",
      "Iteracion: 16999 Gradiente: [4.775852731313535e-05,-0.000823963485251511] Loss: 22.842737375879185\n",
      "Iteracion: 17000 Gradiente: [4.773313739197723e-05,-0.0008235254409715509] Loss: 22.842737375198165\n",
      "Iteracion: 17001 Gradiente: [4.770776094839372e-05,-0.0008230876295713803] Loss: 22.842737374517867\n",
      "Iteracion: 17002 Gradiente: [4.7682398097019056e-05,-0.0008226500509200226] Loss: 22.84273737383831\n",
      "Iteracion: 17003 Gradiente: [4.765704869006034e-05,-0.0008222127049009487] Loss: 22.84273737315947\n",
      "Iteracion: 17004 Gradiente: [4.7631712687727185e-05,-0.000821775591392182] Loss: 22.84273737248135\n",
      "Iteracion: 17005 Gradiente: [4.760639013265215e-05,-0.0008213387102677198] Loss: 22.842737371803935\n",
      "Iteracion: 17006 Gradiente: [4.758108117736507e-05,-0.000820902061394572] Loss: 22.842737371127257\n",
      "Iteracion: 17007 Gradiente: [4.7555785622913994e-05,-0.000820465644660473] Loss: 22.84273737045128\n",
      "Iteracion: 17008 Gradiente: [4.753050355456405e-05,-0.0008200294599388277] Loss: 22.842737369776053\n",
      "Iteracion: 17009 Gradiente: [4.750523479894279e-05,-0.0008195935071113307] Loss: 22.842737369101517\n",
      "Iteracion: 17010 Gradiente: [4.7479979568265665e-05,-0.0008191577860453473] Loss: 22.842737368427695\n",
      "Iteracion: 17011 Gradiente: [4.745473776968841e-05,-0.0008187222966217434] Loss: 22.842737367754605\n",
      "Iteracion: 17012 Gradiente: [4.7429509402263646e-05,-0.000818287038716529] Loss: 22.84273736708223\n",
      "Iteracion: 17013 Gradiente: [4.740429437598929e-05,-0.0008178520122125831] Loss: 22.84273736641057\n",
      "Iteracion: 17014 Gradiente: [4.737909284244779e-05,-0.0008174172169759686] Loss: 22.842737365739612\n",
      "Iteracion: 17015 Gradiente: [4.735390460647674e-05,-0.0008169826528965511] Loss: 22.842737365069375\n",
      "Iteracion: 17016 Gradiente: [4.732872982818511e-05,-0.0008165483198413408] Loss: 22.84273736439985\n",
      "Iteracion: 17017 Gradiente: [4.7303568454519036e-05,-0.0008161142176885979] Loss: 22.84273736373103\n",
      "Iteracion: 17018 Gradiente: [4.7278420452319855e-05,-0.0008156803463187145] Loss: 22.842737363062938\n",
      "Iteracion: 17019 Gradiente: [4.725328573158549e-05,-0.0008152467056125564] Loss: 22.84273736239555\n",
      "Iteracion: 17020 Gradiente: [4.722816443916145e-05,-0.0008148132954398572] Loss: 22.84273736172885\n",
      "Iteracion: 17021 Gradiente: [4.720305644241307e-05,-0.0008143801156836142] Loss: 22.842737361062884\n",
      "Iteracion: 17022 Gradiente: [4.7177961973450996e-05,-0.0008139471662104824] Loss: 22.84273736039762\n",
      "Iteracion: 17023 Gradiente: [4.715288064573997e-05,-0.0008135144469177884] Loss: 22.84273735973306\n",
      "Iteracion: 17024 Gradiente: [4.7127812699917133e-05,-0.000813081957668634] Loss: 22.84273735906921\n",
      "Iteracion: 17025 Gradiente: [4.7102758114192514e-05,-0.0008126496983425824] Loss: 22.84273735840607\n",
      "Iteracion: 17026 Gradiente: [4.707771683077529e-05,-0.0008122176688191966] Loss: 22.84273735774362\n",
      "Iteracion: 17027 Gradiente: [4.705268888566631e-05,-0.0008117858689750789] Loss: 22.84273735708189\n",
      "Iteracion: 17028 Gradiente: [4.7027674263707316e-05,-0.000811354298687661] Loss: 22.84273735642085\n",
      "Iteracion: 17029 Gradiente: [4.7002672945003116e-05,-0.0008109229578347291] Loss: 22.842737355760512\n",
      "Iteracion: 17030 Gradiente: [4.697768488123681e-05,-0.0008104918463003467] Loss: 22.842737355100876\n",
      "Iteracion: 17031 Gradiente: [4.695271008661924e-05,-0.0008100609639579185] Loss: 22.84273735444195\n",
      "Iteracion: 17032 Gradiente: [4.692774854599217e-05,-0.0008096303106879551] Loss: 22.842737353783725\n",
      "Iteracion: 17033 Gradiente: [4.6902800312409455e-05,-0.0008091998863622034] Loss: 22.842737353126203\n",
      "Iteracion: 17034 Gradiente: [4.687786533471202e-05,-0.000808769690865437] Loss: 22.842737352469367\n",
      "Iteracion: 17035 Gradiente: [4.6852943636584614e-05,-0.0008083397240716532] Loss: 22.842737351813234\n",
      "Iteracion: 17036 Gradiente: [4.682803512613039e-05,-0.0008079099858657439] Loss: 22.842737351157805\n",
      "Iteracion: 17037 Gradiente: [4.680313990850967e-05,-0.0008074804761177983] Loss: 22.842737350503068\n",
      "Iteracion: 17038 Gradiente: [4.677825798561723e-05,-0.000807051194708445] Loss: 22.84273734984903\n",
      "Iteracion: 17039 Gradiente: [4.67533892172393e-05,-0.0008066221415219843] Loss: 22.842737349195673\n",
      "Iteracion: 17040 Gradiente: [4.672853368295667e-05,-0.0008061933164325315] Loss: 22.842737348543025\n",
      "Iteracion: 17041 Gradiente: [4.6703691353400245e-05,-0.0008057647193195313] Loss: 22.84273734789106\n",
      "Iteracion: 17042 Gradiente: [4.667886223235958e-05,-0.0008053363500625466] Loss: 22.8427373472398\n",
      "Iteracion: 17043 Gradiente: [4.6654046282886456e-05,-0.0008049082085413772] Loss: 22.84273734658923\n",
      "Iteracion: 17044 Gradiente: [4.662924364519465e-05,-0.0008044802946258756] Loss: 22.842737345939362\n",
      "Iteracion: 17045 Gradiente: [4.6604454102331756e-05,-0.0008040526082081574] Loss: 22.842737345290157\n",
      "Iteracion: 17046 Gradiente: [4.657967772535206e-05,-0.0008036251491631674] Loss: 22.842737344641662\n",
      "Iteracion: 17047 Gradiente: [4.6554914596678525e-05,-0.0008031979173631262] Loss: 22.842737343993853\n",
      "Iteracion: 17048 Gradiente: [4.6530164492727026e-05,-0.000802770912701097] Loss: 22.842737343346737\n",
      "Iteracion: 17049 Gradiente: [4.650542767781947e-05,-0.0008023441350407741] Loss: 22.842737342700307\n",
      "Iteracion: 17050 Gradiente: [4.648070392552957e-05,-0.0008019175842734446] Loss: 22.842737342054548\n",
      "Iteracion: 17051 Gradiente: [4.6455993338175476e-05,-0.000801491260271329] Loss: 22.84273734140948\n",
      "Iteracion: 17052 Gradiente: [4.6431295939441955e-05,-0.0008010651629139905] Loss: 22.84273734076512\n",
      "Iteracion: 17053 Gradiente: [4.640661163837952e-05,-0.0008006392920847816] Loss: 22.842737340121417\n",
      "Iteracion: 17054 Gradiente: [4.638194044919904e-05,-0.0008002136476627915] Loss: 22.842737339478408\n",
      "Iteracion: 17055 Gradiente: [4.635728236621617e-05,-0.0007997882295260438] Loss: 22.8427373388361\n",
      "Iteracion: 17056 Gradiente: [4.633263739606264e-05,-0.0007993630375549306] Loss: 22.84273733819446\n",
      "Iteracion: 17057 Gradiente: [4.630800553115932e-05,-0.0007989380716278305] Loss: 22.842737337553498\n",
      "Iteracion: 17058 Gradiente: [4.628338682266531e-05,-0.0007985133316225302] Loss: 22.842737336913224\n",
      "Iteracion: 17059 Gradiente: [4.6258781099102936e-05,-0.0007980888174292507] Loss: 22.84273733627363\n",
      "Iteracion: 17060 Gradiente: [4.6234188603951526e-05,-0.0007976645289108575] Loss: 22.842737335634705\n",
      "Iteracion: 17061 Gradiente: [4.620960898099232e-05,-0.0007972404659695324] Loss: 22.842737334996464\n",
      "Iteracion: 17062 Gradiente: [4.618504255896975e-05,-0.000796816628465417] Loss: 22.842737334358908\n",
      "Iteracion: 17063 Gradiente: [4.616048920145961e-05,-0.0007963930162856532] Loss: 22.842737333722027\n",
      "Iteracion: 17064 Gradiente: [4.613594886488196e-05,-0.0007959696293138308] Loss: 22.842737333085825\n",
      "Iteracion: 17065 Gradiente: [4.611142160134326e-05,-0.0007955464674250124] Loss: 22.84273733245031\n",
      "Iteracion: 17066 Gradiente: [4.608690740231699e-05,-0.0007951235305011295] Loss: 22.842737331815453\n",
      "Iteracion: 17067 Gradiente: [4.606240613232634e-05,-0.0007947008184299165] Loss: 22.842737331181286\n",
      "Iteracion: 17068 Gradiente: [4.603791799032327e-05,-0.0007942783310787386] Loss: 22.84273733054777\n",
      "Iteracion: 17069 Gradiente: [4.6013442786829725e-05,-0.0007938560683399487] Loss: 22.842737329914943\n",
      "Iteracion: 17070 Gradiente: [4.598898060900562e-05,-0.0007934340300876623] Loss: 22.842737329282777\n",
      "Iteracion: 17071 Gradiente: [4.596453142558706e-05,-0.0007930122162052318] Loss: 22.842737328651296\n",
      "Iteracion: 17072 Gradiente: [4.5940095275417056e-05,-0.0007925906265694967] Loss: 22.842737328020487\n",
      "Iteracion: 17073 Gradiente: [4.5915672095967845e-05,-0.0007921692610653489] Loss: 22.842737327390356\n",
      "Iteracion: 17074 Gradiente: [4.589126190902941e-05,-0.0007917481195705752] Loss: 22.842737326760876\n",
      "Iteracion: 17075 Gradiente: [4.5866864720286076e-05,-0.0007913272019666333] Loss: 22.84273732613207\n",
      "Iteracion: 17076 Gradiente: [4.584248047384184e-05,-0.0007909065081373493] Loss: 22.842737325503933\n",
      "Iteracion: 17077 Gradiente: [4.5818109247382686e-05,-0.0007904860379587338] Loss: 22.84273732487646\n",
      "Iteracion: 17078 Gradiente: [4.579375089406312e-05,-0.0007900657913191129] Loss: 22.84273732424966\n",
      "Iteracion: 17079 Gradiente: [4.576940556262343e-05,-0.0007896457680929577] Loss: 22.84273732362353\n",
      "Iteracion: 17080 Gradiente: [4.574507310242855e-05,-0.0007892259681668179] Loss: 22.842737322998058\n",
      "Iteracion: 17081 Gradiente: [4.572075368116657e-05,-0.0007888063914132696] Loss: 22.842737322373257\n",
      "Iteracion: 17082 Gradiente: [4.569644711030681e-05,-0.0007883870377250209] Loss: 22.842737321749112\n",
      "Iteracion: 17083 Gradiente: [4.567215350637828e-05,-0.000787967906975003] Loss: 22.842737321125647\n",
      "Iteracion: 17084 Gradiente: [4.5647872739588516e-05,-0.0007875489990510687] Loss: 22.842737320502827\n",
      "Iteracion: 17085 Gradiente: [4.562360487720222e-05,-0.0007871303138332546] Loss: 22.842737319880673\n",
      "Iteracion: 17086 Gradiente: [4.559935002343233e-05,-0.000786711851193426] Loss: 22.842737319259193\n",
      "Iteracion: 17087 Gradiente: [4.557510799353774e-05,-0.0007862936110261861] Loss: 22.84273731863836\n",
      "Iteracion: 17088 Gradiente: [4.5550878929626985e-05,-0.000785875593204229] Loss: 22.842737318018184\n",
      "Iteracion: 17089 Gradiente: [4.552666272938192e-05,-0.0007854577976131575] Loss: 22.842737317398665\n",
      "Iteracion: 17090 Gradiente: [4.550245938143386e-05,-0.0007850402241370346] Loss: 22.842737316779825\n",
      "Iteracion: 17091 Gradiente: [4.547826885925588e-05,-0.0007846228726582656] Loss: 22.842737316161617\n",
      "Iteracion: 17092 Gradiente: [4.545409117232187e-05,-0.000784205743057124] Loss: 22.842737315544085\n",
      "Iteracion: 17093 Gradiente: [4.542992638410699e-05,-0.0007837888352117516] Loss: 22.842737314927213\n",
      "Iteracion: 17094 Gradiente: [4.5405774347765755e-05,-0.0007833721490155672] Loss: 22.842737314310984\n",
      "Iteracion: 17095 Gradiente: [4.538163533614655e-05,-0.0007829556843298965] Loss: 22.84273731369541\n",
      "Iteracion: 17096 Gradiente: [4.5357509037557975e-05,-0.0007825394410568028] Loss: 22.8427373130805\n",
      "Iteracion: 17097 Gradiente: [4.533339560547726e-05,-0.0007821234190700466] Loss: 22.842737312466237\n",
      "Iteracion: 17098 Gradiente: [4.530929488074283e-05,-0.0007817076182585462] Loss: 22.842737311852627\n",
      "Iteracion: 17099 Gradiente: [4.528520707083317e-05,-0.0007812920384954699] Loss: 22.84273731123967\n",
      "Iteracion: 17100 Gradiente: [4.526113208385141e-05,-0.0007808766796640517] Loss: 22.842737310627363\n",
      "Iteracion: 17101 Gradiente: [4.5237069842111546e-05,-0.0007804615416551049] Loss: 22.842737310015714\n",
      "Iteracion: 17102 Gradiente: [4.521302045645825e-05,-0.0007800466243416792] Loss: 22.842737309404708\n",
      "Iteracion: 17103 Gradiente: [4.518898387099549e-05,-0.0007796319276094958] Loss: 22.842737308794355\n",
      "Iteracion: 17104 Gradiente: [4.516496001087944e-05,-0.0007792174513468808] Loss: 22.842737308184645\n",
      "Iteracion: 17105 Gradiente: [4.5140948964217385e-05,-0.0007788031954293709] Loss: 22.842737307575586\n",
      "Iteracion: 17106 Gradiente: [4.5116950673218525e-05,-0.0007783891597443452] Loss: 22.84273730696718\n",
      "Iteracion: 17107 Gradiente: [4.509296511704027e-05,-0.0007779753441752746] Loss: 22.84273730635942\n",
      "Iteracion: 17108 Gradiente: [4.506899228147176e-05,-0.0007775617486038537] Loss: 22.842737305752287\n",
      "Iteracion: 17109 Gradiente: [4.504503220156645e-05,-0.0007771483729127245] Loss: 22.842737305145825\n",
      "Iteracion: 17110 Gradiente: [4.502108488300867e-05,-0.0007767352169832265] Loss: 22.842737304539995\n",
      "Iteracion: 17111 Gradiente: [4.499715033337755e-05,-0.0007763222806982384] Loss: 22.84273730393481\n",
      "Iteracion: 17112 Gradiente: [4.497322852898833e-05,-0.0007759095639407576] Loss: 22.84273730333028\n",
      "Iteracion: 17113 Gradiente: [4.4949319346680264e-05,-0.0007754970666018342] Loss: 22.84273730272638\n",
      "Iteracion: 17114 Gradiente: [4.492542289350846e-05,-0.0007750847885583075] Loss: 22.842737302123112\n",
      "Iteracion: 17115 Gradiente: [4.490153913820905e-05,-0.0007746727296942406] Loss: 22.842737301520508\n",
      "Iteracion: 17116 Gradiente: [4.487766809025591e-05,-0.0007742608898934596] Loss: 22.84273730091853\n",
      "Iteracion: 17117 Gradiente: [4.48538097856499e-05,-0.0007738492690369488] Loss: 22.8427373003172\n",
      "Iteracion: 17118 Gradiente: [4.482996411923068e-05,-0.0007734378670126792] Loss: 22.84273729971649\n",
      "Iteracion: 17119 Gradiente: [4.4806131065418715e-05,-0.0007730266837058982] Loss: 22.84273729911645\n",
      "Iteracion: 17120 Gradiente: [4.4782310810849896e-05,-0.0007726157189890633] Loss: 22.842737298517033\n",
      "Iteracion: 17121 Gradiente: [4.475850316604616e-05,-0.0007722049727570142] Loss: 22.84273729791824\n",
      "Iteracion: 17122 Gradiente: [4.473470815563966e-05,-0.0007717944448926299] Loss: 22.842737297320095\n",
      "Iteracion: 17123 Gradiente: [4.471092582321034e-05,-0.0007713841352746442] Loss: 22.842737296722586\n",
      "Iteracion: 17124 Gradiente: [4.46871561024409e-05,-0.0007709740437916205] Loss: 22.842737296125716\n",
      "Iteracion: 17125 Gradiente: [4.466339897343611e-05,-0.0007705641703295167] Loss: 22.84273729552947\n",
      "Iteracion: 17126 Gradiente: [4.463965455935674e-05,-0.0007701545147628035] Loss: 22.842737294933876\n",
      "Iteracion: 17127 Gradiente: [4.461592273893681e-05,-0.0007697450769838335] Loss: 22.8427372943389\n",
      "Iteracion: 17128 Gradiente: [4.4592203543440216e-05,-0.0007693358568734728] Loss: 22.842737293744563\n",
      "Iteracion: 17129 Gradiente: [4.456849701834168e-05,-0.0007689268543145999] Loss: 22.842737293150858\n",
      "Iteracion: 17130 Gradiente: [4.4544803018690496e-05,-0.0007685180691971993] Loss: 22.842737292557775\n",
      "Iteracion: 17131 Gradiente: [4.452112163543613e-05,-0.0007681095014026103] Loss: 22.842737291965328\n",
      "Iteracion: 17132 Gradiente: [4.4497452809840375e-05,-0.0007677011508163171] Loss: 22.84273729137351\n",
      "Iteracion: 17133 Gradiente: [4.447379661485229e-05,-0.0007672930173191853] Loss: 22.84273729078232\n",
      "Iteracion: 17134 Gradiente: [4.445015295289068e-05,-0.0007668851008011993] Loss: 22.842737290191774\n",
      "Iteracion: 17135 Gradiente: [4.442652196701147e-05,-0.0007664774011376589] Loss: 22.842737289601846\n",
      "Iteracion: 17136 Gradiente: [4.4402903382471476e-05,-0.0007660699182290879] Loss: 22.84273728901253\n",
      "Iteracion: 17137 Gradiente: [4.437929750622516e-05,-0.0007656626519423782] Loss: 22.84273728842385\n",
      "Iteracion: 17138 Gradiente: [4.435570405689759e-05,-0.0007652556021778168] Loss: 22.84273728783579\n",
      "Iteracion: 17139 Gradiente: [4.433212325617812e-05,-0.0007648487688072692] Loss: 22.842737287248376\n",
      "Iteracion: 17140 Gradiente: [4.4308554918378226e-05,-0.0007644421517251014] Loss: 22.842737286661563\n",
      "Iteracion: 17141 Gradiente: [4.42849991335e-05,-0.0007640357508133632] Loss: 22.842737286075387\n",
      "Iteracion: 17142 Gradiente: [4.426145585891088e-05,-0.0007636295659564732] Loss: 22.842737285489832\n",
      "Iteracion: 17143 Gradiente: [4.423792512208517e-05,-0.0007632235970387313] Loss: 22.842737284904896\n",
      "Iteracion: 17144 Gradiente: [4.4214406869021636e-05,-0.0007628178439488191] Loss: 22.842737284320588\n",
      "Iteracion: 17145 Gradiente: [4.419090118119584e-05,-0.0007624123065659443] Loss: 22.842737283736895\n",
      "Iteracion: 17146 Gradiente: [4.416740792407836e-05,-0.0007620069847825779] Loss: 22.842737283153824\n",
      "Iteracion: 17147 Gradiente: [4.414392717914476e-05,-0.0007616018784797044] Loss: 22.842737282571367\n",
      "Iteracion: 17148 Gradiente: [4.4120458883867283e-05,-0.0007611969875455316] Loss: 22.84273728198955\n",
      "Iteracion: 17149 Gradiente: [4.4097003147195817e-05,-0.0007607923118585565] Loss: 22.84273728140833\n",
      "Iteracion: 17150 Gradiente: [4.407355979765271e-05,-0.0007603878513144479] Loss: 22.84273728082774\n",
      "Iteracion: 17151 Gradiente: [4.405012894134567e-05,-0.0007599836057926505] Loss: 22.842737280247754\n",
      "Iteracion: 17152 Gradiente: [4.40267105602743e-05,-0.0007595795751792404] Loss: 22.84273727966839\n",
      "Iteracion: 17153 Gradiente: [4.400330461275341e-05,-0.0007591757593631361] Loss: 22.84273727908965\n",
      "Iteracion: 17154 Gradiente: [4.3979911076045634e-05,-0.0007587721582288746] Loss: 22.842737278511525\n",
      "Iteracion: 17155 Gradiente: [4.395652998615181e-05,-0.0007583687716605188] Loss: 22.84273727793401\n",
      "Iteracion: 17156 Gradiente: [4.3933161396125794e-05,-0.0007579655995395266] Loss: 22.842737277357106\n",
      "Iteracion: 17157 Gradiente: [4.390980519322814e-05,-0.0007575626417612114] Loss: 22.84273727678082\n",
      "Iteracion: 17158 Gradiente: [4.3886461417249244e-05,-0.0007571598982062019] Loss: 22.84273727620513\n",
      "Iteracion: 17159 Gradiente: [4.386312995639703e-05,-0.0007567573687673246] Loss: 22.842737275630064\n",
      "Iteracion: 17160 Gradiente: [4.383981095467486e-05,-0.0007563550533236688] Loss: 22.84273727505562\n",
      "Iteracion: 17161 Gradiente: [4.381650437702926e-05,-0.0007559529517603636] Loss: 22.84273727448177\n",
      "Iteracion: 17162 Gradiente: [4.3793210147669014e-05,-0.0007555510639694063] Loss: 22.842737273908543\n",
      "Iteracion: 17163 Gradiente: [4.376992829217367e-05,-0.0007551493898346232] Loss: 22.842737273335935\n",
      "Iteracion: 17164 Gradiente: [4.3746658887281836e-05,-0.0007547479292374722] Loss: 22.84273727276391\n",
      "Iteracion: 17165 Gradiente: [4.3723401793727136e-05,-0.0007543466820730297] Loss: 22.8427372721925\n",
      "Iteracion: 17166 Gradiente: [4.3700157150775945e-05,-0.0007539456482183719] Loss: 22.842737271621694\n",
      "Iteracion: 17167 Gradiente: [4.367692487316314e-05,-0.0007535448275656146] Loss: 22.8427372710515\n",
      "Iteracion: 17168 Gradiente: [4.365370481783278e-05,-0.0007531442200084134] Loss: 22.842737270481926\n",
      "Iteracion: 17169 Gradiente: [4.3630497157209905e-05,-0.0007527438254228969] Loss: 22.842737269912956\n",
      "Iteracion: 17170 Gradiente: [4.360730180602938e-05,-0.0007523436437031942] Loss: 22.84273726934458\n",
      "Iteracion: 17171 Gradiente: [4.358411886566197e-05,-0.0007519436747267368] Loss: 22.84273726877679\n",
      "Iteracion: 17172 Gradiente: [4.3560948223368236e-05,-0.0007515439183874169] Loss: 22.842737268209614\n",
      "Iteracion: 17173 Gradiente: [4.353778995967635e-05,-0.0007511443745678766] Loss: 22.842737267643052\n",
      "Iteracion: 17174 Gradiente: [4.351464382542266e-05,-0.0007507450431680477] Loss: 22.842737267077105\n",
      "Iteracion: 17175 Gradiente: [4.349151010103469e-05,-0.000750345924060151] Loss: 22.84273726651174\n",
      "Iteracion: 17176 Gradiente: [4.346838866050954e-05,-0.0007499470171358287] Loss: 22.84273726594698\n",
      "Iteracion: 17177 Gradiente: [4.3445279558795846e-05,-0.0007495483222810388] Loss: 22.842737265382823\n",
      "Iteracion: 17178 Gradiente: [4.342218268410155e-05,-0.0007491498393875417] Loss: 22.842737264819263\n",
      "Iteracion: 17179 Gradiente: [4.33990980800066e-05,-0.000748751568339993] Loss: 22.84273726425629\n",
      "Iteracion: 17180 Gradiente: [4.337602586588218e-05,-0.0007483535090187843] Loss: 22.84273726369393\n",
      "Iteracion: 17181 Gradiente: [4.335296584940806e-05,-0.000747955661322545] Loss: 22.842737263132168\n",
      "Iteracion: 17182 Gradiente: [4.33299180836381e-05,-0.0007475580251342724] Loss: 22.842737262571013\n",
      "Iteracion: 17183 Gradiente: [4.330688253162407e-05,-0.0007471606003441877] Loss: 22.842737262010427\n",
      "Iteracion: 17184 Gradiente: [4.328385925494634e-05,-0.0007467633868362356] Loss: 22.84273726145046\n",
      "Iteracion: 17185 Gradiente: [4.326084830855355e-05,-0.0007463663844924658] Loss: 22.842737260891077\n",
      "Iteracion: 17186 Gradiente: [4.323784951528372e-05,-0.0007459695932137578] Loss: 22.84273726033229\n",
      "Iteracion: 17187 Gradiente: [4.3214862915874616e-05,-0.000745573012882043] Loss: 22.84273725977409\n",
      "Iteracion: 17188 Gradiente: [4.319188852453711e-05,-0.0007451766433857662] Loss: 22.8427372592165\n",
      "Iteracion: 17189 Gradiente: [4.316892644643152e-05,-0.0007447804846055561] Loss: 22.842737258659497\n",
      "Iteracion: 17190 Gradiente: [4.314597649207978e-05,-0.0007443845364405159] Loss: 22.842737258103067\n",
      "Iteracion: 17191 Gradiente: [4.312303874106268e-05,-0.0007439887987741163] Loss: 22.842737257547274\n",
      "Iteracion: 17192 Gradiente: [4.3100113249276246e-05,-0.0007435932714892364] Loss: 22.84273725699202\n",
      "Iteracion: 17193 Gradiente: [4.307719991440232e-05,-0.0007431979544816632] Loss: 22.842737256437385\n",
      "Iteracion: 17194 Gradiente: [4.305429881033736e-05,-0.0007428028476328545] Loss: 22.842737255883335\n",
      "Iteracion: 17195 Gradiente: [4.303140976844588e-05,-0.0007424079508419131] Loss: 22.84273725532986\n",
      "Iteracion: 17196 Gradiente: [4.3008532931783826e-05,-0.0007420132639884021] Loss: 22.842737254776992\n",
      "Iteracion: 17197 Gradiente: [4.298566823782342e-05,-0.0007416187869646744] Loss: 22.842737254224723\n",
      "Iteracion: 17198 Gradiente: [4.296281582298889e-05,-0.000741224519648398] Loss: 22.842737253673004\n",
      "Iteracion: 17199 Gradiente: [4.293997547790696e-05,-0.000740830461941755] Loss: 22.8427372531219\n",
      "Iteracion: 17200 Gradiente: [4.291714723384151e-05,-0.0007404366137304663] Loss: 22.842737252571375\n",
      "Iteracion: 17201 Gradiente: [4.289433115995204e-05,-0.000740042974898832] Loss: 22.84273725202143\n",
      "Iteracion: 17202 Gradiente: [4.287152717002603e-05,-0.0007396495453415734] Loss: 22.84273725147208\n",
      "Iteracion: 17203 Gradiente: [4.284873535596034e-05,-0.0007392563249393191] Loss: 22.8427372509233\n",
      "Iteracion: 17204 Gradiente: [4.2825955623015945e-05,-0.0007388633135876195] Loss: 22.842737250375112\n",
      "Iteracion: 17205 Gradiente: [4.280318801287801e-05,-0.0007384705111711298] Loss: 22.8427372498275\n",
      "Iteracion: 17206 Gradiente: [4.278043253975738e-05,-0.0007380779175788869] Loss: 22.842737249280468\n",
      "Iteracion: 17207 Gradiente: [4.275768915723196e-05,-0.0007376855327032435] Loss: 22.84273724873403\n",
      "Iteracion: 17208 Gradiente: [4.2734957851090864e-05,-0.0007372933564314603] Loss: 22.84273724818817\n",
      "Iteracion: 17209 Gradiente: [4.271223874449485e-05,-0.0007369013886454685] Loss: 22.842737247642884\n",
      "Iteracion: 17210 Gradiente: [4.268953155417421e-05,-0.0007365096292514769] Loss: 22.842737247098178\n",
      "Iteracion: 17211 Gradiente: [4.266683651223957e-05,-0.0007361180781251402] Loss: 22.842737246554055\n",
      "Iteracion: 17212 Gradiente: [4.264415352584668e-05,-0.0007357267351586927] Loss: 22.84273724601051\n",
      "Iteracion: 17213 Gradiente: [4.262148254383646e-05,-0.0007353356002454348] Loss: 22.84273724546754\n",
      "Iteracion: 17214 Gradiente: [4.2598823678948365e-05,-0.0007349446732686004] Loss: 22.842737244925157\n",
      "Iteracion: 17215 Gradiente: [4.2576176882865485e-05,-0.0007345539541182925] Loss: 22.842737244383343\n",
      "Iteracion: 17216 Gradiente: [4.2553542042848375e-05,-0.0007341634426914822] Loss: 22.842737243842105\n",
      "Iteracion: 17217 Gradiente: [4.25309192886895e-05,-0.0007337731388692722] Loss: 22.84273724330144\n",
      "Iteracion: 17218 Gradiente: [4.25083085483872e-05,-0.0007333830425460282] Loss: 22.842737242761352\n",
      "Iteracion: 17219 Gradiente: [4.248570974425547e-05,-0.00073299315361434] Loss: 22.84273724222184\n",
      "Iteracion: 17220 Gradiente: [4.246312304777196e-05,-0.0007326034719540075] Loss: 22.842737241682887\n",
      "Iteracion: 17221 Gradiente: [4.2440548348092004e-05,-0.0007322139974612914] Loss: 22.84273724114452\n",
      "Iteracion: 17222 Gradiente: [4.2417985672689916e-05,-0.0007318247300238075] Loss: 22.842737240606734\n",
      "Iteracion: 17223 Gradiente: [4.2395434995038765e-05,-0.0007314356695311848] Loss: 22.842737240069507\n",
      "Iteracion: 17224 Gradiente: [4.237289627534816e-05,-0.0007310468158790921] Loss: 22.842737239532845\n",
      "Iteracion: 17225 Gradiente: [4.235036953919765e-05,-0.000730658168952895] Loss: 22.842737238996765\n",
      "Iteracion: 17226 Gradiente: [4.232785478469244e-05,-0.0007302697286426963] Loss: 22.842737238461247\n",
      "Iteracion: 17227 Gradiente: [4.230535205541249e-05,-0.0007298814948361117] Loss: 22.842737237926315\n",
      "Iteracion: 17228 Gradiente: [4.2282861169458856e-05,-0.0007294934674327181] Loss: 22.842737237391937\n",
      "Iteracion: 17229 Gradiente: [4.226038228504573e-05,-0.0007291056463146835] Loss: 22.84273723685813\n",
      "Iteracion: 17230 Gradiente: [4.2237915411647006e-05,-0.0007287180313706898] Loss: 22.842737236324897\n",
      "Iteracion: 17231 Gradiente: [4.221546043273368e-05,-0.0007283306224977082] Loss: 22.84273723579222\n",
      "Iteracion: 17232 Gradiente: [4.2193017375780076e-05,-0.0007279434195838282] Loss: 22.84273723526012\n",
      "Iteracion: 17233 Gradiente: [4.217058629573482e-05,-0.0007275564225167841] Loss: 22.842737234728574\n",
      "Iteracion: 17234 Gradiente: [4.214816705996327e-05,-0.0007271696311936655] Loss: 22.842737234197607\n",
      "Iteracion: 17235 Gradiente: [4.212575984183786e-05,-0.0007267830454955752] Loss: 22.842737233667187\n",
      "Iteracion: 17236 Gradiente: [4.2103364555146074e-05,-0.0007263966653159315] Loss: 22.84273723313735\n",
      "Iteracion: 17237 Gradiente: [4.2080981097569746e-05,-0.0007260104905534822] Loss: 22.842737232608066\n",
      "Iteracion: 17238 Gradiente: [4.205860947858279e-05,-0.0007256245210963167] Loss: 22.842737232079333\n",
      "Iteracion: 17239 Gradiente: [4.203624984313592e-05,-0.0007252387568276693] Loss: 22.842737231551176\n",
      "Iteracion: 17240 Gradiente: [4.201390208796359e-05,-0.0007248531976428533] Loss: 22.84273723102359\n",
      "Iteracion: 17241 Gradiente: [4.1991566184644095e-05,-0.0007244678434345768] Loss: 22.842737230496542\n",
      "Iteracion: 17242 Gradiente: [4.196924218244173e-05,-0.0007240826940915213] Loss: 22.84273722997006\n",
      "Iteracion: 17243 Gradiente: [4.1946930123989054e-05,-0.0007236977495014211] Loss: 22.842737229444147\n",
      "Iteracion: 17244 Gradiente: [4.192462975538547e-05,-0.0007233130095697741] Loss: 22.842737228918793\n",
      "Iteracion: 17245 Gradiente: [4.19023413475846e-05,-0.0007229284741716431] Loss: 22.84273722839399\n",
      "Iteracion: 17246 Gradiente: [4.1880064793531345e-05,-0.0007225441432036442] Loss: 22.842737227869748\n",
      "Iteracion: 17247 Gradiente: [4.185780006954095e-05,-0.0007221600165575381] Loss: 22.842737227346053\n",
      "Iteracion: 17248 Gradiente: [4.183554720119294e-05,-0.0007217760941250854] Loss: 22.842737226822926\n",
      "Iteracion: 17249 Gradiente: [4.1813306113643495e-05,-0.0007213923757994678] Loss: 22.842737226300358\n",
      "Iteracion: 17250 Gradiente: [4.1791076862788636e-05,-0.000721008861470788] Loss: 22.842737225778347\n",
      "Iteracion: 17251 Gradiente: [4.176885947136573e-05,-0.0007206255510278462] Loss: 22.842737225256883\n",
      "Iteracion: 17252 Gradiente: [4.174665383421446e-05,-0.0007202424443676136] Loss: 22.842737224735977\n",
      "Iteracion: 17253 Gradiente: [4.1724460004388675e-05,-0.0007198595413764034] Loss: 22.84273722421563\n",
      "Iteracion: 17254 Gradiente: [4.17022779799936e-05,-0.000719476841950358] Loss: 22.842737223695817\n",
      "Iteracion: 17255 Gradiente: [4.168010777618747e-05,-0.0007190943459766193] Loss: 22.842737223176574\n",
      "Iteracion: 17256 Gradiente: [4.1657949368338146e-05,-0.0007187120533484877] Loss: 22.842737222657885\n",
      "Iteracion: 17257 Gradiente: [4.1635802799078195e-05,-0.0007183299639553553] Loss: 22.84273722213974\n",
      "Iteracion: 17258 Gradiente: [4.161366787513998e-05,-0.0007179480777004699] Loss: 22.842737221622134\n",
      "Iteracion: 17259 Gradiente: [4.159154482863414e-05,-0.0007175663944617365] Loss: 22.842737221105097\n",
      "Iteracion: 17260 Gradiente: [4.156943343597656e-05,-0.0007171849141447713] Loss: 22.842737220588607\n",
      "Iteracion: 17261 Gradiente: [4.1547333857276196e-05,-0.0007168036366296111] Loss: 22.842737220072657\n",
      "Iteracion: 17262 Gradiente: [4.152524606126917e-05,-0.0007164225618119247] Loss: 22.842737219557257\n",
      "Iteracion: 17263 Gradiente: [4.1503169898267817e-05,-0.0007160416895914068] Loss: 22.842737219042416\n",
      "Iteracion: 17264 Gradiente: [4.148110565248923e-05,-0.0007156610198437126] Loss: 22.842737218528114\n",
      "Iteracion: 17265 Gradiente: [4.145905297339899e-05,-0.000715280552481327] Loss: 22.842737218014356\n",
      "Iteracion: 17266 Gradiente: [4.1437012025843005e-05,-0.0007149002873861813] Loss: 22.842737217501135\n",
      "Iteracion: 17267 Gradiente: [4.141498280129478e-05,-0.000714520224452168] Loss: 22.84273721698849\n",
      "Iteracion: 17268 Gradiente: [4.1392965379335085e-05,-0.0007141403635659553] Loss: 22.84273721647637\n",
      "Iteracion: 17269 Gradiente: [4.137095965196143e-05,-0.0007137607046265278] Loss: 22.842737215964796\n",
      "Iteracion: 17270 Gradiente: [4.134896559643645e-05,-0.0007133812475258831] Loss: 22.842737215453763\n",
      "Iteracion: 17271 Gradiente: [4.1326983150232385e-05,-0.0007130019921621766] Loss: 22.84273721494327\n",
      "Iteracion: 17272 Gradiente: [4.130501240240392e-05,-0.000712622938421011] Loss: 22.842737214433324\n",
      "Iteracion: 17273 Gradiente: [4.128305347232223e-05,-0.0007122440861881074] Loss: 22.84273721392393\n",
      "Iteracion: 17274 Gradiente: [4.1261106150614066e-05,-0.0007118654353686082] Loss: 22.842737213415067\n",
      "Iteracion: 17275 Gradiente: [4.1239170490333286e-05,-0.0007114869858520241] Loss: 22.842737212906744\n",
      "Iteracion: 17276 Gradiente: [4.1217246474426855e-05,-0.000711108737532129] Loss: 22.842737212398962\n",
      "Iteracion: 17277 Gradiente: [4.119533411142129e-05,-0.0007107306902994992] Loss: 22.842737211891734\n",
      "Iteracion: 17278 Gradiente: [4.117343342784352e-05,-0.0007103528440479086] Loss: 22.842737211385035\n",
      "Iteracion: 17279 Gradiente: [4.115154436211317e-05,-0.0007099751986725522] Loss: 22.84273721087887\n",
      "Iteracion: 17280 Gradiente: [4.112966689812462e-05,-0.0007095977540667301] Loss: 22.84273721037324\n",
      "Iteracion: 17281 Gradiente: [4.110780105198349e-05,-0.000709220510122203] Loss: 22.842737209868158\n",
      "Iteracion: 17282 Gradiente: [4.108594695727182e-05,-0.0007088434667256394] Loss: 22.84273720936361\n",
      "Iteracion: 17283 Gradiente: [4.106410444251196e-05,-0.0007084666237782737] Loss: 22.842737208859585\n",
      "Iteracion: 17284 Gradiente: [4.104227354465214e-05,-0.0007080899811725772] Loss: 22.842737208356123\n",
      "Iteracion: 17285 Gradiente: [4.1020454151900294e-05,-0.0007077135388068238] Loss: 22.84273720785317\n",
      "Iteracion: 17286 Gradiente: [4.099864640162802e-05,-0.0007073372965681557] Loss: 22.842737207350776\n",
      "Iteracion: 17287 Gradiente: [4.097685022278104e-05,-0.0007069612543516494] Loss: 22.842737206848888\n",
      "Iteracion: 17288 Gradiente: [4.095506571578274e-05,-0.0007065854120469339] Loss: 22.842737206347564\n",
      "Iteracion: 17289 Gradiente: [4.0933292700628955e-05,-0.0007062097695555991] Loss: 22.842737205846745\n",
      "Iteracion: 17290 Gradiente: [4.0911531285322175e-05,-0.0007058343267663266] Loss: 22.842737205346467\n",
      "Iteracion: 17291 Gradiente: [4.08897814509146e-05,-0.0007054590835737192] Loss: 22.84273720484672\n",
      "Iteracion: 17292 Gradiente: [4.086804315003671e-05,-0.000705084039873564] Loss: 22.84273720434752\n",
      "Iteracion: 17293 Gradiente: [4.0846316472690584e-05,-0.0007047091955544242] Loss: 22.84273720384883\n",
      "Iteracion: 17294 Gradiente: [4.0824601294768094e-05,-0.0007043345505158764] Loss: 22.842737203350676\n",
      "Iteracion: 17295 Gradiente: [4.080289771385045e-05,-0.0007039601046481418] Loss: 22.842737202853062\n",
      "Iteracion: 17296 Gradiente: [4.078120563330382e-05,-0.0007035858578485469] Loss: 22.842737202355966\n",
      "Iteracion: 17297 Gradiente: [4.075952516586767e-05,-0.0007032118100060103] Loss: 22.8427372018594\n",
      "Iteracion: 17298 Gradiente: [4.073785619311821e-05,-0.0007028379610204638] Loss: 22.842737201363374\n",
      "Iteracion: 17299 Gradiente: [4.0716198688528495e-05,-0.0007024643107881682] Loss: 22.84273720086786\n",
      "Iteracion: 17300 Gradiente: [4.069455269567849e-05,-0.0007020908591992262] Loss: 22.842737200372877\n",
      "Iteracion: 17301 Gradiente: [4.0672918202252126e-05,-0.0007017176061489513] Loss: 22.842737199878435\n",
      "Iteracion: 17302 Gradiente: [4.0651295253724126e-05,-0.0007013445515288671] Loss: 22.842737199384487\n",
      "Iteracion: 17303 Gradiente: [4.062968378282979e-05,-0.0007009716952372476] Loss: 22.842737198891083\n",
      "Iteracion: 17304 Gradiente: [4.060808374314699e-05,-0.0007005990371711827] Loss: 22.842737198398204\n",
      "Iteracion: 17305 Gradiente: [4.0586495215203894e-05,-0.0007002265772203013] Loss: 22.84273719790586\n",
      "Iteracion: 17306 Gradiente: [4.0564918157315334e-05,-0.0006998543152799167] Loss: 22.842737197414014\n",
      "Iteracion: 17307 Gradiente: [4.054335266137817e-05,-0.0006994822512409608] Loss: 22.842737196922712\n",
      "Iteracion: 17308 Gradiente: [4.052179855686215e-05,-0.0006991103850057337] Loss: 22.84273719643191\n",
      "Iteracion: 17309 Gradiente: [4.050025592997978e-05,-0.0006987387164665885] Loss: 22.842737195941652\n",
      "Iteracion: 17310 Gradiente: [4.0478724719150706e-05,-0.0006983672455186015] Loss: 22.8427371954519\n",
      "Iteracion: 17311 Gradiente: [4.045720496795487e-05,-0.0006979959720560203] Loss: 22.842737194962687\n",
      "Iteracion: 17312 Gradiente: [4.0435696670707934e-05,-0.0006976248959727372] Loss: 22.842737194473973\n",
      "Iteracion: 17313 Gradiente: [4.0414199921201545e-05,-0.0006972540171583812] Loss: 22.842737193985787\n",
      "Iteracion: 17314 Gradiente: [4.039271441342862e-05,-0.0006968833355259108] Loss: 22.84273719349812\n",
      "Iteracion: 17315 Gradiente: [4.037124037002589e-05,-0.0006965128509559548] Loss: 22.842737193010965\n",
      "Iteracion: 17316 Gradiente: [4.0349777809941166e-05,-0.000696142563342761] Loss: 22.842737192524343\n",
      "Iteracion: 17317 Gradiente: [4.032832666401494e-05,-0.0006957724725860241] Loss: 22.84273719203823\n",
      "Iteracion: 17318 Gradiente: [4.030688691140464e-05,-0.0006954025785818866] Loss: 22.842737191552622\n",
      "Iteracion: 17319 Gradiente: [4.028545849621423e-05,-0.000695032881228741] Loss: 22.842737191067553\n",
      "Iteracion: 17320 Gradiente: [4.0264041538762284e-05,-0.0006946633804136108] Loss: 22.842737190582998\n",
      "Iteracion: 17321 Gradiente: [4.024263596231018e-05,-0.000694294076036428] Loss: 22.84273719009894\n",
      "Iteracion: 17322 Gradiente: [4.022124174791012e-05,-0.0006939249679934534] Loss: 22.84273718961539\n",
      "Iteracion: 17323 Gradiente: [4.0199858901246444e-05,-0.0006935560561810661] Loss: 22.84273718913238\n",
      "Iteracion: 17324 Gradiente: [4.017848739295005e-05,-0.000693187340494461] Loss: 22.842737188649874\n",
      "Iteracion: 17325 Gradiente: [4.0157127264706106e-05,-0.0006928188208270569] Loss: 22.842737188167874\n",
      "Iteracion: 17326 Gradiente: [4.013577852125157e-05,-0.0006924504970747591] Loss: 22.8427371876864\n",
      "Iteracion: 17327 Gradiente: [4.011444111995388e-05,-0.0006920823691359601] Loss: 22.842737187205426\n",
      "Iteracion: 17328 Gradiente: [4.009311502007525e-05,-0.0006917144369073942] Loss: 22.842737186724957\n",
      "Iteracion: 17329 Gradiente: [4.007180028035388e-05,-0.0006913467002816513] Loss: 22.84273718624502\n",
      "Iteracion: 17330 Gradiente: [4.00504968979476e-05,-0.000690979159153926] Loss: 22.842737185765586\n",
      "Iteracion: 17331 Gradiente: [4.002920477811737e-05,-0.0006906118134258084] Loss: 22.84273718528664\n",
      "Iteracion: 17332 Gradiente: [4.000792403360265e-05,-0.0006902446629878748] Loss: 22.842737184808225\n",
      "Iteracion: 17333 Gradiente: [3.998665456398006e-05,-0.000689877707739465] Loss: 22.842737184330325\n",
      "Iteracion: 17334 Gradiente: [3.996539638630263e-05,-0.0006895109475775503] Loss: 22.84273718385291\n",
      "Iteracion: 17335 Gradiente: [3.994414962941543e-05,-0.0006891443823879702] Loss: 22.842737183376016\n",
      "Iteracion: 17336 Gradiente: [3.992291414647298e-05,-0.0006887780120790647] Loss: 22.842737182899622\n",
      "Iteracion: 17337 Gradiente: [3.9901689917580066e-05,-0.0006884118365442523] Loss: 22.84273718242375\n",
      "Iteracion: 17338 Gradiente: [3.988047690578848e-05,-0.000688045855684886] Loss: 22.842737181948376\n",
      "Iteracion: 17339 Gradiente: [3.9859275215311145e-05,-0.0006876800693890553] Loss: 22.842737181473503\n",
      "Iteracion: 17340 Gradiente: [3.9838084814884195e-05,-0.0006873144775553896] Loss: 22.842737180999148\n",
      "Iteracion: 17341 Gradiente: [3.981690564103246e-05,-0.0006869490800837023] Loss: 22.842737180525273\n",
      "Iteracion: 17342 Gradiente: [3.9795737806495404e-05,-0.0006865838768636223] Loss: 22.842737180051923\n",
      "Iteracion: 17343 Gradiente: [3.977458117105925e-05,-0.0006862188677994633] Loss: 22.842737179579046\n",
      "Iteracion: 17344 Gradiente: [3.975343578777786e-05,-0.0006858540527851176] Loss: 22.842737179106702\n",
      "Iteracion: 17345 Gradiente: [3.973230156380699e-05,-0.0006854894317228855] Loss: 22.842737178634856\n",
      "Iteracion: 17346 Gradiente: [3.9711178654518635e-05,-0.0006851250044999091] Loss: 22.8427371781635\n",
      "Iteracion: 17347 Gradiente: [3.9690067031491104e-05,-0.0006847607710156467] Loss: 22.84273717769266\n",
      "Iteracion: 17348 Gradiente: [3.966896653745759e-05,-0.0006843967311731091] Loss: 22.842737177222308\n",
      "Iteracion: 17349 Gradiente: [3.9647877297473615e-05,-0.0006840328848631098] Loss: 22.84273717675246\n",
      "Iteracion: 17350 Gradiente: [3.962679935417176e-05,-0.0006836692319798962] Loss: 22.842737176283112\n",
      "Iteracion: 17351 Gradiente: [3.96057324441775e-05,-0.0006833057724363082] Loss: 22.842737175814257\n",
      "Iteracion: 17352 Gradiente: [3.958467682139144e-05,-0.0006829425061132117] Loss: 22.842737175345913\n",
      "Iteracion: 17353 Gradiente: [3.956363239391673e-05,-0.0006825794329145651] Loss: 22.84273717487805\n",
      "Iteracion: 17354 Gradiente: [3.9542599178806386e-05,-0.0006822165527348526] Loss: 22.84273717441071\n",
      "Iteracion: 17355 Gradiente: [3.952157711637483e-05,-0.000681853865475072] Loss: 22.842737173943842\n",
      "Iteracion: 17356 Gradiente: [3.950056624546505e-05,-0.000681491371030063] Loss: 22.84273717347748\n",
      "Iteracion: 17357 Gradiente: [3.94795665717614e-05,-0.0006811290692963231] Loss: 22.842737173011617\n",
      "Iteracion: 17358 Gradiente: [3.9458578036525675e-05,-0.0006807669601750869] Loss: 22.842737172546247\n",
      "Iteracion: 17359 Gradiente: [3.943760066154785e-05,-0.0006804050435611942] Loss: 22.842737172081364\n",
      "Iteracion: 17360 Gradiente: [3.941663437293149e-05,-0.0006800433193574188] Loss: 22.842737171616978\n",
      "Iteracion: 17361 Gradiente: [3.9395679238888684e-05,-0.0006796817874577717] Loss: 22.842737171153097\n",
      "Iteracion: 17362 Gradiente: [3.937473537500106e-05,-0.0006793204477511713] Loss: 22.84273717068969\n",
      "Iteracion: 17363 Gradiente: [3.935380241936552e-05,-0.0006789593001579182] Loss: 22.84273717022679\n",
      "Iteracion: 17364 Gradiente: [3.9332880738622104e-05,-0.0006785983445534309] Loss: 22.842737169764384\n",
      "Iteracion: 17365 Gradiente: [3.9311970192557054e-05,-0.0006782375808435622] Loss: 22.842737169302463\n",
      "Iteracion: 17366 Gradiente: [3.9291070773591245e-05,-0.000677877008924573] Loss: 22.84273716884103\n",
      "Iteracion: 17367 Gradiente: [3.927018242014431e-05,-0.0006775166287001848] Loss: 22.842737168380093\n",
      "Iteracion: 17368 Gradiente: [3.924930520706009e-05,-0.0006771564400627502] Loss: 22.84273716791964\n",
      "Iteracion: 17369 Gradiente: [3.922843904623126e-05,-0.0006767964429156355] Loss: 22.84273716745968\n",
      "Iteracion: 17370 Gradiente: [3.920758409113508e-05,-0.0006764366371470488] Loss: 22.84273716700021\n",
      "Iteracion: 17371 Gradiente: [3.9186740093555275e-05,-0.0006760770226706588] Loss: 22.842737166541237\n",
      "Iteracion: 17372 Gradiente: [3.9165907192758213e-05,-0.0006757175993755027] Loss: 22.842737166082745\n",
      "Iteracion: 17373 Gradiente: [3.9145085356532645e-05,-0.000675358367161157] Loss: 22.842737165624737\n",
      "Iteracion: 17374 Gradiente: [3.912427462940589e-05,-0.0006749993259222246] Loss: 22.842737165167215\n",
      "Iteracion: 17375 Gradiente: [3.910347492516545e-05,-0.0006746404755650322] Loss: 22.842737164710186\n",
      "Iteracion: 17376 Gradiente: [3.908268633286601e-05,-0.0006742818159799195] Loss: 22.842737164253638\n",
      "Iteracion: 17377 Gradiente: [3.906190876440026e-05,-0.0006739233470695419] Loss: 22.842737163797572\n",
      "Iteracion: 17378 Gradiente: [3.9041142287980316e-05,-0.0006735650687305157] Loss: 22.842737163341997\n",
      "Iteracion: 17379 Gradiente: [3.902038681076192e-05,-0.0006732069808657333] Loss: 22.84273716288691\n",
      "Iteracion: 17380 Gradiente: [3.899964237537764e-05,-0.0006728490833705081] Loss: 22.842737162432293\n",
      "Iteracion: 17381 Gradiente: [3.8978908939194905e-05,-0.0006724913761464298] Loss: 22.842737161978167\n",
      "Iteracion: 17382 Gradiente: [3.8958186580847115e-05,-0.000672133859087154] Loss: 22.842737161524536\n",
      "Iteracion: 17383 Gradiente: [3.893747518759483e-05,-0.0006717765320975862] Loss: 22.842737161071355\n",
      "Iteracion: 17384 Gradiente: [3.8916774839966214e-05,-0.0006714193950733953] Loss: 22.842737160618682\n",
      "Iteracion: 17385 Gradiente: [3.889608546974917e-05,-0.0006710624479152235] Loss: 22.842737160166482\n",
      "Iteracion: 17386 Gradiente: [3.887540713378712e-05,-0.0006707056905187396] Loss: 22.842737159714773\n",
      "Iteracion: 17387 Gradiente: [3.8854739795131836e-05,-0.0006703491227848228] Loss: 22.842737159263525\n",
      "Iteracion: 17388 Gradiente: [3.88340834073612e-05,-0.0006699927446163656] Loss: 22.842737158812778\n",
      "Iteracion: 17389 Gradiente: [3.88134379714226e-05,-0.0006696365559112868] Loss: 22.84273715836249\n",
      "Iteracion: 17390 Gradiente: [3.879280355173857e-05,-0.0006692805565640706] Loss: 22.84273715791268\n",
      "Iteracion: 17391 Gradiente: [3.877218014262477e-05,-0.0006689247464753597] Loss: 22.842737157463375\n",
      "Iteracion: 17392 Gradiente: [3.875156768534301e-05,-0.0006685691255455595] Loss: 22.84273715701453\n",
      "Iteracion: 17393 Gradiente: [3.873096616473504e-05,-0.0006682136936770888] Loss: 22.84273715656616\n",
      "Iteracion: 17394 Gradiente: [3.871037558837998e-05,-0.0006678584507664453] Loss: 22.84273715611828\n",
      "Iteracion: 17395 Gradiente: [3.868979602638471e-05,-0.0006675033967109556] Loss: 22.842737155670843\n",
      "Iteracion: 17396 Gradiente: [3.866922731769288e-05,-0.000667148531417657] Loss: 22.842737155223926\n",
      "Iteracion: 17397 Gradiente: [3.864866954946441e-05,-0.0006667938547812706] Loss: 22.84273715477746\n",
      "Iteracion: 17398 Gradiente: [3.862812269327757e-05,-0.0006664393667037416] Loss: 22.842737154331466\n",
      "Iteracion: 17399 Gradiente: [3.860758682397621e-05,-0.0006660850670790808] Loss: 22.842737153885952\n",
      "Iteracion: 17400 Gradiente: [3.8587061796609606e-05,-0.0006657309558151544] Loss: 22.842737153440904\n",
      "Iteracion: 17401 Gradiente: [3.8566547820551024e-05,-0.0006653770327988677] Loss: 22.842737152996346\n",
      "Iteracion: 17402 Gradiente: [3.8546044681690246e-05,-0.000665023297943416] Loss: 22.842737152552235\n",
      "Iteracion: 17403 Gradiente: [3.8525552480450645e-05,-0.000664669751141389] Loss: 22.842737152108608\n",
      "Iteracion: 17404 Gradiente: [3.8505071028301545e-05,-0.00066431639230539] Loss: 22.842737151665467\n",
      "Iteracion: 17405 Gradiente: [3.848460051093146e-05,-0.0006639632213224426] Loss: 22.84273715122278\n",
      "Iteracion: 17406 Gradiente: [3.8464140917919094e-05,-0.0006636102380940183] Loss: 22.84273715078058\n",
      "Iteracion: 17407 Gradiente: [3.8443692200000136e-05,-0.0006632574425205225] Loss: 22.842737150338827\n",
      "Iteracion: 17408 Gradiente: [3.842325434296375e-05,-0.0006629048345075716] Loss: 22.842737149897566\n",
      "Iteracion: 17409 Gradiente: [3.840282739323205e-05,-0.0006625524139469263] Loss: 22.84273714945676\n",
      "Iteracion: 17410 Gradiente: [3.838241130627769e-05,-0.0006622001807455054] Loss: 22.84273714901643\n",
      "Iteracion: 17411 Gradiente: [3.8362006068837215e-05,-0.0006618481348006355] Loss: 22.842737148576564\n",
      "Iteracion: 17412 Gradiente: [3.8341611695121473e-05,-0.000661496276014617] Loss: 22.84273714813717\n",
      "Iteracion: 17413 Gradiente: [3.8321228082812316e-05,-0.000661144604291053] Loss: 22.842737147698237\n",
      "Iteracion: 17414 Gradiente: [3.83008552451732e-05,-0.0006607931195302305] Loss: 22.842737147259765\n",
      "Iteracion: 17415 Gradiente: [3.828049337357697e-05,-0.0006604418216237917] Loss: 22.84273714682178\n",
      "Iteracion: 17416 Gradiente: [3.8260142279492966e-05,-0.0006600907104797216] Loss: 22.84273714638424\n",
      "Iteracion: 17417 Gradiente: [3.8239802042501957e-05,-0.0006597397859946359] Loss: 22.84273714594717\n",
      "Iteracion: 17418 Gradiente: [3.821947254796972e-05,-0.0006593890480759275] Loss: 22.842737145510576\n",
      "Iteracion: 17419 Gradiente: [3.819915396737391e-05,-0.00065903849661512] Loss: 22.842737145074427\n",
      "Iteracion: 17420 Gradiente: [3.8178846062919546e-05,-0.0006586881315233957] Loss: 22.842737144638754\n",
      "Iteracion: 17421 Gradiente: [3.815854900892646e-05,-0.0006583379526954758] Loss: 22.84273714420355\n",
      "Iteracion: 17422 Gradiente: [3.813826280728942e-05,-0.0006579879600288052] Loss: 22.842737143768794\n",
      "Iteracion: 17423 Gradiente: [3.811798730832076e-05,-0.0006576381534346847] Loss: 22.84273714333451\n",
      "Iteracion: 17424 Gradiente: [3.809772268728769e-05,-0.000657288532801914] Loss: 22.842737142900695\n",
      "Iteracion: 17425 Gradiente: [3.807746869218439e-05,-0.0006569390980475968] Loss: 22.842737142467325\n",
      "Iteracion: 17426 Gradiente: [3.805722553143672e-05,-0.000656589849058283] Loss: 22.842737142034412\n",
      "Iteracion: 17427 Gradiente: [3.8036993138727364e-05,-0.0006562407857408914] Loss: 22.842737141601976\n",
      "Iteracion: 17428 Gradiente: [3.8016771488476785e-05,-0.0006558919079971304] Loss: 22.842737141169987\n",
      "Iteracion: 17429 Gradiente: [3.7996560644160125e-05,-0.0006555432157240896] Loss: 22.842737140738464\n",
      "Iteracion: 17430 Gradiente: [3.7976360547039195e-05,-0.000655194708826438] Loss: 22.842737140307396\n",
      "Iteracion: 17431 Gradiente: [3.7956171115638424e-05,-0.0006548463872099101] Loss: 22.842737139876792\n",
      "Iteracion: 17432 Gradiente: [3.79359924882768e-05,-0.0006544982507673325] Loss: 22.842737139446633\n",
      "Iteracion: 17433 Gradiente: [3.7915824501055793e-05,-0.0006541502994101241] Loss: 22.84273713901695\n",
      "Iteracion: 17434 Gradiente: [3.789566732166349e-05,-0.0006538025330299273] Loss: 22.842737138587708\n",
      "Iteracion: 17435 Gradiente: [3.787552083546568e-05,-0.0006534549515333054] Loss: 22.84273713815892\n",
      "Iteracion: 17436 Gradiente: [3.785538502919887e-05,-0.0006531075548245723] Loss: 22.842737137730612\n",
      "Iteracion: 17437 Gradiente: [3.783525989717873e-05,-0.0006527603428035415] Loss: 22.842737137302734\n",
      "Iteracion: 17438 Gradiente: [3.781514560330379e-05,-0.0006524133153632761] Loss: 22.84273713687532\n",
      "Iteracion: 17439 Gradiente: [3.779504180556614e-05,-0.0006520664724253796] Loss: 22.842737136448363\n",
      "Iteracion: 17440 Gradiente: [3.77749488251311e-05,-0.0006517198138729678] Loss: 22.842737136021864\n",
      "Iteracion: 17441 Gradiente: [3.7754866539785326e-05,-0.0006513733396133148] Loss: 22.8427371355958\n",
      "Iteracion: 17442 Gradiente: [3.77347948813167e-05,-0.0006510270495528658] Loss: 22.842737135170204\n",
      "Iteracion: 17443 Gradiente: [3.7714733933095584e-05,-0.0006506809435880001] Loss: 22.842737134745054\n",
      "Iteracion: 17444 Gradiente: [3.769468353596039e-05,-0.0006503350216320314] Loss: 22.84273713432036\n",
      "Iteracion: 17445 Gradiente: [3.767464391444264e-05,-0.0006499892835707991] Loss: 22.84273713389611\n",
      "Iteracion: 17446 Gradiente: [3.765461491127553e-05,-0.0006496437293165514] Loss: 22.842737133472326\n",
      "Iteracion: 17447 Gradiente: [3.763459655014382e-05,-0.0006492983587699304] Loss: 22.842737133048995\n",
      "Iteracion: 17448 Gradiente: [3.761458884715315e-05,-0.0006489531718318157] Loss: 22.84273713262609\n",
      "Iteracion: 17449 Gradiente: [3.7594591809882634e-05,-0.0006486081684048628] Loss: 22.84273713220365\n",
      "Iteracion: 17450 Gradiente: [3.757460533127717e-05,-0.0006482633483953985] Loss: 22.84273713178165\n",
      "Iteracion: 17451 Gradiente: [3.755462949944407e-05,-0.0006479187117026441] Loss: 22.842737131360106\n",
      "Iteracion: 17452 Gradiente: [3.753466439964844e-05,-0.0006475742582214394] Loss: 22.842737130939007\n",
      "Iteracion: 17453 Gradiente: [3.751470977514752e-05,-0.0006472299878709009] Loss: 22.84273713051836\n",
      "Iteracion: 17454 Gradiente: [3.749476581920893e-05,-0.0006468859005418419] Loss: 22.842737130098147\n",
      "Iteracion: 17455 Gradiente: [3.747483249204227e-05,-0.0006465419961381021] Loss: 22.84273712967841\n",
      "Iteracion: 17456 Gradiente: [3.7454909683750275e-05,-0.0006461982745696797] Loss: 22.8427371292591\n",
      "Iteracion: 17457 Gradiente: [3.743499757528449e-05,-0.0006458547357271508] Loss: 22.842737128840234\n",
      "Iteracion: 17458 Gradiente: [3.741509589569129e-05,-0.0006455113795305797] Loss: 22.84273712842182\n",
      "Iteracion: 17459 Gradiente: [3.7395204877081294e-05,-0.0006451682058672266] Loss: 22.842737128003836\n",
      "Iteracion: 17460 Gradiente: [3.7375324394398984e-05,-0.0006448252146488661] Loss: 22.84273712758631\n",
      "Iteracion: 17461 Gradiente: [3.7355454567962926e-05,-0.0006444824057695086] Loss: 22.84273712716923\n",
      "Iteracion: 17462 Gradiente: [3.733559527745456e-05,-0.0006441397791393891] Loss: 22.842737126752585\n",
      "Iteracion: 17463 Gradiente: [3.731574654655863e-05,-0.0006437973346595053] Loss: 22.84273712633638\n",
      "Iteracion: 17464 Gradiente: [3.729590835348518e-05,-0.0006434550722353549] Loss: 22.84273712592063\n",
      "Iteracion: 17465 Gradiente: [3.727608067928638e-05,-0.0006431129917692384] Loss: 22.842737125505305\n",
      "Iteracion: 17466 Gradiente: [3.72562636054378e-05,-0.0006427710931610874] Loss: 22.842737125090444\n",
      "Iteracion: 17467 Gradiente: [3.7236456986988745e-05,-0.0006424293763203082] Loss: 22.842737124676002\n",
      "Iteracion: 17468 Gradiente: [3.721666091867822e-05,-0.0006420878411460033] Loss: 22.84273712426201\n",
      "Iteracion: 17469 Gradiente: [3.7196875418506654e-05,-0.0006417464875408286] Loss: 22.842737123848462\n",
      "Iteracion: 17470 Gradiente: [3.717710043057802e-05,-0.000641405315408979] Loss: 22.842737123435356\n",
      "Iteracion: 17471 Gradiente: [3.715733588668021e-05,-0.0006410643246596237] Loss: 22.842737123022673\n",
      "Iteracion: 17472 Gradiente: [3.713758196018565e-05,-0.0006407235151835759] Loss: 22.842737122610448\n",
      "Iteracion: 17473 Gradiente: [3.7117838490985375e-05,-0.0006403828868954519] Loss: 22.842737122198642\n",
      "Iteracion: 17474 Gradiente: [3.70981054686581e-05,-0.0006400424396994472] Loss: 22.842737121787287\n",
      "Iteracion: 17475 Gradiente: [3.7078382983205906e-05,-0.0006397021734917038] Loss: 22.842737121376366\n",
      "Iteracion: 17476 Gradiente: [3.7058670971153634e-05,-0.0006393620881805617] Loss: 22.842737120965875\n",
      "Iteracion: 17477 Gradiente: [3.7038969441975196e-05,-0.0006390221836696242] Loss: 22.84273712055584\n",
      "Iteracion: 17478 Gradiente: [3.701927840798665e-05,-0.0006386824598603625] Loss: 22.842737120146218\n",
      "Iteracion: 17479 Gradiente: [3.6999597883398866e-05,-0.0006383429166572085] Loss: 22.842737119737045\n",
      "Iteracion: 17480 Gradiente: [3.697992775831456e-05,-0.0006380035539687393] Loss: 22.842737119328305\n",
      "Iteracion: 17481 Gradiente: [3.696026805547111e-05,-0.000637664371698321] Loss: 22.84273711892\n",
      "Iteracion: 17482 Gradiente: [3.69406188733971e-05,-0.0006373253697445828] Loss: 22.842737118512126\n",
      "Iteracion: 17483 Gradiente: [3.692098011166915e-05,-0.0006369865480146804] Loss: 22.842737118104676\n",
      "Iteracion: 17484 Gradiente: [3.690135183470981e-05,-0.0006366479064111512] Loss: 22.842737117697673\n",
      "Iteracion: 17485 Gradiente: [3.6881733878620554e-05,-0.0006363094448455323] Loss: 22.842737117291097\n",
      "Iteracion: 17486 Gradiente: [3.6862126393089056e-05,-0.0006359711632151506] Loss: 22.842737116884955\n",
      "Iteracion: 17487 Gradiente: [3.684252932979841e-05,-0.0006356330614245564] Loss: 22.842737116479253\n",
      "Iteracion: 17488 Gradiente: [3.6822942751276365e-05,-0.0006352951393774712] Loss: 22.842737116073973\n",
      "Iteracion: 17489 Gradiente: [3.680336657604736e-05,-0.000634957396978919] Loss: 22.842737115669127\n",
      "Iteracion: 17490 Gradiente: [3.6783800716951494e-05,-0.0006346198341402006] Loss: 22.8427371152647\n",
      "Iteracion: 17491 Gradiente: [3.676424533315033e-05,-0.0006342824507563923] Loss: 22.84273711486071\n",
      "Iteracion: 17492 Gradiente: [3.674470029769357e-05,-0.0006339452467376105] Loss: 22.842737114457147\n",
      "Iteracion: 17493 Gradiente: [3.6725165657950734e-05,-0.0006336082219882874] Loss: 22.842737114054025\n",
      "Iteracion: 17494 Gradiente: [3.6705641507713455e-05,-0.0006332713764050387] Loss: 22.84273711365133\n",
      "Iteracion: 17495 Gradiente: [3.668612770297841e-05,-0.0006329347099004679] Loss: 22.84273711324905\n",
      "Iteracion: 17496 Gradiente: [3.6666624175533495e-05,-0.0006325982223846911] Loss: 22.842737112847207\n",
      "Iteracion: 17497 Gradiente: [3.664713104474989e-05,-0.0006322619137533773] Loss: 22.84273711244579\n",
      "Iteracion: 17498 Gradiente: [3.662764829831152e-05,-0.0006319257839132083] Loss: 22.842737112044805\n",
      "Iteracion: 17499 Gradiente: [3.660817595137663e-05,-0.0006315898327673134] Loss: 22.842737111644222\n",
      "Iteracion: 17500 Gradiente: [3.6588713906364014e-05,-0.0006312540602261644] Loss: 22.842737111244094\n",
      "Iteracion: 17501 Gradiente: [3.6569262262749666e-05,-0.000630918466189101] Loss: 22.84273711084438\n",
      "Iteracion: 17502 Gradiente: [3.654982090495196e-05,-0.000630583050567779] Loss: 22.842737110445093\n",
      "Iteracion: 17503 Gradiente: [3.653038985665565e-05,-0.000630247813264854] Loss: 22.842737110046237\n",
      "Iteracion: 17504 Gradiente: [3.651096921165238e-05,-0.0006299127541803765] Loss: 22.84273710964779\n",
      "Iteracion: 17505 Gradiente: [3.649155889888789e-05,-0.000629577873222568] Loss: 22.842737109249764\n",
      "Iteracion: 17506 Gradiente: [3.647215882836008e-05,-0.0006292431703033212] Loss: 22.84273710885217\n",
      "Iteracion: 17507 Gradiente: [3.645276908628148e-05,-0.0006289086453201994] Loss: 22.842737108454994\n",
      "Iteracion: 17508 Gradiente: [3.643338971623204e-05,-0.0006285742981783452] Loss: 22.84273710805825\n",
      "Iteracion: 17509 Gradiente: [3.6414020639578365e-05,-0.0006282401287852697] Loss: 22.842737107661915\n",
      "Iteracion: 17510 Gradiente: [3.639466175684447e-05,-0.0006279061370536946] Loss: 22.842737107266007\n",
      "Iteracion: 17511 Gradiente: [3.637531330961489e-05,-0.0006275723228734857] Loss: 22.84273710687053\n",
      "Iteracion: 17512 Gradiente: [3.635597510556939e-05,-0.0006272386861616279] Loss: 22.842737106475465\n",
      "Iteracion: 17513 Gradiente: [3.633664707175891e-05,-0.0006269052268272901] Loss: 22.842737106080833\n",
      "Iteracion: 17514 Gradiente: [3.631732946776841e-05,-0.0006265719447632989] Loss: 22.84273710568659\n",
      "Iteracion: 17515 Gradiente: [3.629802200843339e-05,-0.0006262388398875866] Loss: 22.84273710529279\n",
      "Iteracion: 17516 Gradiente: [3.627872490881145e-05,-0.0006259059120954665] Loss: 22.842737104899403\n",
      "Iteracion: 17517 Gradiente: [3.6259437994582795e-05,-0.0006255731613019103] Loss: 22.842737104506423\n",
      "Iteracion: 17518 Gradiente: [3.624016134817036e-05,-0.0006252405874094554] Loss: 22.84273710411388\n",
      "Iteracion: 17519 Gradiente: [3.622089503020713e-05,-0.0006249081903168493] Loss: 22.842737103721742\n",
      "Iteracion: 17520 Gradiente: [3.620163890900585e-05,-0.0006245759699396558] Loss: 22.842737103330027\n",
      "Iteracion: 17521 Gradiente: [3.618239292298616e-05,-0.0006242439261879913] Loss: 22.842737102938724\n",
      "Iteracion: 17522 Gradiente: [3.616315723225701e-05,-0.000623912058957643] Loss: 22.842737102547847\n",
      "Iteracion: 17523 Gradiente: [3.614393187092446e-05,-0.0006235803681505558] Loss: 22.842737102157358\n",
      "Iteracion: 17524 Gradiente: [3.612471659171964e-05,-0.0006232488536890439] Loss: 22.84273710176732\n",
      "Iteracion: 17525 Gradiente: [3.610551157275192e-05,-0.000622917515469131] Loss: 22.84273710137767\n",
      "Iteracion: 17526 Gradiente: [3.6086316814968695e-05,-0.0006225863533958413] Loss: 22.84273710098844\n",
      "Iteracion: 17527 Gradiente: [3.606713220278834e-05,-0.000622255367380357] Loss: 22.84273710059964\n",
      "Iteracion: 17528 Gradiente: [3.604795780915992e-05,-0.0006219245573272285] Loss: 22.84273710021123\n",
      "Iteracion: 17529 Gradiente: [3.602879365397863e-05,-0.0006215939231401772] Loss: 22.842737099823246\n",
      "Iteracion: 17530 Gradiente: [3.6009639545871625e-05,-0.000621263464737017] Loss: 22.842737099435677\n",
      "Iteracion: 17531 Gradiente: [3.599049575673992e-05,-0.0006209331820060744] Loss: 22.842737099048513\n",
      "Iteracion: 17532 Gradiente: [3.597136214352759e-05,-0.0006206030748646896] Loss: 22.842737098661757\n",
      "Iteracion: 17533 Gradiente: [3.595223862286427e-05,-0.0006202731432229787] Loss: 22.842737098275425\n",
      "Iteracion: 17534 Gradiente: [3.593312536054327e-05,-0.0006199433869783869] Loss: 22.842737097889493\n",
      "Iteracion: 17535 Gradiente: [3.591402216045481e-05,-0.0006196138060478991] Loss: 22.842737097503978\n",
      "Iteracion: 17536 Gradiente: [3.5894929136285704e-05,-0.0006192844003313287] Loss: 22.842737097118874\n",
      "Iteracion: 17537 Gradiente: [3.587584629087815e-05,-0.000618955169735358] Loss: 22.842737096734158\n",
      "Iteracion: 17538 Gradiente: [3.585677360244214e-05,-0.0006186261141678528] Loss: 22.84273709634987\n",
      "Iteracion: 17539 Gradiente: [3.583771108424116e-05,-0.0006182972335347851] Loss: 22.842737095965997\n",
      "Iteracion: 17540 Gradiente: [3.581865863869401e-05,-0.0006179685277475736] Loss: 22.842737095582518\n",
      "Iteracion: 17541 Gradiente: [3.579961630559107e-05,-0.0006176399967117163] Loss: 22.84273709519946\n",
      "Iteracion: 17542 Gradiente: [3.578058407735322e-05,-0.0006173116403346057] Loss: 22.842737094816794\n",
      "Iteracion: 17543 Gradiente: [3.576156204114037e-05,-0.0006169834585175949] Loss: 22.842737094434547\n",
      "Iteracion: 17544 Gradiente: [3.574255014863562e-05,-0.0006166554511700895] Loss: 22.842737094052698\n",
      "Iteracion: 17545 Gradiente: [3.5723548275730836e-05,-0.0006163276182058771] Loss: 22.842737093671253\n",
      "Iteracion: 17546 Gradiente: [3.570455649537507e-05,-0.0006159999595283239] Loss: 22.842737093290218\n",
      "Iteracion: 17547 Gradiente: [3.5685574956308605e-05,-0.0006156724750359406] Loss: 22.842737092909594\n",
      "Iteracion: 17548 Gradiente: [3.566660337147217e-05,-0.000615345164652581] Loss: 22.84273709252937\n",
      "Iteracion: 17549 Gradiente: [3.564764190192212e-05,-0.0006150180282749792] Loss: 22.84273709214954\n",
      "Iteracion: 17550 Gradiente: [3.562869047849896e-05,-0.0006146910658165676] Loss: 22.842737091770122\n",
      "Iteracion: 17551 Gradiente: [3.560974920636302e-05,-0.0006143642771752648] Loss: 22.842737091391108\n",
      "Iteracion: 17552 Gradiente: [3.5590817994564834e-05,-0.0006140376622673452] Loss: 22.842737091012506\n",
      "Iteracion: 17553 Gradiente: [3.557189677110273e-05,-0.0006137112210018596] Loss: 22.842737090634305\n",
      "Iteracion: 17554 Gradiente: [3.5552985660084836e-05,-0.0006133849532792131] Loss: 22.842737090256485\n",
      "Iteracion: 17555 Gradiente: [3.5534084639721185e-05,-0.0006130588590085751] Loss: 22.84273708987909\n",
      "Iteracion: 17556 Gradiente: [3.551519363422055e-05,-0.0006127329380997064] Loss: 22.842737089502076\n",
      "Iteracion: 17557 Gradiente: [3.549631266726768e-05,-0.0006124071904629602] Loss: 22.84273708912548\n",
      "Iteracion: 17558 Gradiente: [3.547744167538743e-05,-0.0006120816160060846] Loss: 22.84273708874928\n",
      "Iteracion: 17559 Gradiente: [3.5458580815846595e-05,-0.0006117562146299586] Loss: 22.842737088373475\n",
      "Iteracion: 17560 Gradiente: [3.5439729981590065e-05,-0.000611430986246475] Loss: 22.842737087998078\n",
      "Iteracion: 17561 Gradiente: [3.542088909019488e-05,-0.0006111059307689477] Loss: 22.842737087623064\n",
      "Iteracion: 17562 Gradiente: [3.540205826482179e-05,-0.0006107810480980192] Loss: 22.842737087248466\n",
      "Iteracion: 17563 Gradiente: [3.5383237521576426e-05,-0.0006104563381397791] Loss: 22.842737086874255\n",
      "Iteracion: 17564 Gradiente: [3.5364426685191576e-05,-0.00061013180081441] Loss: 22.842737086500453\n",
      "Iteracion: 17565 Gradiente: [3.534562583998498e-05,-0.0006098074360230281] Loss: 22.842737086127023\n",
      "Iteracion: 17566 Gradiente: [3.532683508543262e-05,-0.000609483243668052] Loss: 22.842737085754024\n",
      "Iteracion: 17567 Gradiente: [3.530805426237294e-05,-0.0006091592236669404] Loss: 22.84273708538139\n",
      "Iteracion: 17568 Gradiente: [3.528928340585935e-05,-0.000608835375926257] Loss: 22.842737085009176\n",
      "Iteracion: 17569 Gradiente: [3.5270522489364944e-05,-0.0006085117003543416] Loss: 22.842737084637346\n",
      "Iteracion: 17570 Gradiente: [3.525177158678616e-05,-0.000608188196857166] Loss: 22.842737084265913\n",
      "Iteracion: 17571 Gradiente: [3.5233030669701296e-05,-0.0006078648653435436] Loss: 22.842737083894885\n",
      "Iteracion: 17572 Gradiente: [3.52142997220047e-05,-0.0006075417057214594] Loss: 22.842737083524238\n",
      "Iteracion: 17573 Gradiente: [3.519557871716946e-05,-0.00060721871790174] Loss: 22.842737083153985\n",
      "Iteracion: 17574 Gradiente: [3.517686764951122e-05,-0.0006068959017949756] Loss: 22.842737082784126\n",
      "Iteracion: 17575 Gradiente: [3.5158166556925605e-05,-0.0006065732573037034] Loss: 22.84273708241466\n",
      "Iteracion: 17576 Gradiente: [3.513947541004351e-05,-0.0006062507843412372] Loss: 22.842737082045605\n",
      "Iteracion: 17577 Gradiente: [3.512079418896974e-05,-0.0006059284828146143] Loss: 22.842737081676923\n",
      "Iteracion: 17578 Gradiente: [3.51021228728617e-05,-0.0006056063526359641] Loss: 22.842737081308634\n",
      "Iteracion: 17579 Gradiente: [3.508346153466846e-05,-0.0006052843937081794] Loss: 22.842737080940754\n",
      "Iteracion: 17580 Gradiente: [3.506481009101966e-05,-0.0006049626059459949] Loss: 22.842737080573247\n",
      "Iteracion: 17581 Gradiente: [3.5046168515388367e-05,-0.0006046409892576321] Loss: 22.842737080206128\n",
      "Iteracion: 17582 Gradiente: [3.502753697072573e-05,-0.0006043195435447994] Loss: 22.842737079839416\n",
      "Iteracion: 17583 Gradiente: [3.500891527323802e-05,-0.0006039982687240079] Loss: 22.84273707947307\n",
      "Iteracion: 17584 Gradiente: [3.499030340208265e-05,-0.0006036771647088083] Loss: 22.84273707910713\n",
      "Iteracion: 17585 Gradiente: [3.4971701499368164e-05,-0.0006033562313972377] Loss: 22.842737078741592\n",
      "Iteracion: 17586 Gradiente: [3.495310943624948e-05,-0.0006030354687068733] Loss: 22.84273707837642\n",
      "Iteracion: 17587 Gradiente: [3.493452725062222e-05,-0.0006027148765441599] Loss: 22.84273707801165\n",
      "Iteracion: 17588 Gradiente: [3.4915955005961526e-05,-0.0006023944548140036] Loss: 22.842737077647254\n",
      "Iteracion: 17589 Gradiente: [3.4897392595212295e-05,-0.0006020742034329156] Loss: 22.842737077283253\n",
      "Iteracion: 17590 Gradiente: [3.4878840134903534e-05,-0.0006017541223010644] Loss: 22.84273707691964\n",
      "Iteracion: 17591 Gradiente: [3.486029747818975e-05,-0.0006014342113388693] Loss: 22.842737076556418\n",
      "Iteracion: 17592 Gradiente: [3.484176462791311e-05,-0.0006011144704546704] Loss: 22.842737076193583\n",
      "Iteracion: 17593 Gradiente: [3.482324167976003e-05,-0.0006007948995505312] Loss: 22.842737075831106\n",
      "Iteracion: 17594 Gradiente: [3.480472861857228e-05,-0.0006004754985381074] Loss: 22.84273707546905\n",
      "Iteracion: 17595 Gradiente: [3.478622532213649e-05,-0.000600156267333792] Loss: 22.842737075107348\n",
      "Iteracion: 17596 Gradiente: [3.476773193540339e-05,-0.0005998372058379904] Loss: 22.84273707474605\n",
      "Iteracion: 17597 Gradiente: [3.474924827552665e-05,-0.000599518313971122] Loss: 22.842737074385134\n",
      "Iteracion: 17598 Gradiente: [3.473077449124655e-05,-0.0005991995916337108] Loss: 22.842737074024605\n",
      "Iteracion: 17599 Gradiente: [3.47123105607731e-05,-0.0005988810387385967] Loss: 22.842737073664445\n",
      "Iteracion: 17600 Gradiente: [3.469385643389463e-05,-0.0005985626551951857] Loss: 22.842737073304683\n",
      "Iteracion: 17601 Gradiente: [3.467541211913764e-05,-0.0005982444409148968] Loss: 22.842737072945294\n",
      "Iteracion: 17602 Gradiente: [3.465697760892302e-05,-0.0005979263958072541] Loss: 22.8427370725863\n",
      "Iteracion: 17603 Gradiente: [3.4638552983778935e-05,-0.0005976085197768081] Loss: 22.842737072227678\n",
      "Iteracion: 17604 Gradiente: [3.462013804285865e-05,-0.0005972908127451622] Loss: 22.84273707186944\n",
      "Iteracion: 17605 Gradiente: [3.46017328695325e-05,-0.0005969732746185249] Loss: 22.84273707151157\n",
      "Iteracion: 17606 Gradiente: [3.458333752443347e-05,-0.0005966559053020385] Loss: 22.84273707115409\n",
      "Iteracion: 17607 Gradiente: [3.4564951934612506e-05,-0.000596338704709846] Loss: 22.842737070796993\n",
      "Iteracion: 17608 Gradiente: [3.4546576129438716e-05,-0.0005960216727515899] Loss: 22.842737070440286\n",
      "Iteracion: 17609 Gradiente: [3.4528210167650283e-05,-0.0005957048093317023] Loss: 22.84273707008393\n",
      "Iteracion: 17610 Gradiente: [3.450985394977124e-05,-0.0005953881143676416] Loss: 22.842737069727967\n",
      "Iteracion: 17611 Gradiente: [3.4491507460643336e-05,-0.0005950715877703535] Loss: 22.842737069372397\n",
      "Iteracion: 17612 Gradiente: [3.447317071447742e-05,-0.0005947552294484145] Loss: 22.84273706901718\n",
      "Iteracion: 17613 Gradiente: [3.445484370369438e-05,-0.0005944390393131253] Loss: 22.84273706866237\n",
      "Iteracion: 17614 Gradiente: [3.4436526403662054e-05,-0.0005941230172774443] Loss: 22.842737068307915\n",
      "Iteracion: 17615 Gradiente: [3.441821896027856e-05,-0.0005938071632410668] Loss: 22.842737067953824\n",
      "Iteracion: 17616 Gradiente: [3.4399921145222836e-05,-0.0005934914771276094] Loss: 22.842737067600126\n",
      "Iteracion: 17617 Gradiente: [3.438163304754956e-05,-0.0005931759588442276] Loss: 22.842737067246823\n",
      "Iteracion: 17618 Gradiente: [3.436335464831094e-05,-0.0005928606083003274] Loss: 22.84273706689386\n",
      "Iteracion: 17619 Gradiente: [3.4345086139827195e-05,-0.0005925454253989196] Loss: 22.8427370665413\n",
      "Iteracion: 17620 Gradiente: [3.432682721324909e-05,-0.0005922304100624368] Loss: 22.84273706618909\n",
      "Iteracion: 17621 Gradiente: [3.43085780770025e-05,-0.0005919155621950741] Loss: 22.842737065837287\n",
      "Iteracion: 17622 Gradiente: [3.4290338557714976e-05,-0.0005916008817154742] Loss: 22.842737065485835\n",
      "Iteracion: 17623 Gradiente: [3.427210879086336e-05,-0.000591286368525464] Loss: 22.842737065134756\n",
      "Iteracion: 17624 Gradiente: [3.425388869307729e-05,-0.0005909720225414361] Loss: 22.842737064784053\n",
      "Iteracion: 17625 Gradiente: [3.423567830888411e-05,-0.0005906578436723227] Loss: 22.84273706443373\n",
      "Iteracion: 17626 Gradiente: [3.4217477531228727e-05,-0.0005903438318342799] Loss: 22.842737064083767\n",
      "Iteracion: 17627 Gradiente: [3.4199286451060594e-05,-0.0005900299869337525] Loss: 22.842737063734184\n",
      "Iteracion: 17628 Gradiente: [3.418110503995801e-05,-0.0005897163088822784] Loss: 22.842737063384973\n",
      "Iteracion: 17629 Gradiente: [3.416293330644748e-05,-0.0005894027975919869] Loss: 22.84273706303615\n",
      "Iteracion: 17630 Gradiente: [3.414477115673738e-05,-0.0005890894529778497] Loss: 22.84273706268766\n",
      "Iteracion: 17631 Gradiente: [3.412661874998927e-05,-0.0005887762749429963] Loss: 22.842737062339566\n",
      "Iteracion: 17632 Gradiente: [3.4108476093782276e-05,-0.0005884632633969507] Loss: 22.84273706199183\n",
      "Iteracion: 17633 Gradiente: [3.409034298537487e-05,-0.0005881504182629745] Loss: 22.842737061644478\n",
      "Iteracion: 17634 Gradiente: [3.407221948634742e-05,-0.0005878377394478681] Loss: 22.842737061297484\n",
      "Iteracion: 17635 Gradiente: [3.4054105670596374e-05,-0.0005875252268616293] Loss: 22.84273706095086\n",
      "Iteracion: 17636 Gradiente: [3.403600140264492e-05,-0.0005872128804207695] Loss: 22.842737060604613\n",
      "Iteracion: 17637 Gradiente: [3.401790686249721e-05,-0.0005869007000256943] Loss: 22.842737060258724\n",
      "Iteracion: 17638 Gradiente: [3.3999821864464746e-05,-0.0005865886856006123] Loss: 22.8427370599132\n",
      "Iteracion: 17639 Gradiente: [3.398174646633834e-05,-0.0005862768370524426] Loss: 22.84273705956806\n",
      "Iteracion: 17640 Gradiente: [3.3963680760962236e-05,-0.0005859651542897619] Loss: 22.842737059223275\n",
      "Iteracion: 17641 Gradiente: [3.394562463086004e-05,-0.0005856536372274235] Loss: 22.842737058878857\n",
      "Iteracion: 17642 Gradiente: [3.392757814519124e-05,-0.0005853422857738859] Loss: 22.842737058534798\n",
      "Iteracion: 17643 Gradiente: [3.3909541238585915e-05,-0.0005850310998457786] Loss: 22.84273705819112\n",
      "Iteracion: 17644 Gradiente: [3.389151386556932e-05,-0.0005847200793566524] Loss: 22.842737057847806\n",
      "Iteracion: 17645 Gradiente: [3.387349605740534e-05,-0.0005844092242173342] Loss: 22.842737057504845\n",
      "Iteracion: 17646 Gradiente: [3.385548795051818e-05,-0.0005840985343308347] Loss: 22.84273705716225\n",
      "Iteracion: 17647 Gradiente: [3.383748927300682e-05,-0.0005837880096246788] Loss: 22.84273705682003\n",
      "Iteracion: 17648 Gradiente: [3.381950023519191e-05,-0.0005834776499987982] Loss: 22.842737056478168\n",
      "Iteracion: 17649 Gradiente: [3.3801520690227944e-05,-0.0005831674553739674] Loss: 22.84273705613667\n",
      "Iteracion: 17650 Gradiente: [3.3783550774539134e-05,-0.0005828574256518948] Loss: 22.842737055795517\n",
      "Iteracion: 17651 Gradiente: [3.376559037917559e-05,-0.0005825475607558417] Loss: 22.842737055454748\n",
      "Iteracion: 17652 Gradiente: [3.3747639605508084e-05,-0.0005822378605883453] Loss: 22.84273705511435\n",
      "Iteracion: 17653 Gradiente: [3.372969830669111e-05,-0.0005819283250717197] Loss: 22.842737054774293\n",
      "Iteracion: 17654 Gradiente: [3.371176650356726e-05,-0.0005816189541157258] Loss: 22.842737054434604\n",
      "Iteracion: 17655 Gradiente: [3.3693844349613757e-05,-0.0005813097476238483] Loss: 22.84273705409528\n",
      "Iteracion: 17656 Gradiente: [3.367593162124649e-05,-0.0005810007055221907] Loss: 22.842737053756313\n",
      "Iteracion: 17657 Gradiente: [3.3658028469100526e-05,-0.0005806918277123429] Loss: 22.84273705341771\n",
      "Iteracion: 17658 Gradiente: [3.364013490264976e-05,-0.0005803831141097504] Loss: 22.84273705307947\n",
      "Iteracion: 17659 Gradiente: [3.362225075136394e-05,-0.0005800745646325822] Loss: 22.84273705274158\n",
      "Iteracion: 17660 Gradiente: [3.360437610619253e-05,-0.0005797661791902442] Loss: 22.842737052404047\n",
      "Iteracion: 17661 Gradiente: [3.358651099934681e-05,-0.0005794579576926159] Loss: 22.84273705206688\n",
      "Iteracion: 17662 Gradiente: [3.356865542135286e-05,-0.0005791499000537215] Loss: 22.842737051730076\n",
      "Iteracion: 17663 Gradiente: [3.3550809246207794e-05,-0.0005788420061925593] Loss: 22.842737051393616\n",
      "Iteracion: 17664 Gradiente: [3.3532972598019724e-05,-0.0005785342760159295] Loss: 22.84273705105752\n",
      "Iteracion: 17665 Gradiente: [3.351514543984043e-05,-0.0005782267094363174] Loss: 22.842737050721787\n",
      "Iteracion: 17666 Gradiente: [3.3497327705352595e-05,-0.0005779193063727206] Loss: 22.842737050386397\n",
      "Iteracion: 17667 Gradiente: [3.347951953003303e-05,-0.0005776120667283872] Loss: 22.842737050051383\n",
      "Iteracion: 17668 Gradiente: [3.346172076703624e-05,-0.0005773049904248021] Loss: 22.842737049716714\n",
      "Iteracion: 17669 Gradiente: [3.344393152057516e-05,-0.000576998077370187] Loss: 22.84273704938241\n",
      "Iteracion: 17670 Gradiente: [3.342615165896253e-05,-0.0005766913274838951] Loss: 22.842737049048438\n",
      "Iteracion: 17671 Gradiente: [3.340838136599207e-05,-0.0005763847406684637] Loss: 22.84273704871485\n",
      "Iteracion: 17672 Gradiente: [3.339062038018407e-05,-0.0005760783168527202] Loss: 22.842737048381593\n",
      "Iteracion: 17673 Gradiente: [3.3372868929859575e-05,-0.0005757720559343463] Loss: 22.842737048048704\n",
      "Iteracion: 17674 Gradiente: [3.335512694491172e-05,-0.0005754659578331693] Loss: 22.84273704771617\n",
      "Iteracion: 17675 Gradiente: [3.333739432207494e-05,-0.0005751600224668844] Loss: 22.84273704738398\n",
      "Iteracion: 17676 Gradiente: [3.331967110966616e-05,-0.0005748542497457266] Loss: 22.84273704705215\n",
      "Iteracion: 17677 Gradiente: [3.3301957379687035e-05,-0.0005745486395802857] Loss: 22.842737046720668\n",
      "Iteracion: 17678 Gradiente: [3.3284253023187677e-05,-0.0005742431918880204] Loss: 22.84273704638954\n",
      "Iteracion: 17679 Gradiente: [3.326655805532634e-05,-0.0005739379065834289] Loss: 22.84273704605876\n",
      "Iteracion: 17680 Gradiente: [3.324887259168463e-05,-0.0005736327835715353] Loss: 22.842737045728338\n",
      "Iteracion: 17681 Gradiente: [3.3231196505312254e-05,-0.0005733278227751271] Loss: 22.84273704539828\n",
      "Iteracion: 17682 Gradiente: [3.321352976210316e-05,-0.0005730230241073996] Loss: 22.84273704506855\n",
      "Iteracion: 17683 Gradiente: [3.319587246153333e-05,-0.0005727183874764564] Loss: 22.84273704473918\n",
      "Iteracion: 17684 Gradiente: [3.317822450981112e-05,-0.0005724139128022425] Loss: 22.842737044410164\n",
      "Iteracion: 17685 Gradiente: [3.31605858804096e-05,-0.0005721095999992561] Loss: 22.842737044081485\n",
      "Iteracion: 17686 Gradiente: [3.31429567467012e-05,-0.0005718054489726399] Loss: 22.842737043753168\n",
      "Iteracion: 17687 Gradiente: [3.312533685762749e-05,-0.0005715014596484972] Loss: 22.842737043425192\n",
      "Iteracion: 17688 Gradiente: [3.310772642350912e-05,-0.0005711976319303128] Loss: 22.842737043097575\n",
      "Iteracion: 17689 Gradiente: [3.30901253647653e-05,-0.0005708939657343611] Loss: 22.84273704277029\n",
      "Iteracion: 17690 Gradiente: [3.3072533629289565e-05,-0.000570590460980469] Loss: 22.842737042443368\n",
      "Iteracion: 17691 Gradiente: [3.3054951264451425e-05,-0.0005702871175770951] Loss: 22.84273704211678\n",
      "Iteracion: 17692 Gradiente: [3.303737823709222e-05,-0.0005699839354405138] Loss: 22.842737041790546\n",
      "Iteracion: 17693 Gradiente: [3.3019814587002354e-05,-0.0005696809144832097] Loss: 22.842737041464673\n",
      "Iteracion: 17694 Gradiente: [3.300226019102107e-05,-0.0005693780546256022] Loss: 22.842737041139113\n",
      "Iteracion: 17695 Gradiente: [3.298471512872917e-05,-0.0005690753557782812] Loss: 22.842737040813937\n",
      "Iteracion: 17696 Gradiente: [3.296717943612748e-05,-0.0005687728178517186] Loss: 22.84273704048909\n",
      "Iteracion: 17697 Gradiente: [3.2949653101847315e-05,-0.0005684704407624252] Loss: 22.842737040164582\n",
      "Iteracion: 17698 Gradiente: [3.2932136105993476e-05,-0.0005681682244232415] Loss: 22.84273703984041\n",
      "Iteracion: 17699 Gradiente: [3.291462832351044e-05,-0.0005678661687582576] Loss: 22.84273703951662\n",
      "Iteracion: 17700 Gradiente: [3.289712989366459e-05,-0.0005675642736735635] Loss: 22.842737039193132\n",
      "Iteracion: 17701 Gradiente: [3.287964078140249e-05,-0.0005672625390841309] Loss: 22.84273703887002\n",
      "Iteracion: 17702 Gradiente: [3.2862160935565043e-05,-0.0005669609649073] Loss: 22.84273703854722\n",
      "Iteracion: 17703 Gradiente: [3.284469040446917e-05,-0.0005666595510552005] Loss: 22.842737038224794\n",
      "Iteracion: 17704 Gradiente: [3.282722924590568e-05,-0.0005663582974390143] Loss: 22.842737037902697\n",
      "Iteracion: 17705 Gradiente: [3.2809777272291286e-05,-0.0005660572039853188] Loss: 22.842737037580942\n",
      "Iteracion: 17706 Gradiente: [3.279233457836502e-05,-0.0005657562706005593] Loss: 22.842737037259536\n",
      "Iteracion: 17707 Gradiente: [3.2774901107283465e-05,-0.0005654554972047994] Loss: 22.842737036938455\n",
      "Iteracion: 17708 Gradiente: [3.275747706273554e-05,-0.0005651548837003399] Loss: 22.84273703661773\n",
      "Iteracion: 17709 Gradiente: [3.2740062148188066e-05,-0.0005648544300190868] Loss: 22.84273703629735\n",
      "Iteracion: 17710 Gradiente: [3.272265650385483e-05,-0.0005645541360677224] Loss: 22.842737035977294\n",
      "Iteracion: 17711 Gradiente: [3.270526009847193e-05,-0.0005642540017613366] Loss: 22.842737035657596\n",
      "Iteracion: 17712 Gradiente: [3.268787301067277e-05,-0.0005639540270127696] Loss: 22.84273703533823\n",
      "Iteracion: 17713 Gradiente: [3.267049508508535e-05,-0.0005636542117445723] Loss: 22.84273703501919\n",
      "Iteracion: 17714 Gradiente: [3.265312653771465e-05,-0.0005633545558595187] Loss: 22.842737034700512\n",
      "Iteracion: 17715 Gradiente: [3.263576708434357e-05,-0.0005630550592877389] Loss: 22.842737034382157\n",
      "Iteracion: 17716 Gradiente: [3.2618416888870645e-05,-0.0005627557219376911] Loss: 22.842737034064147\n",
      "Iteracion: 17717 Gradiente: [3.260107595508544e-05,-0.0005624565437212681] Loss: 22.84273703374647\n",
      "Iteracion: 17718 Gradiente: [3.258374419298586e-05,-0.000562157524560547] Loss: 22.842737033429138\n",
      "Iteracion: 17719 Gradiente: [3.256642168594226e-05,-0.0005618586643641048] Loss: 22.842737033112137\n",
      "Iteracion: 17720 Gradiente: [3.254910833258388e-05,-0.0005615599630549657] Loss: 22.842737032795473\n",
      "Iteracion: 17721 Gradiente: [3.2531804279756216e-05,-0.0005612614205380358] Loss: 22.842737032479157\n",
      "Iteracion: 17722 Gradiente: [3.251450935977118e-05,-0.000560963036740129] Loss: 22.84273703216316\n",
      "Iteracion: 17723 Gradiente: [3.249722367020998e-05,-0.0005606648115707695] Loss: 22.84273703184751\n",
      "Iteracion: 17724 Gradiente: [3.2479947194966977e-05,-0.0005603667449439816] Loss: 22.842737031532195\n",
      "Iteracion: 17725 Gradiente: [3.246267985825095e-05,-0.0005600688367817241] Loss: 22.842737031217208\n",
      "Iteracion: 17726 Gradiente: [3.2445421680904474e-05,-0.0005597710869973109] Loss: 22.842737030902565\n",
      "Iteracion: 17727 Gradiente: [3.242817270556013e-05,-0.0005594734955059503] Loss: 22.84273703058825\n",
      "Iteracion: 17728 Gradiente: [3.2410932769266763e-05,-0.0005591760622292459] Loss: 22.84273703027426\n",
      "Iteracion: 17729 Gradiente: [3.23937021552941e-05,-0.0005588787870693797] Loss: 22.84273702996063\n",
      "Iteracion: 17730 Gradiente: [3.2376480680795795e-05,-0.0005585816699513894] Loss: 22.84273702964731\n",
      "Iteracion: 17731 Gradiente: [3.235926842630003e-05,-0.0005582847107859834] Loss: 22.84273702933433\n",
      "Iteracion: 17732 Gradiente: [3.2342065256329985e-05,-0.0005579879094963047] Loss: 22.84273702902169\n",
      "Iteracion: 17733 Gradiente: [3.232487120878128e-05,-0.0005576912659967329] Loss: 22.842737028709372\n",
      "Iteracion: 17734 Gradiente: [3.2307686304496504e-05,-0.0005573947802024766] Loss: 22.84273702839739\n",
      "Iteracion: 17735 Gradiente: [3.2290510542528254e-05,-0.000557098452027797] Loss: 22.842737028085743\n",
      "Iteracion: 17736 Gradiente: [3.227334399487821e-05,-0.0005568022813862446] Loss: 22.842737027774415\n",
      "Iteracion: 17737 Gradiente: [3.2256186423751386e-05,-0.0005565062682065284] Loss: 22.84273702746345\n",
      "Iteracion: 17738 Gradiente: [3.223903805746886e-05,-0.0005562104123917778] Loss: 22.842737027152772\n",
      "Iteracion: 17739 Gradiente: [3.2221898790870304e-05,-0.000555914713863596] Loss: 22.84273702684246\n",
      "Iteracion: 17740 Gradiente: [3.220476864574569e-05,-0.0005556191725381391] Loss: 22.84273702653247\n",
      "Iteracion: 17741 Gradiente: [3.2187647574725514e-05,-0.0005553237883332211] Loss: 22.842737026222796\n",
      "Iteracion: 17742 Gradiente: [3.217053560054713e-05,-0.0005550285611632214] Loss: 22.842737025913447\n",
      "Iteracion: 17743 Gradiente: [3.215343280847567e-05,-0.0005547334909409803] Loss: 22.842737025604443\n",
      "Iteracion: 17744 Gradiente: [3.2136339044086526e-05,-0.0005544385775894038] Loss: 22.842737025295765\n",
      "Iteracion: 17745 Gradiente: [3.211925434527529e-05,-0.000554143821026661] Loss: 22.842737024987407\n",
      "Iteracion: 17746 Gradiente: [3.2102178748990204e-05,-0.0005538492211632236] Loss: 22.842737024679373\n",
      "Iteracion: 17747 Gradiente: [3.2085112293126866e-05,-0.0005535547779145371] Loss: 22.84273702437169\n",
      "Iteracion: 17748 Gradiente: [3.206805479862851e-05,-0.0005532604912071785] Loss: 22.842737024064313\n",
      "Iteracion: 17749 Gradiente: [3.205100643792017e-05,-0.0005529663609474748] Loss: 22.842737023757262\n",
      "Iteracion: 17750 Gradiente: [3.203396714184237e-05,-0.0005526723870570294] Loss: 22.84273702345055\n",
      "Iteracion: 17751 Gradiente: [3.201693698429153e-05,-0.0005523785694459586] Loss: 22.842737023144156\n",
      "Iteracion: 17752 Gradiente: [3.199991581747478e-05,-0.0005520849080421423] Loss: 22.842737022838097\n",
      "Iteracion: 17753 Gradiente: [3.198290364328689e-05,-0.0005517914027620918] Loss: 22.842737022532358\n",
      "Iteracion: 17754 Gradiente: [3.1965900502465655e-05,-0.0005514980535179366] Loss: 22.842737022226945\n",
      "Iteracion: 17755 Gradiente: [3.194890648501314e-05,-0.00055120486022228] Loss: 22.842737021921852\n",
      "Iteracion: 17756 Gradiente: [3.19319214592421e-05,-0.000550911822799686] Loss: 22.842737021617076\n",
      "Iteracion: 17757 Gradiente: [3.1914945450732074e-05,-0.0005506189411654816] Loss: 22.842737021312633\n",
      "Iteracion: 17758 Gradiente: [3.189797852011604e-05,-0.0005503262152335727] Loss: 22.842737021008517\n",
      "Iteracion: 17759 Gradiente: [3.188102056791801e-05,-0.0005500336449265101] Loss: 22.842737020704725\n",
      "Iteracion: 17760 Gradiente: [3.186407168224529e-05,-0.0005497412301547655] Loss: 22.842737020401252\n",
      "Iteracion: 17761 Gradiente: [3.1847131815728365e-05,-0.0005494489708399423] Loss: 22.842737020098106\n",
      "Iteracion: 17762 Gradiente: [3.183020084710127e-05,-0.0005491568669047098] Loss: 22.84273701979528\n",
      "Iteracion: 17763 Gradiente: [3.1813278898577364e-05,-0.0005488649182600132] Loss: 22.842737019492763\n",
      "Iteracion: 17764 Gradiente: [3.179636597868315e-05,-0.0005485731248228376] Loss: 22.842737019190583\n",
      "Iteracion: 17765 Gradiente: [3.1779462056154746e-05,-0.0005482814865107599] Loss: 22.84273701888872\n",
      "Iteracion: 17766 Gradiente: [3.17625671177287e-05,-0.0005479900032446731] Loss: 22.842737018587176\n",
      "Iteracion: 17767 Gradiente: [3.174568123635405e-05,-0.000547698674933154] Loss: 22.842737018285963\n",
      "Iteracion: 17768 Gradiente: [3.1728804286975296e-05,-0.0005474075015038456] Loss: 22.842737017985055\n",
      "Iteracion: 17769 Gradiente: [3.1711936230749416e-05,-0.0005471164828770488] Loss: 22.842737017684467\n",
      "Iteracion: 17770 Gradiente: [3.1695077201258455e-05,-0.0005468256189598009] Loss: 22.842737017384206\n",
      "Iteracion: 17771 Gradiente: [3.167822710281598e-05,-0.0005465349096771395] Loss: 22.84273701708427\n",
      "Iteracion: 17772 Gradiente: [3.166138594300112e-05,-0.0005462443549455761] Loss: 22.842737016784636\n",
      "Iteracion: 17773 Gradiente: [3.164455376634123e-05,-0.000545953954681148] Loss: 22.842737016485334\n",
      "Iteracion: 17774 Gradiente: [3.162773048283422e-05,-0.0005456637088052218] Loss: 22.84273701618635\n",
      "Iteracion: 17775 Gradiente: [3.161091628574771e-05,-0.0005453736172244798] Loss: 22.84273701588769\n",
      "Iteracion: 17776 Gradiente: [3.1594110891812005e-05,-0.0005450836798729597] Loss: 22.84273701558932\n",
      "Iteracion: 17777 Gradiente: [3.157731451324253e-05,-0.0005447938966565147] Loss: 22.84273701529129\n",
      "Iteracion: 17778 Gradiente: [3.156052705151069e-05,-0.0005445042674982876] Loss: 22.842737014993567\n",
      "Iteracion: 17779 Gradiente: [3.154374852745908e-05,-0.0005442147923149083] Loss: 22.842737014696176\n",
      "Iteracion: 17780 Gradiente: [3.1526978913613374e-05,-0.0005439254710262038] Loss: 22.842737014399088\n",
      "Iteracion: 17781 Gradiente: [3.151021828387002e-05,-0.0005436363035451326] Loss: 22.84273701410232\n",
      "Iteracion: 17782 Gradiente: [3.149346649233091e-05,-0.0005433472897985089] Loss: 22.842737013805866\n",
      "Iteracion: 17783 Gradiente: [3.1476723535206474e-05,-0.0005430584297059227] Loss: 22.842737013509726\n",
      "Iteracion: 17784 Gradiente: [3.1459989566921344e-05,-0.0005427697231738194] Loss: 22.8427370132139\n",
      "Iteracion: 17785 Gradiente: [3.144326446242e-05,-0.0005424811701290129] Loss: 22.842737012918395\n",
      "Iteracion: 17786 Gradiente: [3.1426548261492826e-05,-0.00054219277048837] Loss: 22.842737012623196\n",
      "Iteracion: 17787 Gradiente: [3.140984092434943e-05,-0.0005419045241699412] Loss: 22.842737012328303\n",
      "Iteracion: 17788 Gradiente: [3.139314248888543e-05,-0.0005416164310911853] Loss: 22.84273701203375\n",
      "Iteracion: 17789 Gradiente: [3.137645295225866e-05,-0.000541328491169916] Loss: 22.84273701173948\n",
      "Iteracion: 17790 Gradiente: [3.135977220646661e-05,-0.0005410407043318818] Loss: 22.84273701144555\n",
      "Iteracion: 17791 Gradiente: [3.134310042961867e-05,-0.0005407530704844751] Loss: 22.84273701115192\n",
      "Iteracion: 17792 Gradiente: [3.132643751939668e-05,-0.0005404655895508389] Loss: 22.842737010858592\n",
      "Iteracion: 17793 Gradiente: [3.130978343411546e-05,-0.0005401782614538794] Loss: 22.842737010565592\n",
      "Iteracion: 17794 Gradiente: [3.129313813587942e-05,-0.0005398910861128314] Loss: 22.8427370102729\n",
      "Iteracion: 17795 Gradiente: [3.12765017649023e-05,-0.0005396040634379299] Loss: 22.842737009980503\n",
      "Iteracion: 17796 Gradiente: [3.125987413075866e-05,-0.0005393171933600153] Loss: 22.842737009688435\n",
      "Iteracion: 17797 Gradiente: [3.124325552429734e-05,-0.000539030475780559] Loss: 22.84273700939667\n",
      "Iteracion: 17798 Gradiente: [3.1226645612984307e-05,-0.0005387439106357306] Loss: 22.842737009105218\n",
      "Iteracion: 17799 Gradiente: [3.1210044484926885e-05,-0.0005384574978421596] Loss: 22.84273700881408\n",
      "Iteracion: 17800 Gradiente: [3.119345226802276e-05,-0.0005381712373081863] Loss: 22.842737008523233\n",
      "Iteracion: 17801 Gradiente: [3.117686888079637e-05,-0.0005378851289589667] Loss: 22.842737008232728\n",
      "Iteracion: 17802 Gradiente: [3.116029432135292e-05,-0.0005375991727136172] Loss: 22.842737007942514\n",
      "Iteracion: 17803 Gradiente: [3.1143728554638984e-05,-0.0005373133684917282] Loss: 22.842737007652598\n",
      "Iteracion: 17804 Gradiente: [3.112717166118273e-05,-0.0005370277162076794] Loss: 22.842737007363\n",
      "Iteracion: 17805 Gradiente: [3.11106234704539e-05,-0.000536742215791719] Loss: 22.842737007073712\n",
      "Iteracion: 17806 Gradiente: [3.109408413024539e-05,-0.0005364568671537266] Loss: 22.84273700678472\n",
      "Iteracion: 17807 Gradiente: [3.107755347286911e-05,-0.0005361716702220557] Loss: 22.84273700649606\n",
      "Iteracion: 17808 Gradiente: [3.1061031739909596e-05,-0.0005358866249012569] Loss: 22.84273700620768\n",
      "Iteracion: 17809 Gradiente: [3.104451878736351e-05,-0.0005356017311193284] Loss: 22.84273700591962\n",
      "Iteracion: 17810 Gradiente: [3.102801458396698e-05,-0.0005353169887979921] Loss: 22.84273700563187\n",
      "Iteracion: 17811 Gradiente: [3.1011519120246096e-05,-0.0005350323978568384] Loss: 22.84273700534441\n",
      "Iteracion: 17812 Gradiente: [3.099503240093782e-05,-0.0005347479582136808] Loss: 22.84273700505726\n",
      "Iteracion: 17813 Gradiente: [3.097855452646551e-05,-0.0005344636697832546] Loss: 22.842737004770427\n",
      "Iteracion: 17814 Gradiente: [3.0962085331983266e-05,-0.0005341795324942685] Loss: 22.842737004483883\n",
      "Iteracion: 17815 Gradiente: [3.0945624955810067e-05,-0.0005338955462574309] Loss: 22.842737004197666\n",
      "Iteracion: 17816 Gradiente: [3.092917330510166e-05,-0.0005336117109968323] Loss: 22.842737003911736\n",
      "Iteracion: 17817 Gradiente: [3.0912730462281006e-05,-0.0005333280266288654] Loss: 22.842737003626105\n",
      "Iteracion: 17818 Gradiente: [3.089629633450386e-05,-0.0005330444930778574] Loss: 22.84273700334079\n",
      "Iteracion: 17819 Gradiente: [3.0879870880085034e-05,-0.0005327611102651749] Loss: 22.842737003055777\n",
      "Iteracion: 17820 Gradiente: [3.086345415681535e-05,-0.0005324778781079213] Loss: 22.84273700277107\n",
      "Iteracion: 17821 Gradiente: [3.084704616090524e-05,-0.0005321947965256868] Loss: 22.842737002486658\n",
      "Iteracion: 17822 Gradiente: [3.083064690751295e-05,-0.0005319118654373512] Loss: 22.84273700220255\n",
      "Iteracion: 17823 Gradiente: [3.081425641842846e-05,-0.000531629084760965] Loss: 22.84273700191874\n",
      "Iteracion: 17824 Gradiente: [3.079787465575616e-05,-0.000531346454418724] Loss: 22.842737001635236\n",
      "Iteracion: 17825 Gradiente: [3.078150152380961e-05,-0.000531063974335666] Loss: 22.842737001352038\n",
      "Iteracion: 17826 Gradiente: [3.0765137122064816e-05,-0.0005307816444261703] Loss: 22.842737001069136\n",
      "Iteracion: 17827 Gradiente: [3.074878144957438e-05,-0.0005304994646104196] Loss: 22.84273700078654\n",
      "Iteracion: 17828 Gradiente: [3.073243443528402e-05,-0.0005302174348122672] Loss: 22.84273700050424\n",
      "Iteracion: 17829 Gradiente: [3.0716096148353245e-05,-0.000529935554948698] Loss: 22.842737000222243\n",
      "Iteracion: 17830 Gradiente: [3.069976656509728e-05,-0.0005296538249388287] Loss: 22.842736999940538\n",
      "Iteracion: 17831 Gradiente: [3.068344564951531e-05,-0.0005293722447067495] Loss: 22.84273699965913\n",
      "Iteracion: 17832 Gradiente: [3.066713338076473e-05,-0.0005290908141726429] Loss: 22.842736999378022\n",
      "Iteracion: 17833 Gradiente: [3.065082984600546e-05,-0.000528809533252191] Loss: 22.84273699909723\n",
      "Iteracion: 17834 Gradiente: [3.063453484533814e-05,-0.0005285284018775371] Loss: 22.842736998816722\n",
      "Iteracion: 17835 Gradiente: [3.061824853508218e-05,-0.0005282474199590344] Loss: 22.842736998536527\n",
      "Iteracion: 17836 Gradiente: [3.0601970915237566e-05,-0.0005279665874179311] Loss: 22.84273699825661\n",
      "Iteracion: 17837 Gradiente: [3.05857019678039e-05,-0.0005276859041733436] Loss: 22.842736997976992\n",
      "Iteracion: 17838 Gradiente: [3.056944167951769e-05,-0.0005274053701497176] Loss: 22.84273699769769\n",
      "Iteracion: 17839 Gradiente: [3.0553190081642846e-05,-0.0005271249852623802] Loss: 22.84273699741868\n",
      "Iteracion: 17840 Gradiente: [3.053694713154679e-05,-0.0005268447494368426] Loss: 22.84273699713996\n",
      "Iteracion: 17841 Gradiente: [3.052071267859446e-05,-0.0005265646626009849] Loss: 22.84273699686153\n",
      "Iteracion: 17842 Gradiente: [3.050448695015954e-05,-0.0005262847246624365] Loss: 22.8427369965834\n",
      "Iteracion: 17843 Gradiente: [3.048826975676396e-05,-0.0005260049355528669] Loss: 22.842736996305568\n",
      "Iteracion: 17844 Gradiente: [3.0472061355150495e-05,-0.0005257252951787213] Loss: 22.84273699602804\n",
      "Iteracion: 17845 Gradiente: [3.045586147341813e-05,-0.0005254458034748665] Loss: 22.842736995750794\n",
      "Iteracion: 17846 Gradiente: [3.0439670251780625e-05,-0.0005251664603533138] Loss: 22.84273699547384\n",
      "Iteracion: 17847 Gradiente: [3.0423487453390406e-05,-0.0005248872657517723] Loss: 22.842736995197185\n",
      "Iteracion: 17848 Gradiente: [3.0407313374780642e-05,-0.0005246082195714763] Loss: 22.842736994920845\n",
      "Iteracion: 17849 Gradiente: [3.0391147966687034e-05,-0.0005243293217352137] Loss: 22.84273699464477\n",
      "Iteracion: 17850 Gradiente: [3.0374991068053227e-05,-0.0005240505721758381] Loss: 22.842736994368995\n",
      "Iteracion: 17851 Gradiente: [3.035884283898819e-05,-0.0005237719708035845] Loss: 22.842736994093514\n",
      "Iteracion: 17852 Gradiente: [3.0342703173384204e-05,-0.0005234935175452667] Loss: 22.842736993818328\n",
      "Iteracion: 17853 Gradiente: [3.0326572008713508e-05,-0.0005232152123254489] Loss: 22.842736993543436\n",
      "Iteracion: 17854 Gradiente: [3.0310449502242895e-05,-0.0005229370550578001] Loss: 22.842736993268836\n",
      "Iteracion: 17855 Gradiente: [3.0294335531759012e-05,-0.0005226590456677134] Loss: 22.842736992994524\n",
      "Iteracion: 17856 Gradiente: [3.027823014557877e-05,-0.0005223811840758449] Loss: 22.842736992720518\n",
      "Iteracion: 17857 Gradiente: [3.0262133287806135e-05,-0.0005221034702055741] Loss: 22.842736992446795\n",
      "Iteracion: 17858 Gradiente: [3.024604503517973e-05,-0.0005218259039742416] Loss: 22.842736992173354\n",
      "Iteracion: 17859 Gradiente: [3.0229965258854462e-05,-0.0005215484853087796] Loss: 22.842736991900203\n",
      "Iteracion: 17860 Gradiente: [3.0213894164414037e-05,-0.0005212712141198969] Loss: 22.842736991627344\n",
      "Iteracion: 17861 Gradiente: [3.019783147143092e-05,-0.0005209940903457759] Loss: 22.842736991354784\n",
      "Iteracion: 17862 Gradiente: [3.018177732675061e-05,-0.0005207171138985463] Loss: 22.84273699108251\n",
      "Iteracion: 17863 Gradiente: [3.0165731809953893e-05,-0.0005204402846951931] Loss: 22.842736990810533\n",
      "Iteracion: 17864 Gradiente: [3.0149694740089217e-05,-0.0005201636026673858] Loss: 22.842736990538846\n",
      "Iteracion: 17865 Gradiente: [3.013366622894864e-05,-0.0005198870677298591] Loss: 22.84273699026743\n",
      "Iteracion: 17866 Gradiente: [3.0117646302111702e-05,-0.0005196106798029139] Loss: 22.842736989996297\n",
      "Iteracion: 17867 Gradiente: [3.010163479662727e-05,-0.000519334438819404] Loss: 22.842736989725477\n",
      "Iteracion: 17868 Gradiente: [3.008563185839345e-05,-0.0005190583446894455] Loss: 22.84273698945493\n",
      "Iteracion: 17869 Gradiente: [3.0069637491199806e-05,-0.0005187823973374843] Loss: 22.84273698918469\n",
      "Iteracion: 17870 Gradiente: [3.0053651518831732e-05,-0.0005185065966925843] Loss: 22.8427369889147\n",
      "Iteracion: 17871 Gradiente: [3.0037674027501755e-05,-0.0005182309426729148] Loss: 22.842736988645022\n",
      "Iteracion: 17872 Gradiente: [3.0021705027631165e-05,-0.0005179554351998424] Loss: 22.84273698837563\n",
      "Iteracion: 17873 Gradiente: [3.0005744558062968e-05,-0.000517680074192602] Loss: 22.84273698810652\n",
      "Iteracion: 17874 Gradiente: [2.9989792639639746e-05,-0.000517404859571613] Loss: 22.842736987837714\n",
      "Iteracion: 17875 Gradiente: [2.9973849219307643e-05,-0.0005171297912619129] Loss: 22.84273698756917\n",
      "Iteracion: 17876 Gradiente: [2.995791414453682e-05,-0.0005168548691955266] Loss: 22.842736987300913\n",
      "Iteracion: 17877 Gradiente: [2.9941987611437072e-05,-0.0005165800932806756] Loss: 22.84273698703295\n",
      "Iteracion: 17878 Gradiente: [2.9926069527164145e-05,-0.0005163054634473714] Loss: 22.842736986765278\n",
      "Iteracion: 17879 Gradiente: [2.9910159930561046e-05,-0.0005160309796140202] Loss: 22.84273698649788\n",
      "Iteracion: 17880 Gradiente: [2.9894258765731745e-05,-0.0005157566417058963] Loss: 22.84273698623077\n",
      "Iteracion: 17881 Gradiente: [2.987836597204326e-05,-0.0005154824496496957] Loss: 22.842736985963935\n",
      "Iteracion: 17882 Gradiente: [2.986248174655278e-05,-0.0005152084033555345] Loss: 22.842736985697396\n",
      "Iteracion: 17883 Gradiente: [2.9846605944309582e-05,-0.0005149345027531875] Loss: 22.842736985431138\n",
      "Iteracion: 17884 Gradiente: [2.9830738603209285e-05,-0.0005146607477641396] Loss: 22.842736985165143\n",
      "Iteracion: 17885 Gradiente: [2.9814879682514098e-05,-0.0005143871383124813] Loss: 22.84273698489947\n",
      "Iteracion: 17886 Gradiente: [2.979902919075054e-05,-0.0005141136743191055] Loss: 22.842736984634044\n",
      "Iteracion: 17887 Gradiente: [2.9783187088128216e-05,-0.0005138403557108262] Loss: 22.84273698436891\n",
      "Iteracion: 17888 Gradiente: [2.9767353465596593e-05,-0.0005135671824034442] Loss: 22.84273698410408\n",
      "Iteracion: 17889 Gradiente: [2.9751528156414982e-05,-0.0005132941543299315] Loss: 22.842736983839508\n",
      "Iteracion: 17890 Gradiente: [2.973571134911405e-05,-0.0005130212714017072] Loss: 22.842736983575215\n",
      "Iteracion: 17891 Gradiente: [2.9719902934743912e-05,-0.0005127485335464143] Loss: 22.842736983311223\n",
      "Iteracion: 17892 Gradiente: [2.9704102861198104e-05,-0.0005124759406905118] Loss: 22.842736983047487\n",
      "Iteracion: 17893 Gradiente: [2.968831124121607e-05,-0.0005122034927503923] Loss: 22.84273698278405\n",
      "Iteracion: 17894 Gradiente: [2.9672528085219105e-05,-0.0005119311896484883] Loss: 22.842736982520883\n",
      "Iteracion: 17895 Gradiente: [2.965675331362642e-05,-0.000511659031312206] Loss: 22.842736982257996\n",
      "Iteracion: 17896 Gradiente: [2.9640986859173306e-05,-0.0005113870176664647] Loss: 22.842736981995394\n",
      "Iteracion: 17897 Gradiente: [2.962522884217833e-05,-0.0005111151486284863] Loss: 22.84273698173307\n",
      "Iteracion: 17898 Gradiente: [2.9609479106322094e-05,-0.0005108434241307691] Loss: 22.84273698147102\n",
      "Iteracion: 17899 Gradiente: [2.9593737807923995e-05,-0.0005105718440848505] Loss: 22.84273698120925\n",
      "Iteracion: 17900 Gradiente: [2.957800482950764e-05,-0.0005103004084226371] Loss: 22.842736980947766\n",
      "Iteracion: 17901 Gradiente: [2.956228019475778e-05,-0.0005100291170658503] Loss: 22.84273698068655\n",
      "Iteracion: 17902 Gradiente: [2.9546564038203846e-05,-0.0005097579699276859] Loss: 22.842736980425613\n",
      "Iteracion: 17903 Gradiente: [2.9530856144788233e-05,-0.0005094869669468002] Loss: 22.842736980164954\n",
      "Iteracion: 17904 Gradiente: [2.9515156622513435e-05,-0.0005092161080367438] Loss: 22.842736979904576\n",
      "Iteracion: 17905 Gradiente: [2.949946547895858e-05,-0.0005089453931221992] Loss: 22.84273697964448\n",
      "Iteracion: 17906 Gradiente: [2.9483782655385463e-05,-0.0005086748221311647] Loss: 22.842736979384654\n",
      "Iteracion: 17907 Gradiente: [2.946810815937321e-05,-0.0005084043949832306] Loss: 22.842736979125107\n",
      "Iteracion: 17908 Gradiente: [2.945244198144792e-05,-0.0005081341116028426] Loss: 22.84273697886583\n",
      "Iteracion: 17909 Gradiente: [2.9436784150031296e-05,-0.0005078639719130251] Loss: 22.84273697860683\n",
      "Iteracion: 17910 Gradiente: [2.9421134703018954e-05,-0.0005075939758345527] Loss: 22.8427369783481\n",
      "Iteracion: 17911 Gradiente: [2.9405493582620085e-05,-0.0005073241232934104] Loss: 22.84273697808965\n",
      "Iteracion: 17912 Gradiente: [2.9389860711148685e-05,-0.0005070544142173598] Loss: 22.842736977831475\n",
      "Iteracion: 17913 Gradiente: [2.9374236137869048e-05,-0.0005067848485275306] Loss: 22.842736977573576\n",
      "Iteracion: 17914 Gradiente: [2.935861988741332e-05,-0.0005065154261465921] Loss: 22.842736977315944\n",
      "Iteracion: 17915 Gradiente: [2.934301193893892e-05,-0.0005062461469981608] Loss: 22.842736977058607\n",
      "Iteracion: 17916 Gradiente: [2.9327412294340625e-05,-0.0005059770110075116] Loss: 22.842736976801525\n",
      "Iteracion: 17917 Gradiente: [2.9311820965934507e-05,-0.0005057080180961293] Loss: 22.842736976544725\n",
      "Iteracion: 17918 Gradiente: [2.9296237890245418e-05,-0.0005054391681920123] Loss: 22.84273697628818\n",
      "Iteracion: 17919 Gradiente: [2.9280663117485043e-05,-0.0005051704612155798] Loss: 22.842736976031933\n",
      "Iteracion: 17920 Gradiente: [2.9265096593652136e-05,-0.0005049018970935275] Loss: 22.842736975775942\n",
      "Iteracion: 17921 Gradiente: [2.9249538336747112e-05,-0.0005046334757491166] Loss: 22.84273697552022\n",
      "Iteracion: 17922 Gradiente: [2.923398845856203e-05,-0.0005043651970999245] Loss: 22.842736975264792\n",
      "Iteracion: 17923 Gradiente: [2.9218446714670185e-05,-0.0005040970610829495] Loss: 22.84273697500963\n",
      "Iteracion: 17924 Gradiente: [2.920291326139098e-05,-0.0005038290676139922] Loss: 22.84273697475472\n",
      "Iteracion: 17925 Gradiente: [2.9187388163146957e-05,-0.0005035612166125247] Loss: 22.842736974500085\n",
      "Iteracion: 17926 Gradiente: [2.9171871277829573e-05,-0.0005032935080123479] Loss: 22.84273697424573\n",
      "Iteracion: 17927 Gradiente: [2.915636258080667e-05,-0.0005030259417370786] Loss: 22.84273697399164\n",
      "Iteracion: 17928 Gradiente: [2.9140862120395166e-05,-0.0005027585177091491] Loss: 22.842736973737825\n",
      "Iteracion: 17929 Gradiente: [2.912536992880632e-05,-0.0005024912358494523] Loss: 22.842736973484275\n",
      "Iteracion: 17930 Gradiente: [2.9109886066673123e-05,-0.0005022240960803023] Loss: 22.84273697323098\n",
      "Iteracion: 17931 Gradiente: [2.9094410290516256e-05,-0.0005019570983388159] Loss: 22.842736972977992\n",
      "Iteracion: 17932 Gradiente: [2.907894289971106e-05,-0.0005016902425334517] Loss: 22.842736972725252\n",
      "Iteracion: 17933 Gradiente: [2.9063483699095134e-05,-0.0005014235285975369] Loss: 22.842736972472782\n",
      "Iteracion: 17934 Gradiente: [2.904803264414113e-05,-0.0005011569564595438] Loss: 22.842736972220575\n",
      "Iteracion: 17935 Gradiente: [2.9032589779376394e-05,-0.0005008905260409573] Loss: 22.84273697196863\n",
      "Iteracion: 17936 Gradiente: [2.901715508869529e-05,-0.000500624237266815] Loss: 22.842736971716963\n",
      "Iteracion: 17937 Gradiente: [2.9001728690521608e-05,-0.000500358090055523] Loss: 22.842736971465566\n",
      "Iteracion: 17938 Gradiente: [2.8986310420956823e-05,-0.000500092084341001] Loss: 22.84273697121444\n",
      "Iteracion: 17939 Gradiente: [2.897090045242597e-05,-0.0004998262200350231] Loss: 22.842736970963568\n",
      "Iteracion: 17940 Gradiente: [2.895549868450568e-05,-0.000499560497071864] Loss: 22.84273697071297\n",
      "Iteracion: 17941 Gradiente: [2.8940105096353363e-05,-0.000499294915375496] Loss: 22.84273697046264\n",
      "Iteracion: 17942 Gradiente: [2.892471956386089e-05,-0.0004990294748781802] Loss: 22.842736970212577\n",
      "Iteracion: 17943 Gradiente: [2.8909342370297962e-05,-0.0004987641754864806] Loss: 22.84273696996277\n",
      "Iteracion: 17944 Gradiente: [2.889397320207839e-05,-0.0004984990171463959] Loss: 22.84273696971325\n",
      "Iteracion: 17945 Gradiente: [2.887861224868023e-05,-0.000498233999769108] Loss: 22.842736969463964\n",
      "Iteracion: 17946 Gradiente: [2.886325954041998e-05,-0.000497969123278826] Loss: 22.842736969214965\n",
      "Iteracion: 17947 Gradiente: [2.8847914963610797e-05,-0.0004977043876067455] Loss: 22.84273696896622\n",
      "Iteracion: 17948 Gradiente: [2.8832578565622194e-05,-0.0004974397926754174] Loss: 22.84273696871776\n",
      "Iteracion: 17949 Gradiente: [2.8817250241293855e-05,-0.0004971753384148532] Loss: 22.84273696846954\n",
      "Iteracion: 17950 Gradiente: [2.8801930172524712e-05,-0.0004969110247400248] Loss: 22.84273696822159\n",
      "Iteracion: 17951 Gradiente: [2.8786618158468022e-05,-0.0004966468515884041] Loss: 22.8427369679739\n",
      "Iteracion: 17952 Gradiente: [2.877131427775718e-05,-0.0004963828188794631] Loss: 22.842736967726488\n",
      "Iteracion: 17953 Gradiente: [2.8756018579656484e-05,-0.0004961189265351607] Loss: 22.84273696747932\n",
      "Iteracion: 17954 Gradiente: [2.874073096658473e-05,-0.000495855174487166] Loss: 22.842736967232433\n",
      "Iteracion: 17955 Gradiente: [2.8725451518122706e-05,-0.0004955915626561354] Loss: 22.842736966985793\n",
      "Iteracion: 17956 Gradiente: [2.871018017363743e-05,-0.000495328090970067] Loss: 22.84273696673943\n",
      "Iteracion: 17957 Gradiente: [2.8694916963445394e-05,-0.0004950647593524593] Loss: 22.842736966493316\n",
      "Iteracion: 17958 Gradiente: [2.8679661879967475e-05,-0.0004948015677301262] Loss: 22.842736966247475\n",
      "Iteracion: 17959 Gradiente: [2.8664414848359836e-05,-0.0004945385160309476] Loss: 22.84273696600188\n",
      "Iteracion: 17960 Gradiente: [2.864917602778405e-05,-0.0004942756041723821] Loss: 22.84273696575657\n",
      "Iteracion: 17961 Gradiente: [2.8633945273289404e-05,-0.0004940128320887045] Loss: 22.8427369655115\n",
      "Iteracion: 17962 Gradiente: [2.861872257824416e-05,-0.000493750199703058] Loss: 22.842736965266678\n",
      "Iteracion: 17963 Gradiente: [2.8603508008965644e-05,-0.0004934877069400063] Loss: 22.842736965022148\n",
      "Iteracion: 17964 Gradiente: [2.8588301509557824e-05,-0.0004932253537270743] Loss: 22.842736964777863\n",
      "Iteracion: 17965 Gradiente: [2.8573103062967675e-05,-0.0004929631399906025] Loss: 22.84273696453384\n",
      "Iteracion: 17966 Gradiente: [2.855791276298684e-05,-0.0004927010656520755] Loss: 22.842736964290072\n",
      "Iteracion: 17967 Gradiente: [2.8542730500665433e-05,-0.0004924391306426894] Loss: 22.84273696404658\n",
      "Iteracion: 17968 Gradiente: [2.8527556306319942e-05,-0.0004921773348847581] Loss: 22.842736963803315\n",
      "Iteracion: 17969 Gradiente: [2.851239014679171e-05,-0.0004919156783090036] Loss: 22.842736963560338\n",
      "Iteracion: 17970 Gradiente: [2.8497232044818095e-05,-0.0004916541608369111] Loss: 22.842736963317613\n",
      "Iteracion: 17971 Gradiente: [2.8482082039242112e-05,-0.0004913927823939919] Loss: 22.842736963075136\n",
      "Iteracion: 17972 Gradiente: [2.8466940069430772e-05,-0.0004911315429082445] Loss: 22.842736962832923\n",
      "Iteracion: 17973 Gradiente: [2.8451806153384494e-05,-0.0004908704423060091] Loss: 22.842736962590966\n",
      "Iteracion: 17974 Gradiente: [2.8436680294892844e-05,-0.0004906094805120867] Loss: 22.84273696234927\n",
      "Iteracion: 17975 Gradiente: [2.8421562497745374e-05,-0.0004903486574529362] Loss: 22.842736962107825\n",
      "Iteracion: 17976 Gradiente: [2.8406452768573822e-05,-0.0004900879730525294] Loss: 22.842736961866645\n",
      "Iteracion: 17977 Gradiente: [2.8391351060956065e-05,-0.0004898274272407596] Loss: 22.84273696162572\n",
      "Iteracion: 17978 Gradiente: [2.8376257281100454e-05,-0.0004895670199490591] Loss: 22.84273696138504\n",
      "Iteracion: 17979 Gradiente: [2.8361171578694667e-05,-0.0004893067510951236] Loss: 22.842736961144624\n",
      "Iteracion: 17980 Gradiente: [2.834609390257962e-05,-0.0004890466206074251] Loss: 22.842736960904453\n",
      "Iteracion: 17981 Gradiente: [2.8331024308651345e-05,-0.0004887866284087513] Loss: 22.84273696066456\n",
      "Iteracion: 17982 Gradiente: [2.831596257522051e-05,-0.0004885267744394165] Loss: 22.84273696042491\n",
      "Iteracion: 17983 Gradiente: [2.8300908921134273e-05,-0.00048826705861214256] Loss: 22.842736960185505\n",
      "Iteracion: 17984 Gradiente: [2.828586331986571e-05,-0.0004880074808537434] Loss: 22.84273695994636\n",
      "Iteracion: 17985 Gradiente: [2.8270825693728815e-05,-0.0004877480410974281] Loss: 22.842736959707484\n",
      "Iteracion: 17986 Gradiente: [2.8255796072092682e-05,-0.0004874887392662212] Loss: 22.842736959468848\n",
      "Iteracion: 17987 Gradiente: [2.8240774396219117e-05,-0.00048722957529131843] Loss: 22.842736959230457\n",
      "Iteracion: 17988 Gradiente: [2.8225760808216667e-05,-0.00048697054908934944] Loss: 22.84273695899233\n",
      "Iteracion: 17989 Gradiente: [2.8210755107238584e-05,-0.0004867116605981418] Loss: 22.84273695875446\n",
      "Iteracion: 17990 Gradiente: [2.8195757436340804e-05,-0.00048645290973823305] Loss: 22.84273695851684\n",
      "Iteracion: 17991 Gradiente: [2.818076765530956e-05,-0.000486194296442477] Loss: 22.84273695827947\n",
      "Iteracion: 17992 Gradiente: [2.8165785899621672e-05,-0.0004859358206299902] Loss: 22.84273695804236\n",
      "Iteracion: 17993 Gradiente: [2.8150812140855427e-05,-0.0004856774822300736] Loss: 22.842736957805492\n",
      "Iteracion: 17994 Gradiente: [2.813584627574528e-05,-0.0004854192811742782] Loss: 22.842736957568885\n",
      "Iteracion: 17995 Gradiente: [2.812088841703068e-05,-0.00048516121738254964] Loss: 22.842736957332516\n",
      "Iteracion: 17996 Gradiente: [2.8105938440603495e-05,-0.00048490329079034684] Loss: 22.84273695709641\n",
      "Iteracion: 17997 Gradiente: [2.8090996486677493e-05,-0.0004846455013156022] Loss: 22.842736956860556\n",
      "Iteracion: 17998 Gradiente: [2.807606242261803e-05,-0.00048438784889377474] Loss: 22.842736956624943\n",
      "Iteracion: 17999 Gradiente: [2.8061136383901915e-05,-0.00048413033344042825] Loss: 22.842736956389583\n",
      "Iteracion: 18000 Gradiente: [2.8046218309896177e-05,-0.0004838729548898376] Loss: 22.842736956154475\n",
      "Iteracion: 18001 Gradiente: [2.8031308036702286e-05,-0.000483615713177817] Loss: 22.842736955919623\n",
      "Iteracion: 18002 Gradiente: [2.8016405742429622e-05,-0.0004833586082195751] Loss: 22.84273695568501\n",
      "Iteracion: 18003 Gradiente: [2.8001511371182156e-05,-0.0004831016399460708] Loss: 22.84273695545066\n",
      "Iteracion: 18004 Gradiente: [2.798662491632816e-05,-0.0004828448082850656] Loss: 22.842736955216548\n",
      "Iteracion: 18005 Gradiente: [2.7971746255654277e-05,-0.0004825881131708343] Loss: 22.842736954982684\n",
      "Iteracion: 18006 Gradiente: [2.7956875649692846e-05,-0.000482331554515388] Loss: 22.84273695474907\n",
      "Iteracion: 18007 Gradiente: [2.7942012881491488e-05,-0.00048207513225821214] Loss: 22.842736954515708\n",
      "Iteracion: 18008 Gradiente: [2.7927158021157085e-05,-0.0004818188463229234] Loss: 22.842736954282593\n",
      "Iteracion: 18009 Gradiente: [2.79123111501652e-05,-0.00048156269663076994] Loss: 22.842736954049727\n",
      "Iteracion: 18010 Gradiente: [2.789747213019685e-05,-0.00048130668311839505] Loss: 22.842736953817113\n",
      "Iteracion: 18011 Gradiente: [2.7882640987778966e-05,-0.0004810508057109549] Loss: 22.842736953584737\n",
      "Iteracion: 18012 Gradiente: [2.786781764522554e-05,-0.00048079506434071104] Loss: 22.842736953352617\n",
      "Iteracion: 18013 Gradiente: [2.7853002307172878e-05,-0.00048053945892358266] Loss: 22.842736953120728\n",
      "Iteracion: 18014 Gradiente: [2.7838194751931646e-05,-0.0004802839894003578] Loss: 22.84273695288911\n",
      "Iteracion: 18015 Gradiente: [2.7823395083714785e-05,-0.000480028655690982] Loss: 22.84273695265771\n",
      "Iteracion: 18016 Gradiente: [2.7808603308206634e-05,-0.0004797734577245194] Loss: 22.842736952426563\n",
      "Iteracion: 18017 Gradiente: [2.779381948604017e-05,-0.0004795183954217445] Loss: 22.842736952195676\n",
      "Iteracion: 18018 Gradiente: [2.7779043398368232e-05,-0.0004792634687269981] Loss: 22.84273695196502\n",
      "Iteracion: 18019 Gradiente: [2.7764275251721908e-05,-0.00047900867755477824] Loss: 22.84273695173463\n",
      "Iteracion: 18020 Gradiente: [2.7749514906834823e-05,-0.00047875402183924126] Loss: 22.84273695150447\n",
      "Iteracion: 18021 Gradiente: [2.773476236465437e-05,-0.00047849950150767503] Loss: 22.84273695127456\n",
      "Iteracion: 18022 Gradiente: [2.7720017758762575e-05,-0.0004782451164829856] Loss: 22.84273695104489\n",
      "Iteracion: 18023 Gradiente: [2.7705280933787436e-05,-0.00047799086670086887] Loss: 22.842736950815468\n",
      "Iteracion: 18024 Gradiente: [2.76905519039398e-05,-0.0004777367520871915] Loss: 22.842736950586275\n",
      "Iteracion: 18025 Gradiente: [2.767583077816956e-05,-0.00047748277256426754] Loss: 22.842736950357335\n",
      "Iteracion: 18026 Gradiente: [2.7661117534686734e-05,-0.00047722892806139803] Loss: 22.842736950128643\n",
      "Iteracion: 18027 Gradiente: [2.7646411988750213e-05,-0.00047697521851712094] Loss: 22.842736949900196\n",
      "Iteracion: 18028 Gradiente: [2.763171425120466e-05,-0.00047672164385375025] Loss: 22.842736949671977\n",
      "Iteracion: 18029 Gradiente: [2.7617024392156965e-05,-0.0004764682039949027] Loss: 22.842736949444017\n",
      "Iteracion: 18030 Gradiente: [2.7602342331078945e-05,-0.0004762148988729583] Loss: 22.84273694921629\n",
      "Iteracion: 18031 Gradiente: [2.75876680291276e-05,-0.0004759617284185206] Loss: 22.84273694898882\n",
      "Iteracion: 18032 Gradiente: [2.757300170515009e-05,-0.0004757086925465615] Loss: 22.842736948761573\n",
      "Iteracion: 18033 Gradiente: [2.7558343043665444e-05,-0.00047545579120461903] Loss: 22.84273694853456\n",
      "Iteracion: 18034 Gradiente: [2.7543692238888675e-05,-0.0004752030243087309] Loss: 22.842736948307802\n",
      "Iteracion: 18035 Gradiente: [2.7529049146816455e-05,-0.0004749503917961325] Loss: 22.8427369480813\n",
      "Iteracion: 18036 Gradiente: [2.751441383281872e-05,-0.00047469789358961143] Loss: 22.842736947855023\n",
      "Iteracion: 18037 Gradiente: [2.7499786308264145e-05,-0.00047444552962107404] Loss: 22.842736947628982\n",
      "Iteracion: 18038 Gradiente: [2.74851665958901e-05,-0.0004741932998127159] Loss: 22.842736947403182\n",
      "Iteracion: 18039 Gradiente: [2.7470554641695344e-05,-0.000473941204099404] Loss: 22.842736947177634\n",
      "Iteracion: 18040 Gradiente: [2.7455950422942503e-05,-0.00047368924240901816] Loss: 22.842736946952332\n",
      "Iteracion: 18041 Gradiente: [2.744135400689629e-05,-0.00047343741466742507] Loss: 22.842736946727246\n",
      "Iteracion: 18042 Gradiente: [2.7426765328186775e-05,-0.00047318572080593907] Loss: 22.842736946502402\n",
      "Iteracion: 18043 Gradiente: [2.7412184376392664e-05,-0.0004729341607552821] Loss: 22.84273694627781\n",
      "Iteracion: 18044 Gradiente: [2.7397611298359455e-05,-0.00047268273443457076] Loss: 22.842736946053453\n",
      "Iteracion: 18045 Gradiente: [2.7383045864818693e-05,-0.0004724314417851853] Loss: 22.842736945829333\n",
      "Iteracion: 18046 Gradiente: [2.73684881714568e-05,-0.0004721802827310976] Loss: 22.842736945605463\n",
      "Iteracion: 18047 Gradiente: [2.7353938249537654e-05,-0.0004719292571995955] Loss: 22.84273694538182\n",
      "Iteracion: 18048 Gradiente: [2.733939605642869e-05,-0.0004716783651203353] Loss: 22.8427369451584\n",
      "Iteracion: 18049 Gradiente: [2.7324861594972086e-05,-0.0004714276064233284] Loss: 22.842736944935236\n",
      "Iteracion: 18050 Gradiente: [2.731033490306345e-05,-0.00047117698103503377] Loss: 22.842736944712303\n",
      "Iteracion: 18051 Gradiente: [2.729581588501636e-05,-0.00047092648888937086] Loss: 22.842736944489605\n",
      "Iteracion: 18052 Gradiente: [2.7281304615674646e-05,-0.0004706761299116143] Loss: 22.842736944267152\n",
      "Iteracion: 18053 Gradiente: [2.7266801020194482e-05,-0.0004704259040347362] Loss: 22.842736944044926\n",
      "Iteracion: 18054 Gradiente: [2.7252305096681083e-05,-0.0004701758111875639] Loss: 22.84273694382295\n",
      "Iteracion: 18055 Gradiente: [2.7237816968295193e-05,-0.00046992585129252974] Loss: 22.84273694360119\n",
      "Iteracion: 18056 Gradiente: [2.7223336538402994e-05,-0.0004696760242841454] Loss: 22.842736943379684\n",
      "Iteracion: 18057 Gradiente: [2.7208863779530173e-05,-0.00046942633009289616] Loss: 22.842736943158407\n",
      "Iteracion: 18058 Gradiente: [2.719439865283372e-05,-0.0004691767686495041] Loss: 22.842736942937353\n",
      "Iteracion: 18059 Gradiente: [2.717994123694704e-05,-0.00046892733988030956] Loss: 22.842736942716563\n",
      "Iteracion: 18060 Gradiente: [2.7165491582081812e-05,-0.0004686780437109424] Loss: 22.842736942495986\n",
      "Iteracion: 18061 Gradiente: [2.7151049554656007e-05,-0.0004684288800779276] Loss: 22.84273694227564\n",
      "Iteracion: 18062 Gradiente: [2.7136615186880894e-05,-0.00046817984890902645] Loss: 22.842736942055538\n",
      "Iteracion: 18063 Gradiente: [2.712218856117943e-05,-0.0004679309501293953] Loss: 22.842736941835675\n",
      "Iteracion: 18064 Gradiente: [2.710776961502385e-05,-0.0004676821836703482] Loss: 22.842736941616028\n",
      "Iteracion: 18065 Gradiente: [2.7093358327571573e-05,-0.00046743354946355474] Loss: 22.842736941396627\n",
      "Iteracion: 18066 Gradiente: [2.707895470166477e-05,-0.0004671850474391448] Loss: 22.84273694117747\n",
      "Iteracion: 18067 Gradiente: [2.7064558731619098e-05,-0.0004669366775253536] Loss: 22.842736940958527\n",
      "Iteracion: 18068 Gradiente: [2.7050170274378615e-05,-0.0004666884396611929] Loss: 22.842736940739822\n",
      "Iteracion: 18069 Gradiente: [2.7035789533632244e-05,-0.00046644033376471344] Loss: 22.84273694052136\n",
      "Iteracion: 18070 Gradiente: [2.7021416450641785e-05,-0.0004661923597674663] Loss: 22.842736940303105\n",
      "Iteracion: 18071 Gradiente: [2.7007051031091578e-05,-0.0004659445176004103] Loss: 22.8427369400851\n",
      "Iteracion: 18072 Gradiente: [2.6992693274981624e-05,-0.0004656968071916623] Loss: 22.84273693986732\n",
      "Iteracion: 18073 Gradiente: [2.697834317189063e-05,-0.0004654492284715891] Loss: 22.84273693964978\n",
      "Iteracion: 18074 Gradiente: [2.6964000672554295e-05,-0.00046520178137458384] Loss: 22.84273693943247\n",
      "Iteracion: 18075 Gradiente: [2.694966576086699e-05,-0.0004649544658294739] Loss: 22.842736939215378\n",
      "Iteracion: 18076 Gradiente: [2.693533850598821e-05,-0.00046470728176295495] Loss: 22.842736938998534\n",
      "Iteracion: 18077 Gradiente: [2.6921018909812727e-05,-0.000464460229104328] Loss: 22.842736938781915\n",
      "Iteracion: 18078 Gradiente: [2.690670680844202e-05,-0.0004642133077941442] Loss: 22.84273693856552\n",
      "Iteracion: 18079 Gradiente: [2.6892402340195075e-05,-0.00046396651775246764] Loss: 22.84273693834937\n",
      "Iteracion: 18080 Gradiente: [2.6878105539177948e-05,-0.0004637198589084809] Loss: 22.84273693813343\n",
      "Iteracion: 18081 Gradiente: [2.686381633717853e-05,-0.00046347333119681384] Loss: 22.842736937917724\n",
      "Iteracion: 18082 Gradiente: [2.6849534738933773e-05,-0.0004632269345458203] Loss: 22.84273693770227\n",
      "Iteracion: 18083 Gradiente: [2.6835260610861646e-05,-0.00046298066889391973] Loss: 22.842736937487008\n",
      "Iteracion: 18084 Gradiente: [2.68209941926519e-05,-0.0004627345341573867] Loss: 22.842736937271997\n",
      "Iteracion: 18085 Gradiente: [2.6806735324195567e-05,-0.00046248853027653546] Loss: 22.842736937057218\n",
      "Iteracion: 18086 Gradiente: [2.6792484115389924e-05,-0.00046224265717427216] Loss: 22.842736936842655\n",
      "Iteracion: 18087 Gradiente: [2.6778240377704304e-05,-0.0004619969147902007] Loss: 22.84273693662832\n",
      "Iteracion: 18088 Gradiente: [2.6764004282616345e-05,-0.00046175130304722717] Loss: 22.842736936414227\n",
      "Iteracion: 18089 Gradiente: [2.674977566622753e-05,-0.00046150582188341595] Loss: 22.842736936200346\n",
      "Iteracion: 18090 Gradiente: [2.6735554646014254e-05,-0.0004612604712240416] Loss: 22.842736935986714\n",
      "Iteracion: 18091 Gradiente: [2.6721341215344788e-05,-0.0004610152509984052] Loss: 22.842736935773292\n",
      "Iteracion: 18092 Gradiente: [2.6707135354323934e-05,-0.0004607701611391235] Loss: 22.842736935560097\n",
      "Iteracion: 18093 Gradiente: [2.669293699852915e-05,-0.00046052520157857657] Loss: 22.84273693534713\n",
      "Iteracion: 18094 Gradiente: [2.6678746206698633e-05,-0.000460280372245947] Loss: 22.842736935134393\n",
      "Iteracion: 18095 Gradiente: [2.666456301199105e-05,-0.000460035673070062] Loss: 22.842736934921888\n",
      "Iteracion: 18096 Gradiente: [2.6650387329141268e-05,-0.00045979110398436753] Loss: 22.842736934709585\n",
      "Iteracion: 18097 Gradiente: [2.663621916951797e-05,-0.00045954666491899353] Loss: 22.842736934497527\n",
      "Iteracion: 18098 Gradiente: [2.6622058484804256e-05,-0.0004593023558092805] Loss: 22.84273693428568\n",
      "Iteracion: 18099 Gradiente: [2.660790536594959e-05,-0.0004590581765793189] Loss: 22.842736934074075\n",
      "Iteracion: 18100 Gradiente: [2.659375972958363e-05,-0.00045881412716515985] Loss: 22.8427369338627\n",
      "Iteracion: 18101 Gradiente: [2.6579621790763972e-05,-0.00045857020748497256] Loss: 22.842736933651533\n",
      "Iteracion: 18102 Gradiente: [2.656549119611403e-05,-0.00045832641748984787] Loss: 22.842736933440595\n",
      "Iteracion: 18103 Gradiente: [2.6551368130374915e-05,-0.000458082757100442] Loss: 22.842736933229876\n",
      "Iteracion: 18104 Gradiente: [2.6537252617231387e-05,-0.00045783922624510845] Loss: 22.8427369330194\n",
      "Iteracion: 18105 Gradiente: [2.6523144593208297e-05,-0.0004575958248595432] Loss: 22.842736932809125\n",
      "Iteracion: 18106 Gradiente: [2.650904412367557e-05,-0.00045735255287174443] Loss: 22.842736932599088\n",
      "Iteracion: 18107 Gradiente: [2.649495105610337e-05,-0.00045710941021811873] Loss: 22.84273693238927\n",
      "Iteracion: 18108 Gradiente: [2.64808654880729e-05,-0.0004568663968274933] Loss: 22.842736932179672\n",
      "Iteracion: 18109 Gradiente: [2.6466787483059305e-05,-0.0004566235126257349] Loss: 22.842736931970304\n",
      "Iteracion: 18110 Gradiente: [2.6452716943481393e-05,-0.00045638075754889464] Loss: 22.842736931761156\n",
      "Iteracion: 18111 Gradiente: [2.6438653882602618e-05,-0.0004561381315293526] Loss: 22.842736931552228\n",
      "Iteracion: 18112 Gradiente: [2.6424598264422153e-05,-0.0004558956344983045] Loss: 22.84273693134352\n",
      "Iteracion: 18113 Gradiente: [2.641055011925649e-05,-0.00045565326638635404] Loss: 22.842736931135043\n",
      "Iteracion: 18114 Gradiente: [2.6396509412999572e-05,-0.000455411027126947] Loss: 22.842736930926776\n",
      "Iteracion: 18115 Gradiente: [2.6382476316181663e-05,-0.0004551689166394368] Loss: 22.842736930718733\n",
      "Iteracion: 18116 Gradiente: [2.6368450470689216e-05,-0.0004549269348794146] Loss: 22.842736930510917\n",
      "Iteracion: 18117 Gradiente: [2.635443218537148e-05,-0.00045468508175782553] Loss: 22.842736930303314\n",
      "Iteracion: 18118 Gradiente: [2.634042143085935e-05,-0.0004544433572066945] Loss: 22.842736930095935\n",
      "Iteracion: 18119 Gradiente: [2.6326418090623823e-05,-0.00045420176116669114] Loss: 22.842736929888773\n",
      "Iteracion: 18120 Gradiente: [2.6312422096452793e-05,-0.00045396029357315607] Loss: 22.842736929681838\n",
      "Iteracion: 18121 Gradiente: [2.629843366340386e-05,-0.00045371895434307417] Loss: 22.842736929475123\n",
      "Iteracion: 18122 Gradiente: [2.6284452568840305e-05,-0.00045347774342291793] Loss: 22.84273692926864\n",
      "Iteracion: 18123 Gradiente: [2.627047898613455e-05,-0.00045323666073227763] Loss: 22.842736929062344\n",
      "Iteracion: 18124 Gradiente: [2.6256512792125857e-05,-0.0004529957062119413] Loss: 22.842736928856294\n",
      "Iteracion: 18125 Gradiente: [2.624255406544762e-05,-0.00045275487978742036] Loss: 22.842736928650464\n",
      "Iteracion: 18126 Gradiente: [2.6228602655464785e-05,-0.000452514181399503] Loss: 22.842736928444825\n",
      "Iteracion: 18127 Gradiente: [2.621465875544497e-05,-0.00045227361097038474] Loss: 22.84273692823943\n",
      "Iteracion: 18128 Gradiente: [2.620072223085875e-05,-0.0004520331684367089] Loss: 22.842736928034242\n",
      "Iteracion: 18129 Gradiente: [2.6186793097811762e-05,-0.0004517928537305001] Loss: 22.84273692782928\n",
      "Iteracion: 18130 Gradiente: [2.6172871347777497e-05,-0.00045155266678437536] Loss: 22.84273692762453\n",
      "Iteracion: 18131 Gradiente: [2.6158957054652396e-05,-0.00045131260752585926] Loss: 22.84273692742\n",
      "Iteracion: 18132 Gradiente: [2.6145050188119968e-05,-0.00045107267588946344] Loss: 22.842736927215686\n",
      "Iteracion: 18133 Gradiente: [2.613115071312677e-05,-0.0004508328718060284] Loss: 22.84273692701158\n",
      "Iteracion: 18134 Gradiente: [2.611725850935424e-05,-0.0004505931952188291] Loss: 22.8427369268077\n",
      "Iteracion: 18135 Gradiente: [2.6103373785228237e-05,-0.00045035364604520585] Loss: 22.842736926604047\n",
      "Iteracion: 18136 Gradiente: [2.6089496536011818e-05,-0.0004501142242165912] Loss: 22.842736926400605\n",
      "Iteracion: 18137 Gradiente: [2.607562649264613e-05,-0.0004498749296828919] Loss: 22.842736926197382\n",
      "Iteracion: 18138 Gradiente: [2.6061763892926138e-05,-0.000449635762360856] Loss: 22.842736925994373\n",
      "Iteracion: 18139 Gradiente: [2.6047908634533692e-05,-0.00044939672218961374] Loss: 22.842736925791556\n",
      "Iteracion: 18140 Gradiente: [2.603406076768048e-05,-0.000449157809098466] Loss: 22.842736925588984\n",
      "Iteracion: 18141 Gradiente: [2.602022031415648e-05,-0.0004489190230174245] Loss: 22.842736925386625\n",
      "Iteracion: 18142 Gradiente: [2.600638713469531e-05,-0.00044868036388715876] Loss: 22.842736925184468\n",
      "Iteracion: 18143 Gradiente: [2.5992561369510743e-05,-0.0004484418316312855] Loss: 22.842736924982535\n",
      "Iteracion: 18144 Gradiente: [2.5978742856599033e-05,-0.00044820342619189546] Loss: 22.8427369247808\n",
      "Iteracion: 18145 Gradiente: [2.5964931769332603e-05,-0.0004479651474933159] Loss: 22.842736924579295\n",
      "Iteracion: 18146 Gradiente: [2.595112802055155e-05,-0.00044772699547076893] Loss: 22.842736924378002\n",
      "Iteracion: 18147 Gradiente: [2.5937331656677998e-05,-0.0004474889700548582] Loss: 22.842736924176926\n",
      "Iteracion: 18148 Gradiente: [2.592354260286811e-05,-0.0004472510711799771] Loss: 22.842736923976062\n",
      "Iteracion: 18149 Gradiente: [2.59097608089102e-05,-0.00044701329878442663] Loss: 22.8427369237754\n",
      "Iteracion: 18150 Gradiente: [2.589598641407065e-05,-0.00044677565279206043] Loss: 22.842736923574964\n",
      "Iteracion: 18151 Gradiente: [2.5882219284767415e-05,-0.0004465381331434296] Loss: 22.842736923374737\n",
      "Iteracion: 18152 Gradiente: [2.586845949584434e-05,-0.000446300739766059] Loss: 22.84273692317472\n",
      "Iteracion: 18153 Gradiente: [2.585470699708973e-05,-0.0004460634725949338] Loss: 22.84273692297494\n",
      "Iteracion: 18154 Gradiente: [2.5840961795135322e-05,-0.0004458263315637367] Loss: 22.842736922775334\n",
      "Iteracion: 18155 Gradiente: [2.5827224012194467e-05,-0.0004455893165972687] Loss: 22.84273692257596\n",
      "Iteracion: 18156 Gradiente: [2.5813493500474276e-05,-0.00044535242763714676] Loss: 22.842736922376798\n",
      "Iteracion: 18157 Gradiente: [2.5799770195552204e-05,-0.00044511566462072477] Loss: 22.84273692217784\n",
      "Iteracion: 18158 Gradiente: [2.5786054336170613e-05,-0.00044487902746451387] Loss: 22.842736921979103\n",
      "Iteracion: 18159 Gradiente: [2.5772345653270654e-05,-0.00044464251612019723] Loss: 22.84273692178058\n",
      "Iteracion: 18160 Gradiente: [2.575864429275043e-05,-0.0004444061305096151] Loss: 22.842736921582265\n",
      "Iteracion: 18161 Gradiente: [2.5744950155133967e-05,-0.00044416987057225297] Loss: 22.842736921384148\n",
      "Iteracion: 18162 Gradiente: [2.5731263339897243e-05,-0.0004439337362356355] Loss: 22.84273692118625\n",
      "Iteracion: 18163 Gradiente: [2.5717583829039842e-05,-0.0004436977274346295] Loss: 22.84273692098857\n",
      "Iteracion: 18164 Gradiente: [2.5703911556244446e-05,-0.0004434618441042204] Loss: 22.842736920791086\n",
      "Iteracion: 18165 Gradiente: [2.5690246496878898e-05,-0.0004432260861808146] Loss: 22.842736920593822\n",
      "Iteracion: 18166 Gradiente: [2.5676588782630462e-05,-0.0004429904535879103] Loss: 22.842736920396764\n",
      "Iteracion: 18167 Gradiente: [2.5662938314970538e-05,-0.0004427549462657036] Loss: 22.84273692019992\n",
      "Iteracion: 18168 Gradiente: [2.5649295182006426e-05,-0.00044251956414195586] Loss: 22.842736920003283\n",
      "Iteracion: 18169 Gradiente: [2.563565925773522e-05,-0.00044228430715757366] Loss: 22.84273691980685\n",
      "Iteracion: 18170 Gradiente: [2.5622030643527676e-05,-0.0004420491752383053] Loss: 22.84273691961063\n",
      "Iteracion: 18171 Gradiente: [2.5608409054219312e-05,-0.00044181416833559694] Loss: 22.842736919414616\n",
      "Iteracion: 18172 Gradiente: [2.5594794883924503e-05,-0.0004415792863598019] Loss: 22.842736919218815\n",
      "Iteracion: 18173 Gradiente: [2.558118788537437e-05,-0.0004413445292582215] Loss: 22.84273691902322\n",
      "Iteracion: 18174 Gradiente: [2.5567588162781856e-05,-0.00044110989695897256] Loss: 22.842736918827832\n",
      "Iteracion: 18175 Gradiente: [2.555399561572358e-05,-0.00044087538939940885] Loss: 22.842736918632646\n",
      "Iteracion: 18176 Gradiente: [2.5540410329464674e-05,-0.00044064100650966035] Loss: 22.842736918437684\n",
      "Iteracion: 18177 Gradiente: [2.5526832290741672e-05,-0.0004404067482235282] Loss: 22.84273691824291\n",
      "Iteracion: 18178 Gradiente: [2.5513261475869817e-05,-0.0004401726144770635] Loss: 22.842736918048352\n",
      "Iteracion: 18179 Gradiente: [2.5499697808110493e-05,-0.00043993860520525154] Loss: 22.842736917854005\n",
      "Iteracion: 18180 Gradiente: [2.5486141333885824e-05,-0.00043970472034272255] Loss: 22.84273691765987\n",
      "Iteracion: 18181 Gradiente: [2.547259209109143e-05,-0.00043947095981771155] Loss: 22.842736917465917\n",
      "Iteracion: 18182 Gradiente: [2.545905008351686e-05,-0.0004392373235657961] Loss: 22.842736917272187\n",
      "Iteracion: 18183 Gradiente: [2.5445515393585084e-05,-0.0004390038115159219] Loss: 22.84273691707867\n",
      "Iteracion: 18184 Gradiente: [2.5431987696341216e-05,-0.00043877042361894304] Loss: 22.842736916885336\n",
      "Iteracion: 18185 Gradiente: [2.5418467336635332e-05,-0.00043853715978983134] Loss: 22.84273691669222\n",
      "Iteracion: 18186 Gradiente: [2.5404954084251585e-05,-0.0004383040199755328] Loss: 22.842736916499312\n",
      "Iteracion: 18187 Gradiente: [2.5391448019718155e-05,-0.0004380710041051117] Loss: 22.842736916306606\n",
      "Iteracion: 18188 Gradiente: [2.5377949093770742e-05,-0.000437838112116277] Loss: 22.84273691611411\n",
      "Iteracion: 18189 Gradiente: [2.5364457445675726e-05,-0.0004376053439335929] Loss: 22.84273691592183\n",
      "Iteracion: 18190 Gradiente: [2.5350972907745018e-05,-0.0004373726995018738] Loss: 22.84273691572973\n",
      "Iteracion: 18191 Gradiente: [2.5337495559559407e-05,-0.00043714017874864434] Loss: 22.842736915537838\n",
      "Iteracion: 18192 Gradiente: [2.5324025337643737e-05,-0.00043690778161339] Loss: 22.842736915346162\n",
      "Iteracion: 18193 Gradiente: [2.531056229694665e-05,-0.000436675508027425] Loss: 22.842736915154674\n",
      "Iteracion: 18194 Gradiente: [2.5297106437468148e-05,-0.0004364433579220635] Loss: 22.842736914963403\n",
      "Iteracion: 18195 Gradiente: [2.5283657747839547e-05,-0.00043621133123584365] Loss: 22.84273691477233\n",
      "Iteracion: 18196 Gradiente: [2.5270216144690494e-05,-0.0004359794279041059] Loss: 22.84273691458146\n",
      "Iteracion: 18197 Gradiente: [2.525678174360261e-05,-0.00043574764785662506] Loss: 22.842736914390795\n",
      "Iteracion: 18198 Gradiente: [2.5243354459310772e-05,-0.0004355159910314654] Loss: 22.84273691420034\n",
      "Iteracion: 18199 Gradiente: [2.522993428613063e-05,-0.0004352844573646782] Loss: 22.84273691401007\n",
      "Iteracion: 18200 Gradiente: [2.521652127048431e-05,-0.0004350530467862749] Loss: 22.842736913820016\n",
      "Iteracion: 18201 Gradiente: [2.520311532900147e-05,-0.0004348217592363331] Loss: 22.842736913630162\n",
      "Iteracion: 18202 Gradiente: [2.5189716647370613e-05,-0.00043459059463941686] Loss: 22.842736913440508\n",
      "Iteracion: 18203 Gradiente: [2.5176324996323274e-05,-0.00043435955294081433] Loss: 22.842736913251063\n",
      "Iteracion: 18204 Gradiente: [2.5162940488598906e-05,-0.000434128634069945] Loss: 22.8427369130618\n",
      "Iteracion: 18205 Gradiente: [2.514956315261922e-05,-0.00043389783796025466] Loss: 22.842736912872745\n",
      "Iteracion: 18206 Gradiente: [2.513619295143599e-05,-0.0004336671645460181] Loss: 22.8427369126839\n",
      "Iteracion: 18207 Gradiente: [2.512282975146718e-05,-0.0004334366137712209] Loss: 22.842736912495248\n",
      "Iteracion: 18208 Gradiente: [2.5109473639872704e-05,-0.0004332061855656377] Loss: 22.84273691230681\n",
      "Iteracion: 18209 Gradiente: [2.509612471423376e-05,-0.0004329758798580959] Loss: 22.84273691211856\n",
      "Iteracion: 18210 Gradiente: [2.5082782790756634e-05,-0.0004327456965938836] Loss: 22.84273691193051\n",
      "Iteracion: 18211 Gradiente: [2.5069448068393285e-05,-0.0004325156356942491] Loss: 22.842736911742648\n",
      "Iteracion: 18212 Gradiente: [2.505612034913914e-05,-0.0004322856971089806] Loss: 22.84273691155502\n",
      "Iteracion: 18213 Gradiente: [2.5042799807314018e-05,-0.00043205588075991843] Loss: 22.842736911367567\n",
      "Iteracion: 18214 Gradiente: [2.502948618522775e-05,-0.0004318261865973246] Loss: 22.842736911180307\n",
      "Iteracion: 18215 Gradiente: [2.5016179777518724e-05,-0.00043159661453972354] Loss: 22.84273691099326\n",
      "Iteracion: 18216 Gradiente: [2.5002880399445833e-05,-0.00043136716453204824] Loss: 22.84273691080641\n",
      "Iteracion: 18217 Gradiente: [2.4989588085115125e-05,-0.00043113783650774453] Loss: 22.84273691061976\n",
      "Iteracion: 18218 Gradiente: [2.497630280136794e-05,-0.000430908630403574] Loss: 22.842736910433295\n",
      "Iteracion: 18219 Gradiente: [2.4963024677996752e-05,-0.0004306795461465877] Loss: 22.842736910247044\n",
      "Iteracion: 18220 Gradiente: [2.494975350941786e-05,-0.0004304505836831396] Loss: 22.842736910060985\n",
      "Iteracion: 18221 Gradiente: [2.4936489442476765e-05,-0.00043022174294122807] Loss: 22.84273690987512\n",
      "Iteracion: 18222 Gradiente: [2.4923232444962196e-05,-0.0004299930238567858] Loss: 22.84273690968946\n",
      "Iteracion: 18223 Gradiente: [2.4909982565191056e-05,-0.00042976442636219285] Loss: 22.842736909504005\n",
      "Iteracion: 18224 Gradiente: [2.489673970652954e-05,-0.00042953595039894784] Loss: 22.84273690931872\n",
      "Iteracion: 18225 Gradiente: [2.4883503758132975e-05,-0.0004293075959083126] Loss: 22.842736909133656\n",
      "Iteracion: 18226 Gradiente: [2.487027497958631e-05,-0.0004290793628096405] Loss: 22.842736908948787\n",
      "Iteracion: 18227 Gradiente: [2.4857053185201038e-05,-0.0004288512510495224] Loss: 22.84273690876411\n",
      "Iteracion: 18228 Gradiente: [2.484383844508405e-05,-0.0004286232605591541] Loss: 22.84273690857962\n",
      "Iteracion: 18229 Gradiente: [2.4830630654075018e-05,-0.00042839539127908686] Loss: 22.842736908395338\n",
      "Iteracion: 18230 Gradiente: [2.481742993059773e-05,-0.00042816764313909533] Loss: 22.842736908211247\n",
      "Iteracion: 18231 Gradiente: [2.4804236233914403e-05,-0.0004279400160765334] Loss: 22.842736908027348\n",
      "Iteracion: 18232 Gradiente: [2.479104953370855e-05,-0.00042771251002863647] Loss: 22.84273690784364\n",
      "Iteracion: 18233 Gradiente: [2.4777869916192687e-05,-0.0004274851249260081] Loss: 22.84273690766015\n",
      "Iteracion: 18234 Gradiente: [2.4764697195678308e-05,-0.0004272578607152392] Loss: 22.84273690747684\n",
      "Iteracion: 18235 Gradiente: [2.47515315114318e-05,-0.0004270307173220781] Loss: 22.842736907293727\n",
      "Iteracion: 18236 Gradiente: [2.4738372814188855e-05,-0.0004268036946873129] Loss: 22.8427369071108\n",
      "Iteracion: 18237 Gradiente: [2.4725221122897287e-05,-0.0004265767927433236] Loss: 22.842736906928078\n",
      "Iteracion: 18238 Gradiente: [2.471207640345104e-05,-0.00042635001142699025] Loss: 22.842736906745547\n",
      "Iteracion: 18239 Gradiente: [2.469893871837788e-05,-0.0004261233506740088] Loss: 22.842736906563214\n",
      "Iteracion: 18240 Gradiente: [2.4685808012729162e-05,-0.00042589681042102254] Loss: 22.842736906381074\n",
      "Iteracion: 18241 Gradiente: [2.4672684294084016e-05,-0.00042567039060183257] Loss: 22.84273690619911\n",
      "Iteracion: 18242 Gradiente: [2.4659567523599434e-05,-0.00042544409115781906] Loss: 22.84273690601736\n",
      "Iteracion: 18243 Gradiente: [2.4646457748644935e-05,-0.00042521791201970415] Loss: 22.842736905835807\n",
      "Iteracion: 18244 Gradiente: [2.4633354972062686e-05,-0.000424991853123539] Loss: 22.84273690565443\n",
      "Iteracion: 18245 Gradiente: [2.4620259141746224e-05,-0.00042476591440833525] Loss: 22.842736905473252\n",
      "Iteracion: 18246 Gradiente: [2.4607170252011203e-05,-0.0004245400958099073] Loss: 22.842736905292274\n",
      "Iteracion: 18247 Gradiente: [2.4594088270646352e-05,-0.0004243143972675038] Loss: 22.842736905111494\n",
      "Iteracion: 18248 Gradiente: [2.4581013283864196e-05,-0.0004240888187095967] Loss: 22.842736904930877\n",
      "Iteracion: 18249 Gradiente: [2.456794529829646e-05,-0.0004238633600742503] Loss: 22.84273690475047\n",
      "Iteracion: 18250 Gradiente: [2.455488431678532e-05,-0.000423638021297279] Loss: 22.842736904570263\n",
      "Iteracion: 18251 Gradiente: [2.4541830099641024e-05,-0.0004234128023264579] Loss: 22.84273690439023\n",
      "Iteracion: 18252 Gradiente: [2.452878284676293e-05,-0.00042318770308777214] Loss: 22.842736904210405\n",
      "Iteracion: 18253 Gradiente: [2.4515742689838287e-05,-0.0004229627235093384] Loss: 22.84273690403077\n",
      "Iteracion: 18254 Gradiente: [2.450270924517402e-05,-0.00042273786355018217] Loss: 22.842736903851307\n",
      "Iteracion: 18255 Gradiente: [2.448968294856968e-05,-0.00042251312312053814] Loss: 22.842736903672055\n",
      "Iteracion: 18256 Gradiente: [2.4476663486439066e-05,-0.00042228850217587894] Loss: 22.84273690349299\n",
      "Iteracion: 18257 Gradiente: [2.446365091846777e-05,-0.00042206400064597935] Loss: 22.842736903314105\n",
      "Iteracion: 18258 Gradiente: [2.4450645324236574e-05,-0.0004218396184661799] Loss: 22.842736903135417\n",
      "Iteracion: 18259 Gradiente: [2.443764662132253e-05,-0.00042161535557501867] Loss: 22.84273690295692\n",
      "Iteracion: 18260 Gradiente: [2.4424654839094728e-05,-0.00042139121190996794] Loss: 22.84273690277861\n",
      "Iteracion: 18261 Gradiente: [2.4411669953868417e-05,-0.000421167187406013] Loss: 22.842736902600475\n",
      "Iteracion: 18262 Gradiente: [2.439869190311583e-05,-0.00042094328200524463] Loss: 22.842736902422555\n",
      "Iteracion: 18263 Gradiente: [2.4385720799576423e-05,-0.000420719495635898] Loss: 22.842736902244813\n",
      "Iteracion: 18264 Gradiente: [2.4372756591143722e-05,-0.0004204958282393534] Loss: 22.84273690206726\n",
      "Iteracion: 18265 Gradiente: [2.4359799244659067e-05,-0.0004202722797521356] Loss: 22.8427369018899\n",
      "Iteracion: 18266 Gradiente: [2.4346848803702414e-05,-0.0004200488501097036] Loss: 22.842736901712723\n",
      "Iteracion: 18267 Gradiente: [2.4333905362065403e-05,-0.0004198255392425428] Loss: 22.842736901535734\n",
      "Iteracion: 18268 Gradiente: [2.4320968700900874e-05,-0.00041960234709961244] Loss: 22.842736901358933\n",
      "Iteracion: 18269 Gradiente: [2.4308038914000462e-05,-0.0004193792736124635] Loss: 22.84273690118232\n",
      "Iteracion: 18270 Gradiente: [2.429511602315415e-05,-0.0004191563187169104] Loss: 22.8427369010059\n",
      "Iteracion: 18271 Gradiente: [2.42822000321515e-05,-0.00041893348234995167] Loss: 22.842736900829667\n",
      "Iteracion: 18272 Gradiente: [2.4269290815936983e-05,-0.0004187107644552176] Loss: 22.842736900653623\n",
      "Iteracion: 18273 Gradiente: [2.4256388554514765e-05,-0.0004184881649588116] Loss: 22.842736900477757\n",
      "Iteracion: 18274 Gradiente: [2.4243493127566276e-05,-0.00041826568380341674] Loss: 22.842736900302082\n",
      "Iteracion: 18275 Gradiente: [2.4230604480142878e-05,-0.0004180433209315974] Loss: 22.842736900126607\n",
      "Iteracion: 18276 Gradiente: [2.4217722765721796e-05,-0.0004178210762692203] Loss: 22.8427368999513\n",
      "Iteracion: 18277 Gradiente: [2.4204847888616618e-05,-0.0004175989497601525] Loss: 22.84273689977618\n",
      "Iteracion: 18278 Gradiente: [2.4191979816616063e-05,-0.00041737694134281377] Loss: 22.842736899601256\n",
      "Iteracion: 18279 Gradiente: [2.417911869846042e-05,-0.00041715505094437334] Loss: 22.842736899426498\n",
      "Iteracion: 18280 Gradiente: [2.4166264300144272e-05,-0.000416933278517817] Loss: 22.84273689925196\n",
      "Iteracion: 18281 Gradiente: [2.415341674767054e-05,-0.0004167116239893668] Loss: 22.842736899077575\n",
      "Iteracion: 18282 Gradiente: [2.4140576007880556e-05,-0.000416490087302653] Loss: 22.842736898903397\n",
      "Iteracion: 18283 Gradiente: [2.412774216414467e-05,-0.00041626866838662124] Loss: 22.842736898729388\n",
      "Iteracion: 18284 Gradiente: [2.4114915048774796e-05,-0.0004160473671899941] Loss: 22.842736898555586\n",
      "Iteracion: 18285 Gradiente: [2.4102094874933756e-05,-0.00041582618363520394] Loss: 22.84273689838195\n",
      "Iteracion: 18286 Gradiente: [2.4089281575356834e-05,-0.00041560511766599955] Loss: 22.842736898208504\n",
      "Iteracion: 18287 Gradiente: [2.4076474912249068e-05,-0.0004153841692319323] Loss: 22.84273689803524\n",
      "Iteracion: 18288 Gradiente: [2.406367517361711e-05,-0.00041516333825377674] Loss: 22.842736897862164\n",
      "Iteracion: 18289 Gradiente: [2.4050882103665572e-05,-0.0004149426246849922] Loss: 22.842736897689274\n",
      "Iteracion: 18290 Gradiente: [2.4038095875766886e-05,-0.0004147220284508535] Loss: 22.842736897516563\n",
      "Iteracion: 18291 Gradiente: [2.4025316489921047e-05,-0.00041450154948918794] Loss: 22.84273689734403\n",
      "Iteracion: 18292 Gradiente: [2.4012543957496745e-05,-0.00041428118773948104] Loss: 22.842736897171687\n",
      "Iteracion: 18293 Gradiente: [2.3999778138280212e-05,-0.0004140609431457184] Loss: 22.842736896999533\n",
      "Iteracion: 18294 Gradiente: [2.398701911185223e-05,-0.00041384081563874037] Loss: 22.84273689682756\n",
      "Iteracion: 18295 Gradiente: [2.3974266940740564e-05,-0.0004136208051551904] Loss: 22.842736896655765\n",
      "Iteracion: 18296 Gradiente: [2.3961521389992412e-05,-0.00041340091164367246] Loss: 22.84273689648415\n",
      "Iteracion: 18297 Gradiente: [2.394878270971882e-05,-0.0004131811350300533] Loss: 22.842736896312715\n",
      "Iteracion: 18298 Gradiente: [2.393605083739203e-05,-0.0004129614752546473] Loss: 22.84273689614148\n",
      "Iteracion: 18299 Gradiente: [2.3923325645114347e-05,-0.0004127419322609664] Loss: 22.842736895970408\n",
      "Iteracion: 18300 Gradiente: [2.3910607357417272e-05,-0.00041252250597558773] Loss: 22.84273689579953\n",
      "Iteracion: 18301 Gradiente: [2.3897895774401454e-05,-0.0004123031963465233] Loss: 22.842736895628825\n",
      "Iteracion: 18302 Gradiente: [2.3885190829749568e-05,-0.00041208400331728493] Loss: 22.8427368954583\n",
      "Iteracion: 18303 Gradiente: [2.3872492684517964e-05,-0.0004118649268159894] Loss: 22.842736895287967\n",
      "Iteracion: 18304 Gradiente: [2.3859801357654455e-05,-0.0004116459667777406] Loss: 22.842736895117817\n",
      "Iteracion: 18305 Gradiente: [2.3847116746840887e-05,-0.0004114271231467607] Loss: 22.84273689494784\n",
      "Iteracion: 18306 Gradiente: [2.3834438915552408e-05,-0.0004112083958575615] Loss: 22.84273689477804\n",
      "Iteracion: 18307 Gradiente: [2.3821767827788185e-05,-0.0004109897848508126] Loss: 22.842736894608414\n",
      "Iteracion: 18308 Gradiente: [2.380910339638831e-05,-0.0004107712900686048] Loss: 22.842736894438985\n",
      "Iteracion: 18309 Gradiente: [2.3796445802304333e-05,-0.0004105529114383444] Loss: 22.842736894269724\n",
      "Iteracion: 18310 Gradiente: [2.378379491005944e-05,-0.0004103346489079248] Loss: 22.842736894100664\n",
      "Iteracion: 18311 Gradiente: [2.3771150662810214e-05,-0.0004101165024170683] Loss: 22.842736893931768\n",
      "Iteracion: 18312 Gradiente: [2.375851320929693e-05,-0.00040989847189483913] Loss: 22.842736893763053\n",
      "Iteracion: 18313 Gradiente: [2.374588251351876e-05,-0.0004096805572815517] Loss: 22.842736893594505\n",
      "Iteracion: 18314 Gradiente: [2.373325841820891e-05,-0.0004094627585250995] Loss: 22.842736893426153\n",
      "Iteracion: 18315 Gradiente: [2.372064104084378e-05,-0.00040924507555667824] Loss: 22.842736893257968\n",
      "Iteracion: 18316 Gradiente: [2.3708030460056763e-05,-0.0004090275083109181] Loss: 22.842736893089974\n",
      "Iteracion: 18317 Gradiente: [2.369542652331802e-05,-0.0004088100567340547] Loss: 22.842736892922154\n",
      "Iteracion: 18318 Gradiente: [2.368282937178871e-05,-0.0004085927207557442] Loss: 22.84273689275452\n",
      "Iteracion: 18319 Gradiente: [2.3670238742094323e-05,-0.00040837550033015664] Loss: 22.842736892587034\n",
      "Iteracion: 18320 Gradiente: [2.3657654985716665e-05,-0.0004081583953753428] Loss: 22.842736892419758\n",
      "Iteracion: 18321 Gradiente: [2.3645077928335922e-05,-0.00040794140583931456] Loss: 22.842736892252663\n",
      "Iteracion: 18322 Gradiente: [2.3632507429738324e-05,-0.00040772453166961024] Loss: 22.842736892085725\n",
      "Iteracion: 18323 Gradiente: [2.3619943561925538e-05,-0.0004075077727986098] Loss: 22.842736891918964\n",
      "Iteracion: 18324 Gradiente: [2.36073865361656e-05,-0.0004072911291547854] Loss: 22.842736891752388\n",
      "Iteracion: 18325 Gradiente: [2.359483610140008e-05,-0.00040707460068982] Loss: 22.84273689158599\n",
      "Iteracion: 18326 Gradiente: [2.358229238173711e-05,-0.0004068581873375147] Loss: 22.84273689141977\n",
      "Iteracion: 18327 Gradiente: [2.356975535159715e-05,-0.0004066418890332102] Loss: 22.842736891253722\n",
      "Iteracion: 18328 Gradiente: [2.355722496645285e-05,-0.00040642570572278677] Loss: 22.842736891087853\n",
      "Iteracion: 18329 Gradiente: [2.354470113914431e-05,-0.00040620963734726935] Loss: 22.842736890922165\n",
      "Iteracion: 18330 Gradiente: [2.3532184092308246e-05,-0.0004059936838339458] Loss: 22.84273689075665\n",
      "Iteracion: 18331 Gradiente: [2.351967360046577e-05,-0.0004057778451330781] Loss: 22.842736890591308\n",
      "Iteracion: 18332 Gradiente: [2.350716985498972e-05,-0.0004055621211733751] Loss: 22.84273689042613\n",
      "Iteracion: 18333 Gradiente: [2.3494672737456314e-05,-0.00040534651190071713] Loss: 22.842736890261147\n",
      "Iteracion: 18334 Gradiente: [2.3482182286708544e-05,-0.00040513101725068166] Loss: 22.842736890096337\n",
      "Iteracion: 18335 Gradiente: [2.3469698462956026e-05,-0.0004049156371655963] Loss: 22.842736889931697\n",
      "Iteracion: 18336 Gradiente: [2.3457221260514416e-05,-0.0004047003715836439] Loss: 22.84273688976723\n",
      "Iteracion: 18337 Gradiente: [2.3444750608329436e-05,-0.00040448522044869153] Loss: 22.84273688960295\n",
      "Iteracion: 18338 Gradiente: [2.3432286688300034e-05,-0.0004042701836883822] Loss: 22.842736889438825\n",
      "Iteracion: 18339 Gradiente: [2.341982931094814e-05,-0.0004040552612526227] Loss: 22.842736889274892\n",
      "Iteracion: 18340 Gradiente: [2.34073785994345e-05,-0.00040384045307367464] Loss: 22.84273688911112\n",
      "Iteracion: 18341 Gradiente: [2.3394934459967467e-05,-0.00040362575909599723] Loss: 22.84273688894753\n",
      "Iteracion: 18342 Gradiente: [2.3382497045076888e-05,-0.00040341117925161524] Loss: 22.842736888784113\n",
      "Iteracion: 18343 Gradiente: [2.3370066214548992e-05,-0.00040319671348332996] Loss: 22.84273688862087\n",
      "Iteracion: 18344 Gradiente: [2.3357642059333255e-05,-0.00040298236172979783] Loss: 22.84273688845779\n",
      "Iteracion: 18345 Gradiente: [2.3345224382372482e-05,-0.00040276812393867564] Loss: 22.842736888294912\n",
      "Iteracion: 18346 Gradiente: [2.33328133579865e-05,-0.0004025540000402117] Loss: 22.842736888132176\n",
      "Iteracion: 18347 Gradiente: [2.3320408892383664e-05,-0.00040233998997874686] Loss: 22.84273688796964\n",
      "Iteracion: 18348 Gradiente: [2.330801100545917e-05,-0.0004021260936924638] Loss: 22.842736887807252\n",
      "Iteracion: 18349 Gradiente: [2.329561973510863e-05,-0.00040191231111788757] Loss: 22.842736887645053\n",
      "Iteracion: 18350 Gradiente: [2.328323515522849e-05,-0.00040169864219237185] Loss: 22.842736887483014\n",
      "Iteracion: 18351 Gradiente: [2.3270857070656348e-05,-0.0004014850868638102] Loss: 22.842736887321156\n",
      "Iteracion: 18352 Gradiente: [2.3258485557183426e-05,-0.00040127164506849057] Loss: 22.842736887159468\n",
      "Iteracion: 18353 Gradiente: [2.3246120626178406e-05,-0.0004010583167451879] Loss: 22.842736886997937\n",
      "Iteracion: 18354 Gradiente: [2.323376228143085e-05,-0.0004008451018333877] Loss: 22.842736886836594\n",
      "Iteracion: 18355 Gradiente: [2.322141048504515e-05,-0.0004006320002749438] Loss: 22.84273688667542\n",
      "Iteracion: 18356 Gradiente: [2.320906523417913e-05,-0.0004004190120075653] Loss: 22.842736886514434\n",
      "Iteracion: 18357 Gradiente: [2.319672654683321e-05,-0.0004002061369722772] Loss: 22.84273688635359\n",
      "Iteracion: 18358 Gradiente: [2.3184394479850803e-05,-0.0003999933751048938] Loss: 22.84273688619294\n",
      "Iteracion: 18359 Gradiente: [2.3172068987757182e-05,-0.00039978072634549257] Loss: 22.842736886032444\n",
      "Iteracion: 18360 Gradiente: [2.3159750044972802e-05,-0.00039956819063888813] Loss: 22.84273688587213\n",
      "Iteracion: 18361 Gradiente: [2.314743752170519e-05,-0.0003993557679296581] Loss: 22.84273688571199\n",
      "Iteracion: 18362 Gradiente: [2.3135131633959342e-05,-0.0003991434581448535] Loss: 22.842736885551997\n",
      "Iteracion: 18363 Gradiente: [2.3122832332470958e-05,-0.0003989312612286966] Loss: 22.842736885392192\n",
      "Iteracion: 18364 Gradiente: [2.3110539565133574e-05,-0.0003987191771219756] Loss: 22.84273688523255\n",
      "Iteracion: 18365 Gradiente: [2.3098253253313792e-05,-0.0003985072057721103] Loss: 22.84273688507308\n",
      "Iteracion: 18366 Gradiente: [2.3085973473750225e-05,-0.0003982953471111254] Loss: 22.842736884913787\n",
      "Iteracion: 18367 Gradiente: [2.307370021317941e-05,-0.000398083601082296] Loss: 22.842736884754647\n",
      "Iteracion: 18368 Gradiente: [2.3061433578656456e-05,-0.00039787196761764676] Loss: 22.842736884595702\n",
      "Iteracion: 18369 Gradiente: [2.304917338638764e-05,-0.000397660446667795] Loss: 22.842736884436906\n",
      "Iteracion: 18370 Gradiente: [2.303691972542765e-05,-0.000397449038169384] Loss: 22.842736884278267\n",
      "Iteracion: 18371 Gradiente: [2.3024672655462077e-05,-0.0003972377420563333] Loss: 22.842736884119816\n",
      "Iteracion: 18372 Gradiente: [2.3012432069435818e-05,-0.0003970265582785496] Loss: 22.84273688396154\n",
      "Iteracion: 18373 Gradiente: [2.3000197934190207e-05,-0.0003968154867755184] Loss: 22.842736883803425\n",
      "Iteracion: 18374 Gradiente: [2.298797034920123e-05,-0.0003966045274804486] Loss: 22.84273688364548\n",
      "Iteracion: 18375 Gradiente: [2.29757491268856e-05,-0.000396393680346326] Loss: 22.84273688348768\n",
      "Iteracion: 18376 Gradiente: [2.296353457703996e-05,-0.0003961829452948725] Loss: 22.842736883330076\n",
      "Iteracion: 18377 Gradiente: [2.2951326470395847e-05,-0.00039597232227931063] Loss: 22.842736883172634\n",
      "Iteracion: 18378 Gradiente: [2.293912484295409e-05,-0.00039576181123841537] Loss: 22.842736883015355\n",
      "Iteracion: 18379 Gradiente: [2.292692971460989e-05,-0.00039555141211107997] Loss: 22.842736882858233\n",
      "Iteracion: 18380 Gradiente: [2.291474105315198e-05,-0.0003953411248393953] Loss: 22.84273688270128\n",
      "Iteracion: 18381 Gradiente: [2.2902558899318138e-05,-0.00039513094936130717] Loss: 22.84273688254451\n",
      "Iteracion: 18382 Gradiente: [2.2890383165948456e-05,-0.0003949208856206828] Loss: 22.842736882387896\n",
      "Iteracion: 18383 Gradiente: [2.2878213946834573e-05,-0.00039471093355617864] Loss: 22.84273688223146\n",
      "Iteracion: 18384 Gradiente: [2.2866051162395705e-05,-0.0003945010931091749] Loss: 22.842736882075187\n",
      "Iteracion: 18385 Gradiente: [2.2853894909265666e-05,-0.0003942913642161964] Loss: 22.84273688191906\n",
      "Iteracion: 18386 Gradiente: [2.284174503017766e-05,-0.00039408174682750523] Loss: 22.842736881763123\n",
      "Iteracion: 18387 Gradiente: [2.2829601679556314e-05,-0.00039387224087370496] Loss: 22.842736881607333\n",
      "Iteracion: 18388 Gradiente: [2.281746472760915e-05,-0.0003936628463022155] Loss: 22.842736881451728\n",
      "Iteracion: 18389 Gradiente: [2.280533424633783e-05,-0.00039345356305110123] Loss: 22.842736881296286\n",
      "Iteracion: 18390 Gradiente: [2.279321023858453e-05,-0.00039324439105890006] Loss: 22.842736881140993\n",
      "Iteracion: 18391 Gradiente: [2.2781092689191003e-05,-0.00039303533026876873] Loss: 22.842736880985875\n",
      "Iteracion: 18392 Gradiente: [2.2768981466469995e-05,-0.00039282638062706116] Loss: 22.842736880830916\n",
      "Iteracion: 18393 Gradiente: [2.2756876924745482e-05,-0.0003926175420578678] Loss: 22.842736880676124\n",
      "Iteracion: 18394 Gradiente: [2.2744778684113952e-05,-0.00039240881452068757] Loss: 22.8427368805215\n",
      "Iteracion: 18395 Gradiente: [2.273268685920963e-05,-0.00039220019794944014] Loss: 22.84273688036705\n",
      "Iteracion: 18396 Gradiente: [2.2720601413084297e-05,-0.0003919916922891768] Loss: 22.842736880212758\n",
      "Iteracion: 18397 Gradiente: [2.270852244521393e-05,-0.00039178329747263283] Loss: 22.84273688005861\n",
      "Iteracion: 18398 Gradiente: [2.2696449957493314e-05,-0.0003915750134423727] Loss: 22.84273687990464\n",
      "Iteracion: 18399 Gradiente: [2.26843837396018e-05,-0.0003913668401513822] Loss: 22.84273687975084\n",
      "Iteracion: 18400 Gradiente: [2.2672324028386963e-05,-0.00039115877752671224] Loss: 22.842736879597194\n",
      "Iteracion: 18401 Gradiente: [2.266027074332063e-05,-0.00039095082551258525] Loss: 22.842736879443724\n",
      "Iteracion: 18402 Gradiente: [2.2648223854086306e-05,-0.00039074298405310514] Loss: 22.842736879290403\n",
      "Iteracion: 18403 Gradiente: [2.263618330857753e-05,-0.0003905352530916654] Loss: 22.84273687913726\n",
      "Iteracion: 18404 Gradiente: [2.262414920816506e-05,-0.00039032763256360663] Loss: 22.84273687898426\n",
      "Iteracion: 18405 Gradiente: [2.2612121568007146e-05,-0.0003901201224095985] Loss: 22.842736878831428\n",
      "Iteracion: 18406 Gradiente: [2.2600100307575607e-05,-0.0003899127225767055] Loss: 22.84273687867877\n",
      "Iteracion: 18407 Gradiente: [2.258808543444957e-05,-0.0003897054330022816] Loss: 22.84273687852626\n",
      "Iteracion: 18408 Gradiente: [2.257607686715346e-05,-0.00038949825363457553] Loss: 22.842736878373934\n",
      "Iteracion: 18409 Gradiente: [2.256407476484886e-05,-0.00038929118440573043] Loss: 22.842736878221764\n",
      "Iteracion: 18410 Gradiente: [2.2552078958900287e-05,-0.0003890842252657715] Loss: 22.84273687806973\n",
      "Iteracion: 18411 Gradiente: [2.2540089619838e-05,-0.0003888773761477893] Loss: 22.84273687791789\n",
      "Iteracion: 18412 Gradiente: [2.252810660555345e-05,-0.0003886706369977825] Loss: 22.842736877766182\n",
      "Iteracion: 18413 Gradiente: [2.2516129898993616e-05,-0.0003884640077610394] Loss: 22.84273687761465\n",
      "Iteracion: 18414 Gradiente: [2.2504159667846578e-05,-0.00038825748836958477] Loss: 22.842736877463274\n",
      "Iteracion: 18415 Gradiente: [2.2492195770003796e-05,-0.00038805107877119366] Loss: 22.842736877312067\n",
      "Iteracion: 18416 Gradiente: [2.2480238308730803e-05,-0.0003878447789009698] Loss: 22.842736877161034\n",
      "Iteracion: 18417 Gradiente: [2.2468287072759572e-05,-0.0003876385887148596] Loss: 22.84273687701013\n",
      "Iteracion: 18418 Gradiente: [2.245634223167296e-05,-0.000387432508143822] Loss: 22.8427368768594\n",
      "Iteracion: 18419 Gradiente: [2.2444403687889764e-05,-0.00038722653713409253] Loss: 22.842736876708834\n",
      "Iteracion: 18420 Gradiente: [2.243247159393983e-05,-0.0003870206756194724] Loss: 22.842736876558423\n",
      "Iteracion: 18421 Gradiente: [2.2420545747081633e-05,-0.0003868149235519998] Loss: 22.842736876408186\n",
      "Iteracion: 18422 Gradiente: [2.2408626356688426e-05,-0.00038660928086239703] Loss: 22.84273687625809\n",
      "Iteracion: 18423 Gradiente: [2.239671330338903e-05,-0.0003864037474980838] Loss: 22.84273687610816\n",
      "Iteracion: 18424 Gradiente: [2.238480647823356e-05,-0.00038619832340707204] Loss: 22.84273687595839\n",
      "Iteracion: 18425 Gradiente: [2.2372906044173155e-05,-0.0003859930085239919] Loss: 22.842736875808782\n",
      "Iteracion: 18426 Gradiente: [2.2361011930153532e-05,-0.0003857878027928289] Loss: 22.84273687565933\n",
      "Iteracion: 18427 Gradiente: [2.2349124119121674e-05,-0.00038558270615496326] Loss: 22.84273687551005\n",
      "Iteracion: 18428 Gradiente: [2.233724260255106e-05,-0.00038537771855592] Loss: 22.842736875360906\n",
      "Iteracion: 18429 Gradiente: [2.2325367458127705e-05,-0.00038517283993032926] Loss: 22.842736875211934\n",
      "Iteracion: 18430 Gradiente: [2.231349858353345e-05,-0.00038496807022667665] Loss: 22.84273687506311\n",
      "Iteracion: 18431 Gradiente: [2.23016360270852e-05,-0.0003847634093851582] Loss: 22.84273687491446\n",
      "Iteracion: 18432 Gradiente: [2.2289779760361247e-05,-0.0003845588573485751] Loss: 22.84273687476595\n",
      "Iteracion: 18433 Gradiente: [2.2277929862942376e-05,-0.000384354414054755] Loss: 22.84273687461761\n",
      "Iteracion: 18434 Gradiente: [2.226608617566702e-05,-0.00038415007945431513] Loss: 22.842736874469427\n",
      "Iteracion: 18435 Gradiente: [2.2254248930645795e-05,-0.00038394585347501696] Loss: 22.842736874321407\n",
      "Iteracion: 18436 Gradiente: [2.224241788250462e-05,-0.00038374173607553055] Loss: 22.84273687417353\n",
      "Iteracion: 18437 Gradiente: [2.2230593068191713e-05,-0.0003835377271932098] Loss: 22.84273687402582\n",
      "Iteracion: 18438 Gradiente: [2.221877462034172e-05,-0.000383333826763869] Loss: 22.84273687387827\n",
      "Iteracion: 18439 Gradiente: [2.220696249158512e-05,-0.0003831300347320858] Loss: 22.84273687373086\n",
      "Iteracion: 18440 Gradiente: [2.2195156562550742e-05,-0.00038292635104705635] Loss: 22.84273687358361\n",
      "Iteracion: 18441 Gradiente: [2.2183356917556316e-05,-0.0003827227756469635] Loss: 22.84273687343652\n",
      "Iteracion: 18442 Gradiente: [2.2171563561338795e-05,-0.0003825193084712926] Loss: 22.842736873289596\n",
      "Iteracion: 18443 Gradiente: [2.2159776559268115e-05,-0.0003823159494598845] Loss: 22.842736873142826\n",
      "Iteracion: 18444 Gradiente: [2.214799577776224e-05,-0.0003821126985647775] Loss: 22.842736872996195\n",
      "Iteracion: 18445 Gradiente: [2.213622113724038e-05,-0.00038190955572995716] Loss: 22.842736872849756\n",
      "Iteracion: 18446 Gradiente: [2.212445288686619e-05,-0.00038170652088448755] Loss: 22.842736872703433\n",
      "Iteracion: 18447 Gradiente: [2.211269080968729e-05,-0.00038150359398419673] Loss: 22.84273687255729\n",
      "Iteracion: 18448 Gradiente: [2.2100935003284878e-05,-0.00038130077496513574] Loss: 22.8427368724113\n",
      "Iteracion: 18449 Gradiente: [2.2089185490396326e-05,-0.00038109806376809276] Loss: 22.84273687226546\n",
      "Iteracion: 18450 Gradiente: [2.2077442118491793e-05,-0.0003808954603438034] Loss: 22.842736872119765\n",
      "Iteracion: 18451 Gradiente: [2.2065705164209248e-05,-0.00038069296462050296] Loss: 22.84273687197424\n",
      "Iteracion: 18452 Gradiente: [2.205397431206772e-05,-0.00038049057655757207] Loss: 22.842736871828873\n",
      "Iteracion: 18453 Gradiente: [2.204224979512522e-05,-0.0003802882960853774] Loss: 22.842736871683638\n",
      "Iteracion: 18454 Gradiente: [2.2030531459904525e-05,-0.0003800861231539443] Loss: 22.842736871538573\n",
      "Iteracion: 18455 Gradiente: [2.2018819367986e-05,-0.0003798840577029949] Loss: 22.842736871393658\n",
      "Iteracion: 18456 Gradiente: [2.200711345589449e-05,-0.0003796820996794755] Loss: 22.842736871248903\n",
      "Iteracion: 18457 Gradiente: [2.1995413770999524e-05,-0.0003794802490225161] Loss: 22.842736871104304\n",
      "Iteracion: 18458 Gradiente: [2.1983720411829685e-05,-0.0003792785056694707] Loss: 22.84273687095985\n",
      "Iteracion: 18459 Gradiente: [2.1972033168064323e-05,-0.0003790768695755749] Loss: 22.842736870815553\n",
      "Iteracion: 18460 Gradiente: [2.1960352160022013e-05,-0.00037887534067545896] Loss: 22.8427368706714\n",
      "Iteracion: 18461 Gradiente: [2.1948677317595867e-05,-0.00037867391891855583] Loss: 22.842736870527418\n",
      "Iteracion: 18462 Gradiente: [2.193700874310404e-05,-0.0003784726042392587] Loss: 22.84273687038359\n",
      "Iteracion: 18463 Gradiente: [2.1925346410019604e-05,-0.0003782713965820269] Loss: 22.842736870239904\n",
      "Iteracion: 18464 Gradiente: [2.1913690175286624e-05,-0.00037807029589984607] Loss: 22.842736870096367\n",
      "Iteracion: 18465 Gradiente: [2.1902040165855398e-05,-0.00037786930212651746] Loss: 22.842736869952986\n",
      "Iteracion: 18466 Gradiente: [2.1890396347619874e-05,-0.00037766841520875025] Loss: 22.842736869809755\n",
      "Iteracion: 18467 Gradiente: [2.187875876605479e-05,-0.00037746763508520094] Loss: 22.842736869666684\n",
      "Iteracion: 18468 Gradiente: [2.1867127357685e-05,-0.0003772669617032894] Loss: 22.842736869523772\n",
      "Iteracion: 18469 Gradiente: [2.1855502105457465e-05,-0.00037706639500652744] Loss: 22.842736869380996\n",
      "Iteracion: 18470 Gradiente: [2.1843883114532522e-05,-0.00037686593493226896] Loss: 22.842736869238372\n",
      "Iteracion: 18471 Gradiente: [2.1832270202063835e-05,-0.00037666558143515755] Loss: 22.84273686909591\n",
      "Iteracion: 18472 Gradiente: [2.1820663518686464e-05,-0.00037646533444923115] Loss: 22.842736868953587\n",
      "Iteracion: 18473 Gradiente: [2.1809062960187476e-05,-0.0003762651939238045] Loss: 22.842736868811425\n",
      "Iteracion: 18474 Gradiente: [2.1797468538882942e-05,-0.0003760651598004946] Loss: 22.84273686866942\n",
      "Iteracion: 18475 Gradiente: [2.1785880343827556e-05,-0.00037586523201795787] Loss: 22.842736868527545\n",
      "Iteracion: 18476 Gradiente: [2.1774298304914433e-05,-0.00037566541052373265] Loss: 22.842736868385845\n",
      "Iteracion: 18477 Gradiente: [2.1762722390879692e-05,-0.0003754656952620413] Loss: 22.842736868244277\n",
      "Iteracion: 18478 Gradiente: [2.1751152747621442e-05,-0.0003752660861683429] Loss: 22.84273686810288\n",
      "Iteracion: 18479 Gradiente: [2.1739589228294184e-05,-0.00037506658319420204] Loss: 22.842736867961612\n",
      "Iteracion: 18480 Gradiente: [2.1728031795002303e-05,-0.0003748671862850254] Loss: 22.842736867820502\n",
      "Iteracion: 18481 Gradiente: [2.1716480413639754e-05,-0.00037466789538835127] Loss: 22.84273686767954\n",
      "Iteracion: 18482 Gradiente: [2.170493524431549e-05,-0.00037446871043584907] Loss: 22.842736867538722\n",
      "Iteracion: 18483 Gradiente: [2.1693396287029525e-05,-0.0003742696313725702] Loss: 22.842736867398063\n",
      "Iteracion: 18484 Gradiente: [2.1681863447042816e-05,-0.00037407065814628975] Loss: 22.842736867257553\n",
      "Iteracion: 18485 Gradiente: [2.1670336705407558e-05,-0.00037387179070170383] Loss: 22.842736867117196\n",
      "Iteracion: 18486 Gradiente: [2.165881607917678e-05,-0.0003736730289827979] Loss: 22.84273686697698\n",
      "Iteracion: 18487 Gradiente: [2.164730158445612e-05,-0.0003734743729307155] Loss: 22.842736866836916\n",
      "Iteracion: 18488 Gradiente: [2.1635793223140355e-05,-0.0003732758224896789] Loss: 22.84273686669701\n",
      "Iteracion: 18489 Gradiente: [2.1624290970597333e-05,-0.0003730773776042658] Loss: 22.84273686655723\n",
      "Iteracion: 18490 Gradiente: [2.161279475577279e-05,-0.00037287903822414613] Loss: 22.84273686641762\n",
      "Iteracion: 18491 Gradiente: [2.160130478425041e-05,-0.0003726808042796866] Loss: 22.842736866278162\n",
      "Iteracion: 18492 Gradiente: [2.1589820897816024e-05,-0.00037248267572351776] Loss: 22.842736866138832\n",
      "Iteracion: 18493 Gradiente: [2.1578343055731845e-05,-0.00037228465250128316] Loss: 22.84273686599967\n",
      "Iteracion: 18494 Gradiente: [2.1566871327157364e-05,-0.00037208673455317863] Loss: 22.842736865860623\n",
      "Iteracion: 18495 Gradiente: [2.155540568556565e-05,-0.0003718889218257952] Loss: 22.842736865721754\n",
      "Iteracion: 18496 Gradiente: [2.1543946206747933e-05,-0.00037169121425909186] Loss: 22.842736865583028\n",
      "Iteracion: 18497 Gradiente: [2.1532492731542636e-05,-0.0003714936118022649] Loss: 22.842736865444433\n",
      "Iteracion: 18498 Gradiente: [2.152104542763785e-05,-0.000371296114393734] Loss: 22.84273686530601\n",
      "Iteracion: 18499 Gradiente: [2.1509604171872827e-05,-0.00037109872198399786] Loss: 22.842736865167733\n",
      "Iteracion: 18500 Gradiente: [2.1498169049512703e-05,-0.00037090143450958145] Loss: 22.842736865029586\n",
      "Iteracion: 18501 Gradiente: [2.1486739946870632e-05,-0.00037070425192394413] Loss: 22.842736864891595\n",
      "Iteracion: 18502 Gradiente: [2.1475316894263113e-05,-0.00037050717416763724] Loss: 22.84273686475375\n",
      "Iteracion: 18503 Gradiente: [2.1463899960849632e-05,-0.00037031020118121204] Loss: 22.84273686461604\n",
      "Iteracion: 18504 Gradiente: [2.145248911820848e-05,-0.00037011333290971985] Loss: 22.842736864478493\n",
      "Iteracion: 18505 Gradiente: [2.1441084348339247e-05,-0.0003699165692999884] Loss: 22.842736864341088\n",
      "Iteracion: 18506 Gradiente: [2.1429685608609363e-05,-0.0003697199102968322] Loss: 22.842736864203815\n",
      "Iteracion: 18507 Gradiente: [2.1418292921756194e-05,-0.0003695233558451842] Loss: 22.842736864066712\n",
      "Iteracion: 18508 Gradiente: [2.140690626788455e-05,-0.00036932690588855623] Loss: 22.842736863929744\n",
      "Iteracion: 18509 Gradiente: [2.139552568678482e-05,-0.00036913056036998643] Loss: 22.842736863792904\n",
      "Iteracion: 18510 Gradiente: [2.1384151155719642e-05,-0.00036893431923452625] Loss: 22.84273686365624\n",
      "Iteracion: 18511 Gradiente: [2.137278272300591e-05,-0.0003687381824240295] Loss: 22.84273686351971\n",
      "Iteracion: 18512 Gradiente: [2.1361420329905438e-05,-0.00036854214988508717] Loss: 22.84273686338332\n",
      "Iteracion: 18513 Gradiente: [2.1350063885468747e-05,-0.00036834622156879014] Loss: 22.84273686324706\n",
      "Iteracion: 18514 Gradiente: [2.1338713527067436e-05,-0.00036815039741308435] Loss: 22.842736863110964\n",
      "Iteracion: 18515 Gradiente: [2.132736921491111e-05,-0.00036795467736065274] Loss: 22.842736862975013\n",
      "Iteracion: 18516 Gradiente: [2.131603093857848e-05,-0.0003677590613596256] Loss: 22.842736862839217\n",
      "Iteracion: 18517 Gradiente: [2.1304698660173926e-05,-0.00036756354935635707] Loss: 22.842736862703543\n",
      "Iteracion: 18518 Gradiente: [2.1293372443172607e-05,-0.00036736814129045093] Loss: 22.84273686256802\n",
      "Iteracion: 18519 Gradiente: [2.128205227241627e-05,-0.00036717283710861656] Loss: 22.842736862432652\n",
      "Iteracion: 18520 Gradiente: [2.127073805316589e-05,-0.0003669776367575632] Loss: 22.842736862297404\n",
      "Iteracion: 18521 Gradiente: [2.1259429900055693e-05,-0.0003667825401816316] Loss: 22.842736862162322\n",
      "Iteracion: 18522 Gradiente: [2.1248127706030572e-05,-0.00036658754732551795] Loss: 22.842736862027383\n",
      "Iteracion: 18523 Gradiente: [2.123683152414439e-05,-0.00036639265813415514] Loss: 22.84273686189257\n",
      "Iteracion: 18524 Gradiente: [2.1225541383766238e-05,-0.00036619787254998926] Loss: 22.842736861757917\n",
      "Iteracion: 18525 Gradiente: [2.1214257237526606e-05,-0.00036600319052055853] Loss: 22.8427368616234\n",
      "Iteracion: 18526 Gradiente: [2.120297907879376e-05,-0.00036580861199055905] Loss: 22.842736861489026\n",
      "Iteracion: 18527 Gradiente: [2.1191706944515925e-05,-0.0003656141369036211] Loss: 22.842736861354798\n",
      "Iteracion: 18528 Gradiente: [2.1180440827113976e-05,-0.00036541976520325646] Loss: 22.842736861220715\n",
      "Iteracion: 18529 Gradiente: [2.1169180681113175e-05,-0.0003652254968384246] Loss: 22.84273686108677\n",
      "Iteracion: 18530 Gradiente: [2.115792645535445e-05,-0.0003650313317564269] Loss: 22.84273686095297\n",
      "Iteracion: 18531 Gradiente: [2.1146678222786855e-05,-0.000364837269897104] Loss: 22.8427368608193\n",
      "Iteracion: 18532 Gradiente: [2.113543598909473e-05,-0.00036464331120621786] Loss: 22.842736860685786\n",
      "Iteracion: 18533 Gradiente: [2.1124199716382463e-05,-0.0003644494556314252] Loss: 22.842736860552407\n",
      "Iteracion: 18534 Gradiente: [2.111296947759911e-05,-0.00036425570311173774] Loss: 22.84273686041917\n",
      "Iteracion: 18535 Gradiente: [2.1101745199795612e-05,-0.00036406205359860164] Loss: 22.84273686028609\n",
      "Iteracion: 18536 Gradiente: [2.1090526831812895e-05,-0.0003638685070380158] Loss: 22.842736860153128\n",
      "Iteracion: 18537 Gradiente: [2.1079314461758258e-05,-0.0003636750633710051] Loss: 22.842736860020327\n",
      "Iteracion: 18538 Gradiente: [2.1068108032788284e-05,-0.0003634817225456999] Loss: 22.842736859887644\n",
      "Iteracion: 18539 Gradiente: [2.1056907623536366e-05,-0.0003632884845018225] Loss: 22.842736859755117\n",
      "Iteracion: 18540 Gradiente: [2.1045713168632574e-05,-0.0003630953491899902] Loss: 22.842736859622736\n",
      "Iteracion: 18541 Gradiente: [2.103452461123349e-05,-0.00036290231655821493] Loss: 22.842736859490497\n",
      "Iteracion: 18542 Gradiente: [2.102334198070821e-05,-0.0003627093865494165] Loss: 22.84273685935839\n",
      "Iteracion: 18543 Gradiente: [2.101216531684713e-05,-0.00036251655910698826] Loss: 22.84273685922642\n",
      "Iteracion: 18544 Gradiente: [2.1000994556175102e-05,-0.00036232383418036325] Loss: 22.8427368590946\n",
      "Iteracion: 18545 Gradiente: [2.098982989006496e-05,-0.00036213121170144784] Loss: 22.842736858962912\n",
      "Iteracion: 18546 Gradiente: [2.097867102387833e-05,-0.0003619386916349517] Loss: 22.842736858831376\n",
      "Iteracion: 18547 Gradiente: [2.096751813477719e-05,-0.00036174627391550494] Loss: 22.842736858699958\n",
      "Iteracion: 18548 Gradiente: [2.095637113181207e-05,-0.0003615539584941985] Loss: 22.8427368585687\n",
      "Iteracion: 18549 Gradiente: [2.0945230121090694e-05,-0.00036136174531051777] Loss: 22.842736858437586\n",
      "Iteracion: 18550 Gradiente: [2.0934094921661504e-05,-0.0003611696343189882] Loss: 22.842736858306594\n",
      "Iteracion: 18551 Gradiente: [2.092296571258127e-05,-0.00036097762545459486] Loss: 22.84273685817574\n",
      "Iteracion: 18552 Gradiente: [2.09118423858475e-05,-0.0003607857186716264] Loss: 22.84273685804504\n",
      "Iteracion: 18553 Gradiente: [2.090072500683012e-05,-0.00036059391390850235] Loss: 22.842736857914456\n",
      "Iteracion: 18554 Gradiente: [2.0889613607740407e-05,-0.0003604022111120505] Loss: 22.842736857784036\n",
      "Iteracion: 18555 Gradiente: [2.0878508028469393e-05,-0.0003602106102353749] Loss: 22.842736857653748\n",
      "Iteracion: 18556 Gradiente: [2.0867408374177406e-05,-0.0003600191112185532] Loss: 22.84273685752358\n",
      "Iteracion: 18557 Gradiente: [2.0856314587073637e-05,-0.00035982771400959733] Loss: 22.842736857393575\n",
      "Iteracion: 18558 Gradiente: [2.084522671642238e-05,-0.00035963641855332187] Loss: 22.842736857263706\n",
      "Iteracion: 18559 Gradiente: [2.0834144788750563e-05,-0.0003594452247919359] Loss: 22.84273685713396\n",
      "Iteracion: 18560 Gradiente: [2.0823068691318743e-05,-0.00035925413267795157] Loss: 22.842736857004358\n",
      "Iteracion: 18561 Gradiente: [2.081199858044632e-05,-0.00035906314215049896] Loss: 22.84273685687489\n",
      "Iteracion: 18562 Gradiente: [2.080093426760262e-05,-0.00035887225316303767] Loss: 22.84273685674557\n",
      "Iteracion: 18563 Gradiente: [2.0789875802051937e-05,-0.0003586814656607373] Loss: 22.84273685661639\n",
      "Iteracion: 18564 Gradiente: [2.0778823259585503e-05,-0.0003584907795852151] Loss: 22.84273685648734\n",
      "Iteracion: 18565 Gradiente: [2.0767776610834214e-05,-0.00035830019488175915] Loss: 22.842736856358403\n",
      "Iteracion: 18566 Gradiente: [2.0756735867166754e-05,-0.00035810971149814464] Loss: 22.84273685622965\n",
      "Iteracion: 18567 Gradiente: [2.0745700881737625e-05,-0.0003579193293874757] Loss: 22.84273685610101\n",
      "Iteracion: 18568 Gradiente: [2.073467184876184e-05,-0.00035772904848414556] Loss: 22.842736855972497\n",
      "Iteracion: 18569 Gradiente: [2.0723648726554227e-05,-0.0003575388687387715] Loss: 22.842736855844137\n",
      "Iteracion: 18570 Gradiente: [2.0712631366374503e-05,-0.00035734879010327345] Loss: 22.842736855715902\n",
      "Iteracion: 18571 Gradiente: [2.0701619884751684e-05,-0.0003571588125170185] Loss: 22.84273685558782\n",
      "Iteracion: 18572 Gradiente: [2.0690614260843176e-05,-0.00035696893593026855] Loss: 22.842736855459858\n",
      "Iteracion: 18573 Gradiente: [2.0679614465279883e-05,-0.00035677916028854876] Loss: 22.84273685533203\n",
      "Iteracion: 18574 Gradiente: [2.066862058237954e-05,-0.0003565894855332393] Loss: 22.84273685520435\n",
      "Iteracion: 18575 Gradiente: [2.065763251550834e-05,-0.0003563999116170891] Loss: 22.842736855076808\n",
      "Iteracion: 18576 Gradiente: [2.0646650257087154e-05,-0.00035621043848503103] Loss: 22.84273685494939\n",
      "Iteracion: 18577 Gradiente: [2.063567399564666e-05,-0.0003560210660754848] Loss: 22.84273685482211\n",
      "Iteracion: 18578 Gradiente: [2.0624703347493777e-05,-0.00035583179435339695] Loss: 22.84273685469497\n",
      "Iteracion: 18579 Gradiente: [2.061373865084685e-05,-0.0003556426232473579] Loss: 22.842736854567956\n",
      "Iteracion: 18580 Gradiente: [2.06027797077013e-05,-0.00035545355271544566] Loss: 22.842736854441082\n",
      "Iteracion: 18581 Gradiente: [2.0591826645007433e-05,-0.0003552645826959614] Loss: 22.842736854314342\n",
      "Iteracion: 18582 Gradiente: [2.0580879362341874e-05,-0.00035507571314118044] Loss: 22.842736854187745\n",
      "Iteracion: 18583 Gradiente: [2.0569937944022362e-05,-0.00035488694399331185] Loss: 22.84273685406128\n",
      "Iteracion: 18584 Gradiente: [2.0559002331310693e-05,-0.00035469827519977554] Loss: 22.84273685393496\n",
      "Iteracion: 18585 Gradiente: [2.0548072591471585e-05,-0.00035450970670597806] Loss: 22.84273685380874\n",
      "Iteracion: 18586 Gradiente: [2.0537148620292102e-05,-0.00035432123846372104] Loss: 22.842736853682673\n",
      "Iteracion: 18587 Gradiente: [2.052623041493007e-05,-0.00035413287041829257] Loss: 22.84273685355674\n",
      "Iteracion: 18588 Gradiente: [2.051531800191242e-05,-0.0003539446025165205] Loss: 22.842736853430957\n",
      "Iteracion: 18589 Gradiente: [2.0504411447556475e-05,-0.00035375643470025864] Loss: 22.842736853305286\n",
      "Iteracion: 18590 Gradiente: [2.0493510605964125e-05,-0.0003535683669255718] Loss: 22.842736853179744\n",
      "Iteracion: 18591 Gradiente: [2.0482615657139528e-05,-0.00035338039912637955] Loss: 22.84273685305435\n",
      "Iteracion: 18592 Gradiente: [2.0471726479816728e-05,-0.000353192531258865] Loss: 22.842736852929093\n",
      "Iteracion: 18593 Gradiente: [2.0460843104312212e-05,-0.00035300476326654006] Loss: 22.842736852803956\n",
      "Iteracion: 18594 Gradiente: [2.044996551357296e-05,-0.00035281709509860093] Loss: 22.842736852678968\n",
      "Iteracion: 18595 Gradiente: [2.0439093677282472e-05,-0.0003526295267012832] Loss: 22.842736852554093\n",
      "Iteracion: 18596 Gradiente: [2.042822761154639e-05,-0.00035244205802141456] Loss: 22.842736852429365\n",
      "Iteracion: 18597 Gradiente: [2.0417367374155522e-05,-0.0003522546890035727] Loss: 22.842736852304764\n",
      "Iteracion: 18598 Gradiente: [2.0406512862791715e-05,-0.00035206741959920387] Loss: 22.842736852180295\n",
      "Iteracion: 18599 Gradiente: [2.0395664105876676e-05,-0.00035188024975478055] Loss: 22.842736852055967\n",
      "Iteracion: 18600 Gradiente: [2.0384821158359046e-05,-0.00035169317941168287] Loss: 22.84273685193176\n",
      "Iteracion: 18601 Gradiente: [2.0373983959605844e-05,-0.0003515062085226598] Loss: 22.842736851807686\n",
      "Iteracion: 18602 Gradiente: [2.0363152506774897e-05,-0.000351319337033947] Loss: 22.842736851683743\n",
      "Iteracion: 18603 Gradiente: [2.035232683776182e-05,-0.0003511325648905957] Loss: 22.842736851559945\n",
      "Iteracion: 18604 Gradiente: [2.0341506879617554e-05,-0.0003509458920432233] Loss: 22.842736851436268\n",
      "Iteracion: 18605 Gradiente: [2.0330692722344188e-05,-0.00035075931843320994] Loss: 22.842736851312722\n",
      "Iteracion: 18606 Gradiente: [2.031988429962439e-05,-0.00035057284401389665] Loss: 22.842736851189322\n",
      "Iteracion: 18607 Gradiente: [2.0309081721355447e-05,-0.00035038646872394] Loss: 22.842736851066032\n",
      "Iteracion: 18608 Gradiente: [2.029828472416284e-05,-0.00035020019252603635] Loss: 22.842736850942877\n",
      "Iteracion: 18609 Gradiente: [2.028749349468247e-05,-0.0003500140153563554] Loss: 22.842736850819865\n",
      "Iteracion: 18610 Gradiente: [2.0276708102073827e-05,-0.0003498279371581721] Loss: 22.842736850696973\n",
      "Iteracion: 18611 Gradiente: [2.026592839854402e-05,-0.00034964195788790656] Loss: 22.842736850574216\n",
      "Iteracion: 18612 Gradiente: [2.0255154349987e-05,-0.0003494560774940444] Loss: 22.842736850451583\n",
      "Iteracion: 18613 Gradiente: [2.0244386071036995e-05,-0.0003492702959173736] Loss: 22.842736850329093\n",
      "Iteracion: 18614 Gradiente: [2.023362352758795e-05,-0.0003490846131075642] Loss: 22.842736850206734\n",
      "Iteracion: 18615 Gradiente: [2.022286677648329e-05,-0.0003488990290087202] Loss: 22.842736850084496\n",
      "Iteracion: 18616 Gradiente: [2.0212115701194003e-05,-0.0003487135435738272] Loss: 22.84273684996237\n",
      "Iteracion: 18617 Gradiente: [2.0201370275193162e-05,-0.0003485281567525552] Loss: 22.842736849840403\n",
      "Iteracion: 18618 Gradiente: [2.0190630592272404e-05,-0.00034834286848723175] Loss: 22.842736849718552\n",
      "Iteracion: 18619 Gradiente: [2.0179896702643417e-05,-0.0003481576787219609] Loss: 22.84273684959683\n",
      "Iteracion: 18620 Gradiente: [2.0169168446197244e-05,-0.00034797258741316265] Loss: 22.842736849475237\n",
      "Iteracion: 18621 Gradiente: [2.0158445965042424e-05,-0.00034778759450091454] Loss: 22.842736849353788\n",
      "Iteracion: 18622 Gradiente: [2.0147729037489627e-05,-0.0003476026999451894] Loss: 22.842736849232452\n",
      "Iteracion: 18623 Gradiente: [2.0137017895649478e-05,-0.00034741790368026195] Loss: 22.84273684911126\n",
      "Iteracion: 18624 Gradiente: [2.0126312462783366e-05,-0.000347233205656039] Loss: 22.84273684899018\n",
      "Iteracion: 18625 Gradiente: [2.011561269152177e-05,-0.0003470486058266905] Loss: 22.842736848869244\n",
      "Iteracion: 18626 Gradiente: [2.0104918559127326e-05,-0.00034686410413857044] Loss: 22.842736848748427\n",
      "Iteracion: 18627 Gradiente: [2.0094230160339065e-05,-0.00034667970053483545] Loss: 22.842736848627734\n",
      "Iteracion: 18628 Gradiente: [2.008354746863006e-05,-0.00034649539496385274] Loss: 22.84273684850718\n",
      "Iteracion: 18629 Gradiente: [2.0072870386419104e-05,-0.00034631118738014756] Loss: 22.842736848386753\n",
      "Iteracion: 18630 Gradiente: [2.00621990406565e-05,-0.0003461270777220212] Loss: 22.842736848266448\n",
      "Iteracion: 18631 Gradiente: [2.0051533366919707e-05,-0.00034594306594305145] Loss: 22.842736848146266\n",
      "Iteracion: 18632 Gradiente: [2.0040873392683047e-05,-0.0003457591519879344] Loss: 22.842736848026217\n",
      "Iteracion: 18633 Gradiente: [2.003021908194569e-05,-0.00034557533580716895] Loss: 22.8427368479063\n",
      "Iteracion: 18634 Gradiente: [2.001957037691682e-05,-0.0003453916173537408] Loss: 22.842736847786515\n",
      "Iteracion: 18635 Gradiente: [2.0008927335387247e-05,-0.0003452079965697408] Loss: 22.842736847666842\n",
      "Iteracion: 18636 Gradiente: [1.9998289977252172e-05,-0.0003450244734017597] Loss: 22.84273684754732\n",
      "Iteracion: 18637 Gradiente: [1.9987658284511174e-05,-0.0003448410477998228] Loss: 22.8427368474279\n",
      "Iteracion: 18638 Gradiente: [1.997703226000643e-05,-0.0003446577197130078] Loss: 22.842736847308625\n",
      "Iteracion: 18639 Gradiente: [1.9966411846894518e-05,-0.0003444744890913398] Loss: 22.842736847189475\n",
      "Iteracion: 18640 Gradiente: [1.9955797099176684e-05,-0.0003442913558776202] Loss: 22.842736847070423\n",
      "Iteracion: 18641 Gradiente: [1.9945188015905538e-05,-0.0003441083200234137] Loss: 22.842736846951524\n",
      "Iteracion: 18642 Gradiente: [1.9934584546869397e-05,-0.00034392538147756115] Loss: 22.842736846832754\n",
      "Iteracion: 18643 Gradiente: [1.992398663806701e-05,-0.000343742540192693] Loss: 22.842736846714104\n",
      "Iteracion: 18644 Gradiente: [1.9913394420238244e-05,-0.00034355979610805796] Loss: 22.842736846595585\n",
      "Iteracion: 18645 Gradiente: [1.9902807811907525e-05,-0.0003433771491771154] Loss: 22.842736846477184\n",
      "Iteracion: 18646 Gradiente: [1.989222684623352e-05,-0.0003431945993456272] Loss: 22.842736846358896\n",
      "Iteracion: 18647 Gradiente: [1.9881651636903066e-05,-0.00034301214655639475] Loss: 22.842736846240758\n",
      "Iteracion: 18648 Gradiente: [1.987108193664729e-05,-0.000342829790770575] Loss: 22.84273684612274\n",
      "Iteracion: 18649 Gradiente: [1.9860517845889565e-05,-0.0003426475319317982] Loss: 22.842736846004847\n",
      "Iteracion: 18650 Gradiente: [1.984995935705077e-05,-0.000342465369987958] Loss: 22.842736845887075\n",
      "Iteracion: 18651 Gradiente: [1.9839406478657415e-05,-0.0003422833048869478] Loss: 22.84273684576943\n",
      "Iteracion: 18652 Gradiente: [1.9828859228709918e-05,-0.0003421013365760691] Loss: 22.842736845651906\n",
      "Iteracion: 18653 Gradiente: [1.9818317604366105e-05,-0.0003419194650049917] Loss: 22.84273684553451\n",
      "Iteracion: 18654 Gradiente: [1.9807781592362515e-05,-0.0003417376901216092] Loss: 22.84273684541724\n",
      "Iteracion: 18655 Gradiente: [1.9797251184172637e-05,-0.00034155601187417045] Loss: 22.84273684530009\n",
      "Iteracion: 18656 Gradiente: [1.9786726361796052e-05,-0.0003413744302147137] Loss: 22.842736845183076\n",
      "Iteracion: 18657 Gradiente: [1.9776207177339227e-05,-0.00034119294508509296] Loss: 22.842736845066167\n",
      "Iteracion: 18658 Gradiente: [1.976569359574872e-05,-0.0003410115564393597] Loss: 22.8427368449494\n",
      "Iteracion: 18659 Gradiente: [1.9755185558286333e-05,-0.00034083026422824977] Loss: 22.84273684483276\n",
      "Iteracion: 18660 Gradiente: [1.9744683070636406e-05,-0.0003406490683996566] Loss: 22.84273684471623\n",
      "Iteracion: 18661 Gradiente: [1.973418617732629e-05,-0.0003404679688994605] Loss: 22.842736844599838\n",
      "Iteracion: 18662 Gradiente: [1.9723694772248262e-05,-0.00034028696568325263] Loss: 22.84273684448355\n",
      "Iteracion: 18663 Gradiente: [1.971320914814593e-05,-0.00034010605868139976] Loss: 22.842736844367412\n",
      "Iteracion: 18664 Gradiente: [1.9702728956379663e-05,-0.00033992524786405907] Loss: 22.84273684425137\n",
      "Iteracion: 18665 Gradiente: [1.9692254369374496e-05,-0.0003397445331695318] Loss: 22.842736844135466\n",
      "Iteracion: 18666 Gradiente: [1.9681785406078234e-05,-0.00033956391454452727] Loss: 22.842736844019672\n",
      "Iteracion: 18667 Gradiente: [1.967132197933097e-05,-0.00033938339194392595] Loss: 22.842736843904028\n",
      "Iteracion: 18668 Gradiente: [1.9660864159239583e-05,-0.0003392029653112398] Loss: 22.842736843788487\n",
      "Iteracion: 18669 Gradiente: [1.965041170990389e-05,-0.0003390226346112968] Loss: 22.84273684367307\n",
      "Iteracion: 18670 Gradiente: [1.9639964939225744e-05,-0.00033884239977292435] Loss: 22.842736843557784\n",
      "Iteracion: 18671 Gradiente: [1.962952372783396e-05,-0.00033866226075313457] Loss: 22.842736843442605\n",
      "Iteracion: 18672 Gradiente: [1.961908807193898e-05,-0.00033848221750029477] Loss: 22.842736843327568\n",
      "Iteracion: 18673 Gradiente: [1.9608657949750827e-05,-0.0003383022699639563] Loss: 22.842736843212638\n",
      "Iteracion: 18674 Gradiente: [1.959823336032211e-05,-0.00033812241809509186] Loss: 22.842736843097832\n",
      "Iteracion: 18675 Gradiente: [1.9587814444813983e-05,-0.0003379426618314104] Loss: 22.842736842983154\n",
      "Iteracion: 18676 Gradiente: [1.9577400922798916e-05,-0.0003377630011418849] Loss: 22.842736842868597\n",
      "Iteracion: 18677 Gradiente: [1.95669929515437e-05,-0.00033758343596422454] Loss: 22.842736842754167\n",
      "Iteracion: 18678 Gradiente: [1.9556590577470463e-05,-0.000337403966244428] Loss: 22.842736842639844\n",
      "Iteracion: 18679 Gradiente: [1.9546193643312413e-05,-0.0003372245919437707] Loss: 22.842736842525646\n",
      "Iteracion: 18680 Gradiente: [1.9535802341389778e-05,-0.00033704531299581694] Loss: 22.842736842411572\n",
      "Iteracion: 18681 Gradiente: [1.952541648317189e-05,-0.0003368661293630263] Loss: 22.84273684229762\n",
      "Iteracion: 18682 Gradiente: [1.9515036259084204e-05,-0.00033668704098393696] Loss: 22.842736842183797\n",
      "Iteracion: 18683 Gradiente: [1.9504661428489573e-05,-0.0003365080478207716] Loss: 22.842736842070096\n",
      "Iteracion: 18684 Gradiente: [1.94942921581287e-05,-0.000336329149813371] Loss: 22.842736841956505\n",
      "Iteracion: 18685 Gradiente: [1.9483928317261718e-05,-0.00033615034691827363] Loss: 22.84273684184304\n",
      "Iteracion: 18686 Gradiente: [1.9473570117156668e-05,-0.000335971639072478] Loss: 22.842736841729682\n",
      "Iteracion: 18687 Gradiente: [1.9463217355072024e-05,-0.00033579302623702273] Loss: 22.84273684161645\n",
      "Iteracion: 18688 Gradiente: [1.9452870169326768e-05,-0.00033561450835470905] Loss: 22.842736841503356\n",
      "Iteracion: 18689 Gradiente: [1.9442528363811105e-05,-0.00033543608538349665] Loss: 22.842736841390376\n",
      "Iteracion: 18690 Gradiente: [1.9432192184846524e-05,-0.00033525775726050234] Loss: 22.842736841277503\n",
      "Iteracion: 18691 Gradiente: [1.942186146190276e-05,-0.0003350795239445148] Loss: 22.842736841164765\n",
      "Iteracion: 18692 Gradiente: [1.941153615234725e-05,-0.000334901385387217] Loss: 22.842736841052133\n",
      "Iteracion: 18693 Gradiente: [1.940121631302342e-05,-0.00033472334153567354] Loss: 22.84273684093962\n",
      "Iteracion: 18694 Gradiente: [1.9390902048144197e-05,-0.0003345453923318568] Loss: 22.842736840827243\n",
      "Iteracion: 18695 Gradiente: [1.9380593267707507e-05,-0.00033436753773135783] Loss: 22.842736840714966\n",
      "Iteracion: 18696 Gradiente: [1.937028993097556e-05,-0.0003341897776863334] Loss: 22.842736840602832\n",
      "Iteracion: 18697 Gradiente: [1.9359992006684478e-05,-0.00033401211214609814] Loss: 22.842736840490794\n",
      "Iteracion: 18698 Gradiente: [1.9349699682417545e-05,-0.0003338345410528613] Loss: 22.842736840378887\n",
      "Iteracion: 18699 Gradiente: [1.93394127980658e-05,-0.000333657064363635] Loss: 22.842736840267094\n",
      "Iteracion: 18700 Gradiente: [1.9329131432262633e-05,-0.00033347968202311525] Loss: 22.84273684015542\n",
      "Iteracion: 18701 Gradiente: [1.9318855427741256e-05,-0.0003333023939902091] Loss: 22.84273684004388\n",
      "Iteracion: 18702 Gradiente: [1.930858497303234e-05,-0.00033312520020558625] Loss: 22.842736839932442\n",
      "Iteracion: 18703 Gradiente: [1.9298319999923783e-05,-0.00033294810061986385] Loss: 22.842736839821125\n",
      "Iteracion: 18704 Gradiente: [1.9288060329358814e-05,-0.00033277109519514646] Loss: 22.84273683970993\n",
      "Iteracion: 18705 Gradiente: [1.9277806252186262e-05,-0.0003325941838635771] Loss: 22.842736839598857\n",
      "Iteracion: 18706 Gradiente: [1.9267557542927232e-05,-0.0003324173665882076] Loss: 22.842736839487888\n",
      "Iteracion: 18707 Gradiente: [1.9257314299162924e-05,-0.0003322406433143262] Loss: 22.84273683937704\n",
      "Iteracion: 18708 Gradiente: [1.9247076503840315e-05,-0.00033206401399065534] Loss: 22.842736839266312\n",
      "Iteracion: 18709 Gradiente: [1.9236844141801158e-05,-0.0003318874785705361] Loss: 22.842736839155705\n",
      "Iteracion: 18710 Gradiente: [1.922661729357363e-05,-0.0003317110369961777] Loss: 22.842736839045223\n",
      "Iteracion: 18711 Gradiente: [1.9216395889050848e-05,-0.00033153468922388166] Loss: 22.842736838934847\n",
      "Iteracion: 18712 Gradiente: [1.9206179878968517e-05,-0.00033135843520533116] Loss: 22.84273683882458\n",
      "Iteracion: 18713 Gradiente: [1.919596921500973e-05,-0.0003311822748939856] Loss: 22.842736838714472\n",
      "Iteracion: 18714 Gradiente: [1.918576403170391e-05,-0.00033100620823240946] Loss: 22.84273683860444\n",
      "Iteracion: 18715 Gradiente: [1.9175564311050645e-05,-0.00033083023517027263] Loss: 22.84273683849453\n",
      "Iteracion: 18716 Gradiente: [1.9165369993364343e-05,-0.0003306543556630478] Loss: 22.842736838384745\n",
      "Iteracion: 18717 Gradiente: [1.915518107769761e-05,-0.0003304785696587468] Loss: 22.84273683827507\n",
      "Iteracion: 18718 Gradiente: [1.914499764836819e-05,-0.0003303028771046712] Loss: 22.842736838165525\n",
      "Iteracion: 18719 Gradiente: [1.9134819573688826e-05,-0.0003301272779578331] Loss: 22.842736838056087\n",
      "Iteracion: 18720 Gradiente: [1.9124646942714208e-05,-0.00032995177216174436] Loss: 22.842736837946763\n",
      "Iteracion: 18721 Gradiente: [1.9114479657863135e-05,-0.0003297763596738908] Loss: 22.842736837837556\n",
      "Iteracion: 18722 Gradiente: [1.9104317843243734e-05,-0.0003296010404370738] Loss: 22.842736837728467\n",
      "Iteracion: 18723 Gradiente: [1.9094161338747047e-05,-0.0003294258144109108] Loss: 22.842736837619505\n",
      "Iteracion: 18724 Gradiente: [1.908401032248245e-05,-0.00032925068153441355] Loss: 22.842736837510635\n",
      "Iteracion: 18725 Gradiente: [1.907386464855184e-05,-0.00032907564176731796] Loss: 22.842736837401908\n",
      "Iteracion: 18726 Gradiente: [1.906372436432472e-05,-0.00032890069505728075] Loss: 22.842736837293266\n",
      "Iteracion: 18727 Gradiente: [1.9053589528539306e-05,-0.0003287258413506559] Loss: 22.842736837184773\n",
      "Iteracion: 18728 Gradiente: [1.904346003035092e-05,-0.0003285510806050477] Loss: 22.84273683707637\n",
      "Iteracion: 18729 Gradiente: [1.9033336007131158e-05,-0.00032837641276112586] Loss: 22.84273683696809\n",
      "Iteracion: 18730 Gradiente: [1.9023217271296743e-05,-0.0003282018377833632] Loss: 22.842736836859924\n",
      "Iteracion: 18731 Gradiente: [1.9013103967798393e-05,-0.0003280273556110084] Loss: 22.842736836751865\n",
      "Iteracion: 18732 Gradiente: [1.9002995988633604e-05,-0.0003278529662022578] Loss: 22.84273683664394\n",
      "Iteracion: 18733 Gradiente: [1.899289344085749e-05,-0.0003276786694997943] Loss: 22.84273683653612\n",
      "Iteracion: 18734 Gradiente: [1.898279623446797e-05,-0.00032750446546086684] Loss: 22.842736836428415\n",
      "Iteracion: 18735 Gradiente: [1.8972704394097186e-05,-0.0003273303540337243] Loss: 22.842736836320817\n",
      "Iteracion: 18736 Gradiente: [1.8962617969009443e-05,-0.00032715633516744447] Loss: 22.84273683621334\n",
      "Iteracion: 18737 Gradiente: [1.8952536897624365e-05,-0.0003269824088151315] Loss: 22.84273683610598\n",
      "Iteracion: 18738 Gradiente: [1.8942461100361167e-05,-0.000326808574932258] Loss: 22.84273683599873\n",
      "Iteracion: 18739 Gradiente: [1.893239069374886e-05,-0.00032663483346352017] Loss: 22.842736835891603\n",
      "Iteracion: 18740 Gradiente: [1.892232560199621e-05,-0.0003264611843632063] Loss: 22.842736835784585\n",
      "Iteracion: 18741 Gradiente: [1.8912265869630573e-05,-0.0003262876275794468] Loss: 22.842736835677677\n",
      "Iteracion: 18742 Gradiente: [1.8902211540231898e-05,-0.00032611416305966165] Loss: 22.842736835570886\n",
      "Iteracion: 18743 Gradiente: [1.889216256169372e-05,-0.00032594079075979704] Loss: 22.84273683546421\n",
      "Iteracion: 18744 Gradiente: [1.8882118949174278e-05,-0.00032576751062833863] Loss: 22.842736835357638\n",
      "Iteracion: 18745 Gradiente: [1.8872080629724527e-05,-0.0003255943226206407] Loss: 22.84273683525118\n",
      "Iteracion: 18746 Gradiente: [1.886204765545093e-05,-0.0003254212266842416] Loss: 22.84273683514485\n",
      "Iteracion: 18747 Gradiente: [1.885201998656309e-05,-0.00032524822277366636] Loss: 22.84273683503862\n",
      "Iteracion: 18748 Gradiente: [1.8841997727273942e-05,-0.0003250753108313612] Loss: 22.842736834932506\n",
      "Iteracion: 18749 Gradiente: [1.8831980708000628e-05,-0.0003249024908197858] Loss: 22.84273683482651\n",
      "Iteracion: 18750 Gradiente: [1.8821969068009518e-05,-0.0003247297626817414] Loss: 22.84273683472061\n",
      "Iteracion: 18751 Gradiente: [1.881196271445636e-05,-0.0003245571263737664] Loss: 22.84273683461483\n",
      "Iteracion: 18752 Gradiente: [1.8801961747764532e-05,-0.00032438458184043856] Loss: 22.842736834509182\n",
      "Iteracion: 18753 Gradiente: [1.879196598982465e-05,-0.0003242121290429149] Loss: 22.842736834403624\n",
      "Iteracion: 18754 Gradiente: [1.8781975605482633e-05,-0.00032403976792293084] Loss: 22.84273683429819\n",
      "Iteracion: 18755 Gradiente: [1.877199057105372e-05,-0.0003238674984356038] Loss: 22.842736834192863\n",
      "Iteracion: 18756 Gradiente: [1.8762010763377173e-05,-0.0003236953205341564] Loss: 22.842736834087646\n",
      "Iteracion: 18757 Gradiente: [1.8752036292350265e-05,-0.00032352323416612685] Loss: 22.842736833982542\n",
      "Iteracion: 18758 Gradiente: [1.874206710112958e-05,-0.0003233512392858036] Loss: 22.84273683387755\n",
      "Iteracion: 18759 Gradiente: [1.873210332329715e-05,-0.0003231793358372907] Loss: 22.842736833772666\n",
      "Iteracion: 18760 Gradiente: [1.8722144727689737e-05,-0.00032300752378494244] Loss: 22.842736833667903\n",
      "Iteracion: 18761 Gradiente: [1.8712191426099405e-05,-0.0003228358030725076] Loss: 22.842736833563258\n",
      "Iteracion: 18762 Gradiente: [1.8702243475369567e-05,-0.0003226641736494192] Loss: 22.84273683345871\n",
      "Iteracion: 18763 Gradiente: [1.8692300794024657e-05,-0.0003224926354700841] Loss: 22.842736833354266\n",
      "Iteracion: 18764 Gradiente: [1.868236339153858e-05,-0.00032232118848642225] Loss: 22.84273683324994\n",
      "Iteracion: 18765 Gradiente: [1.867243126317438e-05,-0.00032214983264940617] Loss: 22.842736833145725\n",
      "Iteracion: 18766 Gradiente: [1.8662504456301576e-05,-0.0003219785679088242] Loss: 22.842736833041638\n",
      "Iteracion: 18767 Gradiente: [1.8652582910287188e-05,-0.00032180739421789896] Loss: 22.842736832937653\n",
      "Iteracion: 18768 Gradiente: [1.8642666684816808e-05,-0.0003216363115247608] Loss: 22.842736832833754\n",
      "Iteracion: 18769 Gradiente: [1.8632755616939296e-05,-0.0003214653197922246] Loss: 22.842736832729987\n",
      "Iteracion: 18770 Gradiente: [1.8622849918870087e-05,-0.0003212944189579995] Loss: 22.84273683262633\n",
      "Iteracion: 18771 Gradiente: [1.861294937934114e-05,-0.0003211236089863216] Loss: 22.84273683252279\n",
      "Iteracion: 18772 Gradiente: [1.860305416319837e-05,-0.0003209528898189262] Loss: 22.842736832419344\n",
      "Iteracion: 18773 Gradiente: [1.8593164170018403e-05,-0.0003207822614138915] Loss: 22.84273683231601\n",
      "Iteracion: 18774 Gradiente: [1.8583279511593295e-05,-0.00032061172371508444] Loss: 22.84273683221279\n",
      "Iteracion: 18775 Gradiente: [1.8573400029708865e-05,-0.0003204412766841358] Loss: 22.842736832109683\n",
      "Iteracion: 18776 Gradiente: [1.856352590342188e-05,-0.00032027092026136] Loss: 22.84273683200669\n",
      "Iteracion: 18777 Gradiente: [1.8553657003887262e-05,-0.00032010065440672973] Loss: 22.842736831903782\n",
      "Iteracion: 18778 Gradiente: [1.8543793211733826e-05,-0.0003199304790798626] Loss: 22.842736831801016\n",
      "Iteracion: 18779 Gradiente: [1.8533934781809572e-05,-0.00031976039421598065] Loss: 22.842736831698346\n",
      "Iteracion: 18780 Gradiente: [1.852408155211075e-05,-0.00031959039977695153] Loss: 22.84273683159578\n",
      "Iteracion: 18781 Gradiente: [1.8514233572849056e-05,-0.0003194204957105503] Loss: 22.842736831493323\n",
      "Iteracion: 18782 Gradiente: [1.8504390918868315e-05,-0.00031925068196644684] Loss: 22.842736831390983\n",
      "Iteracion: 18783 Gradiente: [1.8494553419638275e-05,-0.00031908095850425867] Loss: 22.842736831288743\n",
      "Iteracion: 18784 Gradiente: [1.8484721105475426e-05,-0.00031891132527412936] Loss: 22.84273683118662\n",
      "Iteracion: 18785 Gradiente: [1.8474894069224017e-05,-0.0003187417822241893] Loss: 22.8427368310846\n",
      "Iteracion: 18786 Gradiente: [1.846507222277675e-05,-0.00031857232931062167] Loss: 22.842736830982684\n",
      "Iteracion: 18787 Gradiente: [1.845525567034656e-05,-0.00031840296647954366] Loss: 22.84273683088088\n",
      "Iteracion: 18788 Gradiente: [1.8445444258456215e-05,-0.00031823369369104646] Loss: 22.842736830779195\n",
      "Iteracion: 18789 Gradiente: [1.8435638045843918e-05,-0.0003180645108933788] Loss: 22.842736830677595\n",
      "Iteracion: 18790 Gradiente: [1.8425837108300887e-05,-0.0003178954180356186] Loss: 22.84273683057612\n",
      "Iteracion: 18791 Gradiente: [1.841604135677244e-05,-0.0003177264150751332] Loss: 22.842736830474752\n",
      "Iteracion: 18792 Gradiente: [1.8406250919156265e-05,-0.00031755750195401334] Loss: 22.8427368303735\n",
      "Iteracion: 18793 Gradiente: [1.8396465547236098e-05,-0.00031738867864099517] Loss: 22.84273683027234\n",
      "Iteracion: 18794 Gradiente: [1.8386685452279985e-05,-0.00031721994507497204] Loss: 22.842736830171283\n",
      "Iteracion: 18795 Gradiente: [1.8376910464705058e-05,-0.00031705130121923255] Loss: 22.842736830070354\n",
      "Iteracion: 18796 Gradiente: [1.8367140737041154e-05,-0.0003168827470158675] Loss: 22.842736829969507\n",
      "Iteracion: 18797 Gradiente: [1.8357376229497884e-05,-0.0003167142824189284] Loss: 22.84273682986879\n",
      "Iteracion: 18798 Gradiente: [1.834761687196836e-05,-0.0003165459073856643] Loss: 22.842736829768178\n",
      "Iteracion: 18799 Gradiente: [1.8337862679610833e-05,-0.0003163776218666925] Loss: 22.84273682966766\n",
      "Iteracion: 18800 Gradiente: [1.832811363063532e-05,-0.0003162094258173672] Loss: 22.842736829567254\n",
      "Iteracion: 18801 Gradiente: [1.831836992020423e-05,-0.000316041319175279] Loss: 22.842736829466965\n",
      "Iteracion: 18802 Gradiente: [1.8308631357892106e-05,-0.0003158733019062036] Loss: 22.84273682936678\n",
      "Iteracion: 18803 Gradiente: [1.8298897956962415e-05,-0.000315705373960995] Loss: 22.842736829266677\n",
      "Iteracion: 18804 Gradiente: [1.828916972404689e-05,-0.00031553753529240205] Loss: 22.842736829166707\n",
      "Iteracion: 18805 Gradiente: [1.8279446609881234e-05,-0.00031536978585495015] Loss: 22.84273682906684\n",
      "Iteracion: 18806 Gradiente: [1.826972869594101e-05,-0.0003152021255955854] Loss: 22.842736828967066\n",
      "Iteracion: 18807 Gradiente: [1.8260015924435417e-05,-0.00031503455447297785] Loss: 22.842736828867416\n",
      "Iteracion: 18808 Gradiente: [1.8250308343681354e-05,-0.000314867072432771] Loss: 22.842736828767844\n",
      "Iteracion: 18809 Gradiente: [1.8240605929046675e-05,-0.00031469967943174024] Loss: 22.84273682866841\n",
      "Iteracion: 18810 Gradiente: [1.8230908667267918e-05,-0.000314532375422516] Loss: 22.84273682856906\n",
      "Iteracion: 18811 Gradiente: [1.8221216573503323e-05,-0.00031436516035642604] Loss: 22.842736828469832\n",
      "Iteracion: 18812 Gradiente: [1.821152957480384e-05,-0.0003141980341903642] Loss: 22.842736828370697\n",
      "Iteracion: 18813 Gradiente: [1.8201847795277595e-05,-0.00031403099686902654] Loss: 22.842736828271676\n",
      "Iteracion: 18814 Gradiente: [1.8192171106079512e-05,-0.00031386404835380687] Loss: 22.842736828172754\n",
      "Iteracion: 18815 Gradiente: [1.818249957163213e-05,-0.00031369718859319086] Loss: 22.842736828073946\n",
      "Iteracion: 18816 Gradiente: [1.8172833215620206e-05,-0.00031353041753744056] Loss: 22.842736827975227\n",
      "Iteracion: 18817 Gradiente: [1.816317200488508e-05,-0.00031336373514250225] Loss: 22.842736827876625\n",
      "Iteracion: 18818 Gradiente: [1.815351582005557e-05,-0.00031319714136669084] Loss: 22.842736827778126\n",
      "Iteracion: 18819 Gradiente: [1.8143864861978423e-05,-0.00031303063615221544] Loss: 22.84273682767973\n",
      "Iteracion: 18820 Gradiente: [1.81342190567572e-05,-0.00031286421945632507] Loss: 22.842736827581422\n",
      "Iteracion: 18821 Gradiente: [1.812457842997143e-05,-0.0003126978912298739] Loss: 22.842736827483243\n",
      "Iteracion: 18822 Gradiente: [1.811494276561613e-05,-0.0003125316514366242] Loss: 22.842736827385167\n",
      "Iteracion: 18823 Gradiente: [1.810531235454012e-05,-0.00031236550001487726] Loss: 22.84273682728718\n",
      "Iteracion: 18824 Gradiente: [1.809568704895052e-05,-0.00031219943692531633] Loss: 22.842736827189313\n",
      "Iteracion: 18825 Gradiente: [1.8086066789161728e-05,-0.00031203346212388774] Loss: 22.842736827091535\n",
      "Iteracion: 18826 Gradiente: [1.807645168980798e-05,-0.000311867575556235] Loss: 22.84273682699387\n",
      "Iteracion: 18827 Gradiente: [1.8066841646676343e-05,-0.00031170177718375193] Loss: 22.84273682689632\n",
      "Iteracion: 18828 Gradiente: [1.8057236800927967e-05,-0.00031153606694900304] Loss: 22.84273682679885\n",
      "Iteracion: 18829 Gradiente: [1.804763698392738e-05,-0.0003113704448157506] Loss: 22.8427368267015\n",
      "Iteracion: 18830 Gradiente: [1.8038042326414447e-05,-0.0003112049107294013] Loss: 22.842736826604252\n",
      "Iteracion: 18831 Gradiente: [1.8028452689122788e-05,-0.00031103946464921726] Loss: 22.842736826507096\n",
      "Iteracion: 18832 Gradiente: [1.8018868225529634e-05,-0.00031087410652261835] Loss: 22.842736826410057\n",
      "Iteracion: 18833 Gradiente: [1.8009288882581132e-05,-0.00031070883630270887] Loss: 22.842736826313114\n",
      "Iteracion: 18834 Gradiente: [1.7999714549432612e-05,-0.0003105436539506456] Loss: 22.842736826216278\n",
      "Iteracion: 18835 Gradiente: [1.7990145358718716e-05,-0.00031037855941053275] Loss: 22.84273682611954\n",
      "Iteracion: 18836 Gradiente: [1.7980581276333396e-05,-0.0003102135526395008] Loss: 22.842736826022904\n",
      "Iteracion: 18837 Gradiente: [1.7971022219853695e-05,-0.0003100486335949171] Loss: 22.842736825926387\n",
      "Iteracion: 18838 Gradiente: [1.796146833233555e-05,-0.00030988380222053046] Loss: 22.842736825829963\n",
      "Iteracion: 18839 Gradiente: [1.7951919444196087e-05,-0.00030971905848116893] Loss: 22.842736825733628\n",
      "Iteracion: 18840 Gradiente: [1.7942375606594396e-05,-0.00030955440232496303] Loss: 22.842736825637402\n",
      "Iteracion: 18841 Gradiente: [1.793283691142733e-05,-0.00030938983370134567] Loss: 22.842736825541284\n",
      "Iteracion: 18842 Gradiente: [1.7923303190059414e-05,-0.0003092253525745529] Loss: 22.84273682544525\n",
      "Iteracion: 18843 Gradiente: [1.791377462249481e-05,-0.0003090609588837149] Loss: 22.842736825349345\n",
      "Iteracion: 18844 Gradiente: [1.7904251078941038e-05,-0.00030889665259413354] Loss: 22.84273682525353\n",
      "Iteracion: 18845 Gradiente: [1.7894732701506653e-05,-0.0003087324336487285] Loss: 22.842736825157825\n",
      "Iteracion: 18846 Gradiente: [1.788521928839752e-05,-0.00030856830201138054] Loss: 22.842736825062218\n",
      "Iteracion: 18847 Gradiente: [1.7875710943826563e-05,-0.0003084042576319964] Loss: 22.84273682496669\n",
      "Iteracion: 18848 Gradiente: [1.78662077056894e-05,-0.0003082403004602459] Loss: 22.84273682487129\n",
      "Iteracion: 18849 Gradiente: [1.7856709543669543e-05,-0.00030807643045112815] Loss: 22.842736824775994\n",
      "Iteracion: 18850 Gradiente: [1.7847216386712717e-05,-0.00030791264756343156] Loss: 22.84273682468079\n",
      "Iteracion: 18851 Gradiente: [1.7837728222502847e-05,-0.00030774895175108934] Loss: 22.842736824585685\n",
      "Iteracion: 18852 Gradiente: [1.782824518272719e-05,-0.0003075853429597449] Loss: 22.84273682449068\n",
      "Iteracion: 18853 Gradiente: [1.7818767115803288e-05,-0.0003074218211520948] Loss: 22.842736824395786\n",
      "Iteracion: 18854 Gradiente: [1.7809294104154105e-05,-0.0003072583862755588] Loss: 22.842736824300978\n",
      "Iteracion: 18855 Gradiente: [1.7799826167674837e-05,-0.0003070950382850176] Loss: 22.842736824206277\n",
      "Iteracion: 18856 Gradiente: [1.7790363221100353e-05,-0.0003069317771372463] Loss: 22.842736824111675\n",
      "Iteracion: 18857 Gradiente: [1.778090530422105e-05,-0.0003067686027835729] Loss: 22.842736824017177\n",
      "Iteracion: 18858 Gradiente: [1.777145239808912e-05,-0.0003066055151800621] Loss: 22.842736823922785\n",
      "Iteracion: 18859 Gradiente: [1.7762004572811443e-05,-0.0003064425142763838] Loss: 22.842736823828485\n",
      "Iteracion: 18860 Gradiente: [1.775256172512248e-05,-0.00030627960003037913] Loss: 22.84273682373428\n",
      "Iteracion: 18861 Gradiente: [1.7743123987656872e-05,-0.0003061167723899416] Loss: 22.842736823640198\n",
      "Iteracion: 18862 Gradiente: [1.773369111504053e-05,-0.0003059540313212021] Loss: 22.842736823546204\n",
      "Iteracion: 18863 Gradiente: [1.7724263335594516e-05,-0.00030579137676755387] Loss: 22.842736823452295\n",
      "Iteracion: 18864 Gradiente: [1.771484053657938e-05,-0.0003056288086882593] Loss: 22.8427368233585\n",
      "Iteracion: 18865 Gradiente: [1.770542285252456e-05,-0.00030546632702789603] Loss: 22.842736823264794\n",
      "Iteracion: 18866 Gradiente: [1.7696010075951563e-05,-0.0003053039317530685] Loss: 22.84273682317119\n",
      "Iteracion: 18867 Gradiente: [1.7686602410549314e-05,-0.00030514162280776227] Loss: 22.842736823077708\n",
      "Iteracion: 18868 Gradiente: [1.7677199582522008e-05,-0.0003049794001597661] Loss: 22.8427368229843\n",
      "Iteracion: 18869 Gradiente: [1.766780190071889e-05,-0.0003048172637469075] Loss: 22.84273682289099\n",
      "Iteracion: 18870 Gradiente: [1.765840911976587e-05,-0.0003046552135348435] Loss: 22.8427368227978\n",
      "Iteracion: 18871 Gradiente: [1.764902135713934e-05,-0.00030449324947336246] Loss: 22.8427368227047\n",
      "Iteracion: 18872 Gradiente: [1.7639638585364992e-05,-0.00030433137151545016] Loss: 22.84273682261169\n",
      "Iteracion: 18873 Gradiente: [1.7630260772231546e-05,-0.0003041695796196583] Loss: 22.84273682251879\n",
      "Iteracion: 18874 Gradiente: [1.762088802100455e-05,-0.00030400787373222233] Loss: 22.84273682242598\n",
      "Iteracion: 18875 Gradiente: [1.761152015925897e-05,-0.0003038462538191548] Loss: 22.84273682233328\n",
      "Iteracion: 18876 Gradiente: [1.760215732247161e-05,-0.0003036847198247964] Loss: 22.84273682224067\n",
      "Iteracion: 18877 Gradiente: [1.759279953053768e-05,-0.00030352327170225145] Loss: 22.842736822148158\n",
      "Iteracion: 18878 Gradiente: [1.7583446697244655e-05,-0.0003033619094127952] Loss: 22.84273682205576\n",
      "Iteracion: 18879 Gradiente: [1.7574098854803803e-05,-0.0003032006329065714] Loss: 22.842736821963435\n",
      "Iteracion: 18880 Gradiente: [1.7564755916055218e-05,-0.00030303944214414477] Loss: 22.84273682187123\n",
      "Iteracion: 18881 Gradiente: [1.7555417917947126e-05,-0.0003028783370770801] Loss: 22.842736821779106\n",
      "Iteracion: 18882 Gradiente: [1.7546084872795594e-05,-0.0003027173176585999] Loss: 22.842736821687083\n",
      "Iteracion: 18883 Gradiente: [1.7536756904708756e-05,-0.00030255638383671626] Loss: 22.84273682159517\n",
      "Iteracion: 18884 Gradiente: [1.752743377968121e-05,-0.0003023955355786256] Loss: 22.842736821503344\n",
      "Iteracion: 18885 Gradiente: [1.751811572224445e-05,-0.00030223477282606364] Loss: 22.84273682141163\n",
      "Iteracion: 18886 Gradiente: [1.750880254292042e-05,-0.00030207409554445044] Loss: 22.842736821319992\n",
      "Iteracion: 18887 Gradiente: [1.749949424928824e-05,-0.0003019135036879561] Loss: 22.842736821228463\n",
      "Iteracion: 18888 Gradiente: [1.7490190979666902e-05,-0.00030175299720198724] Loss: 22.842736821137024\n",
      "Iteracion: 18889 Gradiente: [1.7480892636475195e-05,-0.0003015925760475824] Loss: 22.842736821045694\n",
      "Iteracion: 18890 Gradiente: [1.747159926708264e-05,-0.0003014322401761878] Loss: 22.842736820954443\n",
      "Iteracion: 18891 Gradiente: [1.7462310845909695e-05,-0.00030127198954292095] Loss: 22.84273682086331\n",
      "Iteracion: 18892 Gradiente: [1.7453027301902086e-05,-0.0003011118241085834] Loss: 22.84273682077226\n",
      "Iteracion: 18893 Gradiente: [1.7443748791379222e-05,-0.000300951743817753] Loss: 22.842736820681306\n",
      "Iteracion: 18894 Gradiente: [1.743447509170437e-05,-0.0003007917486371525] Loss: 22.842736820590453\n",
      "Iteracion: 18895 Gradiente: [1.7425206382881698e-05,-0.000300631838510886] Loss: 22.842736820499717\n",
      "Iteracion: 18896 Gradiente: [1.7415942628910368e-05,-0.0003004720133965577] Loss: 22.842736820409048\n",
      "Iteracion: 18897 Gradiente: [1.740668379568433e-05,-0.00030031227325082454] Loss: 22.842736820318482\n",
      "Iteracion: 18898 Gradiente: [1.7397429796991067e-05,-0.00030015261803271185] Loss: 22.84273682022801\n",
      "Iteracion: 18899 Gradiente: [1.738818088199423e-05,-0.00029999304768324464] Loss: 22.84273682013764\n",
      "Iteracion: 18900 Gradiente: [1.7378936793003655e-05,-0.0002998335621706853] Loss: 22.84273682004737\n",
      "Iteracion: 18901 Gradiente: [1.7369697650337913e-05,-0.0002996741614438747] Loss: 22.842736819957192\n",
      "Iteracion: 18902 Gradiente: [1.7360463384837507e-05,-0.00029951484546124617] Loss: 22.842736819867095\n",
      "Iteracion: 18903 Gradiente: [1.7351234049556297e-05,-0.00029935561417471966] Loss: 22.84273681977711\n",
      "Iteracion: 18904 Gradiente: [1.7342009616072574e-05,-0.00029919646754142566] Loss: 22.842736819687232\n",
      "Iteracion: 18905 Gradiente: [1.733279005691202e-05,-0.0002990374055157711] Loss: 22.842736819597437\n",
      "Iteracion: 18906 Gradiente: [1.7323575358811166e-05,-0.00029887842805536023] Loss: 22.842736819507742\n",
      "Iteracion: 18907 Gradiente: [1.7314365589982116e-05,-0.00029871953511069195] Loss: 22.842736819418125\n",
      "Iteracion: 18908 Gradiente: [1.730516076653051e-05,-0.0002985607266356993] Loss: 22.842736819328632\n",
      "Iteracion: 18909 Gradiente: [1.7295960793717315e-05,-0.00029840200258976306] Loss: 22.842736819239203\n",
      "Iteracion: 18910 Gradiente: [1.7286765702806407e-05,-0.000298243362928711] Loss: 22.84273681914989\n",
      "Iteracion: 18911 Gradiente: [1.7277575482429106e-05,-0.0002980848076047001] Loss: 22.842736819060672\n",
      "Iteracion: 18912 Gradiente: [1.7268390192271e-05,-0.00029792633657130805] Loss: 22.84273681897154\n",
      "Iteracion: 18913 Gradiente: [1.725920981054211e-05,-0.00029776794978471816] Loss: 22.842736818882496\n",
      "Iteracion: 18914 Gradiente: [1.7250034302189e-05,-0.0002976096472027715] Loss: 22.84273681879357\n",
      "Iteracion: 18915 Gradiente: [1.724086369847555e-05,-0.0002974514287763223] Loss: 22.84273681870472\n",
      "Iteracion: 18916 Gradiente: [1.723169795392702e-05,-0.0002972932944643958] Loss: 22.84273681861596\n",
      "Iteracion: 18917 Gradiente: [1.7222537095070342e-05,-0.00029713524422187256] Loss: 22.842736818527307\n",
      "Iteracion: 18918 Gradiente: [1.721338101674519e-05,-0.0002969772780076596] Loss: 22.842736818438745\n",
      "Iteracion: 18919 Gradiente: [1.7204229878113134e-05,-0.00029681939577047936] Loss: 22.842736818350264\n",
      "Iteracion: 18920 Gradiente: [1.719508359296166e-05,-0.0002966615974678177] Loss: 22.842736818261905\n",
      "Iteracion: 18921 Gradiente: [1.718594216223816e-05,-0.00029650388305668685] Loss: 22.842736818173634\n",
      "Iteracion: 18922 Gradiente: [1.7176805545204844e-05,-0.00029634625249457257] Loss: 22.842736818085427\n",
      "Iteracion: 18923 Gradiente: [1.7167673859338114e-05,-0.0002961887057285395] Loss: 22.842736817997356\n",
      "Iteracion: 18924 Gradiente: [1.7158546941686837e-05,-0.00029603124272483685] Loss: 22.842736817909348\n",
      "Iteracion: 18925 Gradiente: [1.7149424854778773e-05,-0.00029587386343360813] Loss: 22.842736817821436\n",
      "Iteracion: 18926 Gradiente: [1.7140307825040206e-05,-0.000295716567798839] Loss: 22.842736817733627\n",
      "Iteracion: 18927 Gradiente: [1.713119547162023e-05,-0.0002955593557962525] Loss: 22.84273681764591\n",
      "Iteracion: 18928 Gradiente: [1.7122088006734278e-05,-0.0002954022273701895] Loss: 22.842736817558283\n",
      "Iteracion: 18929 Gradiente: [1.711298541617149e-05,-0.00029524518247635946] Loss: 22.84273681747076\n",
      "Iteracion: 18930 Gradiente: [1.710388759761372e-05,-0.0002950882210773405] Loss: 22.84273681738331\n",
      "Iteracion: 18931 Gradiente: [1.7094794616430895e-05,-0.0002949313431222104] Loss: 22.84273681729597\n",
      "Iteracion: 18932 Gradiente: [1.708570649914994e-05,-0.0002947745485675076] Loss: 22.84273681720871\n",
      "Iteracion: 18933 Gradiente: [1.707662324956042e-05,-0.00029461783736763893] Loss: 22.842736817121548\n",
      "Iteracion: 18934 Gradiente: [1.706754481555587e-05,-0.00029446120948056396] Loss: 22.842736817034485\n",
      "Iteracion: 18935 Gradiente: [1.7058471215136706e-05,-0.0002943046648619922] Loss: 22.842736816947504\n",
      "Iteracion: 18936 Gradiente: [1.7049402333668696e-05,-0.00029414820347284376] Loss: 22.842736816860633\n",
      "Iteracion: 18937 Gradiente: [1.70403383217869e-05,-0.0002939918252612491] Loss: 22.842736816773833\n",
      "Iteracion: 18938 Gradiente: [1.7031279220229105e-05,-0.00029383553017948337] Loss: 22.842736816687136\n",
      "Iteracion: 18939 Gradiente: [1.7022224835727683e-05,-0.0002936793181943879] Loss: 22.842736816600524\n",
      "Iteracion: 18940 Gradiente: [1.701317531986509e-05,-0.0002935231892540931] Loss: 22.842736816514016\n",
      "Iteracion: 18941 Gradiente: [1.7004130617692682e-05,-0.00029336714331454534] Loss: 22.84273681642759\n",
      "Iteracion: 18942 Gradiente: [1.699509071973656e-05,-0.00029321118033500684] Loss: 22.842736816341258\n",
      "Iteracion: 18943 Gradiente: [1.6986055612733253e-05,-0.00029305530027130545] Loss: 22.84273681625502\n",
      "Iteracion: 18944 Gradiente: [1.697702526826106e-05,-0.0002928995030808087] Loss: 22.842736816168873\n",
      "Iteracion: 18945 Gradiente: [1.6967999692951706e-05,-0.00029274378871816016] Loss: 22.84273681608282\n",
      "Iteracion: 18946 Gradiente: [1.6958979063019797e-05,-0.0002925881571298324] Loss: 22.84273681599686\n",
      "Iteracion: 18947 Gradiente: [1.6949963074353037e-05,-0.00029243260828918] Loss: 22.84273681591097\n",
      "Iteracion: 18948 Gradiente: [1.694095206706455e-05,-0.0002922771421314252] Loss: 22.8427368158252\n",
      "Iteracion: 18949 Gradiente: [1.6931945711462505e-05,-0.0002921217586334753] Loss: 22.84273681573951\n",
      "Iteracion: 18950 Gradiente: [1.6922944173340208e-05,-0.00029196645773967115] Loss: 22.842736815653907\n",
      "Iteracion: 18951 Gradiente: [1.6913947409117706e-05,-0.00029181123940915656] Loss: 22.8427368155684\n",
      "Iteracion: 18952 Gradiente: [1.6904955469006685e-05,-0.00029165610359503565] Loss: 22.842736815482986\n",
      "Iteracion: 18953 Gradiente: [1.689596831321675e-05,-0.0002915010502559786] Loss: 22.842736815397664\n",
      "Iteracion: 18954 Gradiente: [1.6886985797744576e-05,-0.0002913460793553924] Loss: 22.842736815312414\n",
      "Iteracion: 18955 Gradiente: [1.6878008173648597e-05,-0.0002911911908359599] Loss: 22.84273681522728\n",
      "Iteracion: 18956 Gradiente: [1.686903530071504e-05,-0.00029103638466144354] Loss: 22.84273681514222\n",
      "Iteracion: 18957 Gradiente: [1.6860067182733475e-05,-0.0002908816607868422] Loss: 22.842736815057247\n",
      "Iteracion: 18958 Gradiente: [1.6851103886021217e-05,-0.000290727019166089] Loss: 22.84273681497239\n",
      "Iteracion: 18959 Gradiente: [1.684214532626053e-05,-0.0002905724597585646] Loss: 22.842736814887605\n",
      "Iteracion: 18960 Gradiente: [1.6833191554610494e-05,-0.00029041798251772853] Loss: 22.842736814802908\n",
      "Iteracion: 18961 Gradiente: [1.6824242559702423e-05,-0.00029026358740118495] Loss: 22.842736814718318\n",
      "Iteracion: 18962 Gradiente: [1.6815298123636543e-05,-0.0002901092743771964] Loss: 22.84273681463378\n",
      "Iteracion: 18963 Gradiente: [1.6806358658527643e-05,-0.0002899550433795639] Loss: 22.84273681454937\n",
      "Iteracion: 18964 Gradiente: [1.67974238337365e-05,-0.00028980089438081317] Loss: 22.842736814465038\n",
      "Iteracion: 18965 Gradiente: [1.6788493780002986e-05,-0.0002896468273313246] Loss: 22.84273681438079\n",
      "Iteracion: 18966 Gradiente: [1.6779568575960487e-05,-0.00028949284218325507] Loss: 22.84273681429664\n",
      "Iteracion: 18967 Gradiente: [1.677064799328794e-05,-0.000289338938905459] Loss: 22.842736814212568\n",
      "Iteracion: 18968 Gradiente: [1.6761732229989926e-05,-0.0002891851174420405] Loss: 22.842736814128592\n",
      "Iteracion: 18969 Gradiente: [1.67528211723796e-05,-0.00028903137775853813] Loss: 22.842736814044713\n",
      "Iteracion: 18970 Gradiente: [1.6743914885826903e-05,-0.0002888777198045034] Loss: 22.842736813960915\n",
      "Iteracion: 18971 Gradiente: [1.673501330211972e-05,-0.0002887241435420407] Loss: 22.842736813877206\n",
      "Iteracion: 18972 Gradiente: [1.6726116411784155e-05,-0.00028857064892745163] Loss: 22.842736813793582\n",
      "Iteracion: 18973 Gradiente: [1.6717224247031483e-05,-0.0002884172359154983] Loss: 22.842736813710058\n",
      "Iteracion: 18974 Gradiente: [1.6708336895968994e-05,-0.0002882639044582191] Loss: 22.842736813626605\n",
      "Iteracion: 18975 Gradiente: [1.6699454273331565e-05,-0.0002881106545146395] Loss: 22.84273681354326\n",
      "Iteracion: 18976 Gradiente: [1.6690576359223996e-05,-0.0002879574860461531] Loss: 22.84273681345999\n",
      "Iteracion: 18977 Gradiente: [1.6681703142277606e-05,-0.0002878043990074038] Loss: 22.842736813376813\n",
      "Iteracion: 18978 Gradiente: [1.667283462059762e-05,-0.00028765139335552213] Loss: 22.84273681329373\n",
      "Iteracion: 18979 Gradiente: [1.666397078755229e-05,-0.00028749846904882286] Loss: 22.842736813210728\n",
      "Iteracion: 18980 Gradiente: [1.6655111676300293e-05,-0.0002873456260388707] Loss: 22.84273681312782\n",
      "Iteracion: 18981 Gradiente: [1.664625730673682e-05,-0.0002871928642836252] Loss: 22.842736813044997\n",
      "Iteracion: 18982 Gradiente: [1.6637407730020943e-05,-0.000287040183737138] Loss: 22.842736812962276\n",
      "Iteracion: 18983 Gradiente: [1.6628562796465e-05,-0.000286887584363645] Loss: 22.842736812879618\n",
      "Iteracion: 18984 Gradiente: [1.6619722499437253e-05,-0.00028673506612015843] Loss: 22.842736812797046\n",
      "Iteracion: 18985 Gradiente: [1.6610886987677986e-05,-0.00028658262895611133] Loss: 22.842736812714588\n",
      "Iteracion: 18986 Gradiente: [1.660205614465819e-05,-0.0002864302728320685] Loss: 22.8427368126322\n",
      "Iteracion: 18987 Gradiente: [1.6593229937219196e-05,-0.0002862779977096608] Loss: 22.842736812549912\n",
      "Iteracion: 18988 Gradiente: [1.6584408443994412e-05,-0.0002861258035406896] Loss: 22.842736812467695\n",
      "Iteracion: 18989 Gradiente: [1.6575591688668587e-05,-0.000285973690279917] Loss: 22.842736812385578\n",
      "Iteracion: 18990 Gradiente: [1.6566779552817935e-05,-0.00028582165788956597] Loss: 22.842736812303542\n",
      "Iteracion: 18991 Gradiente: [1.655797217097188e-05,-0.0002856697063216747] Loss: 22.842736812221606\n",
      "Iteracion: 18992 Gradiente: [1.6549169488181784e-05,-0.00028551783553408446] Loss: 22.842736812139734\n",
      "Iteracion: 18993 Gradiente: [1.6540371341496514e-05,-0.0002853660454956497] Loss: 22.84273681205796\n",
      "Iteracion: 18994 Gradiente: [1.653157803029141e-05,-0.0002852143361430137] Loss: 22.84273681197628\n",
      "Iteracion: 18995 Gradiente: [1.6522789296876304e-05,-0.0002850627074498865] Loss: 22.84273681189468\n",
      "Iteracion: 18996 Gradiente: [1.651400527388584e-05,-0.0002849111593645167] Loss: 22.84273681181317\n",
      "Iteracion: 18997 Gradiente: [1.6505225919634842e-05,-0.0002847596918464035] Loss: 22.84273681173174\n",
      "Iteracion: 18998 Gradiente: [1.649645126633459e-05,-0.00028460830485267744] Loss: 22.8427368116504\n",
      "Iteracion: 18999 Gradiente: [1.6487681223982995e-05,-0.00028445699834283764] Loss: 22.842736811569146\n",
      "Iteracion: 19000 Gradiente: [1.6478915910056458e-05,-0.0002843057722690408] Loss: 22.842736811487985\n",
      "Iteracion: 19001 Gradiente: [1.647015511233955e-05,-0.0002841546265993126] Loss: 22.8427368114069\n",
      "Iteracion: 19002 Gradiente: [1.6461399173787564e-05,-0.00028400356127183577] Loss: 22.842736811325913\n",
      "Iteracion: 19003 Gradiente: [1.6452647823446872e-05,-0.00028385257625842544] Loss: 22.842736811245\n",
      "Iteracion: 19004 Gradiente: [1.644390105942269e-05,-0.0002837016715182254] Loss: 22.84273681116418\n",
      "Iteracion: 19005 Gradiente: [1.6435158928137148e-05,-0.0002835508470043398] Loss: 22.842736811083448\n",
      "Iteracion: 19006 Gradiente: [1.642642146559107e-05,-0.0002834001026723598] Loss: 22.84273681100279\n",
      "Iteracion: 19007 Gradiente: [1.641768867557403e-05,-0.000283249438477758] Loss: 22.84273681092223\n",
      "Iteracion: 19008 Gradiente: [1.6408960496505644e-05,-0.00028309885438441523] Loss: 22.84273681084175\n",
      "Iteracion: 19009 Gradiente: [1.6400237067652295e-05,-0.0002829483503389222] Loss: 22.84273681076136\n",
      "Iteracion: 19010 Gradiente: [1.639151820427287e-05,-0.00028279792631037047] Loss: 22.842736810681057\n",
      "Iteracion: 19011 Gradiente: [1.638280400015901e-05,-0.000282647582250443] Loss: 22.84273681060083\n",
      "Iteracion: 19012 Gradiente: [1.6374094349203007e-05,-0.00028249731812384957] Loss: 22.842736810520698\n",
      "Iteracion: 19013 Gradiente: [1.636538942667206e-05,-0.0002823471338755231] Loss: 22.84273681044065\n",
      "Iteracion: 19014 Gradiente: [1.6356689008034665e-05,-0.0002821970294762129] Loss: 22.842736810360684\n",
      "Iteracion: 19015 Gradiente: [1.634799328655845e-05,-0.000282047004873102] Loss: 22.84273681028079\n",
      "Iteracion: 19016 Gradiente: [1.633930225466429e-05,-0.0002818970600243868] Loss: 22.842736810200993\n",
      "Iteracion: 19017 Gradiente: [1.633061574087454e-05,-0.000281747194897027] Loss: 22.842736810121288\n",
      "Iteracion: 19018 Gradiente: [1.6321933891087308e-05,-0.00028159740944021884] Loss: 22.84273681004166\n",
      "Iteracion: 19019 Gradiente: [1.631325672993474e-05,-0.0002814477036087245] Loss: 22.842736809962126\n",
      "Iteracion: 19020 Gradiente: [1.630458404235924e-05,-0.00028129807737317474] Loss: 22.84273680988266\n",
      "Iteracion: 19021 Gradiente: [1.629591603583928e-05,-0.0002811485306798052] Loss: 22.84273680980329\n",
      "Iteracion: 19022 Gradiente: [1.62872526383732e-05,-0.0002809990634904835] Loss: 22.842736809723988\n",
      "Iteracion: 19023 Gradiente: [1.6278593851855778e-05,-0.00028084967576245864] Loss: 22.842736809644794\n",
      "Iteracion: 19024 Gradiente: [1.626993961754882e-05,-0.00028070036745582173] Loss: 22.842736809565682\n",
      "Iteracion: 19025 Gradiente: [1.6261290083245208e-05,-0.00028055113852071636] Loss: 22.842736809486645\n",
      "Iteracion: 19026 Gradiente: [1.6252645013044762e-05,-0.0002804019889282472] Loss: 22.842736809407686\n",
      "Iteracion: 19027 Gradiente: [1.624400461158378e-05,-0.0002802529186232287] Loss: 22.84273680932881\n",
      "Iteracion: 19028 Gradiente: [1.623536878980758e-05,-0.0002801039275706074] Loss: 22.84273680925002\n",
      "Iteracion: 19029 Gradiente: [1.622673757424309e-05,-0.00027995501572490865] Loss: 22.842736809171335\n",
      "Iteracion: 19030 Gradiente: [1.621811094499511e-05,-0.00027980618304503937] Loss: 22.842736809092717\n",
      "Iteracion: 19031 Gradiente: [1.620948896648618e-05,-0.00027965742948540633] Loss: 22.842736809014184\n",
      "Iteracion: 19032 Gradiente: [1.6200871533555984e-05,-0.000279508755010364] Loss: 22.842736808935737\n",
      "Iteracion: 19033 Gradiente: [1.619225863862539e-05,-0.00027936015957716147] Loss: 22.842736808857364\n",
      "Iteracion: 19034 Gradiente: [1.618365029780004e-05,-0.00027921164314363977] Loss: 22.842736808779076\n",
      "Iteracion: 19035 Gradiente: [1.6175046589713322e-05,-0.0002790632056618373] Loss: 22.842736808700884\n",
      "Iteracion: 19036 Gradiente: [1.6166447415836653e-05,-0.0002789148470969375] Loss: 22.842736808622757\n",
      "Iteracion: 19037 Gradiente: [1.6157852776170027e-05,-0.00027876656740666306] Loss: 22.842736808544732\n",
      "Iteracion: 19038 Gradiente: [1.614926274176772e-05,-0.00027861836654494707] Loss: 22.842736808466782\n",
      "Iteracion: 19039 Gradiente: [1.614067726147065e-05,-0.00027847024447117026] Loss: 22.842736808388903\n",
      "Iteracion: 19040 Gradiente: [1.6132096407280492e-05,-0.00027832220114033156] Loss: 22.84273680831112\n",
      "Iteracion: 19041 Gradiente: [1.6123520134669887e-05,-0.0002781742365137063] Loss: 22.842736808233422\n",
      "Iteracion: 19042 Gradiente: [1.6114948401006283e-05,-0.00027802635054949103] Loss: 22.84273680815579\n",
      "Iteracion: 19043 Gradiente: [1.610638116270972e-05,-0.00027787854320896106] Loss: 22.84273680807826\n",
      "Iteracion: 19044 Gradiente: [1.6097818544835718e-05,-0.00027773081444522064] Loss: 22.84273680800081\n",
      "Iteracion: 19045 Gradiente: [1.6089260400538783e-05,-0.00027758316422167676] Loss: 22.842736807923437\n",
      "Iteracion: 19046 Gradiente: [1.608070688613831e-05,-0.000277435592490131] Loss: 22.84273680784614\n",
      "Iteracion: 19047 Gradiente: [1.6072157865210102e-05,-0.0002772880992147009] Loss: 22.84273680776893\n",
      "Iteracion: 19048 Gradiente: [1.6063613465651845e-05,-0.000277140684346359] Loss: 22.8427368076918\n",
      "Iteracion: 19049 Gradiente: [1.6055073492301137e-05,-0.0002769933478554994] Loss: 22.842736807614767\n",
      "Iteracion: 19050 Gradiente: [1.6046538130846482e-05,-0.00027684608968906826] Loss: 22.842736807537804\n",
      "Iteracion: 19051 Gradiente: [1.603800734339226e-05,-0.0002766989098082225] Loss: 22.842736807460916\n",
      "Iteracion: 19052 Gradiente: [1.602948100961991e-05,-0.00027655180817755346] Loss: 22.842736807384117\n",
      "Iteracion: 19053 Gradiente: [1.602095922900541e-05,-0.0002764047847473231] Loss: 22.842736807307393\n",
      "Iteracion: 19054 Gradiente: [1.6012442051760446e-05,-0.00027625783947620163] Loss: 22.842736807230775\n",
      "Iteracion: 19055 Gradiente: [1.6003929358513837e-05,-0.00027611097232783287] Loss: 22.84273680715421\n",
      "Iteracion: 19056 Gradiente: [1.599542117958208e-05,-0.00027596418325993947] Loss: 22.84273680707775\n",
      "Iteracion: 19057 Gradiente: [1.5986917471385217e-05,-0.0002758174722326127] Loss: 22.84273680700134\n",
      "Iteracion: 19058 Gradiente: [1.597841833434662e-05,-0.00027567083919871984] Loss: 22.84273680692504\n",
      "Iteracion: 19059 Gradiente: [1.5969923709728087e-05,-0.0002755242841191811] Loss: 22.842736806848812\n",
      "Iteracion: 19060 Gradiente: [1.596143370268995e-05,-0.0002753778069471006] Loss: 22.84273680677267\n",
      "Iteracion: 19061 Gradiente: [1.5952948040383792e-05,-0.000275231407655833] Loss: 22.84273680669659\n",
      "Iteracion: 19062 Gradiente: [1.5944466976710222e-05,-0.0002750850861910218] Loss: 22.842736806620604\n",
      "Iteracion: 19063 Gradiente: [1.5935990334507247e-05,-0.0002749388425202189] Loss: 22.842736806544696\n",
      "Iteracion: 19064 Gradiente: [1.592751835156984e-05,-0.00027479267658717295] Loss: 22.842736806468874\n",
      "Iteracion: 19065 Gradiente: [1.5919050737996562e-05,-0.00027464658836950185] Loss: 22.842736806393134\n",
      "Iteracion: 19066 Gradiente: [1.5910587689897206e-05,-0.00027450057781284917] Loss: 22.84273680631747\n",
      "Iteracion: 19067 Gradiente: [1.5902129139059677e-05,-0.0002743546448801482] Loss: 22.842736806241877\n",
      "Iteracion: 19068 Gradiente: [1.589367509401048e-05,-0.0002742087895277005] Loss: 22.84273680616638\n",
      "Iteracion: 19069 Gradiente: [1.588522555569701e-05,-0.00027406301171808425] Loss: 22.84273680609096\n",
      "Iteracion: 19070 Gradiente: [1.5876780471065406e-05,-0.0002739173114085484] Loss: 22.842736806015612\n",
      "Iteracion: 19071 Gradiente: [1.5868339856221305e-05,-0.00027377168855953943] Loss: 22.84273680594035\n",
      "Iteracion: 19072 Gradiente: [1.5859903812535472e-05,-0.0002736261431238063] Loss: 22.84273680586517\n",
      "Iteracion: 19073 Gradiente: [1.585147222916324e-05,-0.00027348067506582176] Loss: 22.842736805790068\n",
      "Iteracion: 19074 Gradiente: [1.5843045107999388e-05,-0.00027333528434390073] Loss: 22.842736805715035\n",
      "Iteracion: 19075 Gradiente: [1.5834622450938697e-05,-0.00027318997091647647] Loss: 22.842736805640097\n",
      "Iteracion: 19076 Gradiente: [1.5826204226717285e-05,-0.0002730447347455348] Loss: 22.84273680556524\n",
      "Iteracion: 19077 Gradiente: [1.581779059260195e-05,-0.00027289957578039055] Loss: 22.842736805490453\n",
      "Iteracion: 19078 Gradiente: [1.580938132595596e-05,-0.0002727544939916745] Loss: 22.842736805415736\n",
      "Iteracion: 19079 Gradiente: [1.5800976541413547e-05,-0.0002726094893326092] Loss: 22.842736805341115\n",
      "Iteracion: 19080 Gradiente: [1.5792576285396838e-05,-0.0002724645617594964] Loss: 22.842736805266586\n",
      "Iteracion: 19081 Gradiente: [1.578418042527119e-05,-0.00027231971123740093] Loss: 22.84273680519211\n",
      "Iteracion: 19082 Gradiente: [1.5775789115461218e-05,-0.0002721749377172955] Loss: 22.842736805117717\n",
      "Iteracion: 19083 Gradiente: [1.5767402232806185e-05,-0.00027203024116495556] Loss: 22.84273680504342\n",
      "Iteracion: 19084 Gradiente: [1.5759019779200873e-05,-0.00027188562153928806] Loss: 22.84273680496918\n",
      "Iteracion: 19085 Gradiente: [1.575064175464528e-05,-0.00027174107880050257] Loss: 22.84273680489504\n",
      "Iteracion: 19086 Gradiente: [1.574226831451142e-05,-0.000271596612896019] Loss: 22.84273680482095\n",
      "Iteracion: 19087 Gradiente: [1.5733899235215177e-05,-0.0002714522238001393] Loss: 22.84273680474696\n",
      "Iteracion: 19088 Gradiente: [1.5725534575494747e-05,-0.00027130791146786254] Loss: 22.84273680467306\n",
      "Iteracion: 19089 Gradiente: [1.5717174362824457e-05,-0.0002711636758562008] Loss: 22.842736804599216\n",
      "Iteracion: 19090 Gradiente: [1.5708818641731643e-05,-0.0002710195169231137] Loss: 22.842736804525455\n",
      "Iteracion: 19091 Gradiente: [1.5700467280529058e-05,-0.0002708754346323635] Loss: 22.842736804451782\n",
      "Iteracion: 19092 Gradiente: [1.569212045637869e-05,-0.000270731428936107] Loss: 22.842736804378173\n",
      "Iteracion: 19093 Gradiente: [1.5683778041382842e-05,-0.00027058749979881706] Loss: 22.84273680430466\n",
      "Iteracion: 19094 Gradiente: [1.567544007059496e-05,-0.0002704436471784533] Loss: 22.84273680423123\n",
      "Iteracion: 19095 Gradiente: [1.5667106581910653e-05,-0.0002702998710309619] Loss: 22.84273680415785\n",
      "Iteracion: 19096 Gradiente: [1.5658777534592143e-05,-0.00027015617131821067] Loss: 22.842736804084566\n",
      "Iteracion: 19097 Gradiente: [1.565045279505739e-05,-0.0002700125480078697] Loss: 22.84273680401135\n",
      "Iteracion: 19098 Gradiente: [1.5642132530047094e-05,-0.0002698690010504379] Loss: 22.842736803938223\n",
      "Iteracion: 19099 Gradiente: [1.563381674903515e-05,-0.00026972553040150636] Loss: 22.84273680386516\n",
      "Iteracion: 19100 Gradiente: [1.5625505349703416e-05,-0.00026958213602969275] Loss: 22.842736803792185\n",
      "Iteracion: 19101 Gradiente: [1.5617198304577565e-05,-0.00026943881789437776] Loss: 22.84273680371928\n",
      "Iteracion: 19102 Gradiente: [1.560889576145049e-05,-0.000269295575945705] Loss: 22.84273680364645\n",
      "Iteracion: 19103 Gradiente: [1.560059761421447e-05,-0.00026915241015039725] Loss: 22.842736803573725\n",
      "Iteracion: 19104 Gradiente: [1.559230377665699e-05,-0.0002690093204724538] Loss: 22.84273680350105\n",
      "Iteracion: 19105 Gradiente: [1.5584014463835652e-05,-0.0002688663068590576] Loss: 22.84273680342847\n",
      "Iteracion: 19106 Gradiente: [1.557572950995715e-05,-0.00026872336927776056] Loss: 22.842736803355958\n",
      "Iteracion: 19107 Gradiente: [1.556744888944195e-05,-0.0002685805076917328] Loss: 22.842736803283522\n",
      "Iteracion: 19108 Gradiente: [1.5559172752925102e-05,-0.0002684377220498154] Loss: 22.84273680321116\n",
      "Iteracion: 19109 Gradiente: [1.5550901031247123e-05,-0.0002682950123167179] Loss: 22.842736803138873\n",
      "Iteracion: 19110 Gradiente: [1.554263373672408e-05,-0.0002681523784496894] Loss: 22.84273680306666\n",
      "Iteracion: 19111 Gradiente: [1.553437075282697e-05,-0.00026800982041722915] Loss: 22.84273680299454\n",
      "Iteracion: 19112 Gradiente: [1.5526112261454727e-05,-0.00026786733816699384] Loss: 22.842736802922502\n",
      "Iteracion: 19113 Gradiente: [1.5517858190605694e-05,-0.0002677249316629826] Loss: 22.84273680285052\n",
      "Iteracion: 19114 Gradiente: [1.550960842280347e-05,-0.0002675826008707342] Loss: 22.84273680277862\n",
      "Iteracion: 19115 Gradiente: [1.5501362957100657e-05,-0.00026744034575105027] Loss: 22.842736802706796\n",
      "Iteracion: 19116 Gradiente: [1.5493121998133576e-05,-0.00026729816625229813] Loss: 22.842736802635052\n",
      "Iteracion: 19117 Gradiente: [1.548488537821413e-05,-0.0002671560623426217] Loss: 22.842736802563362\n",
      "Iteracion: 19118 Gradiente: [1.5476653121974474e-05,-0.00026701403398009894] Loss: 22.842736802491796\n",
      "Iteracion: 19119 Gradiente: [1.546842527488934e-05,-0.0002668720811229264] Loss: 22.842736802420255\n",
      "Iteracion: 19120 Gradiente: [1.546020183695873e-05,-0.0002667302037298924] Loss: 22.84273680234884\n",
      "Iteracion: 19121 Gradiente: [1.5451982695443196e-05,-0.0002665884017670095] Loss: 22.84273680227746\n",
      "Iteracion: 19122 Gradiente: [1.54437678247632e-05,-0.0002664466751970925] Loss: 22.842736802206176\n",
      "Iteracion: 19123 Gradiente: [1.543555743997634e-05,-0.0002663050239666139] Loss: 22.842736802134972\n",
      "Iteracion: 19124 Gradiente: [1.5427351460554443e-05,-0.000266163448039336] Loss: 22.842736802063822\n",
      "Iteracion: 19125 Gradiente: [1.541914983439104e-05,-0.0002660219473797317] Loss: 22.84273680199277\n",
      "Iteracion: 19126 Gradiente: [1.5410952610750427e-05,-0.00026588052194244464] Loss: 22.84273680192178\n",
      "Iteracion: 19127 Gradiente: [1.540275964752406e-05,-0.0002657391716973952] Loss: 22.842736801850865\n",
      "Iteracion: 19128 Gradiente: [1.539457099018667e-05,-0.00026559789660230614] Loss: 22.84273680178003\n",
      "Iteracion: 19129 Gradiente: [1.538638679221549e-05,-0.000265456696605663] Loss: 22.84273680170927\n",
      "Iteracion: 19130 Gradiente: [1.537820690392285e-05,-0.00026531557167857044] Loss: 22.842736801638587\n",
      "Iteracion: 19131 Gradiente: [1.5370031369836094e-05,-0.0002651745217768564] Loss: 22.84273680156798\n",
      "Iteracion: 19132 Gradiente: [1.5361860160586124e-05,-0.0002650335468628621] Loss: 22.84273680149744\n",
      "Iteracion: 19133 Gradiente: [1.53536933188055e-05,-0.0002648926468944287] Loss: 22.842736801426973\n",
      "Iteracion: 19134 Gradiente: [1.534553081323035e-05,-0.00026475182183354206] Loss: 22.842736801356597\n",
      "Iteracion: 19135 Gradiente: [1.533737265143979e-05,-0.0002646110716380434] Loss: 22.842736801286286\n",
      "Iteracion: 19136 Gradiente: [1.5329218826802085e-05,-0.0002644703962720503] Loss: 22.842736801216056\n",
      "Iteracion: 19137 Gradiente: [1.5321069405634564e-05,-0.00026432979568795644] Loss: 22.8427368011459\n",
      "Iteracion: 19138 Gradiente: [1.5312924271408217e-05,-0.00026418926985449787] Loss: 22.842736801075798\n",
      "Iteracion: 19139 Gradiente: [1.5304783439281285e-05,-0.00026404881872916044] Loss: 22.84273680100579\n",
      "Iteracion: 19140 Gradiente: [1.5296646949991555e-05,-0.00026390844227286436] Loss: 22.842736800935846\n",
      "Iteracion: 19141 Gradiente: [1.528851473248475e-05,-0.00026376814044759556] Loss: 22.842736800865996\n",
      "Iteracion: 19142 Gradiente: [1.5280386895710764e-05,-0.00026362791320811617] Loss: 22.842736800796203\n",
      "Iteracion: 19143 Gradiente: [1.5272263337351432e-05,-0.00026348776051960955] Loss: 22.842736800726488\n",
      "Iteracion: 19144 Gradiente: [1.5264144244042656e-05,-0.0002633476823316272] Loss: 22.842736800656848\n",
      "Iteracion: 19145 Gradiente: [1.5256029417779852e-05,-0.00026320767861681325] Loss: 22.84273680058728\n",
      "Iteracion: 19146 Gradiente: [1.5247918808351339e-05,-0.00026306774933774574] Loss: 22.842736800517795\n",
      "Iteracion: 19147 Gradiente: [1.5239812591971713e-05,-0.0002629278944438577] Loss: 22.842736800448375\n",
      "Iteracion: 19148 Gradiente: [1.5231710596215938e-05,-0.0002627881139077933] Loss: 22.842736800379026\n",
      "Iteracion: 19149 Gradiente: [1.5223612900664799e-05,-0.0002626484076837225] Loss: 22.842736800309755\n",
      "Iteracion: 19150 Gradiente: [1.5215519606689062e-05,-0.00026250877572581524] Loss: 22.84273680024057\n",
      "Iteracion: 19151 Gradiente: [1.5207430578811909e-05,-0.00026236921800280774] Loss: 22.842736800171437\n",
      "Iteracion: 19152 Gradiente: [1.5199345770611217e-05,-0.0002622297344763306] Loss: 22.842736800102383\n",
      "Iteracion: 19153 Gradiente: [1.5191265316616408e-05,-0.0002620903251024487] Loss: 22.842736800033396\n",
      "Iteracion: 19154 Gradiente: [1.5183189180826654e-05,-0.00026195098984006885] Loss: 22.842736799964495\n",
      "Iteracion: 19155 Gradiente: [1.5175117381242368e-05,-0.00026181172864987446] Loss: 22.842736799895658\n",
      "Iteracion: 19156 Gradiente: [1.5167049800387153e-05,-0.0002616725415000095] Loss: 22.842736799826906\n",
      "Iteracion: 19157 Gradiente: [1.5158986474261838e-05,-0.00026153342834819664] Loss: 22.84273679975823\n",
      "Iteracion: 19158 Gradiente: [1.5150927514658482e-05,-0.0002613943891495533] Loss: 22.842736799689614\n",
      "Iteracion: 19159 Gradiente: [1.5142872751994218e-05,-0.0002612554238728156] Loss: 22.842736799621076\n",
      "Iteracion: 19160 Gradiente: [1.5134822390905356e-05,-0.0002611165324664692] Loss: 22.84273679955261\n",
      "Iteracion: 19161 Gradiente: [1.5126776335705473e-05,-0.00026097771489806595] Loss: 22.84273679948421\n",
      "Iteracion: 19162 Gradiente: [1.5118734463233826e-05,-0.0002608389711350393] Loss: 22.842736799415892\n",
      "Iteracion: 19163 Gradiente: [1.511069687391379e-05,-0.0002607003011322699] Loss: 22.842736799347655\n",
      "Iteracion: 19164 Gradiente: [1.5102663522270631e-05,-0.00026056170485233565] Loss: 22.84273679927947\n",
      "Iteracion: 19165 Gradiente: [1.5094634461358207e-05,-0.00026042318225378837] Loss: 22.842736799211373\n",
      "Iteracion: 19166 Gradiente: [1.5086609697808248e-05,-0.00026028473329612706] Loss: 22.84273679914335\n",
      "Iteracion: 19167 Gradiente: [1.5078589208883387e-05,-0.000260146357943114] Loss: 22.842736799075386\n",
      "Iteracion: 19168 Gradiente: [1.5070572898897201e-05,-0.0002600080561567353] Loss: 22.84273679900749\n",
      "Iteracion: 19169 Gradiente: [1.506256102553986e-05,-0.0002598698278868975] Loss: 22.842736798939693\n",
      "Iteracion: 19170 Gradiente: [1.5054553338700316e-05,-0.00025973167310742914] Loss: 22.842736798871943\n",
      "Iteracion: 19171 Gradiente: [1.5046549815641205e-05,-0.0002595935917818556] Loss: 22.84273679880427\n",
      "Iteracion: 19172 Gradiente: [1.5038550612681926e-05,-0.00025945558386103094] Loss: 22.842736798736674\n",
      "Iteracion: 19173 Gradiente: [1.5030555642662572e-05,-0.0002593176493083623] Loss: 22.842736798669147\n",
      "Iteracion: 19174 Gradiente: [1.5022564908425313e-05,-0.00025917978808796723] Loss: 22.842736798601706\n",
      "Iteracion: 19175 Gradiente: [1.5014578487656156e-05,-0.00025904200015484474] Loss: 22.842736798534315\n",
      "Iteracion: 19176 Gradiente: [1.5006596280879117e-05,-0.0002589042854755993] Loss: 22.842736798466998\n",
      "Iteracion: 19177 Gradiente: [1.4998618332621542e-05,-0.00025876664401020357] Loss: 22.842736798399763\n",
      "Iteracion: 19178 Gradiente: [1.4990644592671742e-05,-0.00025862907572005156] Loss: 22.842736798332613\n",
      "Iteracion: 19179 Gradiente: [1.4982675139663115e-05,-0.00025849158056322116] Loss: 22.842736798265495\n",
      "Iteracion: 19180 Gradiente: [1.4974709887383143e-05,-0.0002583541585050142] Loss: 22.842736798198477\n",
      "Iteracion: 19181 Gradiente: [1.496674880077838e-05,-0.0002582168095087193] Loss: 22.842736798131526\n",
      "Iteracion: 19182 Gradiente: [1.49587920882747e-05,-0.00025807953352223006] Loss: 22.842736798064646\n",
      "Iteracion: 19183 Gradiente: [1.495083953007755e-05,-0.0002579423305213879] Loss: 22.842736797997834\n",
      "Iteracion: 19184 Gradiente: [1.4942891234189422e-05,-0.00025780520045988925] Loss: 22.842736797931096\n",
      "Iteracion: 19185 Gradiente: [1.4934947155135584e-05,-0.00025766814330007527] Loss: 22.84273679786443\n",
      "Iteracion: 19186 Gradiente: [1.4927007294810815e-05,-0.0002575311590057083] Loss: 22.84273679779783\n",
      "Iteracion: 19187 Gradiente: [1.4919071636162093e-05,-0.00025739424753676114] Loss: 22.8427367977313\n",
      "Iteracion: 19188 Gradiente: [1.4911140180136803e-05,-0.0002572574088556934] Loss: 22.842736797664845\n",
      "Iteracion: 19189 Gradiente: [1.4903212978841414e-05,-0.00025712064291868824] Loss: 22.84273679759846\n",
      "Iteracion: 19190 Gradiente: [1.4895289980169462e-05,-0.00025698394969258707] Loss: 22.84273679753214\n",
      "Iteracion: 19191 Gradiente: [1.4887371196437017e-05,-0.0002568473291358231] Loss: 22.8427367974659\n",
      "Iteracion: 19192 Gradiente: [1.4879456611538443e-05,-0.00025671078121168497] Loss: 22.84273679739973\n",
      "Iteracion: 19193 Gradiente: [1.4871546189472914e-05,-0.00025657430588429026] Loss: 22.842736797333615\n",
      "Iteracion: 19194 Gradiente: [1.4863639987083842e-05,-0.0002564379031087564] Loss: 22.842736797267584\n",
      "Iteracion: 19195 Gradiente: [1.4855738023319039e-05,-0.0002563015728480167] Loss: 22.842736797201624\n",
      "Iteracion: 19196 Gradiente: [1.4847840237545521e-05,-0.00025616531506571504] Loss: 22.842736797135725\n",
      "Iteracion: 19197 Gradiente: [1.4839946615552435e-05,-0.00025602912972490325] Loss: 22.842736797069904\n",
      "Iteracion: 19198 Gradiente: [1.4832057293763986e-05,-0.0002558930167777381] Loss: 22.842736797004147\n",
      "Iteracion: 19199 Gradiente: [1.482417207985994e-05,-0.00025575697619733737] Loss: 22.842736796938468\n",
      "Iteracion: 19200 Gradiente: [1.481629110647494e-05,-0.0002556210079379895] Loss: 22.84273679687286\n",
      "Iteracion: 19201 Gradiente: [1.480841437834594e-05,-0.0002554851119595488] Loss: 22.84273679680731\n",
      "Iteracion: 19202 Gradiente: [1.480054178273349e-05,-0.0002553492882308698] Loss: 22.84273679674184\n",
      "Iteracion: 19203 Gradiente: [1.47926733774284e-05,-0.00025521353671109637] Loss: 22.84273679667643\n",
      "Iteracion: 19204 Gradiente: [1.4784809140640694e-05,-0.0002550778573597275] Loss: 22.8427367966111\n",
      "Iteracion: 19205 Gradiente: [1.4776949026895635e-05,-0.0002549422501443151] Loss: 22.84273679654582\n",
      "Iteracion: 19206 Gradiente: [1.4769093173564821e-05,-0.0002548067150177265] Loss: 22.84273679648063\n",
      "Iteracion: 19207 Gradiente: [1.4761241414854946e-05,-0.00025467125194976367] Loss: 22.842736796415505\n",
      "Iteracion: 19208 Gradiente: [1.4753393882453262e-05,-0.0002545358608957808] Loss: 22.842736796350447\n",
      "Iteracion: 19209 Gradiente: [1.4745550551727622e-05,-0.0002544005418181191] Loss: 22.842736796285458\n",
      "Iteracion: 19210 Gradiente: [1.4737711357308095e-05,-0.00025426529468184353] Loss: 22.84273679622055\n",
      "Iteracion: 19211 Gradiente: [1.4729876360775051e-05,-0.00025413011944467695] Loss: 22.842736796155688\n",
      "Iteracion: 19212 Gradiente: [1.4722045449389043e-05,-0.0002539950160757106] Loss: 22.842736796090904\n",
      "Iteracion: 19213 Gradiente: [1.4714218811680743e-05,-0.0002538599845261539] Loss: 22.84273679602619\n",
      "Iteracion: 19214 Gradiente: [1.4706396273330332e-05,-0.0002537250247673484] Loss: 22.842736795961546\n",
      "Iteracion: 19215 Gradiente: [1.4698577875075595e-05,-0.0002535901367580825] Loss: 22.842736795896975\n",
      "Iteracion: 19216 Gradiente: [1.4690763653864755e-05,-0.00025345532045631576] Loss: 22.842736795832465\n",
      "Iteracion: 19217 Gradiente: [1.46829535812761e-05,-0.00025332057582936327] Loss: 22.842736795768033\n",
      "Iteracion: 19218 Gradiente: [1.467514768383656e-05,-0.0002531859028355399] Loss: 22.842736795703658\n",
      "Iteracion: 19219 Gradiente: [1.4667345902807938e-05,-0.00025305130143919995] Loss: 22.84273679563935\n",
      "Iteracion: 19220 Gradiente: [1.4659548230611109e-05,-0.000252916771603869] Loss: 22.842736795575117\n",
      "Iteracion: 19221 Gradiente: [1.4651754819775912e-05,-0.0002527823132821775] Loss: 22.84273679551095\n",
      "Iteracion: 19222 Gradiente: [1.4643965456192138e-05,-0.0002526479264473617] Loss: 22.842736795446857\n",
      "Iteracion: 19223 Gradiente: [1.4636180312284826e-05,-0.00025251361105370996] Loss: 22.84273679538283\n",
      "Iteracion: 19224 Gradiente: [1.4628399327420993e-05,-0.0002523793670630899] Loss: 22.84273679531888\n",
      "Iteracion: 19225 Gradiente: [1.4620622401177268e-05,-0.00025224519444589554] Loss: 22.84273679525499\n",
      "Iteracion: 19226 Gradiente: [1.4612849512711061e-05,-0.00025211109316458646] Loss: 22.842736795191154\n",
      "Iteracion: 19227 Gradiente: [1.4605080958555542e-05,-0.0002519770631653984] Loss: 22.842736795127404\n",
      "Iteracion: 19228 Gradiente: [1.459731646302013e-05,-0.00025184310442358064] Loss: 22.84273679506372\n",
      "Iteracion: 19229 Gradiente: [1.4589556093369538e-05,-0.0002517092168973297] Loss: 22.842736795000086\n",
      "Iteracion: 19230 Gradiente: [1.458179989697328e-05,-0.0002515754005480394] Loss: 22.84273679493654\n",
      "Iteracion: 19231 Gradiente: [1.4574047678668952e-05,-0.0002514416553484722] Loss: 22.84273679487306\n",
      "Iteracion: 19232 Gradiente: [1.4566299644987643e-05,-0.00025130798124735066] Loss: 22.84273679480964\n",
      "Iteracion: 19233 Gradiente: [1.455855570403249e-05,-0.0002511743782138846] Loss: 22.842736794746276\n",
      "Iteracion: 19234 Gradiente: [1.455081601780724e-05,-0.00025104084619952024] Loss: 22.842736794682992\n",
      "Iteracion: 19235 Gradiente: [1.4543080340937802e-05,-0.00025090738518069125] Loss: 22.84273679461978\n",
      "Iteracion: 19236 Gradiente: [1.4535348863849625e-05,-0.0002507739951083702] Loss: 22.842736794556625\n",
      "Iteracion: 19237 Gradiente: [1.452762136674816e-05,-0.00025064067595839863] Loss: 22.842736794493554\n",
      "Iteracion: 19238 Gradiente: [1.451989807416491e-05,-0.00025050742767985433] Loss: 22.84273679443053\n",
      "Iteracion: 19239 Gradiente: [1.4512178829780471e-05,-0.00025037425024313127] Loss: 22.842736794367607\n",
      "Iteracion: 19240 Gradiente: [1.4504463695175219e-05,-0.00025024114360749173] Loss: 22.842736794304706\n",
      "Iteracion: 19241 Gradiente: [1.4496752752772106e-05,-0.0002501081077293558] Loss: 22.842736794241894\n",
      "Iteracion: 19242 Gradiente: [1.4489045858567806e-05,-0.0002499751425808938] Loss: 22.84273679417914\n",
      "Iteracion: 19243 Gradiente: [1.4481342937718485e-05,-0.0002498422481277629] Loss: 22.84273679411645\n",
      "Iteracion: 19244 Gradiente: [1.4473644232756063e-05,-0.0002497094243184487] Loss: 22.842736794053852\n",
      "Iteracion: 19245 Gradiente: [1.4465949581676795e-05,-0.0002495766711242927] Loss: 22.842736793991307\n",
      "Iteracion: 19246 Gradiente: [1.445825900627066e-05,-0.00024944398850716237] Loss: 22.842736793928815\n",
      "Iteracion: 19247 Gradiente: [1.4450572596539738e-05,-0.0002493113764240699] Loss: 22.842736793866415\n",
      "Iteracion: 19248 Gradiente: [1.4442890285219315e-05,-0.0002491788348401987] Loss: 22.842736793804065\n",
      "Iteracion: 19249 Gradiente: [1.4435211990833826e-05,-0.0002490463637236928] Loss: 22.84273679374177\n",
      "Iteracion: 19250 Gradiente: [1.4427537801490567e-05,-0.00024891396303191964] Loss: 22.842736793679567\n",
      "Iteracion: 19251 Gradiente: [1.4419867626240073e-05,-0.0002487816327311284] Loss: 22.842736793617405\n",
      "Iteracion: 19252 Gradiente: [1.4412201664034303e-05,-0.0002486493727741864] Loss: 22.84273679355532\n",
      "Iteracion: 19253 Gradiente: [1.4404539685604808e-05,-0.0002485171831363431] Loss: 22.84273679349331\n",
      "Iteracion: 19254 Gradiente: [1.4396881706109829e-05,-0.0002483850637776897] Loss: 22.84273679343136\n",
      "Iteracion: 19255 Gradiente: [1.438922788660572e-05,-0.00024825301465334346] Loss: 22.842736793369465\n",
      "Iteracion: 19256 Gradiente: [1.4381578170249061e-05,-0.000248121035728488] Loss: 22.842736793307648\n",
      "Iteracion: 19257 Gradiente: [1.4373932515354682e-05,-0.0002479891269679513] Loss: 22.84273679324589\n",
      "Iteracion: 19258 Gradiente: [1.4366290844236574e-05,-0.00024785728833892997] Loss: 22.842736793184205\n",
      "Iteracion: 19259 Gradiente: [1.43586533170037e-05,-0.00024772551979488353] Loss: 22.842736793122583\n",
      "Iteracion: 19260 Gradiente: [1.4351019744178e-05,-0.00024759382130786395] Loss: 22.842736793061025\n",
      "Iteracion: 19261 Gradiente: [1.4343390351238364e-05,-0.0002474621928302649] Loss: 22.842736792999524\n",
      "Iteracion: 19262 Gradiente: [1.433576499512886e-05,-0.0002473306343308224] Loss: 22.84273679293811\n",
      "Iteracion: 19263 Gradiente: [1.4328143622795627e-05,-0.00024719914577625937] Loss: 22.842736792876746\n",
      "Iteracion: 19264 Gradiente: [1.4320526354557235e-05,-0.00024706772712193017] Loss: 22.842736792815444\n",
      "Iteracion: 19265 Gradiente: [1.4312913098516825e-05,-0.0002469363783360971] Loss: 22.842736792754216\n",
      "Iteracion: 19266 Gradiente: [1.4305303945623867e-05,-0.0002468050993756539] Loss: 22.84273679269305\n",
      "Iteracion: 19267 Gradiente: [1.4297698733874616e-05,-0.00024667389021371854] Loss: 22.842736792631943\n",
      "Iteracion: 19268 Gradiente: [1.4290097592114156e-05,-0.0002465427508044608] Loss: 22.842736792570907\n",
      "Iteracion: 19269 Gradiente: [1.4282500636871494e-05,-0.0002464116811047745] Loss: 22.842736792509946\n",
      "Iteracion: 19270 Gradiente: [1.427490752992829e-05,-0.0002462806810976067] Loss: 22.84273679244903\n",
      "Iteracion: 19271 Gradiente: [1.4267318605713324e-05,-0.000246149750725048] Loss: 22.84273679238819\n",
      "Iteracion: 19272 Gradiente: [1.4259733664327238e-05,-0.00024601888996080847] Loss: 22.842736792327415\n",
      "Iteracion: 19273 Gradiente: [1.4252152788192991e-05,-0.00024588809876521605] Loss: 22.8427367922667\n",
      "Iteracion: 19274 Gradiente: [1.4244575921414555e-05,-0.00024575737710380943] Loss: 22.842736792206054\n",
      "Iteracion: 19275 Gradiente: [1.4237003017569805e-05,-0.00024562672494106154] Loss: 22.842736792145466\n",
      "Iteracion: 19276 Gradiente: [1.4229434248136385e-05,-0.00024549614223208965] Loss: 22.842736792084953\n",
      "Iteracion: 19277 Gradiente: [1.4221869417951893e-05,-0.0002453656289488274] Loss: 22.842736792024496\n",
      "Iteracion: 19278 Gradiente: [1.4214308572491064e-05,-0.0002452351850525503] Loss: 22.842736791964096\n",
      "Iteracion: 19279 Gradiente: [1.420675181501944e-05,-0.00024510481050098084] Loss: 22.84273679190379\n",
      "Iteracion: 19280 Gradiente: [1.419919910669402e-05,-0.00024497450525776305] Loss: 22.842736791843528\n",
      "Iteracion: 19281 Gradiente: [1.4191650338564917e-05,-0.0002448442692928173] Loss: 22.842736791783317\n",
      "Iteracion: 19282 Gradiente: [1.4184105714321049e-05,-0.000244714102558774] Loss: 22.84273679172319\n",
      "Iteracion: 19283 Gradiente: [1.417656497248269e-05,-0.00024458400503230373] Loss: 22.842736791663118\n",
      "Iteracion: 19284 Gradiente: [1.4169028216315382e-05,-0.00024445397667112917] Loss: 22.842736791603116\n",
      "Iteracion: 19285 Gradiente: [1.4161495539610768e-05,-0.00024432401743356514] Loss: 22.842736791543167\n",
      "Iteracion: 19286 Gradiente: [1.4153966924368433e-05,-0.00024419412728207137] Loss: 22.84273679148329\n",
      "Iteracion: 19287 Gradiente: [1.41464422019529e-05,-0.00024406430618976552] Loss: 22.842736791423466\n",
      "Iteracion: 19288 Gradiente: [1.4138921548578764e-05,-0.00024393455411164666] Loss: 22.84273679136372\n",
      "Iteracion: 19289 Gradiente: [1.4131404837295728e-05,-0.0002438048710165693] Loss: 22.842736791304038\n",
      "Iteracion: 19290 Gradiente: [1.4123892123999818e-05,-0.00024367525686545358] Loss: 22.842736791244416\n",
      "Iteracion: 19291 Gradiente: [1.4116383475008358e-05,-0.00024354571161554854] Loss: 22.842736791184855\n",
      "Iteracion: 19292 Gradiente: [1.4108878845794e-05,-0.000243416235235235] Loss: 22.842736791125347\n",
      "Iteracion: 19293 Gradiente: [1.4101377964455726e-05,-0.0002432868277037888] Loss: 22.842736791065914\n",
      "Iteracion: 19294 Gradiente: [1.4093881225107907e-05,-0.00024315748895986644] Loss: 22.842736791006544\n",
      "Iteracion: 19295 Gradiente: [1.4086388533011511e-05,-0.00024302821897196716] Loss: 22.84273679094724\n",
      "Iteracion: 19296 Gradiente: [1.4078899827533557e-05,-0.00024289901770740602] Loss: 22.84273679088799\n",
      "Iteracion: 19297 Gradiente: [1.4071415078357556e-05,-0.00024276988513278753] Loss: 22.842736790828795\n",
      "Iteracion: 19298 Gradiente: [1.4063934250430065e-05,-0.00024264082121128183] Loss: 22.842736790769685\n",
      "Iteracion: 19299 Gradiente: [1.4056457433753167e-05,-0.00024251182590250646] Loss: 22.84273679071062\n",
      "Iteracion: 19300 Gradiente: [1.4048984674748984e-05,-0.00024238289916832893] Loss: 22.84273679065164\n",
      "Iteracion: 19301 Gradiente: [1.4041515757412527e-05,-0.00024225404098091966] Loss: 22.842736790592703\n",
      "Iteracion: 19302 Gradiente: [1.4034050799220192e-05,-0.0002421252512996593] Loss: 22.842736790533834\n",
      "Iteracion: 19303 Gradiente: [1.4026589903437526e-05,-0.00024199653008191528] Loss: 22.842736790475033\n",
      "Iteracion: 19304 Gradiente: [1.401913290237644e-05,-0.00024186787730151593] Loss: 22.842736790416286\n",
      "Iteracion: 19305 Gradiente: [1.4011679855722529e-05,-0.0002417392929163024] Loss: 22.842736790357613\n",
      "Iteracion: 19306 Gradiente: [1.4004230872425675e-05,-0.00024161077688423423] Loss: 22.842736790298993\n",
      "Iteracion: 19307 Gradiente: [1.3996785808482552e-05,-0.00024148232917783712] Loss: 22.84273679024042\n",
      "Iteracion: 19308 Gradiente: [1.398934469799921e-05,-0.00024135394975921542] Loss: 22.84273679018194\n",
      "Iteracion: 19309 Gradiente: [1.3981907489816573e-05,-0.0002412256385931973] Loss: 22.8427367901235\n",
      "Iteracion: 19310 Gradiente: [1.3974474321306236e-05,-0.00024109739563786073] Loss: 22.842736790065135\n",
      "Iteracion: 19311 Gradiente: [1.3967045091097436e-05,-0.00024096922085933652] Loss: 22.84273679000683\n",
      "Iteracion: 19312 Gradiente: [1.3959619771715855e-05,-0.00024084111422434756] Loss: 22.84273678994858\n",
      "Iteracion: 19313 Gradiente: [1.3952198434215765e-05,-0.0002407130756944061] Loss: 22.842736789890388\n",
      "Iteracion: 19314 Gradiente: [1.3944780949752082e-05,-0.000240585105238722] Loss: 22.84273678983228\n",
      "Iteracion: 19315 Gradiente: [1.3937367521066335e-05,-0.00024045720280980732] Loss: 22.84273678977422\n",
      "Iteracion: 19316 Gradiente: [1.3929957978575657e-05,-0.00024032936838042456] Loss: 22.842736789716227\n",
      "Iteracion: 19317 Gradiente: [1.3922552334596124e-05,-0.0002402016019136255] Loss: 22.84273678965828\n",
      "Iteracion: 19318 Gradiente: [1.3915150682919375e-05,-0.00024007390336914605] Loss: 22.842736789600412\n",
      "Iteracion: 19319 Gradiente: [1.3907752972386334e-05,-0.00023994627271205123] Loss: 22.8427367895426\n",
      "Iteracion: 19320 Gradiente: [1.3900359179312243e-05,-0.00023981870990835336] Loss: 22.842736789484835\n",
      "Iteracion: 19321 Gradiente: [1.3892969413594376e-05,-0.00023969121491482782] Loss: 22.842736789427153\n",
      "Iteracion: 19322 Gradiente: [1.388558338438391e-05,-0.00023956378771453993] Loss: 22.842736789369507\n",
      "Iteracion: 19323 Gradiente: [1.3878201415688334e-05,-0.00023943642824934366] Loss: 22.842736789311953\n",
      "Iteracion: 19324 Gradiente: [1.387082328960787e-05,-0.00023930913649744905] Loss: 22.842736789254445\n",
      "Iteracion: 19325 Gradiente: [1.3863449112250237e-05,-0.00023918191241705243] Loss: 22.842736789196998\n",
      "Iteracion: 19326 Gradiente: [1.3856078918668876e-05,-0.00023905475596919246] Loss: 22.842736789139625\n",
      "Iteracion: 19327 Gradiente: [1.3848712547807433e-05,-0.00023892766712544736] Loss: 22.842736789082288\n",
      "Iteracion: 19328 Gradiente: [1.3841350150300968e-05,-0.0002388006458435399] Loss: 22.842736789025025\n",
      "Iteracion: 19329 Gradiente: [1.3833991699622554e-05,-0.00023867369208829814] Loss: 22.842736788967837\n",
      "Iteracion: 19330 Gradiente: [1.3826637130402257e-05,-0.000238546805827274] Loss: 22.842736788910678\n",
      "Iteracion: 19331 Gradiente: [1.3819286423692272e-05,-0.00023841998702624304] Loss: 22.8427367888536\n",
      "Iteracion: 19332 Gradiente: [1.3811939757601976e-05,-0.0002382932356368883] Loss: 22.84273678879658\n",
      "Iteracion: 19333 Gradiente: [1.3804596870651645e-05,-0.00023816655164038043] Loss: 22.842736788739625\n",
      "Iteracion: 19334 Gradiente: [1.3797257944740219e-05,-0.00023803993499017887] Loss: 22.84273678868273\n",
      "Iteracion: 19335 Gradiente: [1.3789922825443075e-05,-0.00023791338565845404] Loss: 22.842736788625874\n",
      "Iteracion: 19336 Gradiente: [1.3782591702238279e-05,-0.00023778690359913904] Loss: 22.842736788569105\n",
      "Iteracion: 19337 Gradiente: [1.3775264456702037e-05,-0.00023766048878286482] Loss: 22.84273678851239\n",
      "Iteracion: 19338 Gradiente: [1.3767941107782159e-05,-0.00023753414117197262] Loss: 22.842736788455735\n",
      "Iteracion: 19339 Gradiente: [1.3760621700005989e-05,-0.00023740786072785626] Loss: 22.84273678839914\n",
      "Iteracion: 19340 Gradiente: [1.3753306091264978e-05,-0.00023728164742422563] Loss: 22.842736788342588\n",
      "Iteracion: 19341 Gradiente: [1.3745994399982919e-05,-0.00023715550121821137] Loss: 22.84273678828613\n",
      "Iteracion: 19342 Gradiente: [1.3738686617633296e-05,-0.00023702942207393107] Loss: 22.842736788229693\n",
      "Iteracion: 19343 Gradiente: [1.3731382724320914e-05,-0.0002369034099569234] Loss: 22.842736788173355\n",
      "Iteracion: 19344 Gradiente: [1.3724082699203184e-05,-0.00023677746483189802] Loss: 22.84273678811705\n",
      "Iteracion: 19345 Gradiente: [1.3716786560280525e-05,-0.00023665158666368312] Loss: 22.842736788060815\n",
      "Iteracion: 19346 Gradiente: [1.3709494251656906e-05,-0.00023652577541805425] Loss: 22.842736788004636\n",
      "Iteracion: 19347 Gradiente: [1.3702205904072191e-05,-0.00023640003105439197] Loss: 22.84273678794853\n",
      "Iteracion: 19348 Gradiente: [1.3694921376365225e-05,-0.00023627435354344565] Loss: 22.842736787892466\n",
      "Iteracion: 19349 Gradiente: [1.3687640747169401e-05,-0.00023614874284305643] Loss: 22.84273678783646\n",
      "Iteracion: 19350 Gradiente: [1.3680363972904766e-05,-0.00023602319892302622] Loss: 22.842736787780517\n",
      "Iteracion: 19351 Gradiente: [1.3673091052623931e-05,-0.00023589772174711736] Loss: 22.842736787724643\n",
      "Iteracion: 19352 Gradiente: [1.3665822012853824e-05,-0.0002357723112783816] Loss: 22.842736787668823\n",
      "Iteracion: 19353 Gradiente: [1.3658556868752688e-05,-0.00023564696747833125] Loss: 22.842736787613074\n",
      "Iteracion: 19354 Gradiente: [1.3651295503791516e-05,-0.0002355216903206762] Loss: 22.842736787557367\n",
      "Iteracion: 19355 Gradiente: [1.3644038109343152e-05,-0.00023539647975769166] Loss: 22.842736787501718\n",
      "Iteracion: 19356 Gradiente: [1.36367845186669e-05,-0.00023527133576415338] Loss: 22.84273678744614\n",
      "Iteracion: 19357 Gradiente: [1.362953479144835e-05,-0.0002351462582995604] Loss: 22.84273678739062\n",
      "Iteracion: 19358 Gradiente: [1.3622288844317153e-05,-0.00023502124733489893] Loss: 22.842736787335145\n",
      "Iteracion: 19359 Gradiente: [1.3615046845908789e-05,-0.00023489630282445736] Loss: 22.84273678727974\n",
      "Iteracion: 19360 Gradiente: [1.3607808685378587e-05,-0.00023477142473862973] Loss: 22.842736787224396\n",
      "Iteracion: 19361 Gradiente: [1.3600574405359112e-05,-0.00023464661304011257] Loss: 22.84273678716911\n",
      "Iteracion: 19362 Gradiente: [1.3593343890268746e-05,-0.00023452186769977365] Loss: 22.842736787113882\n",
      "Iteracion: 19363 Gradiente: [1.3586117281268647e-05,-0.0002343971886752172] Loss: 22.842736787058712\n",
      "Iteracion: 19364 Gradiente: [1.3578894398354653e-05,-0.00023427257594003473] Loss: 22.8427367870036\n",
      "Iteracion: 19365 Gradiente: [1.3571675447110465e-05,-0.00023414802944733045] Loss: 22.842736786948535\n",
      "Iteracion: 19366 Gradiente: [1.3564460405746104e-05,-0.00023402354916323513] Loss: 22.84273678689355\n",
      "Iteracion: 19367 Gradiente: [1.355724912267912e-05,-0.0002338991350615771] Loss: 22.84273678683861\n",
      "Iteracion: 19368 Gradiente: [1.3550041612120366e-05,-0.00023377478710517134] Loss: 22.842736786783743\n",
      "Iteracion: 19369 Gradiente: [1.3542838107127864e-05,-0.00023365050524593774] Loss: 22.842736786728928\n",
      "Iteracion: 19370 Gradiente: [1.3535638269483266e-05,-0.0002335262894674154] Loss: 22.842736786674152\n",
      "Iteracion: 19371 Gradiente: [1.3528442306665055e-05,-0.00023340213972507703] Loss: 22.84273678661946\n",
      "Iteracion: 19372 Gradiente: [1.3521250150461128e-05,-0.00023327805598446125] Loss: 22.84273678656481\n",
      "Iteracion: 19373 Gradiente: [1.3514061865294025e-05,-0.00023315403820731718] Loss: 22.84273678651021\n",
      "Iteracion: 19374 Gradiente: [1.3506877393372936e-05,-0.0002330300863630915] Loss: 22.8427367864557\n",
      "Iteracion: 19375 Gradiente: [1.3499696687328347e-05,-0.00023290620041862553] Loss: 22.84273678640123\n",
      "Iteracion: 19376 Gradiente: [1.3492519820109312e-05,-0.0002327823803346026] Loss: 22.84273678634681\n",
      "Iteracion: 19377 Gradiente: [1.3485346770873246e-05,-0.00023265862607596926] Loss: 22.84273678629245\n",
      "Iteracion: 19378 Gradiente: [1.3478177547199265e-05,-0.00023253493760885628] Loss: 22.84273678623815\n",
      "Iteracion: 19379 Gradiente: [1.347101215571911e-05,-0.00023241131489761812] Loss: 22.842736786183917\n",
      "Iteracion: 19380 Gradiente: [1.3463850575590185e-05,-0.00023228775790660924] Loss: 22.842736786129727\n",
      "Iteracion: 19381 Gradiente: [1.3456692795443814e-05,-0.0002321642666053947] Loss: 22.842736786075616\n",
      "Iteracion: 19382 Gradiente: [1.3449538766015697e-05,-0.00023204084095643414] Loss: 22.842736786021543\n",
      "Iteracion: 19383 Gradiente: [1.3442388561202278e-05,-0.00023191748092420047] Loss: 22.84273678596753\n",
      "Iteracion: 19384 Gradiente: [1.3435242174371827e-05,-0.00023179418647233755] Loss: 22.842736785913576\n",
      "Iteracion: 19385 Gradiente: [1.3428099522153996e-05,-0.00023167095757230526] Loss: 22.842736785859678\n",
      "Iteracion: 19386 Gradiente: [1.3420960787395112e-05,-0.00023154779417697096] Loss: 22.84273678580585\n",
      "Iteracion: 19387 Gradiente: [1.3413825793880581e-05,-0.00023142469626122875] Loss: 22.842736785752074\n",
      "Iteracion: 19388 Gradiente: [1.3406694574769062e-05,-0.00023130166379026207] Loss: 22.84273678569833\n",
      "Iteracion: 19389 Gradiente: [1.3399567165113997e-05,-0.000231178696726649] Loss: 22.842736785644682\n",
      "Iteracion: 19390 Gradiente: [1.3392443556388874e-05,-0.00023105579503379658] Loss: 22.842736785591068\n",
      "Iteracion: 19391 Gradiente: [1.3385323656696831e-05,-0.0002309329586851779] Loss: 22.842736785537518\n",
      "Iteracion: 19392 Gradiente: [1.3378207646042028e-05,-0.00023081018763354184] Loss: 22.842736785484025\n",
      "Iteracion: 19393 Gradiente: [1.3371095371894626e-05,-0.00023068748185437472] Loss: 22.842736785430585\n",
      "Iteracion: 19394 Gradiente: [1.336398691194063e-05,-0.00023056484130634658] Loss: 22.84273678537721\n",
      "Iteracion: 19395 Gradiente: [1.3356882292706966e-05,-0.00023044226595428559] Loss: 22.842736785323876\n",
      "Iteracion: 19396 Gradiente: [1.3349781343663381e-05,-0.00023031975577296747] Loss: 22.842736785270606\n",
      "Iteracion: 19397 Gradiente: [1.3342684210707982e-05,-0.00023019731072047023] Loss: 22.842736785217404\n",
      "Iteracion: 19398 Gradiente: [1.3335590770680028e-05,-0.00023007493076813526] Loss: 22.842736785164252\n",
      "Iteracion: 19399 Gradiente: [1.3328501126845065e-05,-0.00022995261587463272] Loss: 22.84273678511114\n",
      "Iteracion: 19400 Gradiente: [1.332141528015048e-05,-0.0002298303660058565] Loss: 22.842736785058115\n",
      "Iteracion: 19401 Gradiente: [1.3314333167121124e-05,-0.00022970818113172698] Loss: 22.84273678500512\n",
      "Iteracion: 19402 Gradiente: [1.3307254949760742e-05,-0.00022958606120534835] Loss: 22.842736784952194\n",
      "Iteracion: 19403 Gradiente: [1.3300180402590437e-05,-0.00022946400620907543] Loss: 22.84273678489932\n",
      "Iteracion: 19404 Gradiente: [1.329310958340102e-05,-0.00022934201610157838] Loss: 22.842736784846508\n",
      "Iteracion: 19405 Gradiente: [1.328604261724801e-05,-0.00022922009084318516] Loss: 22.84273678479374\n",
      "Iteracion: 19406 Gradiente: [1.3278979264441659e-05,-0.00022909823041257957] Loss: 22.842736784741046\n",
      "Iteracion: 19407 Gradiente: [1.327191977319823e-05,-0.0002289764347603788] Loss: 22.84273678468838\n",
      "Iteracion: 19408 Gradiente: [1.3264864011830469e-05,-0.00022885470385887176] Loss: 22.8427367846358\n",
      "Iteracion: 19409 Gradiente: [1.3257812004023132e-05,-0.00022873303767395232] Loss: 22.842736784583263\n",
      "Iteracion: 19410 Gradiente: [1.3250763758302735e-05,-0.00022861143616914603] Loss: 22.842736784530782\n",
      "Iteracion: 19411 Gradiente: [1.3243719211194123e-05,-0.00022848989931496534] Loss: 22.842736784478348\n",
      "Iteracion: 19412 Gradiente: [1.3236678452699379e-05,-0.00022836842707079085] Loss: 22.84273678442599\n",
      "Iteracion: 19413 Gradiente: [1.3229641433554207e-05,-0.00022824701940393766] Loss: 22.842736784373663\n",
      "Iteracion: 19414 Gradiente: [1.3222608205865072e-05,-0.00022812567628029967] Loss: 22.842736784321414\n",
      "Iteracion: 19415 Gradiente: [1.3215578716578117e-05,-0.00022800439766553401] Loss: 22.842736784269217\n",
      "Iteracion: 19416 Gradiente: [1.3208552903165583e-05,-0.00022788318352967945] Loss: 22.84273678421706\n",
      "Iteracion: 19417 Gradiente: [1.320153080352308e-05,-0.00022776203383543248] Loss: 22.842736784164973\n",
      "Iteracion: 19418 Gradiente: [1.319451244986188e-05,-0.00022764094854809495] Loss: 22.842736784112933\n",
      "Iteracion: 19419 Gradiente: [1.3187497875340645e-05,-0.00022751992763190288] Loss: 22.842736784060953\n",
      "Iteracion: 19420 Gradiente: [1.3180486925534752e-05,-0.0002273989710571319] Loss: 22.842736784009023\n",
      "Iteracion: 19421 Gradiente: [1.3173479780448361e-05,-0.00022727807878446525] Loss: 22.84273678395716\n",
      "Iteracion: 19422 Gradiente: [1.3166476317868122e-05,-0.00022715725078358655] Loss: 22.84273678390535\n",
      "Iteracion: 19423 Gradiente: [1.3159476621164383e-05,-0.0002270364870156527] Loss: 22.84273678385359\n",
      "Iteracion: 19424 Gradiente: [1.3152480666652385e-05,-0.0002269157874480972] Loss: 22.84273678380188\n",
      "Iteracion: 19425 Gradiente: [1.3145488370961781e-05,-0.00022679515205332735] Loss: 22.842736783750226\n",
      "Iteracion: 19426 Gradiente: [1.31384998487268e-05,-0.00022667458078740784] Loss: 22.84273678369863\n",
      "Iteracion: 19427 Gradiente: [1.3131514997629286e-05,-0.0002265540736229828] Loss: 22.84273678364709\n",
      "Iteracion: 19428 Gradiente: [1.312453390577654e-05,-0.0002264336305227488] Loss: 22.842736783595612\n",
      "Iteracion: 19429 Gradiente: [1.3117556526746436e-05,-0.00022631325145283654] Loss: 22.84273678354418\n",
      "Iteracion: 19430 Gradiente: [1.3110582802748164e-05,-0.00022619293638375854] Loss: 22.842736783492803\n",
      "Iteracion: 19431 Gradiente: [1.3103612809572951e-05,-0.00022607268527572444] Loss: 22.84273678344148\n",
      "Iteracion: 19432 Gradiente: [1.3096646508377793e-05,-0.00022595249809735188] Loss: 22.842736783390215\n",
      "Iteracion: 19433 Gradiente: [1.308968399200694e-05,-0.0002258323748118111] Loss: 22.84273678333901\n",
      "Iteracion: 19434 Gradiente: [1.3082725093719697e-05,-0.00022571231538914087] Loss: 22.84273678328784\n",
      "Iteracion: 19435 Gradiente: [1.3075769882675558e-05,-0.00022559231979677464] Loss: 22.842736783236738\n",
      "Iteracion: 19436 Gradiente: [1.3068818441297481e-05,-0.00022547238799279037] Loss: 22.842736783185686\n",
      "Iteracion: 19437 Gradiente: [1.3061870630319087e-05,-0.00022535251995220069] Loss: 22.842736783134693\n",
      "Iteracion: 19438 Gradiente: [1.3054926553953312e-05,-0.00022523271563462305] Loss: 22.842736783083755\n",
      "Iteracion: 19439 Gradiente: [1.30479862178845e-05,-0.00022511297500535932] Loss: 22.842736783032866\n",
      "Iteracion: 19440 Gradiente: [1.3041049456319342e-05,-0.00022499329804060627] Loss: 22.84273678298204\n",
      "Iteracion: 19441 Gradiente: [1.3034116498526298e-05,-0.00022487368469370494] Loss: 22.84273678293126\n",
      "Iteracion: 19442 Gradiente: [1.3027187126605592e-05,-0.0002247541349431022] Loss: 22.842736782880525\n",
      "Iteracion: 19443 Gradiente: [1.3020261487402725e-05,-0.00022463464874415232] Loss: 22.842736782829864\n",
      "Iteracion: 19444 Gradiente: [1.3013339474809981e-05,-0.00022451522607281523] Loss: 22.842736782779248\n",
      "Iteracion: 19445 Gradiente: [1.3006421132407317e-05,-0.00022439586689036636] Loss: 22.842736782728682\n",
      "Iteracion: 19446 Gradiente: [1.2999506594724153e-05,-0.00022427657115535736] Loss: 22.842736782678177\n",
      "Iteracion: 19447 Gradiente: [1.2992595650492451e-05,-0.00022415733884682728] Loss: 22.84273678262772\n",
      "Iteracion: 19448 Gradiente: [1.2985688334765655e-05,-0.0002240381699285384] Loss: 22.842736782577308\n",
      "Iteracion: 19449 Gradiente: [1.2978784775441455e-05,-0.00022391906435845026] Loss: 22.84273678252696\n",
      "Iteracion: 19450 Gradiente: [1.2971884890096893e-05,-0.0002238000221097991] Loss: 22.84273678247668\n",
      "Iteracion: 19451 Gradiente: [1.2964988626625502e-05,-0.00022368104314859731] Loss: 22.842736782426435\n",
      "Iteracion: 19452 Gradiente: [1.295809603523897e-05,-0.00022356212744050196] Loss: 22.842736782376235\n",
      "Iteracion: 19453 Gradiente: [1.2951207106463395e-05,-0.00022344327495152545] Loss: 22.842736782326117\n",
      "Iteracion: 19454 Gradiente: [1.2944321884826119e-05,-0.00022332448564649592] Loss: 22.84273678227602\n",
      "Iteracion: 19455 Gradiente: [1.2937440305904602e-05,-0.00022320575949379416] Loss: 22.842736782226\n",
      "Iteracion: 19456 Gradiente: [1.2930562376330574e-05,-0.00022308709646026159] Loss: 22.84273678217603\n",
      "Iteracion: 19457 Gradiente: [1.2923688075261452e-05,-0.00022296849651202896] Loss: 22.842736782126103\n",
      "Iteracion: 19458 Gradiente: [1.29168173960655e-05,-0.00022284995961747712] Loss: 22.84273678207624\n",
      "Iteracion: 19459 Gradiente: [1.2909950396533531e-05,-0.00022273148574107892] Loss: 22.842736782026414\n",
      "Iteracion: 19460 Gradiente: [1.290308706908642e-05,-0.00022261307484576774] Loss: 22.84273678197665\n",
      "Iteracion: 19461 Gradiente: [1.289622742698763e-05,-0.00022249472690099025] Loss: 22.842736781926945\n",
      "Iteracion: 19462 Gradiente: [1.2889371374550744e-05,-0.00022237644187607467] Loss: 22.842736781877296\n",
      "Iteracion: 19463 Gradiente: [1.2882518993251324e-05,-0.000222258219732178] Loss: 22.842736781827682\n",
      "Iteracion: 19464 Gradiente: [1.2875670221509001e-05,-0.00022214006044312857] Loss: 22.84273678177814\n",
      "Iteracion: 19465 Gradiente: [1.2868825103851123e-05,-0.0002220219639685439] Loss: 22.842736781728625\n",
      "Iteracion: 19466 Gradiente: [1.2861983671541566e-05,-0.00022190393027656796] Loss: 22.842736781679193\n",
      "Iteracion: 19467 Gradiente: [1.2855145822262178e-05,-0.0002217859593374764] Loss: 22.842736781629796\n",
      "Iteracion: 19468 Gradiente: [1.2848311600540304e-05,-0.0002216680511158605] Loss: 22.842736781580445\n",
      "Iteracion: 19469 Gradiente: [1.2841481058482411e-05,-0.000221550205574772] Loss: 22.842736781531165\n",
      "Iteracion: 19470 Gradiente: [1.2834654217878475e-05,-0.00022143242268093388] Loss: 22.842736781481925\n",
      "Iteracion: 19471 Gradiente: [1.2827830885460874e-05,-0.0002213147024099508] Loss: 22.842736781432748\n",
      "Iteracion: 19472 Gradiente: [1.2821011257339402e-05,-0.00022119704471883493] Loss: 22.842736781383614\n",
      "Iteracion: 19473 Gradiente: [1.281419515824685e-05,-0.00022107944958401997] Loss: 22.84273678133453\n",
      "Iteracion: 19474 Gradiente: [1.2807382756818696e-05,-0.0002209619169608601] Loss: 22.842736781285506\n",
      "Iteracion: 19475 Gradiente: [1.2800573921367686e-05,-0.00022084444682567058] Loss: 22.84273678123652\n",
      "Iteracion: 19476 Gradiente: [1.2793768765580657e-05,-0.00022072703913818734] Loss: 22.842736781187615\n",
      "Iteracion: 19477 Gradiente: [1.278696723829853e-05,-0.000220609693867857] Loss: 22.84273678113874\n",
      "Iteracion: 19478 Gradiente: [1.2780169240045324e-05,-0.00022049241098448153] Loss: 22.842736781089922\n",
      "Iteracion: 19479 Gradiente: [1.2773374973562567e-05,-0.00022037519044886266] Loss: 22.842736781041136\n",
      "Iteracion: 19480 Gradiente: [1.2766584212423974e-05,-0.00022025803223589454] Loss: 22.842736780992432\n",
      "Iteracion: 19481 Gradiente: [1.2759797092106359e-05,-0.00022014093630543148] Loss: 22.842736780943774\n",
      "Iteracion: 19482 Gradiente: [1.2753013648610552e-05,-0.00022002390262348587] Loss: 22.842736780895166\n",
      "Iteracion: 19483 Gradiente: [1.2746233735091057e-05,-0.00021990693116364924] Loss: 22.84273678084661\n",
      "Iteracion: 19484 Gradiente: [1.2739457477550787e-05,-0.00021979002188731537] Loss: 22.842736780798095\n",
      "Iteracion: 19485 Gradiente: [1.2732684795461562e-05,-0.00021967317476393096] Loss: 22.84273678074962\n",
      "Iteracion: 19486 Gradiente: [1.272591567934948e-05,-0.00021955638976329795] Loss: 22.84273678070123\n",
      "Iteracion: 19487 Gradiente: [1.271915015858364e-05,-0.0002194396668495339] Loss: 22.842736780652874\n",
      "Iteracion: 19488 Gradiente: [1.2712388250217069e-05,-0.0002193230059881775] Loss: 22.84273678060457\n",
      "Iteracion: 19489 Gradiente: [1.2705630004461454e-05,-0.00021920640714358323] Loss: 22.842736780556326\n",
      "Iteracion: 19490 Gradiente: [1.269887522615439e-05,-0.00021908987029253997] Loss: 22.842736780508115\n",
      "Iteracion: 19491 Gradiente: [1.2692124137932599e-05,-0.0002189733953920599] Loss: 22.842736780459973\n",
      "Iteracion: 19492 Gradiente: [1.2685376594845365e-05,-0.00021885698241431347] Loss: 22.842736780411858\n",
      "Iteracion: 19493 Gradiente: [1.2678632713421696e-05,-0.00021874063132152344] Loss: 22.84273678036383\n",
      "Iteracion: 19494 Gradiente: [1.2671892252077062e-05,-0.0002186243420952157] Loss: 22.84273678031583\n",
      "Iteracion: 19495 Gradiente: [1.2665155567977611e-05,-0.00021850811467925743] Loss: 22.842736780267884\n",
      "Iteracion: 19496 Gradiente: [1.2658422400591008e-05,-0.00021839194905706922] Loss: 22.84273678021999\n",
      "Iteracion: 19497 Gradiente: [1.2651692858867137e-05,-0.000218275845190045] Loss: 22.842736780172157\n",
      "Iteracion: 19498 Gradiente: [1.2644966848066967e-05,-0.00021815980304893401] Loss: 22.84273678012436\n",
      "Iteracion: 19499 Gradiente: [1.2638244381453963e-05,-0.00021804382260247243] Loss: 22.842736780076617\n",
      "Iteracion: 19500 Gradiente: [1.2631525440080319e-05,-0.00021792790381690945] Loss: 22.842736780028932\n",
      "Iteracion: 19501 Gradiente: [1.2624810165107191e-05,-0.0002178120466520994] Loss: 22.84273677998128\n",
      "Iteracion: 19502 Gradiente: [1.2618098449479475e-05,-0.00021769625108068643] Loss: 22.8427367799337\n",
      "Iteracion: 19503 Gradiente: [1.2611390296039342e-05,-0.00021758051707045922] Loss: 22.84273677988615\n",
      "Iteracion: 19504 Gradiente: [1.2604685665943787e-05,-0.00021746484459169343] Loss: 22.84273677983868\n",
      "Iteracion: 19505 Gradiente: [1.2597984554455858e-05,-0.0002173492336088619] Loss: 22.84273677979123\n",
      "Iteracion: 19506 Gradiente: [1.2591287046840687e-05,-0.00021723368408620066] Loss: 22.84273677974386\n",
      "Iteracion: 19507 Gradiente: [1.2584593128887415e-05,-0.00021711819599291952] Loss: 22.842736779696516\n",
      "Iteracion: 19508 Gradiente: [1.2577902763647823e-05,-0.00021700276929657037] Loss: 22.84273677964922\n",
      "Iteracion: 19509 Gradiente: [1.2571216009860109e-05,-0.00021688740395996812] Loss: 22.842736779601985\n",
      "Iteracion: 19510 Gradiente: [1.2564532814470416e-05,-0.00021677209995694114] Loss: 22.84273677955481\n",
      "Iteracion: 19511 Gradiente: [1.2557853150951814e-05,-0.00021665685725219913] Loss: 22.842736779507668\n",
      "Iteracion: 19512 Gradiente: [1.2551177011725182e-05,-0.0002165416758170835] Loss: 22.842736779460587\n",
      "Iteracion: 19513 Gradiente: [1.254450439679052e-05,-0.0002164265556172514] Loss: 22.842736779413553\n",
      "Iteracion: 19514 Gradiente: [1.2537835348780391e-05,-0.0002163114966174125] Loss: 22.842736779366568\n",
      "Iteracion: 19515 Gradiente: [1.2531169797587912e-05,-0.00021619649878914515] Loss: 22.842736779319626\n",
      "Iteracion: 19516 Gradiente: [1.2524507837004724e-05,-0.00021608156209396157] Loss: 22.842736779272748\n",
      "Iteracion: 19517 Gradiente: [1.2517849418713922e-05,-0.00021596668650450586] Loss: 22.84273677922591\n",
      "Iteracion: 19518 Gradiente: [1.2511194567347654e-05,-0.00021585187198394824] Loss: 22.842736779179123\n",
      "Iteracion: 19519 Gradiente: [1.2504543276274186e-05,-0.00021573711850043272] Loss: 22.842736779132387\n",
      "Iteracion: 19520 Gradiente: [1.2497895519913982e-05,-0.0002156224260236428] Loss: 22.84273677908571\n",
      "Iteracion: 19521 Gradiente: [1.2491251182685421e-05,-0.00021550779452752522] Loss: 22.842736779039058\n",
      "Iteracion: 19522 Gradiente: [1.2484610475856547e-05,-0.0002153932239667237] Loss: 22.842736778992485\n",
      "Iteracion: 19523 Gradiente: [1.2477973280056176e-05,-0.00021527871431743507] Loss: 22.84273677894593\n",
      "Iteracion: 19524 Gradiente: [1.2471339577283894e-05,-0.0002151642655473296] Loss: 22.842736778899454\n",
      "Iteracion: 19525 Gradiente: [1.2464709474594807e-05,-0.00021504987761720903] Loss: 22.84273677885301\n",
      "Iteracion: 19526 Gradiente: [1.245808283366993e-05,-0.00021493555050161223] Loss: 22.842736778806614\n",
      "Iteracion: 19527 Gradiente: [1.2451459734090045e-05,-0.0002148212841659595] Loss: 22.842736778760283\n",
      "Iteracion: 19528 Gradiente: [1.2444840200487306e-05,-0.00021470707857531579] Loss: 22.84273677871398\n",
      "Iteracion: 19529 Gradiente: [1.2438224123911823e-05,-0.00021459293370256204] Loss: 22.842736778667753\n",
      "Iteracion: 19530 Gradiente: [1.2431611488257961e-05,-0.00021447884951678967] Loss: 22.842736778621564\n",
      "Iteracion: 19531 Gradiente: [1.2425002451739904e-05,-0.00021436482597903724] Loss: 22.84273677857541\n",
      "Iteracion: 19532 Gradiente: [1.241839704372675e-05,-0.00021425086305093544] Loss: 22.842736778529318\n",
      "Iteracion: 19533 Gradiente: [1.241179501789702e-05,-0.00021413696071779972] Loss: 22.842736778483275\n",
      "Iteracion: 19534 Gradiente: [1.2405196573202678e-05,-0.00021402311893332638] Loss: 22.84273677843727\n",
      "Iteracion: 19535 Gradiente: [1.239860154763998e-05,-0.0002139093376770281] Loss: 22.842736778391327\n",
      "Iteracion: 19536 Gradiente: [1.2392010121213086e-05,-0.00021379561690354859] Loss: 22.84273677834543\n",
      "Iteracion: 19537 Gradiente: [1.238542215465562e-05,-0.00021368195659050572] Loss: 22.842736778299578\n",
      "Iteracion: 19538 Gradiente: [1.2378837629967165e-05,-0.00021356835670568823] Loss: 22.84273677825377\n",
      "Iteracion: 19539 Gradiente: [1.2372256571779872e-05,-0.00021345481721771382] Loss: 22.84273677820801\n",
      "Iteracion: 19540 Gradiente: [1.2365679124097066e-05,-0.00021334133808406837] Loss: 22.84273677816233\n",
      "Iteracion: 19541 Gradiente: [1.2359105161863226e-05,-0.00021322791927988287] Loss: 22.842736778116663\n",
      "Iteracion: 19542 Gradiente: [1.2352534717289625e-05,-0.00021311456077152495] Loss: 22.842736778071068\n",
      "Iteracion: 19543 Gradiente: [1.2345967706058521e-05,-0.00021300126253021763] Loss: 22.8427367780255\n",
      "Iteracion: 19544 Gradiente: [1.2339404189750288e-05,-0.00021288802452303912] Loss: 22.842736777979997\n",
      "Iteracion: 19545 Gradiente: [1.2332844182575779e-05,-0.00021277484671505437] Loss: 22.842736777934537\n",
      "Iteracion: 19546 Gradiente: [1.2326287716746265e-05,-0.0002126617290722758] Loss: 22.842736777889115\n",
      "Iteracion: 19547 Gradiente: [1.2319734662469272e-05,-0.00021254867157066332] Loss: 22.842736777843758\n",
      "Iteracion: 19548 Gradiente: [1.2313185122062957e-05,-0.0002124356741724398] Loss: 22.842736777798443\n",
      "Iteracion: 19549 Gradiente: [1.2306639100264268e-05,-0.0002123227368452755] Loss: 22.842736777753178\n",
      "Iteracion: 19550 Gradiente: [1.2300096510860688e-05,-0.00021220985956027505] Loss: 22.842736777707945\n",
      "Iteracion: 19551 Gradiente: [1.2293557429643442e-05,-0.0002120970422834508] Loss: 22.842736777662772\n",
      "Iteracion: 19552 Gradiente: [1.2287021735346571e-05,-0.00021198428498839423] Loss: 22.842736777617645\n",
      "Iteracion: 19553 Gradiente: [1.2280489487655662e-05,-0.0002118715876413546] Loss: 22.842736777572565\n",
      "Iteracion: 19554 Gradiente: [1.2273960849521852e-05,-0.00021175895020040988] Loss: 22.842736777527566\n",
      "Iteracion: 19555 Gradiente: [1.2267435585992341e-05,-0.00021164637264507272] Loss: 22.84273677748257\n",
      "Iteracion: 19556 Gradiente: [1.2260913915914292e-05,-0.00021153385493211848] Loss: 22.84273677743763\n",
      "Iteracion: 19557 Gradiente: [1.225439570191611e-05,-0.0002114213970379808] Loss: 22.84273677739275\n",
      "Iteracion: 19558 Gradiente: [1.224788087010135e-05,-0.00021130899893506694] Loss: 22.842736777347913\n",
      "Iteracion: 19559 Gradiente: [1.224136950289297e-05,-0.0002111966605855997] Loss: 22.842736777303127\n",
      "Iteracion: 19560 Gradiente: [1.223486160408053e-05,-0.00021108438195949948] Loss: 22.842736777258384\n",
      "Iteracion: 19561 Gradiente: [1.2228357131031467e-05,-0.00021097216302585765] Loss: 22.842736777213684\n",
      "Iteracion: 19562 Gradiente: [1.2221856190800887e-05,-0.00021086000374630486] Loss: 22.842736777169037\n",
      "Iteracion: 19563 Gradiente: [1.2215358687702367e-05,-0.00021074790409609062] Loss: 22.842736777124443\n",
      "Iteracion: 19564 Gradiente: [1.2208864592366809e-05,-0.00021063586404324042] Loss: 22.842736777079885\n",
      "Iteracion: 19565 Gradiente: [1.2202374011849316e-05,-0.0002105238835509245] Loss: 22.842736777035377\n",
      "Iteracion: 19566 Gradiente: [1.2195886840989563e-05,-0.00021041196259356335] Loss: 22.842736776990918\n",
      "Iteracion: 19567 Gradiente: [1.2189403202948293e-05,-0.00021030010113077443] Loss: 22.842736776946513\n",
      "Iteracion: 19568 Gradiente: [1.2182922890247028e-05,-0.000210188299145031] Loss: 22.842736776902154\n",
      "Iteracion: 19569 Gradiente: [1.2176446062994728e-05,-0.00021007655659417425] Loss: 22.842736776857826\n",
      "Iteracion: 19570 Gradiente: [1.2169972731612688e-05,-0.00020996487344611127] Loss: 22.842736776813567\n",
      "Iteracion: 19571 Gradiente: [1.2163502777677119e-05,-0.00020985324967585464] Loss: 22.842736776769353\n",
      "Iteracion: 19572 Gradiente: [1.2157036243820584e-05,-0.00020974168524965365] Loss: 22.842736776725175\n",
      "Iteracion: 19573 Gradiente: [1.2150573182149553e-05,-0.00020963018013162583] Loss: 22.842736776681036\n",
      "Iteracion: 19574 Gradiente: [1.2144113590769241e-05,-0.00020951873429252051] Loss: 22.84273677663696\n",
      "Iteracion: 19575 Gradiente: [1.2137657497153972e-05,-0.00020940734769621847] Loss: 22.842736776592922\n",
      "Iteracion: 19576 Gradiente: [1.2131204633192283e-05,-0.00020929602032874564] Loss: 22.842736776548936\n",
      "Iteracion: 19577 Gradiente: [1.2124755335207737e-05,-0.0002091847521394167] Loss: 22.842736776505003\n",
      "Iteracion: 19578 Gradiente: [1.2118309409932711e-05,-0.00020907354310584954] Loss: 22.842736776461102\n",
      "Iteracion: 19579 Gradiente: [1.2111866937895381e-05,-0.00020896239319429338] Loss: 22.84273677641726\n",
      "Iteracion: 19580 Gradiente: [1.2105427996781752e-05,-0.00020885130236578676] Loss: 22.84273677637346\n",
      "Iteracion: 19581 Gradiente: [1.2098992351639027e-05,-0.0002087402706035135] Loss: 22.842736776329705\n",
      "Iteracion: 19582 Gradiente: [1.209256023363044e-05,-0.00020862929786436742] Loss: 22.842736776286007\n",
      "Iteracion: 19583 Gradiente: [1.2086131365170634e-05,-0.00020851838413046646] Loss: 22.842736776242326\n",
      "Iteracion: 19584 Gradiente: [1.2079706040897993e-05,-0.00020840752935467796] Loss: 22.842736776198716\n",
      "Iteracion: 19585 Gradiente: [1.207328402396494e-05,-0.00020829673351935679] Loss: 22.842736776155157\n",
      "Iteracion: 19586 Gradiente: [1.2066865560692955e-05,-0.00020818599657819922] Loss: 22.84273677611163\n",
      "Iteracion: 19587 Gradiente: [1.2060450449287904e-05,-0.00020807531851273117] Loss: 22.84273677606816\n",
      "Iteracion: 19588 Gradiente: [1.2054038746593202e-05,-0.00020796469928564912] Loss: 22.842736776024726\n",
      "Iteracion: 19589 Gradiente: [1.2047630506610101e-05,-0.0002078541388646234] Loss: 22.842736775981333\n",
      "Iteracion: 19590 Gradiente: [1.2041225595756562e-05,-0.00020774363722573242] Loss: 22.842736775938004\n",
      "Iteracion: 19591 Gradiente: [1.2034824104982059e-05,-0.0002076331943314358] Loss: 22.84273677589471\n",
      "Iteracion: 19592 Gradiente: [1.2028426037128762e-05,-0.00020752281015177232] Loss: 22.84273677585147\n",
      "Iteracion: 19593 Gradiente: [1.2022031321142397e-05,-0.00020741248465772817] Loss: 22.84273677580828\n",
      "Iteracion: 19594 Gradiente: [1.2015639995865968e-05,-0.00020730221781673682] Loss: 22.842736775765125\n",
      "Iteracion: 19595 Gradiente: [1.2009252146564602e-05,-0.00020719200959303427] Loss: 22.84273677572202\n",
      "Iteracion: 19596 Gradiente: [1.2002867681341437e-05,-0.00020708185995997516] Loss: 22.842736775678965\n",
      "Iteracion: 19597 Gradiente: [1.1996486587880403e-05,-0.0002069717688871246] Loss: 22.842736775635935\n",
      "Iteracion: 19598 Gradiente: [1.1990108819759371e-05,-0.00020686173634523186] Loss: 22.842736775592968\n",
      "Iteracion: 19599 Gradiente: [1.1983734500139084e-05,-0.00020675176229711194] Loss: 22.842736775550044\n",
      "Iteracion: 19600 Gradiente: [1.197736362807215e-05,-0.00020664184671126407] Loss: 22.842736775507166\n",
      "Iteracion: 19601 Gradiente: [1.197099610881954e-05,-0.00020653198956163503] Loss: 22.842736775464328\n",
      "Iteracion: 19602 Gradiente: [1.1964631930065177e-05,-0.000206422190818382] Loss: 22.842736775421542\n",
      "Iteracion: 19603 Gradiente: [1.195827117896897e-05,-0.00020631245044467524] Loss: 22.842736775378807\n",
      "Iteracion: 19604 Gradiente: [1.1951913718159328e-05,-0.00020620276841825101] Loss: 22.842736775336107\n",
      "Iteracion: 19605 Gradiente: [1.1945559749430383e-05,-0.00020609314469505574] Loss: 22.842736775293442\n",
      "Iteracion: 19606 Gradiente: [1.1939209177095714e-05,-0.00020598357925033877] Loss: 22.842736775250845\n",
      "Iteracion: 19607 Gradiente: [1.1932861911153244e-05,-0.00020587407205792849] Loss: 22.84273677520829\n",
      "Iteracion: 19608 Gradiente: [1.1926518053921124e-05,-0.00020576462308075823] Loss: 22.842736775165765\n",
      "Iteracion: 19609 Gradiente: [1.1920177583609377e-05,-0.00020565523228981418] Loss: 22.842736775123296\n",
      "Iteracion: 19610 Gradiente: [1.1913840421584609e-05,-0.0002055458996579773] Loss: 22.842736775080873\n",
      "Iteracion: 19611 Gradiente: [1.190750669479712e-05,-0.00020543662514604932] Loss: 22.842736775038492\n",
      "Iteracion: 19612 Gradiente: [1.1901176251664463e-05,-0.00020532740873247708] Loss: 22.84273677499615\n",
      "Iteracion: 19613 Gradiente: [1.1894849203978689e-05,-0.0002052182503803124] Loss: 22.842736774953867\n",
      "Iteracion: 19614 Gradiente: [1.188852551194941e-05,-0.00020510915006243617] Loss: 22.842736774911636\n",
      "Iteracion: 19615 Gradiente: [1.1882205243788728e-05,-0.0002050001077402423] Loss: 22.84273677486943\n",
      "Iteracion: 19616 Gradiente: [1.1875888295283707e-05,-0.00020489112339087492] Loss: 22.842736774827284\n",
      "Iteracion: 19617 Gradiente: [1.1869574700540398e-05,-0.00020478219698105704] Loss: 22.842736774785173\n",
      "Iteracion: 19618 Gradiente: [1.186326445671663e-05,-0.0002046733284803537] Loss: 22.84273677474309\n",
      "Iteracion: 19619 Gradiente: [1.1856957652867094e-05,-0.00020456451785217194] Loss: 22.842736774701077\n",
      "Iteracion: 19620 Gradiente: [1.1850654114671973e-05,-0.00020445576507626128] Loss: 22.8427367746591\n",
      "Iteracion: 19621 Gradiente: [1.1844353927396394e-05,-0.0002043470701169762] Loss: 22.842736774617162\n",
      "Iteracion: 19622 Gradiente: [1.18380571355677e-05,-0.0002042384329404475] Loss: 22.84273677457529\n",
      "Iteracion: 19623 Gradiente: [1.1831763574339978e-05,-0.00020412985352488515] Loss: 22.84273677453344\n",
      "Iteracion: 19624 Gradiente: [1.1825473503298175e-05,-0.00020402133182588025] Loss: 22.842736774491648\n",
      "Iteracion: 19625 Gradiente: [1.1819186680857758e-05,-0.00020391286782649824] Loss: 22.842736774449897\n",
      "Iteracion: 19626 Gradiente: [1.1812903207442104e-05,-0.0002038044614886066] Loss: 22.84273677440818\n",
      "Iteracion: 19627 Gradiente: [1.1806623150315925e-05,-0.00020369611277869145] Loss: 22.84273677436653\n",
      "Iteracion: 19628 Gradiente: [1.180034634084374e-05,-0.00020358782167591017] Loss: 22.842736774324905\n",
      "Iteracion: 19629 Gradiente: [1.1794072964714056e-05,-0.00020347958813727492] Loss: 22.842736774283313\n",
      "Iteracion: 19630 Gradiente: [1.1787802877923544e-05,-0.0002033714121418247] Loss: 22.842736774241793\n",
      "Iteracion: 19631 Gradiente: [1.1781536048260933e-05,-0.0002032632936610194] Loss: 22.842736774200297\n",
      "Iteracion: 19632 Gradiente: [1.177527271162641e-05,-0.00020315523264831845] Loss: 22.842736774158855\n",
      "Iteracion: 19633 Gradiente: [1.1769012587592442e-05,-0.00020304722909330052] Loss: 22.842736774117455\n",
      "Iteracion: 19634 Gradiente: [1.1762755768055892e-05,-0.00020293928295653056] Loss: 22.8427367740761\n",
      "Iteracion: 19635 Gradiente: [1.1756502421652234e-05,-0.00020283139419892868] Loss: 22.842736774034783\n",
      "Iteracion: 19636 Gradiente: [1.175025228405957e-05,-0.00020272356280545505] Loss: 22.842736773993515\n",
      "Iteracion: 19637 Gradiente: [1.174400544243781e-05,-0.00020261578873963514] Loss: 22.842736773952296\n",
      "Iteracion: 19638 Gradiente: [1.1737761919524322e-05,-0.00020250807196984985] Loss: 22.84273677391112\n",
      "Iteracion: 19639 Gradiente: [1.1731521812900306e-05,-0.00020240041245950618] Loss: 22.842736773869976\n",
      "Iteracion: 19640 Gradiente: [1.1725285068564517e-05,-0.0002022928101825509] Loss: 22.842736773828893\n",
      "Iteracion: 19641 Gradiente: [1.1719051514091916e-05,-0.00020218526511719403] Loss: 22.84273677378785\n",
      "Iteracion: 19642 Gradiente: [1.1712821326644492e-05,-0.0002020777772221057] Loss: 22.842736773746825\n",
      "Iteracion: 19643 Gradiente: [1.1706594361271527e-05,-0.00020197034647845651] Loss: 22.842736773705884\n",
      "Iteracion: 19644 Gradiente: [1.1700370790398058e-05,-0.00020186297284195595] Loss: 22.84273677366495\n",
      "Iteracion: 19645 Gradiente: [1.1694150459599466e-05,-0.00020175565629235355] Loss: 22.842736773624093\n",
      "Iteracion: 19646 Gradiente: [1.1687933449403924e-05,-0.00020164839679495116] Loss: 22.842736773583244\n",
      "Iteracion: 19647 Gradiente: [1.1681719796759655e-05,-0.0002015411943169454] Loss: 22.842736773542462\n",
      "Iteracion: 19648 Gradiente: [1.1675509482718855e-05,-0.00020143404882908556] Loss: 22.842736773501713\n",
      "Iteracion: 19649 Gradiente: [1.1669302417279444e-05,-0.0002013269603062658] Loss: 22.842736773461024\n",
      "Iteracion: 19650 Gradiente: [1.1663098715075648e-05,-0.00020121992871118265] Loss: 22.84273677342036\n",
      "Iteracion: 19651 Gradiente: [1.1656898208419382e-05,-0.00020111295402417775] Loss: 22.84273677337974\n",
      "Iteracion: 19652 Gradiente: [1.1650701069735685e-05,-0.00020100603620238172] Loss: 22.842736773339173\n",
      "Iteracion: 19653 Gradiente: [1.164450725639199e-05,-0.0002008991752210439] Loss: 22.842736773298654\n",
      "Iteracion: 19654 Gradiente: [1.1638316559015039e-05,-0.00020079237105979548] Loss: 22.84273677325816\n",
      "Iteracion: 19655 Gradiente: [1.1632129358455738e-05,-0.00020068562366735895] Loss: 22.842736773217723\n",
      "Iteracion: 19656 Gradiente: [1.1625945326917038e-05,-0.00020057893303165503] Loss: 22.84273677317731\n",
      "Iteracion: 19657 Gradiente: [1.1619764619770951e-05,-0.00020047229911384078] Loss: 22.842736773136952\n",
      "Iteracion: 19658 Gradiente: [1.1613587161226253e-05,-0.0002003657218890472] Loss: 22.84273677309665\n",
      "Iteracion: 19659 Gradiente: [1.1607413026126778e-05,-0.0002002592013207997] Loss: 22.842736773056366\n",
      "Iteracion: 19660 Gradiente: [1.1601242194577329e-05,-0.00020015273738115033] Loss: 22.842736773016153\n",
      "Iteracion: 19661 Gradiente: [1.1595074627734903e-05,-0.00020004633004179576] Loss: 22.84273677297597\n",
      "Iteracion: 19662 Gradiente: [1.1588910303809523e-05,-0.0001999399792746696] Loss: 22.84273677293582\n",
      "Iteracion: 19663 Gradiente: [1.1582749281539388e-05,-0.00019983368504412625] Loss: 22.84273677289572\n",
      "Iteracion: 19664 Gradiente: [1.157659159408316e-05,-0.0001997274473196124] Loss: 22.842736772855663\n",
      "Iteracion: 19665 Gradiente: [1.1570437064278848e-05,-0.0001996212660817065] Loss: 22.842736772815652\n",
      "Iteracion: 19666 Gradiente: [1.1564285881604518e-05,-0.00019951514128848657] Loss: 22.842736772775677\n",
      "Iteracion: 19667 Gradiente: [1.1558137885951207e-05,-0.00019940907292005742] Loss: 22.842736772735748\n",
      "Iteracion: 19668 Gradiente: [1.155199328574478e-05,-0.0001993030609323654] Loss: 22.842736772695865\n",
      "Iteracion: 19669 Gradiente: [1.1545851885822836e-05,-0.00019919710530776533] Loss: 22.842736772656007\n",
      "Iteracion: 19670 Gradiente: [1.1539713779029625e-05,-0.0001990912060117959] Loss: 22.842736772616213\n",
      "Iteracion: 19671 Gradiente: [1.1533578883889579e-05,-0.00019898536301733808] Loss: 22.842736772576455\n",
      "Iteracion: 19672 Gradiente: [1.1527447255351337e-05,-0.00019887957629182533] Loss: 22.84273677253674\n",
      "Iteracion: 19673 Gradiente: [1.1521318945521368e-05,-0.00019877384580257266] Loss: 22.842736772497066\n",
      "Iteracion: 19674 Gradiente: [1.1515193802817218e-05,-0.00019866817152779012] Loss: 22.842736772457428\n",
      "Iteracion: 19675 Gradiente: [1.1509071963663094e-05,-0.00019856255343005577] Loss: 22.842736772417837\n",
      "Iteracion: 19676 Gradiente: [1.1502953436585509e-05,-0.00019845699147928996] Loss: 22.84273677237828\n",
      "Iteracion: 19677 Gradiente: [1.1496838106002845e-05,-0.00019835148565074215] Loss: 22.842736772338785\n",
      "Iteracion: 19678 Gradiente: [1.1490726092233671e-05,-0.00019824603591006945] Loss: 22.842736772299315\n",
      "Iteracion: 19679 Gradiente: [1.148461724843249e-05,-0.00019814064223323176] Loss: 22.84273677225989\n",
      "Iteracion: 19680 Gradiente: [1.1478511652285306e-05,-0.00019803530458695207] Loss: 22.842736772220512\n",
      "Iteracion: 19681 Gradiente: [1.1472409293370825e-05,-0.0001979300229436376] Loss: 22.842736772181173\n",
      "Iteracion: 19682 Gradiente: [1.1466310267375472e-05,-0.00019782479726456378] Loss: 22.842736772141883\n",
      "Iteracion: 19683 Gradiente: [1.1460214343136007e-05,-0.00019771962753551976] Loss: 22.842736772102622\n",
      "Iteracion: 19684 Gradiente: [1.1454121783079547e-05,-0.0001976145137097281] Loss: 22.84273677206339\n",
      "Iteracion: 19685 Gradiente: [1.1448032399622813e-05,-0.00019750945576954375] Loss: 22.842736772024224\n",
      "Iteracion: 19686 Gradiente: [1.1441946324453055e-05,-0.00019740445367718944] Loss: 22.8427367719851\n",
      "Iteracion: 19687 Gradiente: [1.1435863472305148e-05,-0.0001972995074093357] Loss: 22.842736771946015\n",
      "Iteracion: 19688 Gradiente: [1.1429783863074286e-05,-0.00019719461693152123] Loss: 22.84273677190696\n",
      "Iteracion: 19689 Gradiente: [1.1423707440864443e-05,-0.0001970897822208902] Loss: 22.842736771867944\n",
      "Iteracion: 19690 Gradiente: [1.1417634164937832e-05,-0.00019698500324677088] Loss: 22.842736771829\n",
      "Iteracion: 19691 Gradiente: [1.141156413571783e-05,-0.0001968802799758862] Loss: 22.84273677179007\n",
      "Iteracion: 19692 Gradiente: [1.14054974640491e-05,-0.00019677561237401163] Loss: 22.842736771751202\n",
      "Iteracion: 19693 Gradiente: [1.1399434010665269e-05,-0.00019667100041497558] Loss: 22.842736771712346\n",
      "Iteracion: 19694 Gradiente: [1.139337366566906e-05,-0.00019656644407675116] Loss: 22.842736771673554\n",
      "Iteracion: 19695 Gradiente: [1.1387316643170683e-05,-0.00019646194331836379] Loss: 22.842736771634797\n",
      "Iteracion: 19696 Gradiente: [1.1381262774534662e-05,-0.00019635749812003668] Loss: 22.84273677159606\n",
      "Iteracion: 19697 Gradiente: [1.1375212132710051e-05,-0.00019625310844872956] Loss: 22.8427367715574\n",
      "Iteracion: 19698 Gradiente: [1.1369164728118145e-05,-0.0001961487742716391] Loss: 22.84273677151877\n",
      "Iteracion: 19699 Gradiente: [1.1363120499178574e-05,-0.0001960444955650805] Loss: 22.84273677148017\n",
      "Iteracion: 19700 Gradiente: [1.1357079537788195e-05,-0.00019594027229127657] Loss: 22.84273677144162\n",
      "Iteracion: 19701 Gradiente: [1.1351041770050567e-05,-0.00019583610442855577] Loss: 22.84273677140311\n",
      "Iteracion: 19702 Gradiente: [1.1345007267967351e-05,-0.000195731991940562] Loss: 22.84273677136464\n",
      "Iteracion: 19703 Gradiente: [1.1338975835428754e-05,-0.00019562793481024224] Loss: 22.84273677132621\n",
      "Iteracion: 19704 Gradiente: [1.1332947760441433e-05,-0.00019552393299070067] Loss: 22.842736771287818\n",
      "Iteracion: 19705 Gradiente: [1.1326922834579515e-05,-0.00019541998646630532] Loss: 22.84273677124947\n",
      "Iteracion: 19706 Gradiente: [1.1320901071106467e-05,-0.0001953160952036607] Loss: 22.842736771211168\n",
      "Iteracion: 19707 Gradiente: [1.1314882523076145e-05,-0.00019521225917245032] Loss: 22.84273677117291\n",
      "Iteracion: 19708 Gradiente: [1.1308867090065178e-05,-0.00019510847834934472] Loss: 22.842736771134682\n",
      "Iteracion: 19709 Gradiente: [1.1302854940709039e-05,-0.00019500475269301393] Loss: 22.842736771096487\n",
      "Iteracion: 19710 Gradiente: [1.1296846053217753e-05,-0.0001949010821789443] Loss: 22.842736771058334\n",
      "Iteracion: 19711 Gradiente: [1.129084024853455e-05,-0.00019479746678451685] Loss: 22.842736771020245\n",
      "Iteracion: 19712 Gradiente: [1.1284837699084467e-05,-0.000194693906471007] Loss: 22.842736770982185\n",
      "Iteracion: 19713 Gradiente: [1.1278838379287966e-05,-0.00019459040121105886] Loss: 22.842736770944157\n",
      "Iteracion: 19714 Gradiente: [1.1272842222827724e-05,-0.00019448695097944816] Loss: 22.842736770906164\n",
      "Iteracion: 19715 Gradiente: [1.1266849232545914e-05,-0.00019438355574739793] Loss: 22.84273677086822\n",
      "Iteracion: 19716 Gradiente: [1.1260859434022071e-05,-0.00019428021548151264] Loss: 22.84273677083032\n",
      "Iteracion: 19717 Gradiente: [1.1254872813045343e-05,-0.00019417693015609435] Loss: 22.842736770792474\n",
      "Iteracion: 19718 Gradiente: [1.1248889385721365e-05,-0.00019407369973940548] Loss: 22.842736770754648\n",
      "Iteracion: 19719 Gradiente: [1.1242909106575402e-05,-0.00019397052420562963] Loss: 22.84273677071686\n",
      "Iteracion: 19720 Gradiente: [1.1236932084557339e-05,-0.00019386740351710804] Loss: 22.842736770679117\n",
      "Iteracion: 19721 Gradiente: [1.1230958184190362e-05,-0.00019376433765453763] Loss: 22.84273677064143\n",
      "Iteracion: 19722 Gradiente: [1.1224987457580937e-05,-0.00019366132658570716] Loss: 22.842736770603757\n",
      "Iteracion: 19723 Gradiente: [1.1219019906623847e-05,-0.0001935583702793527] Loss: 22.842736770566137\n",
      "Iteracion: 19724 Gradiente: [1.1213055542687773e-05,-0.00019345546870752628] Loss: 22.842736770528557\n",
      "Iteracion: 19725 Gradiente: [1.120709428050759e-05,-0.0001933526218465431] Loss: 22.842736770491022\n",
      "Iteracion: 19726 Gradiente: [1.1201136182611056e-05,-0.00019324982966123134] Loss: 22.842736770453524\n",
      "Iteracion: 19727 Gradiente: [1.1195181322894616e-05,-0.0001931470921193797] Loss: 22.84273677041606\n",
      "Iteracion: 19728 Gradiente: [1.11892297278852e-05,-0.00019304440919019802] Loss: 22.842736770378636\n",
      "Iteracion: 19729 Gradiente: [1.1183281116207884e-05,-0.0001929417808613702] Loss: 22.842736770341254\n",
      "Iteracion: 19730 Gradiente: [1.1177335759763688e-05,-0.00019283920708659252] Loss: 22.84273677030391\n",
      "Iteracion: 19731 Gradiente: [1.1171393547707945e-05,-0.0001927366878438382] Loss: 22.842736770266605\n",
      "Iteracion: 19732 Gradiente: [1.1165454455408508e-05,-0.00019263422310693555] Loss: 22.842736770229347\n",
      "Iteracion: 19733 Gradiente: [1.1159518533077061e-05,-0.00019253181284308122] Loss: 22.842736770192126\n",
      "Iteracion: 19734 Gradiente: [1.1153585793029682e-05,-0.00019242945702148498] Loss: 22.842736770154943\n",
      "Iteracion: 19735 Gradiente: [1.1147656205897268e-05,-0.00019232715561561992] Loss: 22.8427367701178\n",
      "Iteracion: 19736 Gradiente: [1.1141729819049337e-05,-0.00019222490859374848] Loss: 22.8427367700807\n",
      "Iteracion: 19737 Gradiente: [1.1135806531115122e-05,-0.00019212271593206746] Loss: 22.842736770043633\n",
      "Iteracion: 19738 Gradiente: [1.1129886419780633e-05,-0.00019202057759741819] Loss: 22.8427367700066\n",
      "Iteracion: 19739 Gradiente: [1.112396943388679e-05,-0.00019191849356398432] Loss: 22.84273676996962\n",
      "Iteracion: 19740 Gradiente: [1.1118055543117104e-05,-0.0001918164638053573] Loss: 22.842736769932678\n",
      "Iteracion: 19741 Gradiente: [1.1112144813788897e-05,-0.0001917144882870758] Loss: 22.842736769895758\n",
      "Iteracion: 19742 Gradiente: [1.110623730748254e-05,-0.00019161256697858658] Loss: 22.84273676985889\n",
      "Iteracion: 19743 Gradiente: [1.1100332908616414e-05,-0.00019151069985454683] Loss: 22.842736769822075\n",
      "Iteracion: 19744 Gradiente: [1.1094431673086547e-05,-0.00019140888688487696] Loss: 22.842736769785272\n",
      "Iteracion: 19745 Gradiente: [1.1088533544996912e-05,-0.00019130712804601066] Loss: 22.84273676974853\n",
      "Iteracion: 19746 Gradiente: [1.1082638454240624e-05,-0.0001912054233082235] Loss: 22.842736769711816\n",
      "Iteracion: 19747 Gradiente: [1.107674663103353e-05,-0.00019110377263361993] Loss: 22.84273676967514\n",
      "Iteracion: 19748 Gradiente: [1.1070857886844958e-05,-0.0001910021760028968] Loss: 22.842736769638513\n",
      "Iteracion: 19749 Gradiente: [1.1064972313571768e-05,-0.00019090063338088233] Loss: 22.842736769601917\n",
      "Iteracion: 19750 Gradiente: [1.105908982594883e-05,-0.0001907991447454312] Loss: 22.84273676956536\n",
      "Iteracion: 19751 Gradiente: [1.1053210470398274e-05,-0.00019069771006326638] Loss: 22.842736769528848\n",
      "Iteracion: 19752 Gradiente: [1.1047334174918432e-05,-0.00019059632931070307] Loss: 22.84273676949236\n",
      "Iteracion: 19753 Gradiente: [1.1041461086354805e-05,-0.0001904950024515036] Loss: 22.842736769455925\n",
      "Iteracion: 19754 Gradiente: [1.1035591114705312e-05,-0.0001903937294593779] Loss: 22.84273676941952\n",
      "Iteracion: 19755 Gradiente: [1.1029724263759515e-05,-0.00019029251030922012] Loss: 22.842736769383166\n",
      "Iteracion: 19756 Gradiente: [1.1023860562886511e-05,-0.0001901913449678716] Loss: 22.842736769346836\n",
      "Iteracion: 19757 Gradiente: [1.1017999945768982e-05,-0.00019009023340987123] Loss: 22.842736769310555\n",
      "Iteracion: 19758 Gradiente: [1.1012142465460784e-05,-0.00018998917560502092] Loss: 22.842736769274303\n",
      "Iteracion: 19759 Gradiente: [1.1006288030065055e-05,-0.00018988817152963596] Loss: 22.8427367692381\n",
      "Iteracion: 19760 Gradiente: [1.1000436668003507e-05,-0.00018978722115399195] Loss: 22.84273676920193\n",
      "Iteracion: 19761 Gradiente: [1.099458856685942e-05,-0.00018968632443818004] Loss: 22.842736769165803\n",
      "Iteracion: 19762 Gradiente: [1.0988743491679998e-05,-0.00018958548136633151] Loss: 22.842736769129704\n",
      "Iteracion: 19763 Gradiente: [1.0982901555204687e-05,-0.00018948469190374813] Loss: 22.84273676909366\n",
      "Iteracion: 19764 Gradiente: [1.0977062715748314e-05,-0.0001893839560253241] Loss: 22.842736769057634\n",
      "Iteracion: 19765 Gradiente: [1.0971226910783116e-05,-0.0001892832737051246] Loss: 22.84273676902166\n",
      "Iteracion: 19766 Gradiente: [1.0965394307049791e-05,-0.0001891826449048987] Loss: 22.84273676898572\n",
      "Iteracion: 19767 Gradiente: [1.095956472454418e-05,-0.0001890820696062908] Loss: 22.842736768949813\n",
      "Iteracion: 19768 Gradiente: [1.0953738251373579e-05,-0.00018898154777673425] Loss: 22.84273676891397\n",
      "Iteracion: 19769 Gradiente: [1.09479149159597e-05,-0.0001888810793841363] Loss: 22.84273676887814\n",
      "Iteracion: 19770 Gradiente: [1.0942094630195242e-05,-0.0001887806644061148] Loss: 22.842736768842357\n",
      "Iteracion: 19771 Gradiente: [1.093627755418917e-05,-0.00018868030280655052] Loss: 22.842736768806606\n",
      "Iteracion: 19772 Gradiente: [1.0930463464357369e-05,-0.00018857999456732463] Loss: 22.8427367687709\n",
      "Iteracion: 19773 Gradiente: [1.0924652521756191e-05,-0.00018847973965338365] Loss: 22.842736768735215\n",
      "Iteracion: 19774 Gradiente: [1.0918844681858294e-05,-0.00018837953803524005] Loss: 22.842736768699577\n",
      "Iteracion: 19775 Gradiente: [1.091303981108164e-05,-0.00018827938969513032] Loss: 22.842736768663993\n",
      "Iteracion: 19776 Gradiente: [1.0907238179432473e-05,-0.00018817929458746127] Loss: 22.842736768628424\n",
      "Iteracion: 19777 Gradiente: [1.0901439531115405e-05,-0.00018807925270027207] Loss: 22.842736768592903\n",
      "Iteracion: 19778 Gradiente: [1.0895644029081572e-05,-0.00018797926399365393] Loss: 22.84273676855743\n",
      "Iteracion: 19779 Gradiente: [1.0889851572907598e-05,-0.00018787932844617218] Loss: 22.84273676852197\n",
      "Iteracion: 19780 Gradiente: [1.0884062225121246e-05,-0.00018777944602703655] Loss: 22.842736768486564\n",
      "Iteracion: 19781 Gradiente: [1.0878275840771797e-05,-0.0001876796167143387] Loss: 22.84273676845119\n",
      "Iteracion: 19782 Gradiente: [1.0872492615969047e-05,-0.00018757984046994616] Loss: 22.842736768415868\n",
      "Iteracion: 19783 Gradiente: [1.0866712428499643e-05,-0.0001874801172721874] Loss: 22.842736768380576\n",
      "Iteracion: 19784 Gradiente: [1.086093533994396e-05,-0.00018738044708778526] Loss: 22.84273676834531\n",
      "Iteracion: 19785 Gradiente: [1.0855161391987167e-05,-0.00018728082988689703] Loss: 22.842736768310097\n",
      "Iteracion: 19786 Gradiente: [1.084939044631028e-05,-0.00018718126565021956] Loss: 22.842736768274918\n",
      "Iteracion: 19787 Gradiente: [1.0843622541756304e-05,-0.00018708175434625218] Loss: 22.84273676823976\n",
      "Iteracion: 19788 Gradiente: [1.0837857824223343e-05,-0.0001869822959382835] Loss: 22.84273676820466\n",
      "Iteracion: 19789 Gradiente: [1.0832096097601607e-05,-0.00018688289040914204] Loss: 22.84273676816959\n",
      "Iteracion: 19790 Gradiente: [1.0826337396944533e-05,-0.00018678353772945874] Loss: 22.84273676813455\n",
      "Iteracion: 19791 Gradiente: [1.082058180088552e-05,-0.0001866842378670223] Loss: 22.84273676809956\n",
      "Iteracion: 19792 Gradiente: [1.0814829250686368e-05,-0.00018658499079577951] Loss: 22.8427367680646\n",
      "Iteracion: 19793 Gradiente: [1.0809079699924951e-05,-0.00018648579649050608] Loss: 22.842736768029678\n",
      "Iteracion: 19794 Gradiente: [1.0803333312499793e-05,-0.00018638665491437222] Loss: 22.842736767994793\n",
      "Iteracion: 19795 Gradiente: [1.0797589915985857e-05,-0.0001862875660481933] Loss: 22.842736767959945\n",
      "Iteracion: 19796 Gradiente: [1.0791849568173954e-05,-0.000186188529860587] Loss: 22.842736767925135\n",
      "Iteracion: 19797 Gradiente: [1.0786112254853226e-05,-0.0001860895463252632] Loss: 22.842736767890372\n",
      "Iteracion: 19798 Gradiente: [1.078037804613056e-05,-0.000185990615409537] Loss: 22.84273676785562\n",
      "Iteracion: 19799 Gradiente: [1.0774646798002626e-05,-0.00018589173709434212] Loss: 22.842736767820917\n",
      "Iteracion: 19800 Gradiente: [1.076891874163266e-05,-0.00018579291133586178] Loss: 22.84273676778626\n",
      "Iteracion: 19801 Gradiente: [1.0763193701753456e-05,-0.0001856941381183456] Loss: 22.84273676775164\n",
      "Iteracion: 19802 Gradiente: [1.0757471628153326e-05,-0.00018559541741656934] Loss: 22.842736767717057\n",
      "Iteracion: 19803 Gradiente: [1.0751752665782987e-05,-0.00018549674919370317] Loss: 22.842736767682492\n",
      "Iteracion: 19804 Gradiente: [1.0746036672533894e-05,-0.00018539813342985194] Loss: 22.842736767647985\n",
      "Iteracion: 19805 Gradiente: [1.0740323736513346e-05,-0.00018529957009138324] Loss: 22.8427367676135\n",
      "Iteracion: 19806 Gradiente: [1.0734613827404852e-05,-0.0001852010591543755] Loss: 22.842736767579062\n",
      "Iteracion: 19807 Gradiente: [1.0728907003946612e-05,-0.00018510260058472264] Loss: 22.84273676754467\n",
      "Iteracion: 19808 Gradiente: [1.0723203104134882e-05,-0.0001850041943659638] Loss: 22.84273676751028\n",
      "Iteracion: 19809 Gradiente: [1.0717502322184676e-05,-0.00018490584045688745] Loss: 22.842736767475955\n",
      "Iteracion: 19810 Gradiente: [1.0711804646727311e-05,-0.0001848075388325062] Loss: 22.842736767441657\n",
      "Iteracion: 19811 Gradiente: [1.0706109912916873e-05,-0.0001847092894728064] Loss: 22.842736767407395\n",
      "Iteracion: 19812 Gradiente: [1.0700418206018488e-05,-0.00018461109234569524] Loss: 22.842736767373182\n",
      "Iteracion: 19813 Gradiente: [1.0694729567717332e-05,-0.0001845129474202641] Loss: 22.84273676733898\n",
      "Iteracion: 19814 Gradiente: [1.0689043924116958e-05,-0.00018441485467306505] Loss: 22.842736767304828\n",
      "Iteracion: 19815 Gradiente: [1.0683361325429057e-05,-0.0001843168140746106] Loss: 22.84273676727072\n",
      "Iteracion: 19816 Gradiente: [1.0677681703441522e-05,-0.0001842188256002686] Loss: 22.842736767236648\n",
      "Iteracion: 19817 Gradiente: [1.067200513584036e-05,-0.00018412088921640664] Loss: 22.842736767202602\n",
      "Iteracion: 19818 Gradiente: [1.0666331571466495e-05,-0.00018402300490064267] Loss: 22.842736767168606\n",
      "Iteracion: 19819 Gradiente: [1.0660661000846024e-05,-0.00018392517262479183] Loss: 22.84273676713464\n",
      "Iteracion: 19820 Gradiente: [1.065499343629502e-05,-0.00018382739235877447] Loss: 22.842736767100686\n",
      "Iteracion: 19821 Gradiente: [1.0649328977289467e-05,-0.00018372966407120828] Loss: 22.842736767066803\n",
      "Iteracion: 19822 Gradiente: [1.064366740592959e-05,-0.00018363198774515866] Loss: 22.842736767032928\n",
      "Iteracion: 19823 Gradiente: [1.0638008864323941e-05,-0.00018353436334640112] Loss: 22.84273676699911\n",
      "Iteracion: 19824 Gradiente: [1.0632353375209883e-05,-0.00018343679084473763] Loss: 22.842736766965324\n",
      "Iteracion: 19825 Gradiente: [1.0626700861848803e-05,-0.00018333927021802293] Loss: 22.842736766931576\n",
      "Iteracion: 19826 Gradiente: [1.0621051447401442e-05,-0.00018324180143025615] Loss: 22.842736766897865\n",
      "Iteracion: 19827 Gradiente: [1.061540501723357e-05,-0.00018314438446106844] Loss: 22.842736766864167\n",
      "Iteracion: 19828 Gradiente: [1.0609761527765234e-05,-0.00018304701928535395] Loss: 22.84273676683052\n",
      "Iteracion: 19829 Gradiente: [1.0604121049103318e-05,-0.00018294970587161193] Loss: 22.84273676679692\n",
      "Iteracion: 19830 Gradiente: [1.0598483532930913e-05,-0.00018285244419473655] Loss: 22.842736766763338\n",
      "Iteracion: 19831 Gradiente: [1.0592849123251351e-05,-0.00018275523421908227] Loss: 22.842736766729804\n",
      "Iteracion: 19832 Gradiente: [1.0587217643850029e-05,-0.00018265807592617497] Loss: 22.84273676669631\n",
      "Iteracion: 19833 Gradiente: [1.058158915346515e-05,-0.00018256096928688238] Loss: 22.842736766662835\n",
      "Iteracion: 19834 Gradiente: [1.0575963625569784e-05,-0.00018246391427361174] Loss: 22.842736766629407\n",
      "Iteracion: 19835 Gradiente: [1.0570341074374785e-05,-0.00018236691085829666] Loss: 22.84273676659601\n",
      "Iteracion: 19836 Gradiente: [1.0564721590829624e-05,-0.00018226995900931796] Loss: 22.84273676656265\n",
      "Iteracion: 19837 Gradiente: [1.0559105070721368e-05,-0.00018217305870370145] Loss: 22.842736766529335\n",
      "Iteracion: 19838 Gradiente: [1.0553491492260036e-05,-0.0001820762099156307] Loss: 22.842736766496042\n",
      "Iteracion: 19839 Gradiente: [1.054788093976337e-05,-0.000181979412613605] Loss: 22.842736766462778\n",
      "Iteracion: 19840 Gradiente: [1.0542273345966654e-05,-0.00018188266677251856] Loss: 22.842736766429564\n",
      "Iteracion: 19841 Gradiente: [1.053666880371414e-05,-0.00018178597236169955] Loss: 22.842736766396374\n",
      "Iteracion: 19842 Gradiente: [1.05310671595286e-05,-0.00018168932935971326] Loss: 22.842736766363224\n",
      "Iteracion: 19843 Gradiente: [1.0525468526149477e-05,-0.00018159273773423005] Loss: 22.842736766330106\n",
      "Iteracion: 19844 Gradiente: [1.051987286283899e-05,-0.00018149619746049933] Loss: 22.842736766297033\n",
      "Iteracion: 19845 Gradiente: [1.0514280137385868e-05,-0.00018139970851294152] Loss: 22.842736766263997\n",
      "Iteracion: 19846 Gradiente: [1.0508690418949603e-05,-0.00018130327086147703] Loss: 22.84273676623099\n",
      "Iteracion: 19847 Gradiente: [1.050310371795149e-05,-0.00018120688447484194] Loss: 22.842736766198012\n",
      "Iteracion: 19848 Gradiente: [1.049751995196857e-05,-0.00018111054933290423] Loss: 22.84273676616507\n",
      "Iteracion: 19849 Gradiente: [1.0491939104895209e-05,-0.00018101426540842642] Loss: 22.84273676613218\n",
      "Iteracion: 19850 Gradiente: [1.0486361252522633e-05,-0.00018091803266931568] Loss: 22.8427367660993\n",
      "Iteracion: 19851 Gradiente: [1.0480786350323494e-05,-0.0001808218510922425] Loss: 22.842736766066476\n",
      "Iteracion: 19852 Gradiente: [1.0475214437140796e-05,-0.0001807257206458246] Loss: 22.842736766033674\n",
      "Iteracion: 19853 Gradiente: [1.0469645482658052e-05,-0.00018062964130578508] Loss: 22.8427367660009\n",
      "Iteracion: 19854 Gradiente: [1.0464079630878587e-05,-0.00018053361303695207] Loss: 22.84273676596819\n",
      "Iteracion: 19855 Gradiente: [1.0458516550215791e-05,-0.0001804376358298517] Loss: 22.842736765935495\n",
      "Iteracion: 19856 Gradiente: [1.0452956531518491e-05,-0.00018034170964066713] Loss: 22.84273676590283\n",
      "Iteracion: 19857 Gradiente: [1.0447399412782943e-05,-0.00018024583445341117] Loss: 22.842736765870203\n",
      "Iteracion: 19858 Gradiente: [1.0441845227167808e-05,-0.00018015001023646466] Loss: 22.842736765837625\n",
      "Iteracion: 19859 Gradiente: [1.0436294055201263e-05,-0.0001800542369598664] Loss: 22.84273676580506\n",
      "Iteracion: 19860 Gradiente: [1.0430745763301275e-05,-0.00017995851460301065] Loss: 22.842736765772546\n",
      "Iteracion: 19861 Gradiente: [1.0425200429153847e-05,-0.00017986284313427822] Loss: 22.842736765740053\n",
      "Iteracion: 19862 Gradiente: [1.041965804802203e-05,-0.00017976722252832644] Loss: 22.84273676570761\n",
      "Iteracion: 19863 Gradiente: [1.0414118703276169e-05,-0.000179671652750694] Loss: 22.84273676567521\n",
      "Iteracion: 19864 Gradiente: [1.0408582245228596e-05,-0.00017957613378361733] Loss: 22.84273676564282\n",
      "Iteracion: 19865 Gradiente: [1.0403048685247996e-05,-0.0001794806656016353] Loss: 22.842736765610464\n",
      "Iteracion: 19866 Gradiente: [1.0397518115231227e-05,-0.00017938524816993133] Loss: 22.84273676557815\n",
      "Iteracion: 19867 Gradiente: [1.0391990458439675e-05,-0.0001792898814675444] Loss: 22.842736765545872\n",
      "Iteracion: 19868 Gradiente: [1.0386465786875003e-05,-0.00017919456546167112] Loss: 22.842736765513628\n",
      "Iteracion: 19869 Gradiente: [1.0380944052220306e-05,-0.000179099300128982] Loss: 22.842736765481423\n",
      "Iteracion: 19870 Gradiente: [1.0375425225106482e-05,-0.00017900408544401595] Loss: 22.84273676544925\n",
      "Iteracion: 19871 Gradiente: [1.0369909307428316e-05,-0.00017890892137740385] Loss: 22.8427367654171\n",
      "Iteracion: 19872 Gradiente: [1.0364396309607098e-05,-0.00017881380790593464] Loss: 22.84273676538499\n",
      "Iteracion: 19873 Gradiente: [1.0358886311223613e-05,-0.00017871874499431802] Loss: 22.842736765352925\n",
      "Iteracion: 19874 Gradiente: [1.0353379181538003e-05,-0.00017862373262561942] Loss: 22.842736765320875\n",
      "Iteracion: 19875 Gradiente: [1.0347874967919779e-05,-0.00017852877076940388] Loss: 22.84273676528887\n",
      "Iteracion: 19876 Gradiente: [1.0342373684579798e-05,-0.0001784338593966576] Loss: 22.842736765256905\n",
      "Iteracion: 19877 Gradiente: [1.033687545846836e-05,-0.00017833899847516932] Loss: 22.842736765224974\n",
      "Iteracion: 19878 Gradiente: [1.0331380101054796e-05,-0.00017824418798587278] Loss: 22.842736765193074\n",
      "Iteracion: 19879 Gradiente: [1.032588754507439e-05,-0.00017814942790946495] Loss: 22.842736765161195\n",
      "Iteracion: 19880 Gradiente: [1.0320397999900402e-05,-0.00017805471820520798] Loss: 22.84273676512936\n",
      "Iteracion: 19881 Gradiente: [1.0314911306371262e-05,-0.00017796005885486465] Loss: 22.842736765097563\n",
      "Iteracion: 19882 Gradiente: [1.0309427594279441e-05,-0.00017786544982432891] Loss: 22.842736765065794\n",
      "Iteracion: 19883 Gradiente: [1.0303946842782352e-05,-0.00017777089108742908] Loss: 22.842736765034044\n",
      "Iteracion: 19884 Gradiente: [1.029846897514138e-05,-0.0001776763826237963] Loss: 22.842736765002364\n",
      "Iteracion: 19885 Gradiente: [1.029299397335611e-05,-0.00017758192440607464] Loss: 22.842736764970685\n",
      "Iteracion: 19886 Gradiente: [1.0287521905638642e-05,-0.00017748751640453975] Loss: 22.84273676493906\n",
      "Iteracion: 19887 Gradiente: [1.0282052676302555e-05,-0.0001773931585971648] Loss: 22.842736764907464\n",
      "Iteracion: 19888 Gradiente: [1.0276586398087299e-05,-0.00017729885095008058] Loss: 22.8427367648759\n",
      "Iteracion: 19889 Gradiente: [1.0271123055834627e-05,-0.00017720459343877336] Loss: 22.842736764844368\n",
      "Iteracion: 19890 Gradiente: [1.026566265807105e-05,-0.0001771103860339925] Loss: 22.84273676481286\n",
      "Iteracion: 19891 Gradiente: [1.0260205089214954e-05,-0.0001770162287179744] Loss: 22.842736764781385\n",
      "Iteracion: 19892 Gradiente: [1.0254750436426245e-05,-0.0001769221214575604] Loss: 22.84273676474995\n",
      "Iteracion: 19893 Gradiente: [1.0249298756548342e-05,-0.00017682806422409197] Loss: 22.842736764718563\n",
      "Iteracion: 19894 Gradiente: [1.0243849979474362e-05,-0.00017673405699352902] Loss: 22.842736764687206\n",
      "Iteracion: 19895 Gradiente: [1.023840396593793e-05,-0.00017664009974763435] Loss: 22.842736764655868\n",
      "Iteracion: 19896 Gradiente: [1.0232960948997061e-05,-0.0001765461924468544] Loss: 22.84273676462458\n",
      "Iteracion: 19897 Gradiente: [1.0227520808333187e-05,-0.0001764523350710571] Loss: 22.842736764593308\n",
      "Iteracion: 19898 Gradiente: [1.0222083509840256e-05,-0.00017635852759478136] Loss: 22.842736764562076\n",
      "Iteracion: 19899 Gradiente: [1.0216649198468985e-05,-0.00017626476998510536] Loss: 22.842736764530873\n",
      "Iteracion: 19900 Gradiente: [1.0211217629792676e-05,-0.0001761710622276998] Loss: 22.84273676449971\n",
      "Iteracion: 19901 Gradiente: [1.0205789013184586e-05,-0.00017607740428478755] Loss: 22.842736764468587\n",
      "Iteracion: 19902 Gradiente: [1.0200363291801295e-05,-0.00017598379613256536] Loss: 22.84273676443748\n",
      "Iteracion: 19903 Gradiente: [1.0194940531960129e-05,-0.00017589023774083518] Loss: 22.842736764406418\n",
      "Iteracion: 19904 Gradiente: [1.0189520608605562e-05,-0.0001757967290899387] Loss: 22.84273676437539\n",
      "Iteracion: 19905 Gradiente: [1.0184103571949283e-05,-0.00017570327015169104] Loss: 22.842736764344373\n",
      "Iteracion: 19906 Gradiente: [1.0178689336726165e-05,-0.0001756098609032364] Loss: 22.84273676431341\n",
      "Iteracion: 19907 Gradiente: [1.0173277991990896e-05,-0.00017551650131366613] Loss: 22.84273676428249\n",
      "Iteracion: 19908 Gradiente: [1.0167869561428233e-05,-0.00017542319135361122] Loss: 22.842736764251576\n",
      "Iteracion: 19909 Gradiente: [1.0162463948404365e-05,-0.00017532993100530803] Loss: 22.84273676422071\n",
      "Iteracion: 19910 Gradiente: [1.0157061292185668e-05,-0.0001752367202309794] Loss: 22.84273676418987\n",
      "Iteracion: 19911 Gradiente: [1.0151661511296576e-05,-0.0001751435590106117] Loss: 22.842736764159074\n",
      "Iteracion: 19912 Gradiente: [1.0146264577315378e-05,-0.0001750504473195728] Loss: 22.842736764128304\n",
      "Iteracion: 19913 Gradiente: [1.0140870539506371e-05,-0.00017495738512683564] Loss: 22.842736764097573\n",
      "Iteracion: 19914 Gradiente: [1.0135479370395235e-05,-0.00017486437240895233] Loss: 22.842736764066856\n",
      "Iteracion: 19915 Gradiente: [1.0130091022612456e-05,-0.0001747714091427118] Loss: 22.842736764036186\n",
      "Iteracion: 19916 Gradiente: [1.012470557952838e-05,-0.00017467849529436327] Loss: 22.842736764005547\n",
      "Iteracion: 19917 Gradiente: [1.0119323006089568e-05,-0.0001745856308428273] Loss: 22.84273676397494\n",
      "Iteracion: 19918 Gradiente: [1.0113943301348626e-05,-0.0001744928157601559] Loss: 22.84273676394437\n",
      "Iteracion: 19919 Gradiente: [1.0108566432146896e-05,-0.0001744000500218353] Loss: 22.84273676391382\n",
      "Iteracion: 19920 Gradiente: [1.0103192359641373e-05,-0.0001743073336058387] Loss: 22.84273676388332\n",
      "Iteracion: 19921 Gradiente: [1.0097821167202407e-05,-0.00017421466647829693] Loss: 22.84273676385283\n",
      "Iteracion: 19922 Gradiente: [1.0092452784723112e-05,-0.00017412204861824894] Loss: 22.842736763822398\n",
      "Iteracion: 19923 Gradiente: [1.0087087287994715e-05,-0.0001740294799951414] Loss: 22.84273676379198\n",
      "Iteracion: 19924 Gradiente: [1.0081724722491951e-05,-0.00017393696057924993] Loss: 22.842736763761604\n",
      "Iteracion: 19925 Gradiente: [1.0076364935684978e-05,-0.00017384449035328468] Loss: 22.842736763731253\n",
      "Iteracion: 19926 Gradiente: [1.0071008099998835e-05,-0.00017375206928219218] Loss: 22.842736763700945\n",
      "Iteracion: 19927 Gradiente: [1.0065654090377999e-05,-0.00017365969734643252] Loss: 22.842736763670658\n",
      "Iteracion: 19928 Gradiente: [1.0060302865137297e-05,-0.0001735673745216104] Loss: 22.842736763640406\n",
      "Iteracion: 19929 Gradiente: [1.0054954497225784e-05,-0.00017347510077835674] Loss: 22.842736763610194\n",
      "Iteracion: 19930 Gradiente: [1.0049609000854313e-05,-0.00017338287608860506] Loss: 22.842736763580003\n",
      "Iteracion: 19931 Gradiente: [1.0044266340022052e-05,-0.00017329070042831535] Loss: 22.842736763549848\n",
      "Iteracion: 19932 Gradiente: [1.0038926486307295e-05,-0.0001731985737739213] Loss: 22.842736763519724\n",
      "Iteracion: 19933 Gradiente: [1.0033589409393548e-05,-0.0001731064960997249] Loss: 22.842736763489633\n",
      "Iteracion: 19934 Gradiente: [1.0028255186966816e-05,-0.00017301446737517286] Loss: 22.842736763459573\n",
      "Iteracion: 19935 Gradiente: [1.002292392797699e-05,-0.00017292248757089606] Loss: 22.84273676342955\n",
      "Iteracion: 19936 Gradiente: [1.0017595479894226e-05,-0.00017283055666688084] Loss: 22.842736763399554\n",
      "Iteracion: 19937 Gradiente: [1.0012269821875937e-05,-0.00017273867463695562] Loss: 22.842736763369583\n",
      "Iteracion: 19938 Gradiente: [1.0006946951079954e-05,-0.0001726468414574356] Loss: 22.84273676333966\n",
      "Iteracion: 19939 Gradiente: [1.000162687887496e-05,-0.0001725550571010833] Loss: 22.842736763309755\n",
      "Iteracion: 19940 Gradiente: [9.996309733158645e-06,-0.00017246332153604272] Loss: 22.8427367632799\n",
      "Iteracion: 19941 Gradiente: [9.990995380348977e-06,-0.00017237163474076073] Loss: 22.842736763250052\n",
      "Iteracion: 19942 Gradiente: [9.985683936027575e-06,-0.00017227999668409193] Loss: 22.842736763220255\n",
      "Iteracion: 19943 Gradiente: [9.980375251454158e-06,-0.0001721884073488648] Loss: 22.84273676319047\n",
      "Iteracion: 19944 Gradiente: [9.975069384419536e-06,-0.00017209686670571027] Loss: 22.84273676316074\n",
      "Iteracion: 19945 Gradiente: [9.969766292291145e-06,-0.00017200537473141726] Loss: 22.84273676313103\n",
      "Iteracion: 19946 Gradiente: [9.964466053702381e-06,-0.00017191393139484034] Loss: 22.842736763101357\n",
      "Iteracion: 19947 Gradiente: [9.959168614651996e-06,-0.0001718225366725316] Loss: 22.842736763071716\n",
      "Iteracion: 19948 Gradiente: [9.953873994087795e-06,-0.00017173119053914832] Loss: 22.842736763042094\n",
      "Iteracion: 19949 Gradiente: [9.948582295275325e-06,-0.00017163989296188712] Loss: 22.84273676301251\n",
      "Iteracion: 19950 Gradiente: [9.94329326431398e-06,-0.00017154864392997146] Loss: 22.842736762982963\n",
      "Iteracion: 19951 Gradiente: [9.938007073628796e-06,-0.00017145744340716363] Loss: 22.842736762953436\n",
      "Iteracion: 19952 Gradiente: [9.932723744062362e-06,-0.00017136629136587088] Loss: 22.84273676292395\n",
      "Iteracion: 19953 Gradiente: [9.927443242456017e-06,-0.00017127518778229007] Loss: 22.84273676289449\n",
      "Iteracion: 19954 Gradiente: [9.922165457017703e-06,-0.00017118413263889448] Loss: 22.84273676286507\n",
      "Iteracion: 19955 Gradiente: [9.916890473012548e-06,-0.00017109312590288066] Loss: 22.842736762835667\n",
      "Iteracion: 19956 Gradiente: [9.911618449602127e-06,-0.00017100216754026103] Loss: 22.842736762806307\n",
      "Iteracion: 19957 Gradiente: [9.906349108253683e-06,-0.00017091125754002215] Loss: 22.842736762776983\n",
      "Iteracion: 19958 Gradiente: [9.901082616655307e-06,-0.00017082039586853172] Loss: 22.842736762747673\n",
      "Iteracion: 19959 Gradiente: [9.895818883857525e-06,-0.000170729582503526] Loss: 22.84273676271841\n",
      "Iteracion: 19960 Gradiente: [9.890557995125468e-06,-0.00017063881741421482] Loss: 22.84273676268916\n",
      "Iteracion: 19961 Gradiente: [9.885299898352665e-06,-0.00017054810058034776] Loss: 22.842736762659957\n",
      "Iteracion: 19962 Gradiente: [9.880044524379628e-06,-0.00017045743197824] Loss: 22.842736762630793\n",
      "Iteracion: 19963 Gradiente: [9.874791881732866e-06,-0.00017036681158195678] Loss: 22.842736762601636\n",
      "Iteracion: 19964 Gradiente: [9.869542250839913e-06,-0.00017027623934874706] Loss: 22.842736762572528\n",
      "Iteracion: 19965 Gradiente: [9.864295310535454e-06,-0.00017018571527138704] Loss: 22.84273676254344\n",
      "Iteracion: 19966 Gradiente: [9.85905104281907e-06,-0.00017009523932927096] Loss: 22.842736762514395\n",
      "Iteracion: 19967 Gradiente: [9.853809663695756e-06,-0.0001700048114790557] Loss: 22.84273676248537\n",
      "Iteracion: 19968 Gradiente: [9.848571068005185e-06,-0.00016991443170321456] Loss: 22.84273676245638\n",
      "Iteracion: 19969 Gradiente: [9.843335287958629e-06,-0.00016982409997451004] Loss: 22.84273676242741\n",
      "Iteracion: 19970 Gradiente: [9.838102316924354e-06,-0.00016973381626783635] Loss: 22.842736762398484\n",
      "Iteracion: 19971 Gradiente: [9.832872096164163e-06,-0.00016964358056045608] Loss: 22.842736762369594\n",
      "Iteracion: 19972 Gradiente: [9.827644646520639e-06,-0.0001695533928248949] Loss: 22.84273676234072\n",
      "Iteracion: 19973 Gradiente: [9.822419946203808e-06,-0.0001694632530381786] Loss: 22.84273676231188\n",
      "Iteracion: 19974 Gradiente: [9.817198039741015e-06,-0.00016937316117141183] Loss: 22.842736762283074\n",
      "Iteracion: 19975 Gradiente: [9.811978933763991e-06,-0.00016928311719960714] Loss: 22.842736762254287\n",
      "Iteracion: 19976 Gradiente: [9.806762558165852e-06,-0.00016919312110014555] Loss: 22.842736762225556\n",
      "Iteracion: 19977 Gradiente: [9.801549040844293e-06,-0.00016910317283939472] Loss: 22.84273676219682\n",
      "Iteracion: 19978 Gradiente: [9.796338238743374e-06,-0.000169013272402078] Loss: 22.84273676216815\n",
      "Iteracion: 19979 Gradiente: [9.791130190706099e-06,-0.00016892341975953683] Loss: 22.84273676213949\n",
      "Iteracion: 19980 Gradiente: [9.785924919469836e-06,-0.00016883361488477058] Loss: 22.84273676211087\n",
      "Iteracion: 19981 Gradiente: [9.78072241650807e-06,-0.00016874385775338395] Loss: 22.842736762082275\n",
      "Iteracion: 19982 Gradiente: [9.775522663820387e-06,-0.0001686541483397974] Loss: 22.842736762053725\n",
      "Iteracion: 19983 Gradiente: [9.770325743829743e-06,-0.00016856448661523398] Loss: 22.842736762025194\n",
      "Iteracion: 19984 Gradiente: [9.765131585481867e-06,-0.00016847487255731153] Loss: 22.842736761996687\n",
      "Iteracion: 19985 Gradiente: [9.759940090248164e-06,-0.00016838530614649017] Loss: 22.842736761968215\n",
      "Iteracion: 19986 Gradiente: [9.754751307392932e-06,-0.00016829578735411133] Loss: 22.84273676193977\n",
      "Iteracion: 19987 Gradiente: [9.74956548039548e-06,-0.00016820631614216096] Loss: 22.842736761911357\n",
      "Iteracion: 19988 Gradiente: [9.744382376197791e-06,-0.00016811689249844144] Loss: 22.84273676188299\n",
      "Iteracion: 19989 Gradiente: [9.73920191806125e-06,-0.00016802751639950487] Loss: 22.842736761854628\n",
      "Iteracion: 19990 Gradiente: [9.734024316306507e-06,-0.00016793818781136357] Loss: 22.842736761826302\n",
      "Iteracion: 19991 Gradiente: [9.72884935303379e-06,-0.00016784890671897774] Loss: 22.842736761798008\n",
      "Iteracion: 19992 Gradiente: [9.723677204457696e-06,-0.00016775967308764925] Loss: 22.842736761769775\n",
      "Iteracion: 19993 Gradiente: [9.718507774891804e-06,-0.00016767048689819337] Loss: 22.842736761741527\n",
      "Iteracion: 19994 Gradiente: [9.713341124021705e-06,-0.0001675813481199384] Loss: 22.84273676171334\n",
      "Iteracion: 19995 Gradiente: [9.708177249005227e-06,-0.000167492256729318] Loss: 22.84273676168516\n",
      "Iteracion: 19996 Gradiente: [9.703016095841121e-06,-0.00016740321270406847] Loss: 22.842736761657026\n",
      "Iteracion: 19997 Gradiente: [9.697857605791189e-06,-0.00016731421602097878] Loss: 22.842736761628903\n",
      "Iteracion: 19998 Gradiente: [9.692701883068367e-06,-0.00016722526665079822] Loss: 22.842736761600822\n",
      "Iteracion: 19999 Gradiente: [9.68754889261921e-06,-0.00016713636457043416] Loss: 22.84273676157277\n"
     ]
    }
   ],
   "source": [
    "# Cargamos los datos\n",
    "X, Y = np.loadtxt('pizza.txt', skiprows=1, unpack=True)\n",
    "\n",
    "# Llamamos a la funcion\n",
    "w, b = train(X, Y, 20000, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El descenso de gradiente no funciona si:\n",
    "- La función no es diferenciable\n",
    "- La función no es convexa (tiene múltiples mínimos locales - en una funcion convexa todos los mínimos locales son globales)\n",
    "\n",
    "Por ejemplo, la funcion de perdida implementada con error absoluto medio $y - \\hat{y}$ no es diferenciable en 0. Por eso usamos la función de pérdida con error cuadrático $(y - \\hat{y})^2$ que es diferenciable en todo el dominio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
