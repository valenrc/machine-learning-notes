{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos el dataset MNIST para clasificar digitos con otro modelo de clasificaciÃ³n.\n",
    "\n",
    "El dataset contiene imagenes de 28x28 pixeles en escala de grises (0-255) de digitos escritos a mano.\n",
    "Hay 60,000 imagenes de entrenamiento y 10,000 imagenes de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], shape=(60000, 784), dtype=uint8)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cargamos los datos de las imagenes en /mnist/train-images-idx3-ubyte.gz\n",
    "def load_mnist_images(path):\n",
    "  with gzip.open(path, 'rb') as f:\n",
    "\t\t# desempaquetamos: leemos los 4 enteros en big-endian que indican metadatos\n",
    "    _ignored, n_images, rows, cols = struct.unpack('>IIII', f.read(16))\n",
    "\t\t# leemos el resto de bytes del archivo como enteros de 8 bits sin signo\n",
    "    all_pixels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "\t\t# reshape para que cada imagen sea una fila (n_images) y cada pixel sea una columna (rows*cols)\n",
    "    X = all_pixels.reshape(n_images, rows * cols)\n",
    "  return X\n",
    "\n",
    "X = load_mnist_images('mnist/train-images-idx3-ubyte.gz')\n",
    "X\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5],\n",
       "       [0],\n",
       "       [4],\n",
       "       ...,\n",
       "       [5],\n",
       "       [6],\n",
       "       [8]], shape=(60000, 1), dtype=uint8)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cargamos los datos de las etiquetas en /mnist/train-labels-idx1-ubyte.gz\n",
    "def load_mnist_labels(path):\n",
    "  with gzip.open(path, 'rb') as f:\n",
    "\t\t# desempaquetamos: leemos los 2 enteros en big-endian que indican metadatos\n",
    "    f.read(8)\n",
    "\t\t# leemos el resto de bytes del archivo como enteros de 8 bits sin signo\n",
    "    all_labels = f.read()\n",
    "\t\t# reshape para que cada etiqueta sea una fila\n",
    "    Y = np.frombuffer(all_labels, dtype=np.uint8).reshape(-1, 1)\n",
    "  return Y\n",
    "\n",
    "Y = load_mnist_labels('mnist/train-labels-idx1-ubyte.gz')\n",
    "\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [1],\n",
       "       [0],\n",
       "       [0]], shape=(60000, 1))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para usar el modelo de clasificacion binaria, necesitamos cambiar las etiquetas de 0-9 a 0 o 1\n",
    "# para eso elejimos un numero (5) y lo cambiamos a 1 y el resto a 0\n",
    "Y = (Y == 5).astype(int)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], shape=(5421, 784), dtype=uint8)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits = X[Y.flatten() == 1]\n",
    "digits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualizamos algunos de los digitos\n",
    "r,c = 3,4\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(r*c):\n",
    "\tax = fig.add_subplot(r,c,i+1)\n",
    "\tax.axis('off')\n",
    "\tax.imshow(digits[i].reshape((28,28)), cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las funciones del notebok anterior\n",
    "\n",
    "def sigmoid(x) -> float:\n",
    "\treturn 1 / (1 + np.exp(-x))\n",
    "\n",
    "########ESTAS FUNCIONES SE USAN EN LA FASE DE ENTRENAMIENTO########\n",
    "# esta era nuestra funcion de prediccion\n",
    "def weighted_sum(X,w) -> np.ndarray:\n",
    "\treturn np.matmul(X,w)\n",
    "\n",
    "# apliquemos el sigmoide\n",
    "# el proceso de mover datos procesados a traves del modelo se llama forward propagation\n",
    "def forward(X,w) -> np.ndarray:\n",
    "\treturn sigmoid(weighted_sum(X,w))\n",
    "\n",
    "def log_loss(X, Y, w):\n",
    "  y_hat = forward(X, w)\n",
    "  first_term = Y * np.log(y_hat)\n",
    "  second_term = (1 - Y) * np.log(1 - y_hat)\n",
    "  return -np.average(first_term + second_term)\n",
    "\n",
    "def gradient(X, Y, w):\n",
    "  return np.matmul(X.T, (forward(X, w) - Y)) / X.shape[0]\n",
    "\n",
    "def train(X, Y, iterations, lr) -> np.ndarray:\n",
    "\tw = np.zeros((X.shape[1], 1))\n",
    "\n",
    "\tfor i in range(iterations):\n",
    "\t\tprint(f'Iteration {i} => Loss: {log_loss(X, Y, w)}')\n",
    "\t\tw -= gradient(X,Y,w) * lr\n",
    "\t\n",
    "\treturn w\n",
    "\n",
    "#########ESTA FUNCIÃN SE USA PARA LA FASE DE CLASIFICACIÃN###########\n",
    "# como queremos que el resultado sea 1 o 0 (no un numero entre 1 y 0) redondeamos los resultados al entero mas cercano\n",
    "def classify(X,w) -> np.ndarray:\n",
    "\treturn np.round(forward(X,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]], shape=(60000, 785), dtype=uint8)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ahora corremos el modelo!\n",
    "# insertamos una columna de 1's para el bias\n",
    "X = np.insert(X, 0, 1, axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 => Loss: 0.6931471805599453\n",
      "Iteration 1 => Loss: 0.8004253025949019\n",
      "Iteration 2 => Loss: 0.6037018000801918\n",
      "Iteration 3 => Loss: 0.41561114405207106\n",
      "Iteration 4 => Loss: 0.264885394555525\n",
      "Iteration 5 => Loss: 0.2197496658646801\n",
      "Iteration 6 => Loss: 0.2116971966744956\n",
      "Iteration 7 => Loss: 0.20475082478710038\n",
      "Iteration 8 => Loss: 0.19886878484015674\n",
      "Iteration 9 => Loss: 0.19372162858641284\n",
      "Iteration 10 => Loss: 0.18918194048765816\n",
      "Iteration 11 => Loss: 0.18512959704505969\n",
      "Iteration 12 => Loss: 0.1814815604080142\n",
      "Iteration 13 => Loss: 0.17817479685293547\n",
      "Iteration 14 => Loss: 0.17515915471115795\n",
      "Iteration 15 => Loss: 0.1723953506165786\n",
      "Iteration 16 => Loss: 0.16985095072606282\n",
      "Iteration 17 => Loss: 0.16749932830345124\n",
      "Iteration 18 => Loss: 0.1653180724308886\n",
      "Iteration 19 => Loss: 0.16328826613520675\n",
      "Iteration 20 => Loss: 0.16139375633494252\n",
      "Iteration 21 => Loss: 0.15962067801752663\n",
      "Iteration 22 => Loss: 0.15795704638735247\n",
      "Iteration 23 => Loss: 0.15639244690731702\n",
      "Iteration 24 => Loss: 0.15491778067148085\n",
      "Iteration 25 => Loss: 0.15352506006042058\n",
      "Iteration 26 => Loss: 0.15220724105167435\n",
      "Iteration 27 => Loss: 0.15095808554361245\n",
      "Iteration 28 => Loss: 0.14977204728419308\n",
      "Iteration 29 => Loss: 0.14864417688682038\n",
      "Iteration 30 => Loss: 0.14757004225490838\n",
      "Iteration 31 => Loss: 0.14654566156511656\n",
      "Iteration 32 => Loss: 0.14556744653604964\n",
      "Iteration 33 => Loss: 0.1446321541722572\n",
      "Iteration 34 => Loss: 0.14373684552914012\n",
      "Iteration 35 => Loss: 0.1428788503242973\n",
      "Iteration 36 => Loss: 0.14205573644132233\n",
      "Iteration 37 => Loss: 0.1412652835469923\n",
      "Iteration 38 => Loss: 0.14050546018235405\n",
      "Iteration 39 => Loss: 0.1397744038001722\n",
      "Iteration 40 => Loss: 0.1390704033114991\n",
      "Iteration 41 => Loss: 0.1383918837773224\n",
      "Iteration 42 => Loss: 0.13773739294086704\n",
      "Iteration 43 => Loss: 0.13710558934492645\n",
      "Iteration 44 => Loss: 0.13649523181871498\n",
      "Iteration 45 => Loss: 0.13590517015186068\n",
      "Iteration 46 => Loss: 0.1353343368006255\n",
      "Iteration 47 => Loss: 0.13478173949430847\n",
      "Iteration 48 => Loss: 0.1342464546289012\n",
      "Iteration 49 => Loss: 0.13372762135109859\n",
      "Iteration 50 => Loss: 0.13322443624926733\n",
      "Iteration 51 => Loss: 0.13273614857938065\n",
      "Iteration 52 => Loss: 0.13226205596359317\n",
      "Iteration 53 => Loss: 0.13180150050735345\n",
      "Iteration 54 => Loss: 0.1313538652879622\n",
      "Iteration 55 => Loss: 0.13091857117348435\n",
      "Iteration 56 => Loss: 0.13049507393607074\n",
      "Iteration 57 => Loss: 0.13008286162817387\n",
      "Iteration 58 => Loss: 0.1296814521939619\n",
      "Iteration 59 => Loss: 0.12929039129154007\n",
      "Iteration 60 => Loss: 0.12890925030445236\n",
      "Iteration 61 => Loss: 0.12853762452342626\n",
      "Iteration 62 => Loss: 0.1281751314814919\n",
      "Iteration 63 => Loss: 0.12782140942750006\n",
      "Iteration 64 => Loss: 0.12747611592471905\n",
      "Iteration 65 => Loss: 0.12713892656264417\n",
      "Iteration 66 => Loss: 0.12680953377142654\n",
      "Iteration 67 => Loss: 0.12648764572945337\n",
      "Iteration 68 => Loss: 0.12617298535559987\n",
      "Iteration 69 => Loss: 0.12586528937854832\n",
      "Iteration 70 => Loss: 0.12556430747634387\n",
      "Iteration 71 => Loss: 0.12526980148004305\n",
      "Iteration 72 => Loss: 0.12498154463591929\n",
      "Iteration 73 => Loss: 0.12469932092123305\n",
      "Iteration 74 => Loss: 0.12442292440905582\n",
      "Iteration 75 => Loss: 0.12415215867806853\n",
      "Iteration 76 => Loss: 0.1238868362636398\n",
      "Iteration 77 => Loss: 0.12362677814683308\n",
      "Iteration 78 => Loss: 0.12337181327830136\n",
      "Iteration 79 => Loss: 0.12312177813430428\n",
      "Iteration 80 => Loss: 0.12287651630233092\n",
      "Iteration 81 => Loss: 0.12263587809403648\n",
      "Iteration 82 => Loss: 0.12239972018340117\n",
      "Iteration 83 => Loss: 0.12216790526820255\n",
      "Iteration 84 => Loss: 0.121940301753056\n",
      "Iteration 85 => Loss: 0.12171678345242701\n",
      "Iteration 86 => Loss: 0.12149722931215237\n",
      "Iteration 87 => Loss: 0.12128152314813022\n",
      "Iteration 88 => Loss: 0.12106955340094785\n",
      "Iteration 89 => Loss: 0.12086121290531791\n",
      "Iteration 90 => Loss: 0.12065639867328369\n",
      "Iteration 91 => Loss: 0.12045501169023712\n",
      "Iteration 92 => Loss: 0.12025695672286943\n",
      "Iteration 93 => Loss: 0.12006214213824219\n",
      "Iteration 94 => Loss: 0.11987047973323026\n",
      "Iteration 95 => Loss: 0.11968188457364447\n",
      "Iteration 96 => Loss: 0.11949627484239572\n",
      "Iteration 97 => Loss: 0.11931357169610884\n",
      "Iteration 98 => Loss: 0.11913369912963911\n",
      "Iteration 99 => Loss: 0.11895658384798544\n",
      "Iteration 100 => Loss: 0.11878215514512928\n",
      "Iteration 101 => Loss: 0.11861034478936422\n",
      "Iteration 102 => Loss: 0.11844108691471186\n",
      "Iteration 103 => Loss: 0.11827431791804724\n",
      "Iteration 104 => Loss: 0.11810997636158527\n",
      "Iteration 105 => Loss: 0.11794800288040258\n",
      "Iteration 106 => Loss: 0.11778834009469263\n",
      "Iteration 107 => Loss: 0.1176309325264713\n",
      "Iteration 108 => Loss: 0.11747572652047143\n",
      "Iteration 109 => Loss: 0.11732267016898007\n",
      "Iteration 110 => Loss: 0.11717171324039057\n",
      "Iteration 111 => Loss: 0.11702280711125504\n",
      "Iteration 112 => Loss: 0.11687590470163838\n",
      "Iteration 113 => Loss: 0.11673096041358656\n",
      "Iteration 114 => Loss: 0.11658793007253494\n",
      "Iteration 115 => Loss: 0.11644677087149274\n",
      "Iteration 116 => Loss: 0.11630744131785124\n",
      "Iteration 117 => Loss: 0.11616990118267129\n",
      "Iteration 118 => Loss: 0.11603411145231635\n",
      "Iteration 119 => Loss: 0.11590003428230448\n",
      "Iteration 120 => Loss: 0.11576763295326091\n",
      "Iteration 121 => Loss: 0.11563687182885983\n",
      "Iteration 122 => Loss: 0.11550771631565086\n",
      "Iteration 123 => Loss: 0.11538013282467208\n",
      "Iteration 124 => Loss: 0.1152540887347568\n",
      "Iteration 125 => Loss: 0.11512955235744772\n",
      "Iteration 126 => Loss: 0.11500649290343558\n",
      "Iteration 127 => Loss: 0.11488488045044641\n",
      "Iteration 128 => Loss: 0.11476468591250343\n",
      "Iteration 129 => Loss: 0.1146458810104961\n",
      "Iteration 130 => Loss: 0.11452843824399093\n",
      "Iteration 131 => Loss: 0.11441233086422355\n",
      "Iteration 132 => Loss: 0.1142975328482141\n",
      "Iteration 133 => Loss: 0.11418401887395181\n",
      "Iteration 134 => Loss: 0.11407176429659735\n",
      "Iteration 135 => Loss: 0.11396074512565418\n",
      "Iteration 136 => Loss: 0.11385093800306312\n",
      "Iteration 137 => Loss: 0.11374232018217692\n",
      "Iteration 138 => Loss: 0.11363486950757319\n",
      "Iteration 139 => Loss: 0.1135285643956672\n",
      "Iteration 140 => Loss: 0.11342338381608753\n",
      "Iteration 141 => Loss: 0.11331930727377969\n",
      "Iteration 142 => Loss: 0.1132163147918044\n",
      "Iteration 143 => Loss: 0.11311438689479954\n",
      "Iteration 144 => Loss: 0.11301350459307555\n",
      "Iteration 145 => Loss: 0.11291364936731636\n",
      "Iteration 146 => Loss: 0.1128148031538588\n",
      "Iteration 147 => Loss: 0.1127169483305252\n",
      "Iteration 148 => Loss: 0.11262006770298495\n",
      "Iteration 149 => Loss: 0.11252414449162197\n",
      "Iteration 150 => Loss: 0.11242916231888624\n",
      "Iteration 151 => Loss: 0.11233510519710875\n",
      "Iteration 152 => Loss: 0.11224195751676004\n",
      "Iteration 153 => Loss: 0.11214970403513354\n",
      "Iteration 154 => Loss: 0.11205832986543562\n",
      "Iteration 155 => Loss: 0.1119678204662659\n",
      "Iteration 156 => Loss: 0.11187816163147081\n",
      "Iteration 157 => Loss: 0.1117893394803556\n",
      "Iteration 158 => Loss: 0.11170134044823986\n",
      "Iteration 159 => Loss: 0.11161415127734237\n",
      "Iteration 160 => Loss: 0.11152775900798208\n",
      "Iteration 161 => Loss: 0.11144215097008241\n",
      "Iteration 162 => Loss: 0.11135731477496677\n",
      "Iteration 163 => Loss: 0.11127323830743352\n",
      "Iteration 164 => Loss: 0.11118990971809939\n",
      "Iteration 165 => Loss: 0.11110731741600073\n",
      "Iteration 166 => Loss: 0.11102545006144265\n",
      "Iteration 167 => Loss: 0.11094429655908615\n",
      "Iteration 168 => Loss: 0.11086384605126397\n",
      "Iteration 169 => Loss: 0.11078408791151666\n",
      "Iteration 170 => Loss: 0.1107050117383403\n",
      "Iteration 171 => Loss: 0.11062660734913746\n",
      "Iteration 172 => Loss: 0.11054886477436437\n",
      "Iteration 173 => Loss: 0.11047177425186605\n",
      "Iteration 174 => Loss: 0.11039532622139334\n",
      "Iteration 175 => Loss: 0.11031951131929393\n",
      "Iteration 176 => Loss: 0.11024432037337223\n",
      "Iteration 177 => Loss: 0.11016974439791041\n",
      "Iteration 178 => Loss: 0.11009577458884594\n",
      "Iteration 179 => Loss: 0.11002240231909928\n",
      "Iteration 180 => Loss: 0.10994961913404626\n",
      "Iteration 181 => Loss: 0.1098774167471303\n",
      "Iteration 182 => Loss: 0.10980578703560892\n",
      "Iteration 183 => Loss: 0.10973472203643034\n",
      "Iteration 184 => Loss: 0.10966421394223487\n",
      "Iteration 185 => Loss: 0.10959425509747744\n",
      "Iteration 186 => Loss: 0.10952483799466625\n",
      "Iteration 187 => Loss: 0.10945595527071403\n",
      "Iteration 188 => Loss: 0.10938759970339772\n",
      "Iteration 189 => Loss: 0.10931976420792285\n",
      "Iteration 190 => Loss: 0.10925244183358901\n",
      "Iteration 191 => Loss: 0.1091856257605532\n",
      "Iteration 192 => Loss: 0.10911930929668692\n",
      "Iteration 193 => Loss: 0.10905348587452524\n",
      "Iteration 194 => Loss: 0.10898814904830291\n",
      "Iteration 195 => Loss: 0.10892329249107628\n",
      "Iteration 196 => Loss: 0.10885890999192716\n",
      "Iteration 197 => Loss: 0.10879499545324596\n",
      "Iteration 198 => Loss: 0.10873154288809204\n",
      "Iteration 199 => Loss: 0.10866854641762817\n",
      "Iteration 200 => Loss: 0.10860600026862675\n",
      "Iteration 201 => Loss: 0.10854389877104582\n",
      "Iteration 202 => Loss: 0.1084822363556719\n",
      "Iteration 203 => Loss: 0.10842100755182851\n",
      "Iteration 204 => Loss: 0.10836020698514702\n",
      "Iteration 205 => Loss: 0.10829982937539887\n",
      "Iteration 206 => Loss: 0.10823986953438668\n",
      "Iteration 207 => Loss: 0.10818032236389256\n",
      "Iteration 208 => Loss: 0.10812118285368147\n",
      "Iteration 209 => Loss: 0.10806244607955867\n",
      "Iteration 210 => Loss: 0.10800410720147859\n",
      "Iteration 211 => Loss: 0.1079461614617041\n",
      "Iteration 212 => Loss: 0.10788860418301492\n",
      "Iteration 213 => Loss: 0.10783143076696257\n",
      "Iteration 214 => Loss: 0.10777463669217158\n",
      "Iteration 215 => Loss: 0.1077182175126849\n",
      "Iteration 216 => Loss: 0.10766216885635221\n",
      "Iteration 217 => Loss: 0.1076064864232603\n",
      "Iteration 218 => Loss: 0.1075511659842033\n",
      "Iteration 219 => Loss: 0.10749620337919275\n",
      "Iteration 220 => Loss: 0.10744159451600523\n",
      "Iteration 221 => Loss: 0.1073873353687673\n",
      "Iteration 222 => Loss: 0.10733342197657586\n",
      "Iteration 223 => Loss: 0.10727985044215357\n",
      "Iteration 224 => Loss: 0.10722661693053799\n",
      "Iteration 225 => Loss: 0.10717371766780313\n",
      "Iteration 226 => Loss: 0.10712114893981318\n",
      "Iteration 227 => Loss: 0.10706890709100703\n",
      "Iteration 228 => Loss: 0.10701698852321247\n",
      "Iteration 229 => Loss: 0.10696538969448978\n",
      "Iteration 230 => Loss: 0.10691410711800355\n",
      "Iteration 231 => Loss: 0.10686313736092187\n",
      "Iteration 232 => Loss: 0.10681247704334224\n",
      "Iteration 233 => Loss: 0.10676212283724333\n",
      "Iteration 234 => Loss: 0.1067120714654622\n",
      "Iteration 235 => Loss: 0.10666231970069552\n",
      "Iteration 236 => Loss: 0.10661286436452512\n",
      "Iteration 237 => Loss: 0.1065637023264662\n",
      "Iteration 238 => Loss: 0.10651483050303837\n",
      "Iteration 239 => Loss: 0.1064662458568584\n",
      "Iteration 240 => Loss: 0.10641794539575426\n",
      "Iteration 241 => Loss: 0.10636992617189976\n",
      "Iteration 242 => Loss: 0.10632218528096947\n",
      "Iteration 243 => Loss: 0.10627471986131318\n",
      "Iteration 244 => Loss: 0.1062275270931491\n",
      "Iteration 245 => Loss: 0.1061806041977761\n",
      "Iteration 246 => Loss: 0.10613394843680347\n",
      "Iteration 247 => Loss: 0.1060875571113986\n",
      "Iteration 248 => Loss: 0.1060414275615514\n",
      "Iteration 249 => Loss: 0.10599555716535569\n",
      "Iteration 250 => Loss: 0.10594994333830657\n",
      "Iteration 251 => Loss: 0.10590458353261334\n",
      "Iteration 252 => Loss: 0.10585947523652828\n",
      "Iteration 253 => Loss: 0.10581461597368992\n",
      "Iteration 254 => Loss: 0.10577000330248094\n",
      "Iteration 255 => Loss: 0.1057256348154005\n",
      "Iteration 256 => Loss: 0.1056815081384499\n",
      "Iteration 257 => Loss: 0.10563762093053224\n",
      "Iteration 258 => Loss: 0.10559397088286472\n",
      "Iteration 259 => Loss: 0.10555055571840397\n",
      "Iteration 260 => Loss: 0.10550737319128368\n",
      "Iteration 261 => Loss: 0.10546442108626448\n",
      "Iteration 262 => Loss: 0.10542169721819554\n",
      "Iteration 263 => Loss: 0.10537919943148764\n",
      "Iteration 264 => Loss: 0.10533692559959776\n",
      "Iteration 265 => Loss: 0.1052948736245242\n",
      "Iteration 266 => Loss: 0.10525304143631271\n",
      "Iteration 267 => Loss: 0.10521142699257285\n",
      "Iteration 268 => Loss: 0.10517002827800466\n",
      "Iteration 269 => Loss: 0.10512884330393502\n",
      "Iteration 270 => Loss: 0.10508787010786388\n",
      "Iteration 271 => Loss: 0.1050471067530198\n",
      "Iteration 272 => Loss: 0.10500655132792468\n",
      "Iteration 273 => Loss: 0.10496620194596754\n",
      "Iteration 274 => Loss: 0.10492605674498702\n",
      "Iteration 275 => Loss: 0.10488611388686234\n",
      "Iteration 276 => Loss: 0.10484637155711274\n",
      "Iteration 277 => Loss: 0.10480682796450494\n",
      "Iteration 278 => Loss: 0.10476748134066861\n",
      "Iteration 279 => Loss: 0.10472832993971957\n",
      "Iteration 280 => Loss: 0.10468937203789062\n",
      "Iteration 281 => Loss: 0.10465060593316959\n",
      "Iteration 282 => Loss: 0.10461202994494484\n",
      "Iteration 283 => Loss: 0.10457364241365764\n",
      "Iteration 284 => Loss: 0.10453544170046154\n",
      "Iteration 285 => Loss: 0.10449742618688833\n",
      "Iteration 286 => Loss: 0.10445959427452078\n",
      "Iteration 287 => Loss: 0.10442194438467159\n",
      "Iteration 288 => Loss: 0.10438447495806867\n",
      "Iteration 289 => Loss: 0.1043471844545467\n",
      "Iteration 290 => Loss: 0.10431007135274428\n",
      "Iteration 291 => Loss: 0.10427313414980752\n",
      "Iteration 292 => Loss: 0.10423637136109876\n",
      "Iteration 293 => Loss: 0.10419978151991127\n",
      "Iteration 294 => Loss: 0.10416336317718916\n",
      "Iteration 295 => Loss: 0.1041271149012528\n",
      "Iteration 296 => Loss: 0.10409103527752947\n",
      "Iteration 297 => Loss: 0.10405512290828885\n",
      "Iteration 298 => Loss: 0.10401937641238389\n",
      "Iteration 299 => Loss: 0.10398379442499613\n",
      "Iteration 300 => Loss: 0.10394837559738626\n",
      "Iteration 301 => Loss: 0.1039131185966488\n",
      "Iteration 302 => Loss: 0.10387802210547187\n",
      "Iteration 303 => Loss: 0.1038430848219011\n",
      "Iteration 304 => Loss: 0.10380830545910796\n",
      "Iteration 305 => Loss: 0.1037736827451624\n",
      "Iteration 306 => Loss: 0.10373921542280966\n",
      "Iteration 307 => Loss: 0.10370490224925112\n",
      "Iteration 308 => Loss: 0.10367074199592928\n",
      "Iteration 309 => Loss: 0.10363673344831645\n",
      "Iteration 310 => Loss: 0.10360287540570737\n",
      "Iteration 311 => Loss: 0.10356916668101569\n",
      "Iteration 312 => Loss: 0.10353560610057388\n",
      "Iteration 313 => Loss: 0.10350219250393701\n",
      "Iteration 314 => Loss: 0.1034689247436896\n",
      "Iteration 315 => Loss: 0.10343580168525662\n",
      "Iteration 316 => Loss: 0.10340282220671705\n",
      "Iteration 317 => Loss: 0.10336998519862133\n",
      "Iteration 318 => Loss: 0.10333728956381172\n",
      "Iteration 319 => Loss: 0.10330473421724605\n",
      "Iteration 320 => Loss: 0.10327231808582428\n",
      "Iteration 321 => Loss: 0.10324004010821847\n",
      "Iteration 322 => Loss: 0.10320789923470525\n",
      "Iteration 323 => Loss: 0.10317589442700166\n",
      "Iteration 324 => Loss: 0.1031440246581035\n",
      "Iteration 325 => Loss: 0.10311228891212668\n",
      "Iteration 326 => Loss: 0.10308068618415112\n",
      "Iteration 327 => Loss: 0.10304921548006755\n",
      "Iteration 328 => Loss: 0.10301787581642674\n",
      "Iteration 329 => Loss: 0.10298666622029135\n",
      "Iteration 330 => Loss: 0.10295558572909037\n",
      "Iteration 331 => Loss: 0.10292463339047596\n",
      "Iteration 332 => Loss: 0.1028938082621827\n",
      "Iteration 333 => Loss: 0.1028631094118893\n",
      "Iteration 334 => Loss: 0.10283253591708247\n",
      "Iteration 335 => Loss: 0.10280208686492331\n",
      "Iteration 336 => Loss: 0.1027717613521157\n",
      "Iteration 337 => Loss: 0.10274155848477685\n",
      "Iteration 338 => Loss: 0.10271147737831043\n",
      "Iteration 339 => Loss: 0.10268151715728124\n",
      "Iteration 340 => Loss: 0.10265167695529216\n",
      "Iteration 341 => Loss: 0.10262195591486344\n",
      "Iteration 342 => Loss: 0.10259235318731329\n",
      "Iteration 343 => Loss: 0.10256286793264116\n",
      "Iteration 344 => Loss: 0.10253349931941241\n",
      "Iteration 345 => Loss: 0.10250424652464503\n",
      "Iteration 346 => Loss: 0.10247510873369822\n",
      "Iteration 347 => Loss: 0.1024460851401627\n",
      "Iteration 348 => Loss: 0.10241717494575288\n",
      "Iteration 349 => Loss: 0.10238837736020062\n",
      "Iteration 350 => Loss: 0.10235969160115087\n",
      "Iteration 351 => Loss: 0.1023311168940588\n",
      "Iteration 352 => Loss: 0.10230265247208857\n",
      "Iteration 353 => Loss: 0.1022742975760141\n",
      "Iteration 354 => Loss: 0.10224605145412068\n",
      "Iteration 355 => Loss: 0.1022179133621088\n",
      "Iteration 356 => Loss: 0.10218988256299916\n",
      "Iteration 357 => Loss: 0.1021619583270393\n",
      "Iteration 358 => Loss: 0.10213413993161152\n",
      "Iteration 359 => Loss: 0.10210642666114246\n",
      "Iteration 360 => Loss: 0.10207881780701399\n",
      "Iteration 361 => Loss: 0.10205131266747529\n",
      "Iteration 362 => Loss: 0.10202391054755672\n",
      "Iteration 363 => Loss: 0.10199661075898456\n",
      "Iteration 364 => Loss: 0.1019694126200974\n",
      "Iteration 365 => Loss: 0.10194231545576359\n",
      "Iteration 366 => Loss: 0.10191531859730008\n",
      "Iteration 367 => Loss: 0.10188842138239247\n",
      "Iteration 368 => Loss: 0.10186162315501626\n",
      "Iteration 369 => Loss: 0.10183492326535916\n",
      "Iteration 370 => Loss: 0.101808321069745\n",
      "Iteration 371 => Loss: 0.10178181593055823\n",
      "Iteration 372 => Loss: 0.1017554072161699\n",
      "Iteration 373 => Loss: 0.10172909430086474\n",
      "Iteration 374 => Loss: 0.10170287656476923\n",
      "Iteration 375 => Loss: 0.10167675339378067\n",
      "Iteration 376 => Loss: 0.1016507241794975\n",
      "Iteration 377 => Loss: 0.10162478831915069\n",
      "Iteration 378 => Loss: 0.10159894521553564\n",
      "Iteration 379 => Loss: 0.10157319427694593\n",
      "Iteration 380 => Loss: 0.10154753491710718\n",
      "Iteration 381 => Loss: 0.10152196655511252\n",
      "Iteration 382 => Loss: 0.10149648861535861\n",
      "Iteration 383 => Loss: 0.10147110052748289\n",
      "Iteration 384 => Loss: 0.10144580172630135\n",
      "Iteration 385 => Loss: 0.10142059165174773\n",
      "Iteration 386 => Loss: 0.10139546974881315\n",
      "Iteration 387 => Loss: 0.10137043546748681\n",
      "Iteration 388 => Loss: 0.1013454882626975\n",
      "Iteration 389 => Loss: 0.10132062759425593\n",
      "Iteration 390 => Loss: 0.10129585292679803\n",
      "Iteration 391 => Loss: 0.10127116372972883\n",
      "Iteration 392 => Loss: 0.1012465594771673\n",
      "Iteration 393 => Loss: 0.10122203964789185\n",
      "Iteration 394 => Loss: 0.10119760372528688\n",
      "Iteration 395 => Loss: 0.10117325119728965\n",
      "Iteration 396 => Loss: 0.10114898155633821\n",
      "Iteration 397 => Loss: 0.10112479429932013\n",
      "Iteration 398 => Loss: 0.10110068892752151\n",
      "Iteration 399 => Loss: 0.10107666494657734\n",
      "Iteration 400 => Loss: 0.1010527218664219\n",
      "Iteration 401 => Loss: 0.10102885920124031\n",
      "Iteration 402 => Loss: 0.10100507646942061\n",
      "Iteration 403 => Loss: 0.10098137319350647\n",
      "Iteration 404 => Loss: 0.10095774890015051\n",
      "Iteration 405 => Loss: 0.10093420312006847\n",
      "Iteration 406 => Loss: 0.10091073538799368\n",
      "Iteration 407 => Loss: 0.10088734524263247\n",
      "Iteration 408 => Loss: 0.10086403222662008\n",
      "Iteration 409 => Loss: 0.10084079588647706\n",
      "Iteration 410 => Loss: 0.10081763577256636\n",
      "Iteration 411 => Loss: 0.10079455143905112\n",
      "Iteration 412 => Loss: 0.10077154244385274\n",
      "Iteration 413 => Loss: 0.10074860834860974\n",
      "Iteration 414 => Loss: 0.10072574871863721\n",
      "Iteration 415 => Loss: 0.10070296312288658\n",
      "Iteration 416 => Loss: 0.10068025113390607\n",
      "Iteration 417 => Loss: 0.10065761232780172\n",
      "Iteration 418 => Loss: 0.10063504628419892\n",
      "Iteration 419 => Loss: 0.10061255258620419\n",
      "Iteration 420 => Loss: 0.10059013082036795\n",
      "Iteration 421 => Loss: 0.10056778057664732\n",
      "Iteration 422 => Loss: 0.10054550144836973\n",
      "Iteration 423 => Loss: 0.10052329303219674\n",
      "Iteration 424 => Loss: 0.10050115492808871\n",
      "Iteration 425 => Loss: 0.10047908673926942\n",
      "Iteration 426 => Loss: 0.10045708807219168\n",
      "Iteration 427 => Loss: 0.10043515853650296\n",
      "Iteration 428 => Loss: 0.10041329774501179\n",
      "Iteration 429 => Loss: 0.10039150531365439\n",
      "Iteration 430 => Loss: 0.10036978086146174\n",
      "Iteration 431 => Loss: 0.10034812401052728\n",
      "Iteration 432 => Loss: 0.10032653438597472\n",
      "Iteration 433 => Loss: 0.10030501161592655\n",
      "Iteration 434 => Loss: 0.10028355533147267\n",
      "Iteration 435 => Loss: 0.10026216516663976\n",
      "Iteration 436 => Loss: 0.10024084075836075\n",
      "Iteration 437 => Loss: 0.10021958174644481\n",
      "Iteration 438 => Loss: 0.10019838777354761\n",
      "Iteration 439 => Loss: 0.10017725848514222\n",
      "Iteration 440 => Loss: 0.10015619352949004\n",
      "Iteration 441 => Loss: 0.10013519255761232\n",
      "Iteration 442 => Loss: 0.10011425522326198\n",
      "Iteration 443 => Loss: 0.10009338118289575\n",
      "Iteration 444 => Loss: 0.10007257009564671\n",
      "Iteration 445 => Loss: 0.10005182162329719\n",
      "Iteration 446 => Loss: 0.10003113543025174\n",
      "Iteration 447 => Loss: 0.10001051118351108\n",
      "Iteration 448 => Loss: 0.09998994855264555\n",
      "Iteration 449 => Loss: 0.0999694472097695\n",
      "Iteration 450 => Loss: 0.09994900682951574\n",
      "Iteration 451 => Loss: 0.09992862708901039\n",
      "Iteration 452 => Loss: 0.09990830766784792\n",
      "Iteration 453 => Loss: 0.09988804824806669\n",
      "Iteration 454 => Loss: 0.0998678485141246\n",
      "Iteration 455 => Loss: 0.0998477081528751\n",
      "Iteration 456 => Loss: 0.09982762685354361\n",
      "Iteration 457 => Loss: 0.09980760430770397\n",
      "Iteration 458 => Loss: 0.09978764020925554\n",
      "Iteration 459 => Loss: 0.09976773425440011\n",
      "Iteration 460 => Loss: 0.09974788614161954\n",
      "Iteration 461 => Loss: 0.09972809557165341\n",
      "Iteration 462 => Loss: 0.09970836224747703\n",
      "Iteration 463 => Loss: 0.09968868587427954\n",
      "Iteration 464 => Loss: 0.09966906615944268\n",
      "Iteration 465 => Loss: 0.09964950281251934\n",
      "Iteration 466 => Loss: 0.09962999554521262\n",
      "Iteration 467 => Loss: 0.09961054407135515\n",
      "Iteration 468 => Loss: 0.09959114810688847\n",
      "Iteration 469 => Loss: 0.09957180736984306\n",
      "Iteration 470 => Loss: 0.09955252158031799\n",
      "Iteration 471 => Loss: 0.09953329046046133\n",
      "Iteration 472 => Loss: 0.09951411373445063\n",
      "Iteration 473 => Loss: 0.0994949911284735\n",
      "Iteration 474 => Loss: 0.09947592237070864\n",
      "Iteration 475 => Loss: 0.09945690719130688\n",
      "Iteration 476 => Loss: 0.09943794532237264\n",
      "Iteration 477 => Loss: 0.09941903649794531\n",
      "Iteration 478 => Loss: 0.09940018045398126\n",
      "Iteration 479 => Loss: 0.09938137692833574\n",
      "Iteration 480 => Loss: 0.09936262566074501\n",
      "Iteration 481 => Loss: 0.09934392639280894\n",
      "Iteration 482 => Loss: 0.09932527886797339\n",
      "Iteration 483 => Loss: 0.09930668283151317\n",
      "Iteration 484 => Loss: 0.09928813803051507\n",
      "Iteration 485 => Loss: 0.09926964421386093\n",
      "Iteration 486 => Loss: 0.09925120113221121\n",
      "Iteration 487 => Loss: 0.09923280853798831\n",
      "Iteration 488 => Loss: 0.09921446618536066\n",
      "Iteration 489 => Loss: 0.09919617383022639\n",
      "Iteration 490 => Loss: 0.09917793123019762\n",
      "Iteration 491 => Loss: 0.09915973814458481\n",
      "Iteration 492 => Loss: 0.09914159433438109\n",
      "Iteration 493 => Loss: 0.09912349956224704\n",
      "Iteration 494 => Loss: 0.09910545359249545\n",
      "Iteration 495 => Loss: 0.09908745619107642\n",
      "Iteration 496 => Loss: 0.09906950712556245\n",
      "Iteration 497 => Loss: 0.09905160616513382\n",
      "Iteration 498 => Loss: 0.09903375308056404\n",
      "Iteration 499 => Loss: 0.09901594764420563\n",
      "Iteration 500 => Loss: 0.0989981896299757\n",
      "Iteration 501 => Loss: 0.09898047881334222\n",
      "Iteration 502 => Loss: 0.09896281497130995\n",
      "Iteration 503 => Loss: 0.09894519788240679\n",
      "Iteration 504 => Loss: 0.09892762732667015\n",
      "Iteration 505 => Loss: 0.0989101030856337\n",
      "Iteration 506 => Loss: 0.09889262494231389\n",
      "Iteration 507 => Loss: 0.09887519268119692\n",
      "Iteration 508 => Loss: 0.09885780608822581\n",
      "Iteration 509 => Loss: 0.09884046495078745\n",
      "Iteration 510 => Loss: 0.09882316905769993\n",
      "Iteration 511 => Loss: 0.09880591819920004\n",
      "Iteration 512 => Loss: 0.09878871216693076\n",
      "Iteration 513 => Loss: 0.09877155075392899\n",
      "Iteration 514 => Loss: 0.09875443375461337\n",
      "Iteration 515 => Loss: 0.09873736096477233\n",
      "Iteration 516 => Loss: 0.09872033218155202\n",
      "Iteration 517 => Loss: 0.09870334720344467\n",
      "Iteration 518 => Loss: 0.09868640583027694\n",
      "Iteration 519 => Loss: 0.09866950786319824\n",
      "Iteration 520 => Loss: 0.09865265310466959\n",
      "Iteration 521 => Loss: 0.098635841358452\n",
      "Iteration 522 => Loss: 0.09861907242959558\n",
      "Iteration 523 => Loss: 0.09860234612442836\n",
      "Iteration 524 => Loss: 0.09858566225054549\n",
      "Iteration 525 => Loss: 0.09856902061679818\n",
      "Iteration 526 => Loss: 0.0985524210332832\n",
      "Iteration 527 => Loss: 0.09853586331133223\n",
      "Iteration 528 => Loss: 0.09851934726350141\n",
      "Iteration 529 => Loss: 0.09850287270356094\n",
      "Iteration 530 => Loss: 0.09848643944648476\n",
      "Iteration 531 => Loss: 0.09847004730844053\n",
      "Iteration 532 => Loss: 0.09845369610677941\n",
      "Iteration 533 => Loss: 0.09843738566002634\n",
      "Iteration 534 => Loss: 0.09842111578786995\n",
      "Iteration 535 => Loss: 0.09840488631115296\n",
      "Iteration 536 => Loss: 0.09838869705186253\n",
      "Iteration 537 => Loss: 0.0983725478331207\n",
      "Iteration 538 => Loss: 0.0983564384791749\n",
      "Iteration 539 => Loss: 0.09834036881538871\n",
      "Iteration 540 => Loss: 0.0983243386682325\n",
      "Iteration 541 => Loss: 0.09830834786527427\n",
      "Iteration 542 => Loss: 0.09829239623517073\n",
      "Iteration 543 => Loss: 0.09827648360765819\n",
      "Iteration 544 => Loss: 0.09826060981354365\n",
      "Iteration 545 => Loss: 0.0982447746846962\n",
      "Iteration 546 => Loss: 0.098228978054038\n",
      "Iteration 547 => Loss: 0.09821321975553605\n",
      "Iteration 548 => Loss: 0.09819749962419339\n",
      "Iteration 549 => Loss: 0.0981818174960407\n",
      "Iteration 550 => Loss: 0.09816617320812797\n",
      "Iteration 551 => Loss: 0.0981505665985163\n",
      "Iteration 552 => Loss: 0.09813499750626958\n",
      "Iteration 553 => Loss: 0.09811946577144647\n",
      "Iteration 554 => Loss: 0.0981039712350923\n",
      "Iteration 555 => Loss: 0.09808851373923118\n",
      "Iteration 556 => Loss: 0.0980730931268581\n",
      "Iteration 557 => Loss: 0.09805770924193115\n",
      "Iteration 558 => Loss: 0.09804236192936379\n",
      "Iteration 559 => Loss: 0.09802705103501723\n",
      "Iteration 560 => Loss: 0.09801177640569278\n",
      "Iteration 561 => Loss: 0.09799653788912457\n",
      "Iteration 562 => Loss: 0.09798133533397184\n",
      "Iteration 563 => Loss: 0.09796616858981186\n",
      "Iteration 564 => Loss: 0.09795103750713247\n",
      "Iteration 565 => Loss: 0.09793594193732504\n",
      "Iteration 566 => Loss: 0.09792088173267714\n",
      "Iteration 567 => Loss: 0.09790585674636566\n",
      "Iteration 568 => Loss: 0.09789086683244973\n",
      "Iteration 569 => Loss: 0.0978759118458638\n",
      "Iteration 570 => Loss: 0.09786099164241086\n",
      "Iteration 571 => Loss: 0.09784610607875541\n",
      "Iteration 572 => Loss: 0.09783125501241709\n",
      "Iteration 573 => Loss: 0.09781643830176362\n",
      "Iteration 574 => Loss: 0.0978016558060046\n",
      "Iteration 575 => Loss: 0.09778690738518468\n",
      "Iteration 576 => Loss: 0.09777219290017715\n",
      "Iteration 577 => Loss: 0.09775751221267767\n",
      "Iteration 578 => Loss: 0.09774286518519772\n",
      "Iteration 579 => Loss: 0.09772825168105843\n",
      "Iteration 580 => Loss: 0.09771367156438428\n",
      "Iteration 581 => Loss: 0.09769912470009706\n",
      "Iteration 582 => Loss: 0.09768461095390957\n",
      "Iteration 583 => Loss: 0.09767013019231974\n",
      "Iteration 584 => Loss: 0.09765568228260446\n",
      "Iteration 585 => Loss: 0.09764126709281384\n",
      "Iteration 586 => Loss: 0.0976268844917651\n",
      "Iteration 587 => Loss: 0.09761253434903688\n",
      "Iteration 588 => Loss: 0.09759821653496349\n",
      "Iteration 589 => Loss: 0.09758393092062911\n",
      "Iteration 590 => Loss: 0.09756967737786207\n",
      "Iteration 591 => Loss: 0.0975554557792294\n",
      "Iteration 592 => Loss: 0.09754126599803108\n",
      "Iteration 593 => Loss: 0.09752710790829472\n",
      "Iteration 594 => Loss: 0.09751298138476991\n",
      "Iteration 595 => Loss: 0.09749888630292294\n",
      "Iteration 596 => Loss: 0.09748482253893136\n",
      "Iteration 597 => Loss: 0.09747078996967873\n",
      "Iteration 598 => Loss: 0.09745678847274931\n",
      "Iteration 599 => Loss: 0.09744281792642291\n",
      "Iteration 600 => Loss: 0.09742887820966958\n",
      "Iteration 601 => Loss: 0.09741496920214471\n",
      "Iteration 602 => Loss: 0.0974010907841837\n",
      "Iteration 603 => Loss: 0.09738724283679717\n",
      "Iteration 604 => Loss: 0.09737342524166583\n",
      "Iteration 605 => Loss: 0.09735963788113561\n",
      "Iteration 606 => Loss: 0.09734588063821273\n",
      "Iteration 607 => Loss: 0.0973321533965589\n",
      "Iteration 608 => Loss: 0.09731845604048646\n",
      "Iteration 609 => Loss: 0.09730478845495365\n",
      "Iteration 610 => Loss: 0.09729115052555996\n",
      "Iteration 611 => Loss: 0.09727754213854128\n",
      "Iteration 612 => Loss: 0.09726396318076545\n",
      "Iteration 613 => Loss: 0.09725041353972762\n",
      "Iteration 614 => Loss: 0.09723689310354557\n",
      "Iteration 615 => Loss: 0.09722340176095545\n",
      "Iteration 616 => Loss: 0.09720993940130705\n",
      "Iteration 617 => Loss: 0.09719650591455956\n",
      "Iteration 618 => Loss: 0.09718310119127706\n",
      "Iteration 619 => Loss: 0.09716972512262427\n",
      "Iteration 620 => Loss: 0.09715637760036218\n",
      "Iteration 621 => Loss: 0.09714305851684375\n",
      "Iteration 622 => Loss: 0.09712976776500974\n",
      "Iteration 623 => Loss: 0.09711650523838451\n",
      "Iteration 624 => Loss: 0.0971032708310717\n",
      "Iteration 625 => Loss: 0.09709006443775041\n",
      "Iteration 626 => Loss: 0.09707688595367077\n",
      "Iteration 627 => Loss: 0.09706373527465018\n",
      "Iteration 628 => Loss: 0.09705061229706904\n",
      "Iteration 629 => Loss: 0.09703751691786701\n",
      "Iteration 630 => Loss: 0.0970244490345389\n",
      "Iteration 631 => Loss: 0.09701140854513075\n",
      "Iteration 632 => Loss: 0.09699839534823602\n",
      "Iteration 633 => Loss: 0.09698540934299175\n",
      "Iteration 634 => Loss: 0.09697245042907472\n",
      "Iteration 635 => Loss: 0.09695951850669762\n",
      "Iteration 636 => Loss: 0.09694661347660538\n",
      "Iteration 637 => Loss: 0.09693373524007146\n",
      "Iteration 638 => Loss: 0.0969208836988941\n",
      "Iteration 639 => Loss: 0.09690805875539267\n",
      "Iteration 640 => Loss: 0.09689526031240414\n",
      "Iteration 641 => Loss: 0.09688248827327943\n",
      "Iteration 642 => Loss: 0.09686974254187979\n",
      "Iteration 643 => Loss: 0.09685702302257342\n",
      "Iteration 644 => Loss: 0.09684432962023187\n",
      "Iteration 645 => Loss: 0.09683166224022655\n",
      "Iteration 646 => Loss: 0.09681902078842533\n",
      "Iteration 647 => Loss: 0.0968064051711892\n",
      "Iteration 648 => Loss: 0.09679381529536876\n",
      "Iteration 649 => Loss: 0.09678125106830096\n",
      "Iteration 650 => Loss: 0.09676871239780575\n",
      "Iteration 651 => Loss: 0.09675619919218278\n",
      "Iteration 652 => Loss: 0.09674371136020805\n",
      "Iteration 653 => Loss: 0.09673124881113089\n",
      "Iteration 654 => Loss: 0.09671881145467055\n",
      "Iteration 655 => Loss: 0.09670639920101301\n",
      "Iteration 656 => Loss: 0.09669401196080794\n",
      "Iteration 657 => Loss: 0.09668164964516553\n",
      "Iteration 658 => Loss: 0.09666931216565335\n",
      "Iteration 659 => Loss: 0.09665699943429318\n",
      "Iteration 660 => Loss: 0.0966447113635582\n",
      "Iteration 661 => Loss: 0.09663244786636974\n",
      "Iteration 662 => Loss: 0.09662020885609435\n",
      "Iteration 663 => Loss: 0.0966079942465408\n",
      "Iteration 664 => Loss: 0.09659580395195717\n",
      "Iteration 665 => Loss: 0.09658363788702788\n",
      "Iteration 666 => Loss: 0.09657149596687077\n",
      "Iteration 667 => Loss: 0.09655937810703431\n",
      "Iteration 668 => Loss: 0.09654728422349454\n",
      "Iteration 669 => Loss: 0.09653521423265249\n",
      "Iteration 670 => Loss: 0.09652316805133111\n",
      "Iteration 671 => Loss: 0.09651114559677278\n",
      "Iteration 672 => Loss: 0.09649914678663618\n",
      "Iteration 673 => Loss: 0.09648717153899385\n",
      "Iteration 674 => Loss: 0.0964752197723293\n",
      "Iteration 675 => Loss: 0.09646329140553438\n",
      "Iteration 676 => Loss: 0.09645138635790663\n",
      "Iteration 677 => Loss: 0.09643950454914646\n",
      "Iteration 678 => Loss: 0.0964276458993547\n",
      "Iteration 679 => Loss: 0.09641581032902995\n",
      "Iteration 680 => Loss: 0.09640399775906587\n",
      "Iteration 681 => Loss: 0.09639220811074874\n",
      "Iteration 682 => Loss: 0.09638044130575486\n",
      "Iteration 683 => Loss: 0.09636869726614794\n",
      "Iteration 684 => Loss: 0.09635697591437681\n",
      "Iteration 685 => Loss: 0.09634527717327258\n",
      "Iteration 686 => Loss: 0.0963336009660466\n",
      "Iteration 687 => Loss: 0.09632194721628756\n",
      "Iteration 688 => Loss: 0.09631031584795943\n",
      "Iteration 689 => Loss: 0.09629870678539888\n",
      "Iteration 690 => Loss: 0.09628711995331284\n",
      "Iteration 691 => Loss: 0.09627555527677621\n",
      "Iteration 692 => Loss: 0.09626401268122951\n",
      "Iteration 693 => Loss: 0.09625249209247647\n",
      "Iteration 694 => Loss: 0.0962409934366818\n",
      "Iteration 695 => Loss: 0.09622951664036881\n",
      "Iteration 696 => Loss: 0.09621806163041721\n",
      "Iteration 697 => Loss: 0.09620662833406074\n",
      "Iteration 698 => Loss: 0.09619521667888502\n",
      "Iteration 699 => Loss: 0.09618382659282529\n",
      "Iteration 700 => Loss: 0.09617245800416414\n",
      "Iteration 701 => Loss: 0.09616111084152947\n",
      "Iteration 702 => Loss: 0.09614978503389207\n",
      "Iteration 703 => Loss: 0.09613848051056369\n",
      "Iteration 704 => Loss: 0.09612719720119484\n",
      "Iteration 705 => Loss: 0.09611593503577252\n",
      "Iteration 706 => Loss: 0.0961046939446183\n",
      "Iteration 707 => Loss: 0.09609347385838615\n",
      "Iteration 708 => Loss: 0.09608227470806029\n",
      "Iteration 709 => Loss: 0.09607109642495325\n",
      "Iteration 710 => Loss: 0.09605993894070376\n",
      "Iteration 711 => Loss: 0.09604880218727473\n",
      "Iteration 712 => Loss: 0.0960376860969512\n",
      "Iteration 713 => Loss: 0.09602659060233842\n",
      "Iteration 714 => Loss: 0.09601551563635984\n",
      "Iteration 715 => Loss: 0.09600446113225501\n",
      "Iteration 716 => Loss: 0.0959934270235779\n",
      "Iteration 717 => Loss: 0.09598241324419467\n",
      "Iteration 718 => Loss: 0.09597141972828192\n",
      "Iteration 719 => Loss: 0.09596044641032478\n",
      "Iteration 720 => Loss: 0.09594949322511491\n",
      "Iteration 721 => Loss: 0.09593856010774868\n",
      "Iteration 722 => Loss: 0.0959276469936254\n",
      "Iteration 723 => Loss: 0.09591675381844521\n",
      "Iteration 724 => Loss: 0.09590588051820755\n",
      "Iteration 725 => Loss: 0.09589502702920916\n",
      "Iteration 726 => Loss: 0.09588419328804225\n",
      "Iteration 727 => Loss: 0.09587337923159282\n",
      "Iteration 728 => Loss: 0.09586258479703874\n",
      "Iteration 729 => Loss: 0.09585180992184808\n",
      "Iteration 730 => Loss: 0.09584105454377738\n",
      "Iteration 731 => Loss: 0.09583031860086973\n",
      "Iteration 732 => Loss: 0.09581960203145329\n",
      "Iteration 733 => Loss: 0.09580890477413934\n",
      "Iteration 734 => Loss: 0.09579822676782072\n",
      "Iteration 735 => Loss: 0.0957875679516701\n",
      "Iteration 736 => Loss: 0.09577692826513828\n",
      "Iteration 737 => Loss: 0.09576630764795253\n",
      "Iteration 738 => Loss: 0.09575570604011498\n",
      "Iteration 739 => Loss: 0.09574512338190089\n",
      "Iteration 740 => Loss: 0.0957345596138571\n",
      "Iteration 741 => Loss: 0.09572401467680035\n",
      "Iteration 742 => Loss: 0.09571348851181578\n",
      "Iteration 743 => Loss: 0.09570298106025514\n",
      "Iteration 744 => Loss: 0.09569249226373545\n",
      "Iteration 745 => Loss: 0.09568202206413719\n",
      "Iteration 746 => Loss: 0.09567157040360295\n",
      "Iteration 747 => Loss: 0.09566113722453574\n",
      "Iteration 748 => Loss: 0.09565072246959748\n",
      "Iteration 749 => Loss: 0.09564032608170754\n",
      "Iteration 750 => Loss: 0.09562994800404116\n",
      "Iteration 751 => Loss: 0.09561958818002793\n",
      "Iteration 752 => Loss: 0.09560924655335039\n",
      "Iteration 753 => Loss: 0.09559892306794242\n",
      "Iteration 754 => Loss: 0.09558861766798792\n",
      "Iteration 755 => Loss: 0.09557833029791916\n",
      "Iteration 756 => Loss: 0.09556806090241556\n",
      "Iteration 757 => Loss: 0.09555780942640203\n",
      "Iteration 758 => Loss: 0.09554757581504766\n",
      "Iteration 759 => Loss: 0.09553736001376432\n",
      "Iteration 760 => Loss: 0.09552716196820514\n",
      "Iteration 761 => Loss: 0.09551698162426327\n",
      "Iteration 762 => Loss: 0.09550681892807024\n",
      "Iteration 763 => Loss: 0.09549667382599496\n",
      "Iteration 764 => Loss: 0.09548654626464195\n",
      "Iteration 765 => Loss: 0.09547643619085028\n",
      "Iteration 766 => Loss: 0.09546634355169199\n",
      "Iteration 767 => Loss: 0.09545626829447101\n",
      "Iteration 768 => Loss: 0.09544621036672155\n",
      "Iteration 769 => Loss: 0.095436169716207\n",
      "Iteration 770 => Loss: 0.09542614629091849\n",
      "Iteration 771 => Loss: 0.09541614003907362\n",
      "Iteration 772 => Loss: 0.09540615090911521\n",
      "Iteration 773 => Loss: 0.09539617884970997\n",
      "Iteration 774 => Loss: 0.0953862238097472\n",
      "Iteration 775 => Loss: 0.09537628573833762\n",
      "Iteration 776 => Loss: 0.09536636458481201\n",
      "Iteration 777 => Loss: 0.09535646029871998\n",
      "Iteration 778 => Loss: 0.09534657282982886\n",
      "Iteration 779 => Loss: 0.09533670212812223\n",
      "Iteration 780 => Loss: 0.09532684814379896\n",
      "Iteration 781 => Loss: 0.09531701082727177\n",
      "Iteration 782 => Loss: 0.09530719012916615\n",
      "Iteration 783 => Loss: 0.09529738600031915\n",
      "Iteration 784 => Loss: 0.09528759839177818\n",
      "Iteration 785 => Loss: 0.0952778272547998\n",
      "Iteration 786 => Loss: 0.09526807254084861\n",
      "Iteration 787 => Loss: 0.09525833420159605\n",
      "Iteration 788 => Loss: 0.09524861218891917\n",
      "Iteration 789 => Loss: 0.09523890645489967\n",
      "Iteration 790 => Loss: 0.09522921695182258\n",
      "Iteration 791 => Loss: 0.09521954363217518\n",
      "Iteration 792 => Loss: 0.09520988644864595\n",
      "Iteration 793 => Loss: 0.09520024535412334\n",
      "Iteration 794 => Loss: 0.09519062030169474\n",
      "Iteration 795 => Loss: 0.09518101124464533\n",
      "Iteration 796 => Loss: 0.09517141813645708\n",
      "Iteration 797 => Loss: 0.09516184093080748\n",
      "Iteration 798 => Loss: 0.09515227958156866\n",
      "Iteration 799 => Loss: 0.09514273404280621\n",
      "Iteration 800 => Loss: 0.09513320426877811\n",
      "Iteration 801 => Loss: 0.09512369021393378\n",
      "Iteration 802 => Loss: 0.09511419183291282\n",
      "Iteration 803 => Loss: 0.09510470908054425\n",
      "Iteration 804 => Loss: 0.09509524191184521\n",
      "Iteration 805 => Loss: 0.09508579028202004\n",
      "Iteration 806 => Loss: 0.09507635414645936\n",
      "Iteration 807 => Loss: 0.0950669334607389\n",
      "Iteration 808 => Loss: 0.09505752818061851\n",
      "Iteration 809 => Loss: 0.09504813826204127\n",
      "Iteration 810 => Loss: 0.09503876366113238\n",
      "Iteration 811 => Loss: 0.09502940433419822\n",
      "Iteration 812 => Loss: 0.09502006023772536\n",
      "Iteration 813 => Loss: 0.09501073132837964\n",
      "Iteration 814 => Loss: 0.09500141756300509\n",
      "Iteration 815 => Loss: 0.09499211889862304\n",
      "Iteration 816 => Loss: 0.09498283529243118\n",
      "Iteration 817 => Loss: 0.09497356670180257\n",
      "Iteration 818 => Loss: 0.09496431308428477\n",
      "Iteration 819 => Loss: 0.09495507439759868\n",
      "Iteration 820 => Loss: 0.09494585059963799\n",
      "Iteration 821 => Loss: 0.09493664164846788\n",
      "Iteration 822 => Loss: 0.09492744750232439\n",
      "Iteration 823 => Loss: 0.09491826811961332\n",
      "Iteration 824 => Loss: 0.09490910345890934\n",
      "Iteration 825 => Loss: 0.09489995347895525\n",
      "Iteration 826 => Loss: 0.09489081813866089\n",
      "Iteration 827 => Loss: 0.09488169739710235\n",
      "Iteration 828 => Loss: 0.0948725912135211\n",
      "Iteration 829 => Loss: 0.09486349954732311\n",
      "Iteration 830 => Loss: 0.09485442235807788\n",
      "Iteration 831 => Loss: 0.09484535960551774\n",
      "Iteration 832 => Loss: 0.09483631124953681\n",
      "Iteration 833 => Loss: 0.09482727725019031\n",
      "Iteration 834 => Loss: 0.09481825756769369\n",
      "Iteration 835 => Loss: 0.09480925216242159\n",
      "Iteration 836 => Loss: 0.09480026099490726\n",
      "Iteration 837 => Loss: 0.09479128402584167\n",
      "Iteration 838 => Loss: 0.09478232121607248\n",
      "Iteration 839 => Loss: 0.09477337252660345\n",
      "Iteration 840 => Loss: 0.09476443791859367\n",
      "Iteration 841 => Loss: 0.09475551735335642\n",
      "Iteration 842 => Loss: 0.09474661079235869\n",
      "Iteration 843 => Loss: 0.09473771819722028\n",
      "Iteration 844 => Loss: 0.09472883952971294\n",
      "Iteration 845 => Loss: 0.09471997475175961\n",
      "Iteration 846 => Loss: 0.09471112382543377\n",
      "Iteration 847 => Loss: 0.09470228671295838\n",
      "Iteration 848 => Loss: 0.09469346337670546\n",
      "Iteration 849 => Loss: 0.09468465377919495\n",
      "Iteration 850 => Loss: 0.09467585788309425\n",
      "Iteration 851 => Loss: 0.09466707565121729\n",
      "Iteration 852 => Loss: 0.09465830704652382\n",
      "Iteration 853 => Loss: 0.09464955203211871\n",
      "Iteration 854 => Loss: 0.09464081057125107\n",
      "Iteration 855 => Loss: 0.09463208262731372\n",
      "Iteration 856 => Loss: 0.09462336816384219\n",
      "Iteration 857 => Loss: 0.09461466714451429\n",
      "Iteration 858 => Loss: 0.09460597953314906\n",
      "Iteration 859 => Loss: 0.09459730529370634\n",
      "Iteration 860 => Loss: 0.09458864439028586\n",
      "Iteration 861 => Loss: 0.09457999678712661\n",
      "Iteration 862 => Loss: 0.09457136244860612\n",
      "Iteration 863 => Loss: 0.09456274133923974\n",
      "Iteration 864 => Loss: 0.09455413342367995\n",
      "Iteration 865 => Loss: 0.09454553866671571\n",
      "Iteration 866 => Loss: 0.09453695703327158\n",
      "Iteration 867 => Loss: 0.09452838848840743\n",
      "Iteration 868 => Loss: 0.09451983299731728\n",
      "Iteration 869 => Loss: 0.09451129052532889\n",
      "Iteration 870 => Loss: 0.09450276103790317\n",
      "Iteration 871 => Loss: 0.09449424450063326\n",
      "Iteration 872 => Loss: 0.09448574087924404\n",
      "Iteration 873 => Loss: 0.09447725013959145\n",
      "Iteration 874 => Loss: 0.09446877224766169\n",
      "Iteration 875 => Loss: 0.09446030716957081\n",
      "Iteration 876 => Loss: 0.09445185487156386\n",
      "Iteration 877 => Loss: 0.09444341532001432\n",
      "Iteration 878 => Loss: 0.09443498848142352\n",
      "Iteration 879 => Loss: 0.09442657432241984\n",
      "Iteration 880 => Loss: 0.09441817280975823\n",
      "Iteration 881 => Loss: 0.09440978391031955\n",
      "Iteration 882 => Loss: 0.09440140759110986\n",
      "Iteration 883 => Loss: 0.09439304381925992\n",
      "Iteration 884 => Loss: 0.09438469256202445\n",
      "Iteration 885 => Loss: 0.09437635378678164\n",
      "Iteration 886 => Loss: 0.09436802746103246\n",
      "Iteration 887 => Loss: 0.09435971355240005\n",
      "Iteration 888 => Loss: 0.09435141202862918\n",
      "Iteration 889 => Loss: 0.09434312285758559\n",
      "Iteration 890 => Loss: 0.09433484600725536\n",
      "Iteration 891 => Loss: 0.0943265814457445\n",
      "Iteration 892 => Loss: 0.09431832914127815\n",
      "Iteration 893 => Loss: 0.09431008906220009\n",
      "Iteration 894 => Loss: 0.09430186117697216\n",
      "Iteration 895 => Loss: 0.09429364545417367\n",
      "Iteration 896 => Loss: 0.09428544186250087\n",
      "Iteration 897 => Loss: 0.09427725037076629\n",
      "Iteration 898 => Loss: 0.0942690709478982\n",
      "Iteration 899 => Loss: 0.0942609035629402\n",
      "Iteration 900 => Loss: 0.09425274818505035\n",
      "Iteration 901 => Loss: 0.09424460478350093\n",
      "Iteration 902 => Loss: 0.09423647332767772\n",
      "Iteration 903 => Loss: 0.09422835378707939\n",
      "Iteration 904 => Loss: 0.09422024613131719\n",
      "Iteration 905 => Loss: 0.09421215033011417\n",
      "Iteration 906 => Loss: 0.09420406635330471\n",
      "Iteration 907 => Loss: 0.09419599417083406\n",
      "Iteration 908 => Loss: 0.09418793375275769\n",
      "Iteration 909 => Loss: 0.09417988506924094\n",
      "Iteration 910 => Loss: 0.09417184809055817\n",
      "Iteration 911 => Loss: 0.09416382278709262\n",
      "Iteration 912 => Loss: 0.09415580912933562\n",
      "Iteration 913 => Loss: 0.09414780708788616\n",
      "Iteration 914 => Loss: 0.09413981663345038\n",
      "Iteration 915 => Loss: 0.09413183773684106\n",
      "Iteration 916 => Loss: 0.09412387036897703\n",
      "Iteration 917 => Loss: 0.0941159145008829\n",
      "Iteration 918 => Loss: 0.09410797010368824\n",
      "Iteration 919 => Loss: 0.09410003714862726\n",
      "Iteration 920 => Loss: 0.09409211560703831\n",
      "Iteration 921 => Loss: 0.09408420545036335\n",
      "Iteration 922 => Loss: 0.09407630665014749\n",
      "Iteration 923 => Loss: 0.09406841917803845\n",
      "Iteration 924 => Loss: 0.09406054300578613\n",
      "Iteration 925 => Loss: 0.09405267810524212\n",
      "Iteration 926 => Loss: 0.09404482444835913\n",
      "Iteration 927 => Loss: 0.09403698200719064\n",
      "Iteration 928 => Loss: 0.09402915075389037\n",
      "Iteration 929 => Loss: 0.09402133066071183\n",
      "Iteration 930 => Loss: 0.09401352170000775\n",
      "Iteration 931 => Loss: 0.0940057238442298\n",
      "Iteration 932 => Loss: 0.09399793706592793\n",
      "Iteration 933 => Loss: 0.0939901613377501\n",
      "Iteration 934 => Loss: 0.09398239663244165\n",
      "Iteration 935 => Loss: 0.09397464292284492\n",
      "Iteration 936 => Loss: 0.09396690018189882\n",
      "Iteration 937 => Loss: 0.09395916838263836\n",
      "Iteration 938 => Loss: 0.09395144749819419\n",
      "Iteration 939 => Loss: 0.09394373750179218\n",
      "Iteration 940 => Loss: 0.09393603836675289\n",
      "Iteration 941 => Loss: 0.09392835006649128\n",
      "Iteration 942 => Loss: 0.09392067257451621\n",
      "Iteration 943 => Loss: 0.0939130058644299\n",
      "Iteration 944 => Loss: 0.09390534990992767\n",
      "Iteration 945 => Loss: 0.09389770468479738\n",
      "Iteration 946 => Loss: 0.09389007016291907\n",
      "Iteration 947 => Loss: 0.09388244631826452\n",
      "Iteration 948 => Loss: 0.09387483312489679\n",
      "Iteration 949 => Loss: 0.09386723055696992\n",
      "Iteration 950 => Loss: 0.09385963858872828\n",
      "Iteration 951 => Loss: 0.09385205719450646\n",
      "Iteration 952 => Loss: 0.09384448634872858\n",
      "Iteration 953 => Loss: 0.09383692602590804\n",
      "Iteration 954 => Loss: 0.0938293762006471\n",
      "Iteration 955 => Loss: 0.09382183684763641\n",
      "Iteration 956 => Loss: 0.09381430794165457\n",
      "Iteration 957 => Loss: 0.09380678945756797\n",
      "Iteration 958 => Loss: 0.09379928137033007\n",
      "Iteration 959 => Loss: 0.09379178365498123\n",
      "Iteration 960 => Loss: 0.09378429628664815\n",
      "Iteration 961 => Loss: 0.09377681924054367\n",
      "Iteration 962 => Loss: 0.09376935249196622\n",
      "Iteration 963 => Loss: 0.0937618960162995\n",
      "Iteration 964 => Loss: 0.0937544497890121\n",
      "Iteration 965 => Loss: 0.09374701378565703\n",
      "Iteration 966 => Loss: 0.09373958798187149\n",
      "Iteration 967 => Loss: 0.09373217235337633\n",
      "Iteration 968 => Loss: 0.09372476687597585\n",
      "Iteration 969 => Loss: 0.09371737152555722\n",
      "Iteration 970 => Loss: 0.0937099862780903\n",
      "Iteration 971 => Loss: 0.09370261110962708\n",
      "Iteration 972 => Loss: 0.0936952459963015\n",
      "Iteration 973 => Loss: 0.09368789091432897\n",
      "Iteration 974 => Loss: 0.09368054584000603\n",
      "Iteration 975 => Loss: 0.09367321074970995\n",
      "Iteration 976 => Loss: 0.0936658856198985\n",
      "Iteration 977 => Loss: 0.09365857042710939\n",
      "Iteration 978 => Loss: 0.0936512651479601\n",
      "Iteration 979 => Loss: 0.09364396975914738\n",
      "Iteration 980 => Loss: 0.09363668423744705\n",
      "Iteration 981 => Loss: 0.09362940855971344\n",
      "Iteration 982 => Loss: 0.0936221427028793\n",
      "Iteration 983 => Loss: 0.09361488664395524\n",
      "Iteration 984 => Loss: 0.09360764036002947\n",
      "Iteration 985 => Loss: 0.09360040382826745\n",
      "Iteration 986 => Loss: 0.09359317702591155\n",
      "Iteration 987 => Loss: 0.09358595993028072\n",
      "Iteration 988 => Loss: 0.0935787525187701\n",
      "Iteration 989 => Loss: 0.0935715547688508\n",
      "Iteration 990 => Loss: 0.0935643666580694\n",
      "Iteration 991 => Loss: 0.09355718816404773\n",
      "Iteration 992 => Loss: 0.09355001926448261\n",
      "Iteration 993 => Loss: 0.0935428599371453\n",
      "Iteration 994 => Loss: 0.09353571015988134\n",
      "Iteration 995 => Loss: 0.09352856991061027\n",
      "Iteration 996 => Loss: 0.0935214391673251\n",
      "Iteration 997 => Loss: 0.0935143179080922\n",
      "Iteration 998 => Loss: 0.09350720611105084\n",
      "Iteration 999 => Loss: 0.09350010375441302\n",
      "Iteration 1000 => Loss: 0.09349301081646287\n",
      "Iteration 1001 => Loss: 0.09348592727555673\n",
      "Iteration 1002 => Loss: 0.09347885311012248\n",
      "Iteration 1003 => Loss: 0.09347178829865949\n",
      "Iteration 1004 => Loss: 0.09346473281973813\n",
      "Iteration 1005 => Loss: 0.09345768665199955\n",
      "Iteration 1006 => Loss: 0.09345064977415535\n",
      "Iteration 1007 => Loss: 0.09344362216498729\n",
      "Iteration 1008 => Loss: 0.093436603803347\n",
      "Iteration 1009 => Loss: 0.09342959466815559\n",
      "Iteration 1010 => Loss: 0.09342259473840349\n",
      "Iteration 1011 => Loss: 0.09341560399315008\n",
      "Iteration 1012 => Loss: 0.09340862241152331\n",
      "Iteration 1013 => Loss: 0.09340164997271956\n",
      "Iteration 1014 => Loss: 0.09339468665600323\n",
      "Iteration 1015 => Loss: 0.0933877324407066\n",
      "Iteration 1016 => Loss: 0.09338078730622923\n",
      "Iteration 1017 => Loss: 0.09337385123203806\n",
      "Iteration 1018 => Loss: 0.0933669241976669\n",
      "Iteration 1019 => Loss: 0.09336000618271605\n",
      "Iteration 1020 => Loss: 0.09335309716685232\n",
      "Iteration 1021 => Loss: 0.09334619712980849\n",
      "Iteration 1022 => Loss: 0.0933393060513831\n",
      "Iteration 1023 => Loss: 0.09333242391144018\n",
      "Iteration 1024 => Loss: 0.09332555068990907\n",
      "Iteration 1025 => Loss: 0.09331868636678389\n",
      "Iteration 1026 => Loss: 0.09331183092212356\n",
      "Iteration 1027 => Loss: 0.09330498433605133\n",
      "Iteration 1028 => Loss: 0.09329814658875452\n",
      "Iteration 1029 => Loss: 0.09329131766048442\n",
      "Iteration 1030 => Loss: 0.09328449753155577\n",
      "Iteration 1031 => Loss: 0.09327768618234666\n",
      "Iteration 1032 => Loss: 0.09327088359329826\n",
      "Iteration 1033 => Loss: 0.09326408974491446\n",
      "Iteration 1034 => Loss: 0.09325730461776169\n",
      "Iteration 1035 => Loss: 0.09325052819246864\n",
      "Iteration 1036 => Loss: 0.09324376044972595\n",
      "Iteration 1037 => Loss: 0.09323700137028598\n",
      "Iteration 1038 => Loss: 0.09323025093496262\n",
      "Iteration 1039 => Loss: 0.09322350912463093\n",
      "Iteration 1040 => Loss: 0.0932167759202269\n",
      "Iteration 1041 => Loss: 0.0932100513027473\n",
      "Iteration 1042 => Loss: 0.09320333525324925\n",
      "Iteration 1043 => Loss: 0.09319662775285015\n",
      "Iteration 1044 => Loss: 0.09318992878272728\n",
      "Iteration 1045 => Loss: 0.09318323832411765\n",
      "Iteration 1046 => Loss: 0.09317655635831773\n",
      "Iteration 1047 => Loss: 0.09316988286668317\n",
      "Iteration 1048 => Loss: 0.09316321783062853\n",
      "Iteration 1049 => Loss: 0.0931565612316272\n",
      "Iteration 1050 => Loss: 0.0931499130512109\n",
      "Iteration 1051 => Loss: 0.09314327327096972\n",
      "Iteration 1052 => Loss: 0.09313664187255158\n",
      "Iteration 1053 => Loss: 0.09313001883766225\n",
      "Iteration 1054 => Loss: 0.09312340414806501\n",
      "Iteration 1055 => Loss: 0.09311679778558035\n",
      "Iteration 1056 => Loss: 0.09311019973208584\n",
      "Iteration 1057 => Loss: 0.09310360996951582\n",
      "Iteration 1058 => Loss: 0.09309702847986129\n",
      "Iteration 1059 => Loss: 0.09309045524516944\n",
      "Iteration 1060 => Loss: 0.09308389024754367\n",
      "Iteration 1061 => Loss: 0.09307733346914322\n",
      "Iteration 1062 => Loss: 0.09307078489218297\n",
      "Iteration 1063 => Loss: 0.09306424449893326\n",
      "Iteration 1064 => Loss: 0.09305771227171956\n",
      "Iteration 1065 => Loss: 0.09305118819292237\n",
      "Iteration 1066 => Loss: 0.09304467224497692\n",
      "Iteration 1067 => Loss: 0.0930381644103729\n",
      "Iteration 1068 => Loss: 0.09303166467165438\n",
      "Iteration 1069 => Loss: 0.09302517301141947\n",
      "Iteration 1070 => Loss: 0.09301868941232015\n",
      "Iteration 1071 => Loss: 0.093012213857062\n",
      "Iteration 1072 => Loss: 0.09300574632840412\n",
      "Iteration 1073 => Loss: 0.09299928680915873\n",
      "Iteration 1074 => Loss: 0.09299283528219106\n",
      "Iteration 1075 => Loss: 0.09298639173041909\n",
      "Iteration 1076 => Loss: 0.09297995613681354\n",
      "Iteration 1077 => Loss: 0.09297352848439722\n",
      "Iteration 1078 => Loss: 0.09296710875624521\n",
      "Iteration 1079 => Loss: 0.09296069693548463\n",
      "Iteration 1080 => Loss: 0.0929542930052941\n",
      "Iteration 1081 => Loss: 0.09294789694890392\n",
      "Iteration 1082 => Loss: 0.09294150874959568\n",
      "Iteration 1083 => Loss: 0.09293512839070202\n",
      "Iteration 1084 => Loss: 0.09292875585560648\n",
      "Iteration 1085 => Loss: 0.09292239112774336\n",
      "Iteration 1086 => Loss: 0.09291603419059741\n",
      "Iteration 1087 => Loss: 0.09290968502770365\n",
      "Iteration 1088 => Loss: 0.09290334362264725\n",
      "Iteration 1089 => Loss: 0.09289700995906323\n",
      "Iteration 1090 => Loss: 0.09289068402063633\n",
      "Iteration 1091 => Loss: 0.0928843657911007\n",
      "Iteration 1092 => Loss: 0.09287805525423992\n",
      "Iteration 1093 => Loss: 0.09287175239388658\n",
      "Iteration 1094 => Loss: 0.0928654571939222\n",
      "Iteration 1095 => Loss: 0.09285916963827703\n",
      "Iteration 1096 => Loss: 0.09285288971092985\n",
      "Iteration 1097 => Loss: 0.09284661739590772\n",
      "Iteration 1098 => Loss: 0.09284035267728585\n",
      "Iteration 1099 => Loss: 0.09283409553918745\n",
      "Iteration 1100 => Loss: 0.09282784596578346\n",
      "Iteration 1101 => Loss: 0.09282160394129235\n",
      "Iteration 1102 => Loss: 0.09281536944998005\n",
      "Iteration 1103 => Loss: 0.09280914247615961\n",
      "Iteration 1104 => Loss: 0.09280292300419113\n",
      "Iteration 1105 => Loss: 0.09279671101848157\n",
      "Iteration 1106 => Loss: 0.09279050650348449\n",
      "Iteration 1107 => Loss: 0.09278430944369989\n",
      "Iteration 1108 => Loss: 0.09277811982367408\n",
      "Iteration 1109 => Loss: 0.0927719376279995\n",
      "Iteration 1110 => Loss: 0.09276576284131448\n",
      "Iteration 1111 => Loss: 0.092759595448303\n",
      "Iteration 1112 => Loss: 0.09275343543369481\n",
      "Iteration 1113 => Loss: 0.09274728278226482\n",
      "Iteration 1114 => Loss: 0.09274113747883331\n",
      "Iteration 1115 => Loss: 0.09273499950826543\n",
      "Iteration 1116 => Loss: 0.09272886885547138\n",
      "Iteration 1117 => Loss: 0.09272274550540588\n",
      "Iteration 1118 => Loss: 0.09271662944306826\n",
      "Iteration 1119 => Loss: 0.09271052065350208\n",
      "Iteration 1120 => Loss: 0.09270441912179517\n",
      "Iteration 1121 => Loss: 0.0926983248330793\n",
      "Iteration 1122 => Loss: 0.09269223777253009\n",
      "Iteration 1123 => Loss: 0.0926861579253668\n",
      "Iteration 1124 => Loss: 0.09268008527685213\n",
      "Iteration 1125 => Loss: 0.09267401981229216\n",
      "Iteration 1126 => Loss: 0.09266796151703612\n",
      "Iteration 1127 => Loss: 0.09266191037647618\n",
      "Iteration 1128 => Loss: 0.09265586637604736\n",
      "Iteration 1129 => Loss: 0.09264982950122735\n",
      "Iteration 1130 => Loss: 0.09264379973753632\n",
      "Iteration 1131 => Loss: 0.09263777707053675\n",
      "Iteration 1132 => Loss: 0.09263176148583326\n",
      "Iteration 1133 => Loss: 0.09262575296907258\n",
      "Iteration 1134 => Loss: 0.09261975150594315\n",
      "Iteration 1135 => Loss: 0.09261375708217526\n",
      "Iteration 1136 => Loss: 0.09260776968354058\n",
      "Iteration 1137 => Loss: 0.09260178929585221\n",
      "Iteration 1138 => Loss: 0.09259581590496442\n",
      "Iteration 1139 => Loss: 0.09258984949677262\n",
      "Iteration 1140 => Loss: 0.09258389005721303\n",
      "Iteration 1141 => Loss: 0.09257793757226267\n",
      "Iteration 1142 => Loss: 0.09257199202793909\n",
      "Iteration 1143 => Loss: 0.09256605341030032\n",
      "Iteration 1144 => Loss: 0.09256012170544468\n",
      "Iteration 1145 => Loss: 0.09255419689951055\n",
      "Iteration 1146 => Loss: 0.0925482789786764\n",
      "Iteration 1147 => Loss: 0.09254236792916042\n",
      "Iteration 1148 => Loss: 0.09253646373722053\n",
      "Iteration 1149 => Loss: 0.09253056638915415\n",
      "Iteration 1150 => Loss: 0.0925246758712981\n",
      "Iteration 1151 => Loss: 0.0925187921700284\n",
      "Iteration 1152 => Loss: 0.09251291527176018\n",
      "Iteration 1153 => Loss: 0.09250704516294753\n",
      "Iteration 1154 => Loss: 0.09250118183008323\n",
      "Iteration 1155 => Loss: 0.09249532525969881\n",
      "Iteration 1156 => Loss: 0.09248947543836422\n",
      "Iteration 1157 => Loss: 0.09248363235268782\n",
      "Iteration 1158 => Loss: 0.09247779598931616\n",
      "Iteration 1159 => Loss: 0.09247196633493386\n",
      "Iteration 1160 => Loss: 0.09246614337626342\n",
      "Iteration 1161 => Loss: 0.09246032710006524\n",
      "Iteration 1162 => Loss: 0.0924545174931372\n",
      "Iteration 1163 => Loss: 0.09244871454231483\n",
      "Iteration 1164 => Loss: 0.09244291823447094\n",
      "Iteration 1165 => Loss: 0.09243712855651566\n",
      "Iteration 1166 => Loss: 0.09243134549539603\n",
      "Iteration 1167 => Loss: 0.09242556903809623\n",
      "Iteration 1168 => Loss: 0.09241979917163713\n",
      "Iteration 1169 => Loss: 0.09241403588307634\n",
      "Iteration 1170 => Loss: 0.09240827915950796\n",
      "Iteration 1171 => Loss: 0.09240252898806253\n",
      "Iteration 1172 => Loss: 0.09239678535590684\n",
      "Iteration 1173 => Loss: 0.09239104825024386\n",
      "Iteration 1174 => Loss: 0.0923853176583125\n",
      "Iteration 1175 => Loss: 0.09237959356738756\n",
      "Iteration 1176 => Loss: 0.09237387596477963\n",
      "Iteration 1177 => Loss: 0.09236816483783482\n",
      "Iteration 1178 => Loss: 0.0923624601739348\n",
      "Iteration 1179 => Loss: 0.09235676196049655\n",
      "Iteration 1180 => Loss: 0.09235107018497224\n",
      "Iteration 1181 => Loss: 0.09234538483484916\n",
      "Iteration 1182 => Loss: 0.09233970589764959\n",
      "Iteration 1183 => Loss: 0.09233403336093053\n",
      "Iteration 1184 => Loss: 0.0923283672122838\n",
      "Iteration 1185 => Loss: 0.09232270743933574\n",
      "Iteration 1186 => Loss: 0.0923170540297472\n",
      "Iteration 1187 => Loss: 0.09231140697121326\n",
      "Iteration 1188 => Loss: 0.09230576625146326\n",
      "Iteration 1189 => Loss: 0.09230013185826062\n",
      "Iteration 1190 => Loss: 0.09229450377940265\n",
      "Iteration 1191 => Loss: 0.09228888200272062\n",
      "Iteration 1192 => Loss: 0.09228326651607933\n",
      "Iteration 1193 => Loss: 0.09227765730737733\n",
      "Iteration 1194 => Loss: 0.09227205436454651\n",
      "Iteration 1195 => Loss: 0.09226645767555217\n",
      "Iteration 1196 => Loss: 0.09226086722839276\n",
      "Iteration 1197 => Loss: 0.09225528301109995\n",
      "Iteration 1198 => Loss: 0.09224970501173825\n",
      "Iteration 1199 => Loss: 0.09224413321840513\n",
      "Iteration 1200 => Loss: 0.09223856761923079\n",
      "Iteration 1201 => Loss: 0.092233008202378\n",
      "Iteration 1202 => Loss: 0.09222745495604205\n",
      "Iteration 1203 => Loss: 0.09222190786845069\n",
      "Iteration 1204 => Loss: 0.09221636692786389\n",
      "Iteration 1205 => Loss: 0.09221083212257378\n",
      "Iteration 1206 => Loss: 0.09220530344090451\n",
      "Iteration 1207 => Loss: 0.09219978087121225\n",
      "Iteration 1208 => Loss: 0.09219426440188484\n",
      "Iteration 1209 => Loss: 0.09218875402134197\n",
      "Iteration 1210 => Loss: 0.09218324971803481\n",
      "Iteration 1211 => Loss: 0.09217775148044605\n",
      "Iteration 1212 => Loss: 0.09217225929708972\n",
      "Iteration 1213 => Loss: 0.0921667731565111\n",
      "Iteration 1214 => Loss: 0.09216129304728667\n",
      "Iteration 1215 => Loss: 0.09215581895802386\n",
      "Iteration 1216 => Loss: 0.09215035087736104\n",
      "Iteration 1217 => Loss: 0.09214488879396739\n",
      "Iteration 1218 => Loss: 0.09213943269654278\n",
      "Iteration 1219 => Loss: 0.09213398257381769\n",
      "Iteration 1220 => Loss: 0.09212853841455311\n",
      "Iteration 1221 => Loss: 0.09212310020754026\n",
      "Iteration 1222 => Loss: 0.0921176679416008\n",
      "Iteration 1223 => Loss: 0.0921122416055865\n",
      "Iteration 1224 => Loss: 0.09210682118837908\n",
      "Iteration 1225 => Loss: 0.09210140667889034\n",
      "Iteration 1226 => Loss: 0.09209599806606183\n",
      "Iteration 1227 => Loss: 0.09209059533886489\n",
      "Iteration 1228 => Loss: 0.09208519848630045\n",
      "Iteration 1229 => Loss: 0.09207980749739901\n",
      "Iteration 1230 => Loss: 0.0920744223612204\n",
      "Iteration 1231 => Loss: 0.09206904306685389\n",
      "Iteration 1232 => Loss: 0.09206366960341791\n",
      "Iteration 1233 => Loss: 0.09205830196005999\n",
      "Iteration 1234 => Loss: 0.09205294012595663\n",
      "Iteration 1235 => Loss: 0.09204758409031341\n",
      "Iteration 1236 => Loss: 0.09204223384236447\n",
      "Iteration 1237 => Loss: 0.09203688937137293\n",
      "Iteration 1238 => Loss: 0.09203155066663024\n",
      "Iteration 1239 => Loss: 0.09202621771745659\n",
      "Iteration 1240 => Loss: 0.09202089051320045\n",
      "Iteration 1241 => Loss: 0.09201556904323864\n",
      "Iteration 1242 => Loss: 0.09201025329697617\n",
      "Iteration 1243 => Loss: 0.0920049432638462\n",
      "Iteration 1244 => Loss: 0.09199963893330987\n",
      "Iteration 1245 => Loss: 0.09199434029485627\n",
      "Iteration 1246 => Loss: 0.09198904733800228\n",
      "Iteration 1247 => Loss: 0.09198376005229254\n",
      "Iteration 1248 => Loss: 0.09197847842729927\n",
      "Iteration 1249 => Loss: 0.09197320245262225\n",
      "Iteration 1250 => Loss: 0.09196793211788869\n",
      "Iteration 1251 => Loss: 0.09196266741275318\n",
      "Iteration 1252 => Loss: 0.09195740832689753\n",
      "Iteration 1253 => Loss: 0.09195215485003068\n",
      "Iteration 1254 => Loss: 0.09194690697188868\n",
      "Iteration 1255 => Loss: 0.09194166468223451\n",
      "Iteration 1256 => Loss: 0.09193642797085806\n",
      "Iteration 1257 => Loss: 0.09193119682757597\n",
      "Iteration 1258 => Loss: 0.09192597124223154\n",
      "Iteration 1259 => Loss: 0.09192075120469481\n",
      "Iteration 1260 => Loss: 0.09191553670486215\n",
      "Iteration 1261 => Loss: 0.09191032773265646\n",
      "Iteration 1262 => Loss: 0.09190512427802694\n",
      "Iteration 1263 => Loss: 0.09189992633094904\n",
      "Iteration 1264 => Loss: 0.0918947338814243\n",
      "Iteration 1265 => Loss: 0.09188954691948038\n",
      "Iteration 1266 => Loss: 0.09188436543517087\n",
      "Iteration 1267 => Loss: 0.09187918941857529\n",
      "Iteration 1268 => Loss: 0.09187401885979889\n",
      "Iteration 1269 => Loss: 0.09186885374897268\n",
      "Iteration 1270 => Loss: 0.09186369407625321\n",
      "Iteration 1271 => Loss: 0.09185853983182266\n",
      "Iteration 1272 => Loss: 0.09185339100588862\n",
      "Iteration 1273 => Loss: 0.09184824758868394\n",
      "Iteration 1274 => Loss: 0.09184310957046692\n",
      "Iteration 1275 => Loss: 0.09183797694152085\n",
      "Iteration 1276 => Loss: 0.0918328496921543\n",
      "Iteration 1277 => Loss: 0.09182772781270074\n",
      "Iteration 1278 => Loss: 0.0918226112935186\n",
      "Iteration 1279 => Loss: 0.09181750012499115\n",
      "Iteration 1280 => Loss: 0.09181239429752647\n",
      "Iteration 1281 => Loss: 0.09180729380155724\n",
      "Iteration 1282 => Loss: 0.09180219862754074\n",
      "Iteration 1283 => Loss: 0.09179710876595891\n",
      "Iteration 1284 => Loss: 0.09179202420731791\n",
      "Iteration 1285 => Loss: 0.09178694494214842\n",
      "Iteration 1286 => Loss: 0.09178187096100528\n",
      "Iteration 1287 => Loss: 0.09177680225446755\n",
      "Iteration 1288 => Loss: 0.09177173881313838\n",
      "Iteration 1289 => Loss: 0.09176668062764501\n",
      "Iteration 1290 => Loss: 0.09176162768863853\n",
      "Iteration 1291 => Loss: 0.09175657998679396\n",
      "Iteration 1292 => Loss: 0.09175153751281005\n",
      "Iteration 1293 => Loss: 0.0917465002574093\n",
      "Iteration 1294 => Loss: 0.09174146821133781\n",
      "Iteration 1295 => Loss: 0.09173644136536523\n",
      "Iteration 1296 => Loss: 0.0917314197102847\n",
      "Iteration 1297 => Loss: 0.09172640323691268\n",
      "Iteration 1298 => Loss: 0.09172139193608907\n",
      "Iteration 1299 => Loss: 0.09171638579867686\n",
      "Iteration 1300 => Loss: 0.09171138481556228\n",
      "Iteration 1301 => Loss: 0.09170638897765462\n",
      "Iteration 1302 => Loss: 0.09170139827588619\n",
      "Iteration 1303 => Loss: 0.09169641270121219\n",
      "Iteration 1304 => Loss: 0.09169143224461074\n",
      "Iteration 1305 => Loss: 0.0916864568970826\n",
      "Iteration 1306 => Loss: 0.09168148664965142\n",
      "Iteration 1307 => Loss: 0.0916765214933633\n",
      "Iteration 1308 => Loss: 0.09167156141928702\n",
      "Iteration 1309 => Loss: 0.09166660641851375\n",
      "Iteration 1310 => Loss: 0.0916616564821571\n",
      "Iteration 1311 => Loss: 0.09165671160135302\n",
      "Iteration 1312 => Loss: 0.0916517717672597\n",
      "Iteration 1313 => Loss: 0.0916468369710575\n",
      "Iteration 1314 => Loss: 0.09164190720394888\n",
      "Iteration 1315 => Loss: 0.09163698245715839\n",
      "Iteration 1316 => Loss: 0.09163206272193249\n",
      "Iteration 1317 => Loss: 0.09162714798953955\n",
      "Iteration 1318 => Loss: 0.09162223825126982\n",
      "Iteration 1319 => Loss: 0.09161733349843516\n",
      "Iteration 1320 => Loss: 0.09161243372236924\n",
      "Iteration 1321 => Loss: 0.09160753891442729\n",
      "Iteration 1322 => Loss: 0.09160264906598604\n",
      "Iteration 1323 => Loss: 0.09159776416844377\n",
      "Iteration 1324 => Loss: 0.09159288421322007\n",
      "Iteration 1325 => Loss: 0.09158800919175591\n",
      "Iteration 1326 => Loss: 0.09158313909551351\n",
      "Iteration 1327 => Loss: 0.09157827391597624\n",
      "Iteration 1328 => Loss: 0.09157341364464865\n",
      "Iteration 1329 => Loss: 0.0915685582730563\n",
      "Iteration 1330 => Loss: 0.09156370779274572\n",
      "Iteration 1331 => Loss: 0.09155886219528442\n",
      "Iteration 1332 => Loss: 0.0915540214722607\n",
      "Iteration 1333 => Loss: 0.09154918561528365\n",
      "Iteration 1334 => Loss: 0.09154435461598304\n",
      "Iteration 1335 => Loss: 0.09153952846600938\n",
      "Iteration 1336 => Loss: 0.09153470715703364\n",
      "Iteration 1337 => Loss: 0.0915298906807474\n",
      "Iteration 1338 => Loss: 0.09152507902886267\n",
      "Iteration 1339 => Loss: 0.09152027219311173\n",
      "Iteration 1340 => Loss: 0.09151547016524733\n",
      "Iteration 1341 => Loss: 0.09151067293704239\n",
      "Iteration 1342 => Loss: 0.09150588050028999\n",
      "Iteration 1343 => Loss: 0.09150109284680341\n",
      "Iteration 1344 => Loss: 0.09149630996841589\n",
      "Iteration 1345 => Loss: 0.09149153185698072\n",
      "Iteration 1346 => Loss: 0.09148675850437113\n",
      "Iteration 1347 => Loss: 0.09148198990248017\n",
      "Iteration 1348 => Loss: 0.09147722604322067\n",
      "Iteration 1349 => Loss: 0.09147246691852527\n",
      "Iteration 1350 => Loss: 0.09146771252034622\n",
      "Iteration 1351 => Loss: 0.0914629628406554\n",
      "Iteration 1352 => Loss: 0.09145821787144423\n",
      "Iteration 1353 => Loss: 0.09145347760472364\n",
      "Iteration 1354 => Loss: 0.09144874203252394\n",
      "Iteration 1355 => Loss: 0.09144401114689486\n",
      "Iteration 1356 => Loss: 0.09143928493990534\n",
      "Iteration 1357 => Loss: 0.09143456340364367\n",
      "Iteration 1358 => Loss: 0.09142984653021718\n",
      "Iteration 1359 => Loss: 0.09142513431175248\n",
      "Iteration 1360 => Loss: 0.09142042674039509\n",
      "Iteration 1361 => Loss: 0.09141572380830962\n",
      "Iteration 1362 => Loss: 0.09141102550767953\n",
      "Iteration 1363 => Loss: 0.0914063318307072\n",
      "Iteration 1364 => Loss: 0.09140164276961389\n",
      "Iteration 1365 => Loss: 0.09139695831663949\n",
      "Iteration 1366 => Loss: 0.09139227846404269\n",
      "Iteration 1367 => Loss: 0.09138760320410075\n",
      "Iteration 1368 => Loss: 0.0913829325291095\n",
      "Iteration 1369 => Loss: 0.09137826643138337\n",
      "Iteration 1370 => Loss: 0.0913736049032552\n",
      "Iteration 1371 => Loss: 0.09136894793707621\n",
      "Iteration 1372 => Loss: 0.09136429552521605\n",
      "Iteration 1373 => Loss: 0.09135964766006253\n",
      "Iteration 1374 => Loss: 0.0913550043340218\n",
      "Iteration 1375 => Loss: 0.09135036553951817\n",
      "Iteration 1376 => Loss: 0.09134573126899398\n",
      "Iteration 1377 => Loss: 0.09134110151490972\n",
      "Iteration 1378 => Loss: 0.09133647626974388\n",
      "Iteration 1379 => Loss: 0.09133185552599282\n",
      "Iteration 1380 => Loss: 0.09132723927617091\n",
      "Iteration 1381 => Loss: 0.09132262751281023\n",
      "Iteration 1382 => Loss: 0.09131802022846074\n",
      "Iteration 1383 => Loss: 0.09131341741569\n",
      "Iteration 1384 => Loss: 0.09130881906708345\n",
      "Iteration 1385 => Loss: 0.0913042251752439\n",
      "Iteration 1386 => Loss: 0.09129963573279189\n",
      "Iteration 1387 => Loss: 0.09129505073236538\n",
      "Iteration 1388 => Loss: 0.09129047016661981\n",
      "Iteration 1389 => Loss: 0.09128589402822802\n",
      "Iteration 1390 => Loss: 0.09128132230988019\n",
      "Iteration 1391 => Loss: 0.09127675500428375\n",
      "Iteration 1392 => Loss: 0.09127219210416343\n",
      "Iteration 1393 => Loss: 0.0912676336022611\n",
      "Iteration 1394 => Loss: 0.09126307949133576\n",
      "Iteration 1395 => Loss: 0.09125852976416347\n",
      "Iteration 1396 => Loss: 0.0912539844135374\n",
      "Iteration 1397 => Loss: 0.09124944343226754\n",
      "Iteration 1398 => Loss: 0.0912449068131809\n",
      "Iteration 1399 => Loss: 0.0912403745491214\n",
      "Iteration 1400 => Loss: 0.09123584663294959\n",
      "Iteration 1401 => Loss: 0.09123132305754303\n",
      "Iteration 1402 => Loss: 0.09122680381579576\n",
      "Iteration 1403 => Loss: 0.09122228890061866\n",
      "Iteration 1404 => Loss: 0.09121777830493903\n",
      "Iteration 1405 => Loss: 0.09121327202170094\n",
      "Iteration 1406 => Loss: 0.09120877004386482\n",
      "Iteration 1407 => Loss: 0.09120427236440758\n",
      "Iteration 1408 => Loss: 0.09119977897632256\n",
      "Iteration 1409 => Loss: 0.09119528987261942\n",
      "Iteration 1410 => Loss: 0.09119080504632421\n",
      "Iteration 1411 => Loss: 0.09118632449047913\n",
      "Iteration 1412 => Loss: 0.09118184819814261\n",
      "Iteration 1413 => Loss: 0.09117737616238929\n",
      "Iteration 1414 => Loss: 0.09117290837630987\n",
      "Iteration 1415 => Loss: 0.0911684448330111\n",
      "Iteration 1416 => Loss: 0.0911639855256158\n",
      "Iteration 1417 => Loss: 0.09115953044726269\n",
      "Iteration 1418 => Loss: 0.09115507959110643\n",
      "Iteration 1419 => Loss: 0.09115063295031749\n",
      "Iteration 1420 => Loss: 0.0911461905180822\n",
      "Iteration 1421 => Loss: 0.09114175228760271\n",
      "Iteration 1422 => Loss: 0.09113731825209681\n",
      "Iteration 1423 => Loss: 0.09113288840479797\n",
      "Iteration 1424 => Loss: 0.09112846273895531\n",
      "Iteration 1425 => Loss: 0.09112404124783349\n",
      "Iteration 1426 => Loss: 0.09111962392471278\n",
      "Iteration 1427 => Loss: 0.09111521076288884\n",
      "Iteration 1428 => Loss: 0.0911108017556728\n",
      "Iteration 1429 => Loss: 0.09110639689639122\n",
      "Iteration 1430 => Loss: 0.09110199617838594\n",
      "Iteration 1431 => Loss: 0.09109759959501414\n",
      "Iteration 1432 => Loss: 0.09109320713964818\n",
      "Iteration 1433 => Loss: 0.09108881880567578\n",
      "Iteration 1434 => Loss: 0.09108443458649967\n",
      "Iteration 1435 => Loss: 0.09108005447553771\n",
      "Iteration 1436 => Loss: 0.09107567846622296\n",
      "Iteration 1437 => Loss: 0.09107130655200334\n",
      "Iteration 1438 => Loss: 0.09106693872634179\n",
      "Iteration 1439 => Loss: 0.09106257498271629\n",
      "Iteration 1440 => Loss: 0.09105821531461956\n",
      "Iteration 1441 => Loss: 0.09105385971555928\n",
      "Iteration 1442 => Loss: 0.09104950817905783\n",
      "Iteration 1443 => Loss: 0.09104516069865244\n",
      "Iteration 1444 => Loss: 0.091040817267895\n",
      "Iteration 1445 => Loss: 0.09103647788035207\n",
      "Iteration 1446 => Loss: 0.09103214252960487\n",
      "Iteration 1447 => Loss: 0.09102781120924916\n",
      "Iteration 1448 => Loss: 0.09102348391289525\n",
      "Iteration 1449 => Loss: 0.09101916063416791\n",
      "Iteration 1450 => Loss: 0.09101484136670644\n",
      "Iteration 1451 => Loss: 0.0910105261041645\n",
      "Iteration 1452 => Loss: 0.0910062148402101\n",
      "Iteration 1453 => Loss: 0.09100190756852561\n",
      "Iteration 1454 => Loss: 0.09099760428280765\n",
      "Iteration 1455 => Loss: 0.09099330497676711\n",
      "Iteration 1456 => Loss: 0.09098900964412902\n",
      "Iteration 1457 => Loss: 0.09098471827863262\n",
      "Iteration 1458 => Loss: 0.09098043087403127\n",
      "Iteration 1459 => Loss: 0.09097614742409232\n",
      "Iteration 1460 => Loss: 0.09097186792259725\n",
      "Iteration 1461 => Loss: 0.09096759236334147\n",
      "Iteration 1462 => Loss: 0.09096332074013432\n",
      "Iteration 1463 => Loss: 0.09095905304679909\n",
      "Iteration 1464 => Loss: 0.0909547892771729\n",
      "Iteration 1465 => Loss: 0.09095052942510676\n",
      "Iteration 1466 => Loss: 0.09094627348446534\n",
      "Iteration 1467 => Loss: 0.09094202144912714\n",
      "Iteration 1468 => Loss: 0.09093777331298443\n",
      "Iteration 1469 => Loss: 0.09093352906994297\n",
      "Iteration 1470 => Loss: 0.09092928871392222\n",
      "Iteration 1471 => Loss: 0.0909250522388553\n",
      "Iteration 1472 => Loss: 0.09092081963868875\n",
      "Iteration 1473 => Loss: 0.09091659090738273\n",
      "Iteration 1474 => Loss: 0.09091236603891077\n",
      "Iteration 1475 => Loss: 0.09090814502725983\n",
      "Iteration 1476 => Loss: 0.09090392786643031\n",
      "Iteration 1477 => Loss: 0.09089971455043593\n",
      "Iteration 1478 => Loss: 0.09089550507330368\n",
      "Iteration 1479 => Loss: 0.09089129942907387\n",
      "Iteration 1480 => Loss: 0.09088709761180007\n",
      "Iteration 1481 => Loss: 0.09088289961554889\n",
      "Iteration 1482 => Loss: 0.0908787054344003\n",
      "Iteration 1483 => Loss: 0.0908745150624472\n",
      "Iteration 1484 => Loss: 0.09087032849379569\n",
      "Iteration 1485 => Loss: 0.09086614572256485\n",
      "Iteration 1486 => Loss: 0.0908619667428868\n",
      "Iteration 1487 => Loss: 0.09085779154890657\n",
      "Iteration 1488 => Loss: 0.09085362013478213\n",
      "Iteration 1489 => Loss: 0.0908494524946844\n",
      "Iteration 1490 => Loss: 0.09084528862279706\n",
      "Iteration 1491 => Loss: 0.09084112851331667\n",
      "Iteration 1492 => Loss: 0.09083697216045257\n",
      "Iteration 1493 => Loss: 0.09083281955842674\n",
      "Iteration 1494 => Loss: 0.090828670701474\n",
      "Iteration 1495 => Loss: 0.09082452558384178\n",
      "Iteration 1496 => Loss: 0.09082038419979009\n",
      "Iteration 1497 => Loss: 0.09081624654359162\n",
      "Iteration 1498 => Loss: 0.09081211260953155\n",
      "Iteration 1499 => Loss: 0.0908079823919076\n",
      "Iteration 1500 => Loss: 0.09080385588503001\n",
      "Iteration 1501 => Loss: 0.09079973308322146\n",
      "Iteration 1502 => Loss: 0.09079561398081698\n",
      "Iteration 1503 => Loss: 0.09079149857216408\n",
      "Iteration 1504 => Loss: 0.0907873868516225\n",
      "Iteration 1505 => Loss: 0.09078327881356439\n",
      "Iteration 1506 => Loss: 0.09077917445237414\n",
      "Iteration 1507 => Loss: 0.09077507376244835\n",
      "Iteration 1508 => Loss: 0.09077097673819581\n",
      "Iteration 1509 => Loss: 0.0907668833740376\n",
      "Iteration 1510 => Loss: 0.09076279366440676\n",
      "Iteration 1511 => Loss: 0.09075870760374852\n",
      "Iteration 1512 => Loss: 0.09075462518652021\n",
      "Iteration 1513 => Loss: 0.0907505464071911\n",
      "Iteration 1514 => Loss: 0.09074647126024254\n",
      "Iteration 1515 => Loss: 0.09074239974016779\n",
      "Iteration 1516 => Loss: 0.09073833184147202\n",
      "Iteration 1517 => Loss: 0.09073426755867237\n",
      "Iteration 1518 => Loss: 0.09073020688629777\n",
      "Iteration 1519 => Loss: 0.090726149818889\n",
      "Iteration 1520 => Loss: 0.09072209635099865\n",
      "Iteration 1521 => Loss: 0.09071804647719106\n",
      "Iteration 1522 => Loss: 0.09071400019204225\n",
      "Iteration 1523 => Loss: 0.09070995749014002\n",
      "Iteration 1524 => Loss: 0.09070591836608376\n",
      "Iteration 1525 => Loss: 0.09070188281448453\n",
      "Iteration 1526 => Loss: 0.09069785082996493\n",
      "Iteration 1527 => Loss: 0.09069382240715919\n",
      "Iteration 1528 => Loss: 0.09068979754071302\n",
      "Iteration 1529 => Loss: 0.09068577622528365\n",
      "Iteration 1530 => Loss: 0.09068175845553976\n",
      "Iteration 1531 => Loss: 0.09067774422616148\n",
      "Iteration 1532 => Loss: 0.09067373353184034\n",
      "Iteration 1533 => Loss: 0.09066972636727923\n",
      "Iteration 1534 => Loss: 0.09066572272719232\n",
      "Iteration 1535 => Loss: 0.09066172260630524\n",
      "Iteration 1536 => Loss: 0.09065772599935469\n",
      "Iteration 1537 => Loss: 0.09065373290108873\n",
      "Iteration 1538 => Loss: 0.0906497433062667\n",
      "Iteration 1539 => Loss: 0.09064575720965895\n",
      "Iteration 1540 => Loss: 0.09064177460604705\n",
      "Iteration 1541 => Loss: 0.09063779549022376\n",
      "Iteration 1542 => Loss: 0.0906338198569928\n",
      "Iteration 1543 => Loss: 0.09062984770116903\n",
      "Iteration 1544 => Loss: 0.09062587901757832\n",
      "Iteration 1545 => Loss: 0.09062191380105748\n",
      "Iteration 1546 => Loss: 0.09061795204645437\n",
      "Iteration 1547 => Loss: 0.09061399374862768\n",
      "Iteration 1548 => Loss: 0.09061003890244711\n",
      "Iteration 1549 => Loss: 0.09060608750279313\n",
      "Iteration 1550 => Loss: 0.09060213954455712\n",
      "Iteration 1551 => Loss: 0.09059819502264121\n",
      "Iteration 1552 => Loss: 0.0905942539319584\n",
      "Iteration 1553 => Loss: 0.09059031626743237\n",
      "Iteration 1554 => Loss: 0.09058638202399752\n",
      "Iteration 1555 => Loss: 0.09058245119659895\n",
      "Iteration 1556 => Loss: 0.09057852378019247\n",
      "Iteration 1557 => Loss: 0.09057459976974448\n",
      "Iteration 1558 => Loss: 0.09057067916023195\n",
      "Iteration 1559 => Loss: 0.09056676194664251\n",
      "Iteration 1560 => Loss: 0.09056284812397424\n",
      "Iteration 1561 => Loss: 0.09055893768723579\n",
      "Iteration 1562 => Loss: 0.09055503063144632\n",
      "Iteration 1563 => Loss: 0.0905511269516354\n",
      "Iteration 1564 => Loss: 0.090547226642843\n",
      "Iteration 1565 => Loss: 0.09054332970011959\n",
      "Iteration 1566 => Loss: 0.0905394361185259\n",
      "Iteration 1567 => Loss: 0.09053554589313312\n",
      "Iteration 1568 => Loss: 0.09053165901902263\n",
      "Iteration 1569 => Loss: 0.09052777549128622\n",
      "Iteration 1570 => Loss: 0.09052389530502587\n",
      "Iteration 1571 => Loss: 0.09052001845535375\n",
      "Iteration 1572 => Loss: 0.09051614493739234\n",
      "Iteration 1573 => Loss: 0.09051227474627416\n",
      "Iteration 1574 => Loss: 0.09050840787714204\n",
      "Iteration 1575 => Loss: 0.09050454432514875\n",
      "Iteration 1576 => Loss: 0.0905006840854573\n",
      "Iteration 1577 => Loss: 0.0904968271532407\n",
      "Iteration 1578 => Loss: 0.09049297352368202\n",
      "Iteration 1579 => Loss: 0.09048912319197426\n",
      "Iteration 1580 => Loss: 0.09048527615332048\n",
      "Iteration 1581 => Loss: 0.09048143240293371\n",
      "Iteration 1582 => Loss: 0.09047759193603687\n",
      "Iteration 1583 => Loss: 0.09047375474786273\n",
      "Iteration 1584 => Loss: 0.09046992083365403\n",
      "Iteration 1585 => Loss: 0.09046609018866335\n",
      "Iteration 1586 => Loss: 0.09046226280815298\n",
      "Iteration 1587 => Loss: 0.09045843868739512\n",
      "Iteration 1588 => Loss: 0.09045461782167168\n",
      "Iteration 1589 => Loss: 0.09045080020627433\n",
      "Iteration 1590 => Loss: 0.09044698583650446\n",
      "Iteration 1591 => Loss: 0.0904431747076731\n",
      "Iteration 1592 => Loss: 0.09043936681510104\n",
      "Iteration 1593 => Loss: 0.09043556215411859\n",
      "Iteration 1594 => Loss: 0.09043176072006577\n",
      "Iteration 1595 => Loss: 0.09042796250829209\n",
      "Iteration 1596 => Loss: 0.09042416751415672\n",
      "Iteration 1597 => Loss: 0.09042037573302825\n",
      "Iteration 1598 => Loss: 0.09041658716028488\n",
      "Iteration 1599 => Loss: 0.09041280179131424\n",
      "Iteration 1600 => Loss: 0.09040901962151338\n",
      "Iteration 1601 => Loss: 0.09040524064628888\n",
      "Iteration 1602 => Loss: 0.09040146486105663\n",
      "Iteration 1603 => Loss: 0.09039769226124193\n",
      "Iteration 1604 => Loss: 0.09039392284227947\n",
      "Iteration 1605 => Loss: 0.09039015659961323\n",
      "Iteration 1606 => Loss: 0.09038639352869651\n",
      "Iteration 1607 => Loss: 0.09038263362499183\n",
      "Iteration 1608 => Loss: 0.09037887688397109\n",
      "Iteration 1609 => Loss: 0.09037512330111531\n",
      "Iteration 1610 => Loss: 0.0903713728719148\n",
      "Iteration 1611 => Loss: 0.0903676255918689\n",
      "Iteration 1612 => Loss: 0.09036388145648633\n",
      "Iteration 1613 => Loss: 0.09036014046128472\n",
      "Iteration 1614 => Loss: 0.09035640260179098\n",
      "Iteration 1615 => Loss: 0.09035266787354104\n",
      "Iteration 1616 => Loss: 0.09034893627207982\n",
      "Iteration 1617 => Loss: 0.09034520779296139\n",
      "Iteration 1618 => Loss: 0.09034148243174875\n",
      "Iteration 1619 => Loss: 0.09033776018401392\n",
      "Iteration 1620 => Loss: 0.09033404104533788\n",
      "Iteration 1621 => Loss: 0.09033032501131057\n",
      "Iteration 1622 => Loss: 0.09032661207753079\n",
      "Iteration 1623 => Loss: 0.09032290223960629\n",
      "Iteration 1624 => Loss: 0.09031919549315362\n",
      "Iteration 1625 => Loss: 0.09031549183379829\n",
      "Iteration 1626 => Loss: 0.09031179125717452\n",
      "Iteration 1627 => Loss: 0.09030809375892536\n",
      "Iteration 1628 => Loss: 0.0903043993347027\n",
      "Iteration 1629 => Loss: 0.09030070798016708\n",
      "Iteration 1630 => Loss: 0.09029701969098781\n",
      "Iteration 1631 => Loss: 0.09029333446284292\n",
      "Iteration 1632 => Loss: 0.09028965229141915\n",
      "Iteration 1633 => Loss: 0.0902859731724118\n",
      "Iteration 1634 => Loss: 0.09028229710152495\n",
      "Iteration 1635 => Loss: 0.09027862407447118\n",
      "Iteration 1636 => Loss: 0.09027495408697167\n",
      "Iteration 1637 => Loss: 0.09027128713475627\n",
      "Iteration 1638 => Loss: 0.09026762321356323\n",
      "Iteration 1639 => Loss: 0.09026396231913947\n",
      "Iteration 1640 => Loss: 0.09026030444724029\n",
      "Iteration 1641 => Loss: 0.09025664959362957\n",
      "Iteration 1642 => Loss: 0.09025299775407955\n",
      "Iteration 1643 => Loss: 0.09024934892437098\n",
      "Iteration 1644 => Loss: 0.09024570310029305\n",
      "Iteration 1645 => Loss: 0.09024206027764323\n",
      "Iteration 1646 => Loss: 0.09023842045222742\n",
      "Iteration 1647 => Loss: 0.09023478361985993\n",
      "Iteration 1648 => Loss: 0.09023114977636328\n",
      "Iteration 1649 => Loss: 0.09022751891756839\n",
      "Iteration 1650 => Loss: 0.09022389103931439\n",
      "Iteration 1651 => Loss: 0.09022026613744871\n",
      "Iteration 1652 => Loss: 0.09021664420782706\n",
      "Iteration 1653 => Loss: 0.09021302524631324\n",
      "Iteration 1654 => Loss: 0.09020940924877942\n",
      "Iteration 1655 => Loss: 0.09020579621110578\n",
      "Iteration 1656 => Loss: 0.09020218612918081\n",
      "Iteration 1657 => Loss: 0.090198578998901\n",
      "Iteration 1658 => Loss: 0.09019497481617104\n",
      "Iteration 1659 => Loss: 0.09019137357690361\n",
      "Iteration 1660 => Loss: 0.0901877752770196\n",
      "Iteration 1661 => Loss: 0.09018417991244784\n",
      "Iteration 1662 => Loss: 0.09018058747912527\n",
      "Iteration 1663 => Loss: 0.09017699797299679\n",
      "Iteration 1664 => Loss: 0.09017341139001522\n",
      "Iteration 1665 => Loss: 0.09016982772614147\n",
      "Iteration 1666 => Loss: 0.09016624697734438\n",
      "Iteration 1667 => Loss: 0.09016266913960061\n",
      "Iteration 1668 => Loss: 0.09015909420889485\n",
      "Iteration 1669 => Loss: 0.0901555221812196\n",
      "Iteration 1670 => Loss: 0.09015195305257526\n",
      "Iteration 1671 => Loss: 0.09014838681897001\n",
      "Iteration 1672 => Loss: 0.09014482347641997\n",
      "Iteration 1673 => Loss: 0.09014126302094895\n",
      "Iteration 1674 => Loss: 0.09013770544858862\n",
      "Iteration 1675 => Loss: 0.09013415075537837\n",
      "Iteration 1676 => Loss: 0.09013059893736536\n",
      "Iteration 1677 => Loss: 0.09012704999060449\n",
      "Iteration 1678 => Loss: 0.09012350391115828\n",
      "Iteration 1679 => Loss: 0.09011996069509705\n",
      "Iteration 1680 => Loss: 0.09011642033849873\n",
      "Iteration 1681 => Loss: 0.09011288283744882\n",
      "Iteration 1682 => Loss: 0.09010934818804062\n",
      "Iteration 1683 => Loss: 0.09010581638637485\n",
      "Iteration 1684 => Loss: 0.09010228742856001\n",
      "Iteration 1685 => Loss: 0.09009876131071201\n",
      "Iteration 1686 => Loss: 0.0900952380289543\n",
      "Iteration 1687 => Loss: 0.09009171757941804\n",
      "Iteration 1688 => Loss: 0.09008819995824169\n",
      "Iteration 1689 => Loss: 0.09008468516157135\n",
      "Iteration 1690 => Loss: 0.09008117318556051\n",
      "Iteration 1691 => Loss: 0.09007766402637016\n",
      "Iteration 1692 => Loss: 0.09007415768016867\n",
      "Iteration 1693 => Loss: 0.0900706541431319\n",
      "Iteration 1694 => Loss: 0.09006715341144302\n",
      "Iteration 1695 => Loss: 0.09006365548129266\n",
      "Iteration 1696 => Loss: 0.09006016034887877\n",
      "Iteration 1697 => Loss: 0.09005666801040663\n",
      "Iteration 1698 => Loss: 0.09005317846208886\n",
      "Iteration 1699 => Loss: 0.09004969170014539\n",
      "Iteration 1700 => Loss: 0.09004620772080342\n",
      "Iteration 1701 => Loss: 0.0900427265202974\n",
      "Iteration 1702 => Loss: 0.0900392480948691\n",
      "Iteration 1703 => Loss: 0.0900357724407674\n",
      "Iteration 1704 => Loss: 0.0900322995542485\n",
      "Iteration 1705 => Loss: 0.09002882943157581\n",
      "Iteration 1706 => Loss: 0.09002536206901975\n",
      "Iteration 1707 => Loss: 0.09002189746285806\n",
      "Iteration 1708 => Loss: 0.09001843560937561\n",
      "Iteration 1709 => Loss: 0.09001497650486431\n",
      "Iteration 1710 => Loss: 0.0900115201456232\n",
      "Iteration 1711 => Loss: 0.09000806652795845\n",
      "Iteration 1712 => Loss: 0.09000461564818325\n",
      "Iteration 1713 => Loss: 0.09000116750261786\n",
      "Iteration 1714 => Loss: 0.08999772208758956\n",
      "Iteration 1715 => Loss: 0.08999427939943269\n",
      "Iteration 1716 => Loss: 0.08999083943448853\n",
      "Iteration 1717 => Loss: 0.08998740218910535\n",
      "Iteration 1718 => Loss: 0.08998396765963845\n",
      "Iteration 1719 => Loss: 0.08998053584244997\n",
      "Iteration 1720 => Loss: 0.08997710673390903\n",
      "Iteration 1721 => Loss: 0.08997368033039171\n",
      "Iteration 1722 => Loss: 0.08997025662828088\n",
      "Iteration 1723 => Loss: 0.08996683562396639\n",
      "Iteration 1724 => Loss: 0.08996341731384483\n",
      "Iteration 1725 => Loss: 0.08996000169431978\n",
      "Iteration 1726 => Loss: 0.08995658876180153\n",
      "Iteration 1727 => Loss: 0.08995317851270723\n",
      "Iteration 1728 => Loss: 0.08994977094346078\n",
      "Iteration 1729 => Loss: 0.08994636605049287\n",
      "Iteration 1730 => Loss: 0.08994296383024104\n",
      "Iteration 1731 => Loss: 0.0899395642791494\n",
      "Iteration 1732 => Loss: 0.08993616739366891\n",
      "Iteration 1733 => Loss: 0.0899327731702572\n",
      "Iteration 1734 => Loss: 0.0899293816053786\n",
      "Iteration 1735 => Loss: 0.08992599269550408\n",
      "Iteration 1736 => Loss: 0.08992260643711134\n",
      "Iteration 1737 => Loss: 0.08991922282668463\n",
      "Iteration 1738 => Loss: 0.08991584186071488\n",
      "Iteration 1739 => Loss: 0.08991246353569962\n",
      "Iteration 1740 => Loss: 0.089909087848143\n",
      "Iteration 1741 => Loss: 0.08990571479455566\n",
      "Iteration 1742 => Loss: 0.08990234437145493\n",
      "Iteration 1743 => Loss: 0.08989897657536458\n",
      "Iteration 1744 => Loss: 0.08989561140281489\n",
      "Iteration 1745 => Loss: 0.08989224885034276\n",
      "Iteration 1746 => Loss: 0.08988888891449151\n",
      "Iteration 1747 => Loss: 0.08988553159181097\n",
      "Iteration 1748 => Loss: 0.08988217687885737\n",
      "Iteration 1749 => Loss: 0.08987882477219349\n",
      "Iteration 1750 => Loss: 0.08987547526838846\n",
      "Iteration 1751 => Loss: 0.08987212836401788\n",
      "Iteration 1752 => Loss: 0.08986878405566369\n",
      "Iteration 1753 => Loss: 0.08986544233991428\n",
      "Iteration 1754 => Loss: 0.08986210321336435\n",
      "Iteration 1755 => Loss: 0.08985876667261503\n",
      "Iteration 1756 => Loss: 0.08985543271427372\n",
      "Iteration 1757 => Loss: 0.08985210133495418\n",
      "Iteration 1758 => Loss: 0.0898487725312764\n",
      "Iteration 1759 => Loss: 0.08984544629986678\n",
      "Iteration 1760 => Loss: 0.08984212263735793\n",
      "Iteration 1761 => Loss: 0.08983880154038874\n",
      "Iteration 1762 => Loss: 0.08983548300560434\n",
      "Iteration 1763 => Loss: 0.08983216702965606\n",
      "Iteration 1764 => Loss: 0.08982885360920148\n",
      "Iteration 1765 => Loss: 0.08982554274090443\n",
      "Iteration 1766 => Loss: 0.08982223442143479\n",
      "Iteration 1767 => Loss: 0.08981892864746874\n",
      "Iteration 1768 => Loss: 0.08981562541568854\n",
      "Iteration 1769 => Loss: 0.08981232472278268\n",
      "Iteration 1770 => Loss: 0.08980902656544561\n",
      "Iteration 1771 => Loss: 0.08980573094037807\n",
      "Iteration 1772 => Loss: 0.08980243784428682\n",
      "Iteration 1773 => Loss: 0.08979914727388465\n",
      "Iteration 1774 => Loss: 0.08979585922589055\n",
      "Iteration 1775 => Loss: 0.08979257369702937\n",
      "Iteration 1776 => Loss: 0.08978929068403223\n",
      "Iteration 1777 => Loss: 0.08978601018363605\n",
      "Iteration 1778 => Loss: 0.0897827321925839\n",
      "Iteration 1779 => Loss: 0.0897794567076248\n",
      "Iteration 1780 => Loss: 0.08977618372551376\n",
      "Iteration 1781 => Loss: 0.08977291324301175\n",
      "Iteration 1782 => Loss: 0.08976964525688565\n",
      "Iteration 1783 => Loss: 0.08976637976390833\n",
      "Iteration 1784 => Loss: 0.08976311676085862\n",
      "Iteration 1785 => Loss: 0.08975985624452111\n",
      "Iteration 1786 => Loss: 0.08975659821168647\n",
      "Iteration 1787 => Loss: 0.08975334265915107\n",
      "Iteration 1788 => Loss: 0.08975008958371733\n",
      "Iteration 1789 => Loss: 0.08974683898219332\n",
      "Iteration 1790 => Loss: 0.0897435908513931\n",
      "Iteration 1791 => Loss: 0.08974034518813653\n",
      "Iteration 1792 => Loss: 0.08973710198924925\n",
      "Iteration 1793 => Loss: 0.08973386125156262\n",
      "Iteration 1794 => Loss: 0.08973062297191392\n",
      "Iteration 1795 => Loss: 0.08972738714714619\n",
      "Iteration 1796 => Loss: 0.08972415377410807\n",
      "Iteration 1797 => Loss: 0.08972092284965409\n",
      "Iteration 1798 => Loss: 0.08971769437064447\n",
      "Iteration 1799 => Loss: 0.08971446833394514\n",
      "Iteration 1800 => Loss: 0.0897112447364277\n",
      "Iteration 1801 => Loss: 0.08970802357496946\n",
      "Iteration 1802 => Loss: 0.08970480484645339\n",
      "Iteration 1803 => Loss: 0.08970158854776816\n",
      "Iteration 1804 => Loss: 0.08969837467580805\n",
      "Iteration 1805 => Loss: 0.08969516322747292\n",
      "Iteration 1806 => Loss: 0.08969195419966838\n",
      "Iteration 1807 => Loss: 0.08968874758930553\n",
      "Iteration 1808 => Loss: 0.08968554339330109\n",
      "Iteration 1809 => Loss: 0.08968234160857744\n",
      "Iteration 1810 => Loss: 0.08967914223206236\n",
      "Iteration 1811 => Loss: 0.08967594526068937\n",
      "Iteration 1812 => Loss: 0.08967275069139738\n",
      "Iteration 1813 => Loss: 0.08966955852113087\n",
      "Iteration 1814 => Loss: 0.0896663687468399\n",
      "Iteration 1815 => Loss: 0.08966318136547996\n",
      "Iteration 1816 => Loss: 0.08965999637401205\n",
      "Iteration 1817 => Loss: 0.08965681376940263\n",
      "Iteration 1818 => Loss: 0.08965363354862362\n",
      "Iteration 1819 => Loss: 0.08965045570865238\n",
      "Iteration 1820 => Loss: 0.08964728024647178\n",
      "Iteration 1821 => Loss: 0.08964410715907002\n",
      "Iteration 1822 => Loss: 0.08964093644344073\n",
      "Iteration 1823 => Loss: 0.08963776809658301\n",
      "Iteration 1824 => Loss: 0.08963460211550124\n",
      "Iteration 1825 => Loss: 0.08963143849720524\n",
      "Iteration 1826 => Loss: 0.08962827723871017\n",
      "Iteration 1827 => Loss: 0.08962511833703654\n",
      "Iteration 1828 => Loss: 0.08962196178921017\n",
      "Iteration 1829 => Loss: 0.08961880759226222\n",
      "Iteration 1830 => Loss: 0.0896156557432292\n",
      "Iteration 1831 => Loss: 0.08961250623915284\n",
      "Iteration 1832 => Loss: 0.0896093590770802\n",
      "Iteration 1833 => Loss: 0.08960621425406365\n",
      "Iteration 1834 => Loss: 0.08960307176716072\n",
      "Iteration 1835 => Loss: 0.08959993161343426\n",
      "Iteration 1836 => Loss: 0.08959679378995232\n",
      "Iteration 1837 => Loss: 0.08959365829378824\n",
      "Iteration 1838 => Loss: 0.08959052512202047\n",
      "Iteration 1839 => Loss: 0.08958739427173275\n",
      "Iteration 1840 => Loss: 0.08958426574001392\n",
      "Iteration 1841 => Loss: 0.08958113952395806\n",
      "Iteration 1842 => Loss: 0.08957801562066448\n",
      "Iteration 1843 => Loss: 0.0895748940272374\n",
      "Iteration 1844 => Loss: 0.08957177474078644\n",
      "Iteration 1845 => Loss: 0.08956865775842619\n",
      "Iteration 1846 => Loss: 0.08956554307727645\n",
      "Iteration 1847 => Loss: 0.08956243069446204\n",
      "Iteration 1848 => Loss: 0.08955932060711294\n",
      "Iteration 1849 => Loss: 0.08955621281236416\n",
      "Iteration 1850 => Loss: 0.08955310730735577\n",
      "Iteration 1851 => Loss: 0.08955000408923296\n",
      "Iteration 1852 => Loss: 0.08954690315514592\n",
      "Iteration 1853 => Loss: 0.08954380450224984\n",
      "Iteration 1854 => Loss: 0.08954070812770501\n",
      "Iteration 1855 => Loss: 0.08953761402867669\n",
      "Iteration 1856 => Loss: 0.0895345222023351\n",
      "Iteration 1857 => Loss: 0.08953143264585552\n",
      "Iteration 1858 => Loss: 0.08952834535641813\n",
      "Iteration 1859 => Loss: 0.08952526033120815\n",
      "Iteration 1860 => Loss: 0.08952217756741566\n",
      "Iteration 1861 => Loss: 0.08951909706223576\n",
      "Iteration 1862 => Loss: 0.08951601881286846\n",
      "Iteration 1863 => Loss: 0.08951294281651867\n",
      "Iteration 1864 => Loss: 0.0895098690703962\n",
      "Iteration 1865 => Loss: 0.08950679757171577\n",
      "Iteration 1866 => Loss: 0.08950372831769703\n",
      "Iteration 1867 => Loss: 0.08950066130556435\n",
      "Iteration 1868 => Loss: 0.08949759653254719\n",
      "Iteration 1869 => Loss: 0.08949453399587966\n",
      "Iteration 1870 => Loss: 0.0894914736928008\n",
      "Iteration 1871 => Loss: 0.0894884156205545\n",
      "Iteration 1872 => Loss: 0.08948535977638941\n",
      "Iteration 1873 => Loss: 0.08948230615755896\n",
      "Iteration 1874 => Loss: 0.08947925476132151\n",
      "Iteration 1875 => Loss: 0.08947620558494006\n",
      "Iteration 1876 => Loss: 0.08947315862568245\n",
      "Iteration 1877 => Loss: 0.08947011388082127\n",
      "Iteration 1878 => Loss: 0.08946707134763389\n",
      "Iteration 1879 => Loss: 0.08946403102340238\n",
      "Iteration 1880 => Loss: 0.08946099290541354\n",
      "Iteration 1881 => Loss: 0.08945795699095892\n",
      "Iteration 1882 => Loss: 0.08945492327733476\n",
      "Iteration 1883 => Loss: 0.08945189176184203\n",
      "Iteration 1884 => Loss: 0.08944886244178628\n",
      "Iteration 1885 => Loss: 0.08944583531447785\n",
      "Iteration 1886 => Loss: 0.08944281037723176\n",
      "Iteration 1887 => Loss: 0.0894397876273676\n",
      "Iteration 1888 => Loss: 0.0894367670622096\n",
      "Iteration 1889 => Loss: 0.08943374867908672\n",
      "Iteration 1890 => Loss: 0.08943073247533245\n",
      "Iteration 1891 => Loss: 0.08942771844828494\n",
      "Iteration 1892 => Loss: 0.08942470659528697\n",
      "Iteration 1893 => Loss: 0.08942169691368579\n",
      "Iteration 1894 => Loss: 0.08941868940083339\n",
      "Iteration 1895 => Loss: 0.08941568405408624\n",
      "Iteration 1896 => Loss: 0.08941268087080541\n",
      "Iteration 1897 => Loss: 0.08940967984835645\n",
      "Iteration 1898 => Loss: 0.08940668098410952\n",
      "Iteration 1899 => Loss: 0.08940368427543932\n",
      "Iteration 1900 => Loss: 0.08940068971972501\n",
      "Iteration 1901 => Loss: 0.08939769731435031\n",
      "Iteration 1902 => Loss: 0.08939470705670338\n",
      "Iteration 1903 => Loss: 0.089391718944177\n",
      "Iteration 1904 => Loss: 0.08938873297416826\n",
      "Iteration 1905 => Loss: 0.08938574914407882\n",
      "Iteration 1906 => Loss: 0.0893827674513148\n",
      "Iteration 1907 => Loss: 0.08937978789328675\n",
      "Iteration 1908 => Loss: 0.08937681046740965\n",
      "Iteration 1909 => Loss: 0.08937383517110291\n",
      "Iteration 1910 => Loss: 0.08937086200179035\n",
      "Iteration 1911 => Loss: 0.0893678909569003\n",
      "Iteration 1912 => Loss: 0.08936492203386534\n",
      "Iteration 1913 => Loss: 0.08936195523012254\n",
      "Iteration 1914 => Loss: 0.08935899054311326\n",
      "Iteration 1915 => Loss: 0.08935602797028339\n",
      "Iteration 1916 => Loss: 0.08935306750908298\n",
      "Iteration 1917 => Loss: 0.0893501091569666\n",
      "Iteration 1918 => Loss: 0.08934715291139309\n",
      "Iteration 1919 => Loss: 0.08934419876982558\n",
      "Iteration 1920 => Loss: 0.08934124672973161\n",
      "Iteration 1921 => Loss: 0.08933829678858295\n",
      "Iteration 1922 => Loss: 0.08933534894385578\n",
      "Iteration 1923 => Loss: 0.08933240319303043\n",
      "Iteration 1924 => Loss: 0.08932945953359163\n",
      "Iteration 1925 => Loss: 0.08932651796302832\n",
      "Iteration 1926 => Loss: 0.08932357847883374\n",
      "Iteration 1927 => Loss: 0.08932064107850539\n",
      "Iteration 1928 => Loss: 0.08931770575954497\n",
      "Iteration 1929 => Loss: 0.08931477251945846\n",
      "Iteration 1930 => Loss: 0.08931184135575608\n",
      "Iteration 1931 => Loss: 0.0893089122659522\n",
      "Iteration 1932 => Loss: 0.08930598524756542\n",
      "Iteration 1933 => Loss: 0.0893030602981186\n",
      "Iteration 1934 => Loss: 0.08930013741513876\n",
      "Iteration 1935 => Loss: 0.08929721659615703\n",
      "Iteration 1936 => Loss: 0.08929429783870879\n",
      "Iteration 1937 => Loss: 0.08929138114033357\n",
      "Iteration 1938 => Loss: 0.08928846649857505\n",
      "Iteration 1939 => Loss: 0.08928555391098103\n",
      "Iteration 1940 => Loss: 0.08928264337510347\n",
      "Iteration 1941 => Loss: 0.08927973488849845\n",
      "Iteration 1942 => Loss: 0.08927682844872614\n",
      "Iteration 1943 => Loss: 0.08927392405335087\n",
      "Iteration 1944 => Loss: 0.089271021699941\n",
      "Iteration 1945 => Loss: 0.08926812138606907\n",
      "Iteration 1946 => Loss: 0.0892652231093116\n",
      "Iteration 1947 => Loss: 0.08926232686724925\n",
      "Iteration 1948 => Loss: 0.08925943265746672\n",
      "Iteration 1949 => Loss: 0.08925654047755274\n",
      "Iteration 1950 => Loss: 0.08925365032510016\n",
      "Iteration 1951 => Loss: 0.08925076219770578\n",
      "Iteration 1952 => Loss: 0.08924787609297043\n",
      "Iteration 1953 => Loss: 0.08924499200849904\n",
      "Iteration 1954 => Loss: 0.08924210994190052\n",
      "Iteration 1955 => Loss: 0.08923922989078766\n",
      "Iteration 1956 => Loss: 0.08923635185277741\n",
      "Iteration 1957 => Loss: 0.08923347582549064\n",
      "Iteration 1958 => Loss: 0.08923060180655218\n",
      "Iteration 1959 => Loss: 0.08922772979359082\n",
      "Iteration 1960 => Loss: 0.08922485978423929\n",
      "Iteration 1961 => Loss: 0.08922199177613437\n",
      "Iteration 1962 => Loss: 0.08921912576691667\n",
      "Iteration 1963 => Loss: 0.08921626175423072\n",
      "Iteration 1964 => Loss: 0.08921339973572509\n",
      "Iteration 1965 => Loss: 0.08921053970905218\n",
      "Iteration 1966 => Loss: 0.08920768167186832\n",
      "Iteration 1967 => Loss: 0.08920482562183371\n",
      "Iteration 1968 => Loss: 0.08920197155661244\n",
      "Iteration 1969 => Loss: 0.08919911947387252\n",
      "Iteration 1970 => Loss: 0.08919626937128582\n",
      "Iteration 1971 => Loss: 0.08919342124652803\n",
      "Iteration 1972 => Loss: 0.08919057509727873\n",
      "Iteration 1973 => Loss: 0.08918773092122138\n",
      "Iteration 1974 => Loss: 0.08918488871604324\n",
      "Iteration 1975 => Loss: 0.08918204847943534\n",
      "Iteration 1976 => Loss: 0.08917921020909264\n",
      "Iteration 1977 => Loss: 0.08917637390271387\n",
      "Iteration 1978 => Loss: 0.08917353955800154\n",
      "Iteration 1979 => Loss: 0.08917070717266198\n",
      "Iteration 1980 => Loss: 0.08916787674440532\n",
      "Iteration 1981 => Loss: 0.08916504827094547\n",
      "Iteration 1982 => Loss: 0.08916222175000008\n",
      "Iteration 1983 => Loss: 0.08915939717929056\n",
      "Iteration 1984 => Loss: 0.08915657455654215\n",
      "Iteration 1985 => Loss: 0.08915375387948377\n",
      "Iteration 1986 => Loss: 0.08915093514584807\n",
      "Iteration 1987 => Loss: 0.08914811835337151\n",
      "Iteration 1988 => Loss: 0.08914530349979419\n",
      "Iteration 1989 => Loss: 0.08914249058285999\n",
      "Iteration 1990 => Loss: 0.08913967960031642\n",
      "Iteration 1991 => Loss: 0.08913687054991479\n",
      "Iteration 1992 => Loss: 0.08913406342941002\n",
      "Iteration 1993 => Loss: 0.08913125823656078\n",
      "Iteration 1994 => Loss: 0.08912845496912931\n",
      "Iteration 1995 => Loss: 0.08912565362488166\n",
      "Iteration 1996 => Loss: 0.08912285420158747\n",
      "Iteration 1997 => Loss: 0.08912005669701999\n",
      "Iteration 1998 => Loss: 0.08911726110895615\n",
      "Iteration 1999 => Loss: 0.08911446743517656\n",
      "Iteration 2000 => Loss: 0.08911167567346544\n",
      "Iteration 2001 => Loss: 0.08910888582161058\n",
      "Iteration 2002 => Loss: 0.08910609787740344\n",
      "Iteration 2003 => Loss: 0.08910331183863905\n",
      "Iteration 2004 => Loss: 0.08910052770311606\n",
      "Iteration 2005 => Loss: 0.0890977454686367\n",
      "Iteration 2006 => Loss: 0.08909496513300683\n",
      "Iteration 2007 => Loss: 0.08909218669403575\n",
      "Iteration 2008 => Loss: 0.08908941014953652\n",
      "Iteration 2009 => Loss: 0.08908663549732561\n",
      "Iteration 2010 => Loss: 0.08908386273522313\n",
      "Iteration 2011 => Loss: 0.08908109186105265\n",
      "Iteration 2012 => Loss: 0.08907832287264136\n",
      "Iteration 2013 => Loss: 0.08907555576781996\n",
      "Iteration 2014 => Loss: 0.08907279054442262\n",
      "Iteration 2015 => Loss: 0.08907002720028712\n",
      "Iteration 2016 => Loss: 0.08906726573325469\n",
      "Iteration 2017 => Loss: 0.08906450614116998\n",
      "Iteration 2018 => Loss: 0.08906174842188129\n",
      "Iteration 2019 => Loss: 0.08905899257324035\n",
      "Iteration 2020 => Loss: 0.08905623859310227\n",
      "Iteration 2021 => Loss: 0.0890534864793258\n",
      "Iteration 2022 => Loss: 0.08905073622977303\n",
      "Iteration 2023 => Loss: 0.08904798784230952\n",
      "Iteration 2024 => Loss: 0.08904524131480436\n",
      "Iteration 2025 => Loss: 0.08904249664512995\n",
      "Iteration 2026 => Loss: 0.08903975383116224\n",
      "Iteration 2027 => Loss: 0.08903701287078053\n",
      "Iteration 2028 => Loss: 0.08903427376186761\n",
      "Iteration 2029 => Loss: 0.08903153650230962\n",
      "Iteration 2030 => Loss: 0.08902880108999617\n",
      "Iteration 2031 => Loss: 0.0890260675228202\n",
      "Iteration 2032 => Loss: 0.08902333579867809\n",
      "Iteration 2033 => Loss: 0.08902060591546956\n",
      "Iteration 2034 => Loss: 0.08901787787109774\n",
      "Iteration 2035 => Loss: 0.08901515166346913\n",
      "Iteration 2036 => Loss: 0.0890124272904936\n",
      "Iteration 2037 => Loss: 0.08900970475008434\n",
      "Iteration 2038 => Loss: 0.08900698404015789\n",
      "Iteration 2039 => Loss: 0.0890042651586342\n",
      "Iteration 2040 => Loss: 0.08900154810343652\n",
      "Iteration 2041 => Loss: 0.08899883287249134\n",
      "Iteration 2042 => Loss: 0.08899611946372858\n",
      "Iteration 2043 => Loss: 0.08899340787508146\n",
      "Iteration 2044 => Loss: 0.08899069810448647\n",
      "Iteration 2045 => Loss: 0.08898799014988344\n",
      "Iteration 2046 => Loss: 0.08898528400921546\n",
      "Iteration 2047 => Loss: 0.08898257968042886\n",
      "Iteration 2048 => Loss: 0.08897987716147338\n",
      "Iteration 2049 => Loss: 0.08897717645030193\n",
      "Iteration 2050 => Loss: 0.08897447754487071\n",
      "Iteration 2051 => Loss: 0.08897178044313919\n",
      "Iteration 2052 => Loss: 0.08896908514307009\n",
      "Iteration 2053 => Loss: 0.08896639164262936\n",
      "Iteration 2054 => Loss: 0.08896369993978623\n",
      "Iteration 2055 => Loss: 0.08896101003251305\n",
      "Iteration 2056 => Loss: 0.08895832191878561\n",
      "Iteration 2057 => Loss: 0.08895563559658265\n",
      "Iteration 2058 => Loss: 0.08895295106388636\n",
      "Iteration 2059 => Loss: 0.08895026831868197\n",
      "Iteration 2060 => Loss: 0.088947587358958\n",
      "Iteration 2061 => Loss: 0.08894490818270614\n",
      "Iteration 2062 => Loss: 0.08894223078792125\n",
      "Iteration 2063 => Loss: 0.08893955517260138\n",
      "Iteration 2064 => Loss: 0.08893688133474775\n",
      "Iteration 2065 => Loss: 0.08893420927236471\n",
      "Iteration 2066 => Loss: 0.08893153898345989\n",
      "Iteration 2067 => Loss: 0.08892887046604392\n",
      "Iteration 2068 => Loss: 0.08892620371813066\n",
      "Iteration 2069 => Loss: 0.08892353873773708\n",
      "Iteration 2070 => Loss: 0.08892087552288336\n",
      "Iteration 2071 => Loss: 0.08891821407159267\n",
      "Iteration 2072 => Loss: 0.08891555438189142\n",
      "Iteration 2073 => Loss: 0.08891289645180905\n",
      "Iteration 2074 => Loss: 0.08891024027937819\n",
      "Iteration 2075 => Loss: 0.0889075858626345\n",
      "Iteration 2076 => Loss: 0.08890493319961677\n",
      "Iteration 2077 => Loss: 0.08890228228836684\n",
      "Iteration 2078 => Loss: 0.08889963312692971\n",
      "Iteration 2079 => Loss: 0.08889698571335339\n",
      "Iteration 2080 => Loss: 0.08889434004568897\n",
      "Iteration 2081 => Loss: 0.08889169612199062\n",
      "Iteration 2082 => Loss: 0.08888905394031554\n",
      "Iteration 2083 => Loss: 0.08888641349872402\n",
      "Iteration 2084 => Loss: 0.08888377479527937\n",
      "Iteration 2085 => Loss: 0.0888811378280479\n",
      "Iteration 2086 => Loss: 0.08887850259509902\n",
      "Iteration 2087 => Loss: 0.08887586909450516\n",
      "Iteration 2088 => Loss: 0.08887323732434174\n",
      "Iteration 2089 => Loss: 0.08887060728268717\n",
      "Iteration 2090 => Loss: 0.08886797896762293\n",
      "Iteration 2091 => Loss: 0.08886535237723343\n",
      "Iteration 2092 => Loss: 0.08886272750960615\n",
      "Iteration 2093 => Loss: 0.08886010436283151\n",
      "Iteration 2094 => Loss: 0.08885748293500292\n",
      "Iteration 2095 => Loss: 0.08885486322421676\n",
      "Iteration 2096 => Loss: 0.08885224522857245\n",
      "Iteration 2097 => Loss: 0.08884962894617222\n",
      "Iteration 2098 => Loss: 0.08884701437512146\n",
      "Iteration 2099 => Loss: 0.08884440151352836\n",
      "Iteration 2100 => Loss: 0.08884179035950406\n",
      "Iteration 2101 => Loss: 0.08883918091116273\n",
      "Iteration 2102 => Loss: 0.08883657316662147\n",
      "Iteration 2103 => Loss: 0.08883396712400018\n",
      "Iteration 2104 => Loss: 0.08883136278142184\n",
      "Iteration 2105 => Loss: 0.08882876013701227\n",
      "Iteration 2106 => Loss: 0.08882615918890016\n",
      "Iteration 2107 => Loss: 0.08882355993521719\n",
      "Iteration 2108 => Loss: 0.08882096237409788\n",
      "Iteration 2109 => Loss: 0.08881836650367975\n",
      "Iteration 2110 => Loss: 0.08881577232210298\n",
      "Iteration 2111 => Loss: 0.0888131798275109\n",
      "Iteration 2112 => Loss: 0.08881058901804954\n",
      "Iteration 2113 => Loss: 0.08880799989186783\n",
      "Iteration 2114 => Loss: 0.08880541244711762\n",
      "Iteration 2115 => Loss: 0.08880282668195356\n",
      "Iteration 2116 => Loss: 0.0888002425945332\n",
      "Iteration 2117 => Loss: 0.08879766018301687\n",
      "Iteration 2118 => Loss: 0.08879507944556783\n",
      "Iteration 2119 => Loss: 0.08879250038035212\n",
      "Iteration 2120 => Loss: 0.0887899229855386\n",
      "Iteration 2121 => Loss: 0.08878734725929899\n",
      "Iteration 2122 => Loss: 0.08878477319980778\n",
      "Iteration 2123 => Loss: 0.08878220080524234\n",
      "Iteration 2124 => Loss: 0.0887796300737828\n",
      "Iteration 2125 => Loss: 0.08877706100361209\n",
      "Iteration 2126 => Loss: 0.08877449359291596\n",
      "Iteration 2127 => Loss: 0.08877192783988294\n",
      "Iteration 2128 => Loss: 0.08876936374270432\n",
      "Iteration 2129 => Loss: 0.08876680129957422\n",
      "Iteration 2130 => Loss: 0.0887642405086895\n",
      "Iteration 2131 => Loss: 0.08876168136824979\n",
      "Iteration 2132 => Loss: 0.08875912387645746\n",
      "Iteration 2133 => Loss: 0.08875656803151773\n",
      "Iteration 2134 => Loss: 0.08875401383163839\n",
      "Iteration 2135 => Loss: 0.08875146127503021\n",
      "Iteration 2136 => Loss: 0.08874891035990651\n",
      "Iteration 2137 => Loss: 0.08874636108448346\n",
      "Iteration 2138 => Loss: 0.08874381344697986\n",
      "Iteration 2139 => Loss: 0.08874126744561732\n",
      "Iteration 2140 => Loss: 0.08873872307862014\n",
      "Iteration 2141 => Loss: 0.08873618034421532\n",
      "Iteration 2142 => Loss: 0.08873363924063259\n",
      "Iteration 2143 => Loss: 0.08873109976610435\n",
      "Iteration 2144 => Loss: 0.08872856191886572\n",
      "Iteration 2145 => Loss: 0.08872602569715453\n",
      "Iteration 2146 => Loss: 0.08872349109921124\n",
      "Iteration 2147 => Loss: 0.0887209581232791\n",
      "Iteration 2148 => Loss: 0.08871842676760389\n",
      "Iteration 2149 => Loss: 0.08871589703043413\n",
      "Iteration 2150 => Loss: 0.08871336891002105\n",
      "Iteration 2151 => Loss: 0.08871084240461849\n",
      "Iteration 2152 => Loss: 0.08870831751248293\n",
      "Iteration 2153 => Loss: 0.08870579423187354\n",
      "Iteration 2154 => Loss: 0.0887032725610521\n",
      "Iteration 2155 => Loss: 0.08870075249828303\n",
      "Iteration 2156 => Loss: 0.08869823404183347\n",
      "Iteration 2157 => Loss: 0.08869571718997304\n",
      "Iteration 2158 => Loss: 0.08869320194097406\n",
      "Iteration 2159 => Loss: 0.08869068829311152\n",
      "Iteration 2160 => Loss: 0.08868817624466292\n",
      "Iteration 2161 => Loss: 0.08868566579390845\n",
      "Iteration 2162 => Loss: 0.08868315693913088\n",
      "Iteration 2163 => Loss: 0.08868064967861551\n",
      "Iteration 2164 => Loss: 0.08867814401065034\n",
      "Iteration 2165 => Loss: 0.0886756399335259\n",
      "Iteration 2166 => Loss: 0.0886731374455353\n",
      "Iteration 2167 => Loss: 0.08867063654497424\n",
      "Iteration 2168 => Loss: 0.08866813723014097\n",
      "Iteration 2169 => Loss: 0.0886656394993364\n",
      "Iteration 2170 => Loss: 0.08866314335086381\n",
      "Iteration 2171 => Loss: 0.08866064878302925\n",
      "Iteration 2172 => Loss: 0.08865815579414116\n",
      "Iteration 2173 => Loss: 0.08865566438251066\n",
      "Iteration 2174 => Loss: 0.08865317454645129\n",
      "Iteration 2175 => Loss: 0.08865068628427919\n",
      "Iteration 2176 => Loss: 0.08864819959431307\n",
      "Iteration 2177 => Loss: 0.08864571447487407\n",
      "Iteration 2178 => Loss: 0.08864323092428592\n",
      "Iteration 2179 => Loss: 0.08864074894087483\n",
      "Iteration 2180 => Loss: 0.08863826852296958\n",
      "Iteration 2181 => Loss: 0.08863578966890136\n",
      "Iteration 2182 => Loss: 0.088633312377004\n",
      "Iteration 2183 => Loss: 0.0886308366456137\n",
      "Iteration 2184 => Loss: 0.0886283624730692\n",
      "Iteration 2185 => Loss: 0.08862588985771175\n",
      "Iteration 2186 => Loss: 0.08862341879788503\n",
      "Iteration 2187 => Loss: 0.0886209492919353\n",
      "Iteration 2188 => Loss: 0.08861848133821112\n",
      "Iteration 2189 => Loss: 0.08861601493506371\n",
      "Iteration 2190 => Loss: 0.08861355008084663\n",
      "Iteration 2191 => Loss: 0.08861108677391595\n",
      "Iteration 2192 => Loss: 0.08860862501263017\n",
      "Iteration 2193 => Loss: 0.08860616479535022\n",
      "Iteration 2194 => Loss: 0.08860370612043952\n",
      "Iteration 2195 => Loss: 0.08860124898626394\n",
      "Iteration 2196 => Loss: 0.08859879339119175\n",
      "Iteration 2197 => Loss: 0.08859633933359364\n",
      "Iteration 2198 => Loss: 0.08859388681184273\n",
      "Iteration 2199 => Loss: 0.08859143582431458\n",
      "Iteration 2200 => Loss: 0.08858898636938721\n",
      "Iteration 2201 => Loss: 0.08858653844544097\n",
      "Iteration 2202 => Loss: 0.0885840920508586\n",
      "Iteration 2203 => Loss: 0.08858164718402538\n",
      "Iteration 2204 => Loss: 0.08857920384332882\n",
      "Iteration 2205 => Loss: 0.08857676202715896\n",
      "Iteration 2206 => Loss: 0.08857432173390815\n",
      "Iteration 2207 => Loss: 0.08857188296197113\n",
      "Iteration 2208 => Loss: 0.08856944570974508\n",
      "Iteration 2209 => Loss: 0.08856700997562944\n",
      "Iteration 2210 => Loss: 0.08856457575802612\n",
      "Iteration 2211 => Loss: 0.08856214305533938\n",
      "Iteration 2212 => Loss: 0.0885597118659758\n",
      "Iteration 2213 => Loss: 0.08855728218834437\n",
      "Iteration 2214 => Loss: 0.08855485402085637\n",
      "Iteration 2215 => Loss: 0.08855242736192548\n",
      "Iteration 2216 => Loss: 0.08855000220996768\n",
      "Iteration 2217 => Loss: 0.08854757856340137\n",
      "Iteration 2218 => Loss: 0.08854515642064713\n",
      "Iteration 2219 => Loss: 0.08854273578012804\n",
      "Iteration 2220 => Loss: 0.0885403166402694\n",
      "Iteration 2221 => Loss: 0.08853789899949886\n",
      "Iteration 2222 => Loss: 0.08853548285624645\n",
      "Iteration 2223 => Loss: 0.08853306820894433\n",
      "Iteration 2224 => Loss: 0.08853065505602714\n",
      "Iteration 2225 => Loss: 0.08852824339593178\n",
      "Iteration 2226 => Loss: 0.08852583322709744\n",
      "Iteration 2227 => Loss: 0.08852342454796558\n",
      "Iteration 2228 => Loss: 0.08852101735697994\n",
      "Iteration 2229 => Loss: 0.08851861165258663\n",
      "Iteration 2230 => Loss: 0.08851620743323395\n",
      "Iteration 2231 => Loss: 0.08851380469737254\n",
      "Iteration 2232 => Loss: 0.08851140344345525\n",
      "Iteration 2233 => Loss: 0.08850900366993725\n",
      "Iteration 2234 => Loss: 0.08850660537527595\n",
      "Iteration 2235 => Loss: 0.08850420855793102\n",
      "Iteration 2236 => Loss: 0.08850181321636437\n",
      "Iteration 2237 => Loss: 0.08849941934904024\n",
      "Iteration 2238 => Loss: 0.08849702695442499\n",
      "Iteration 2239 => Loss: 0.0884946360309873\n",
      "Iteration 2240 => Loss: 0.0884922465771981\n",
      "Iteration 2241 => Loss: 0.08848985859153051\n",
      "Iteration 2242 => Loss: 0.08848747207245991\n",
      "Iteration 2243 => Loss: 0.08848508701846386\n",
      "Iteration 2244 => Loss: 0.08848270342802222\n",
      "Iteration 2245 => Loss: 0.08848032129961698\n",
      "Iteration 2246 => Loss: 0.08847794063173241\n",
      "Iteration 2247 => Loss: 0.08847556142285494\n",
      "Iteration 2248 => Loss: 0.08847318367147322\n",
      "Iteration 2249 => Loss: 0.08847080737607815\n",
      "Iteration 2250 => Loss: 0.08846843253516275\n",
      "Iteration 2251 => Loss: 0.0884660591472222\n",
      "Iteration 2252 => Loss: 0.08846368721075404\n",
      "Iteration 2253 => Loss: 0.08846131672425782\n",
      "Iteration 2254 => Loss: 0.08845894768623533\n",
      "Iteration 2255 => Loss: 0.08845658009519056\n",
      "Iteration 2256 => Loss: 0.08845421394962961\n",
      "Iteration 2257 => Loss: 0.08845184924806081\n",
      "Iteration 2258 => Loss: 0.08844948598899464\n",
      "Iteration 2259 => Loss: 0.08844712417094372\n",
      "Iteration 2260 => Loss: 0.08844476379242278\n",
      "Iteration 2261 => Loss: 0.0884424048519488\n",
      "Iteration 2262 => Loss: 0.08844004734804087\n",
      "Iteration 2263 => Loss: 0.08843769127922016\n",
      "Iteration 2264 => Loss: 0.08843533664401006\n",
      "Iteration 2265 => Loss: 0.08843298344093609\n",
      "Iteration 2266 => Loss: 0.08843063166852577\n",
      "Iteration 2267 => Loss: 0.08842828132530896\n",
      "Iteration 2268 => Loss: 0.08842593240981746\n",
      "Iteration 2269 => Loss: 0.08842358492058532\n",
      "Iteration 2270 => Loss: 0.0884212388561486\n",
      "Iteration 2271 => Loss: 0.08841889421504553\n",
      "Iteration 2272 => Loss: 0.08841655099581643\n",
      "Iteration 2273 => Loss: 0.08841420919700373\n",
      "Iteration 2274 => Loss: 0.08841186881715193\n",
      "Iteration 2275 => Loss: 0.08840952985480767\n",
      "Iteration 2276 => Loss: 0.08840719230851966\n",
      "Iteration 2277 => Loss: 0.08840485617683867\n",
      "Iteration 2278 => Loss: 0.08840252145831759\n",
      "Iteration 2279 => Loss: 0.08840018815151142\n",
      "Iteration 2280 => Loss: 0.08839785625497715\n",
      "Iteration 2281 => Loss: 0.08839552576727384\n",
      "Iteration 2282 => Loss: 0.08839319668696279\n",
      "Iteration 2283 => Loss: 0.0883908690126071\n",
      "Iteration 2284 => Loss: 0.08838854274277214\n",
      "Iteration 2285 => Loss: 0.08838621787602524\n",
      "Iteration 2286 => Loss: 0.0883838944109358\n",
      "Iteration 2287 => Loss: 0.0883815723460753\n",
      "Iteration 2288 => Loss: 0.08837925168001722\n",
      "Iteration 2289 => Loss: 0.08837693241133707\n",
      "Iteration 2290 => Loss: 0.08837461453861246\n",
      "Iteration 2291 => Loss: 0.08837229806042299\n",
      "Iteration 2292 => Loss: 0.08836998297535029\n",
      "Iteration 2293 => Loss: 0.08836766928197802\n",
      "Iteration 2294 => Loss: 0.08836535697889185\n",
      "Iteration 2295 => Loss: 0.08836304606467954\n",
      "Iteration 2296 => Loss: 0.08836073653793072\n",
      "Iteration 2297 => Loss: 0.08835842839723723\n",
      "Iteration 2298 => Loss: 0.08835612164119269\n",
      "Iteration 2299 => Loss: 0.08835381626839289\n",
      "Iteration 2300 => Loss: 0.0883515122774356\n",
      "Iteration 2301 => Loss: 0.08834920966692048\n",
      "Iteration 2302 => Loss: 0.08834690843544932\n",
      "Iteration 2303 => Loss: 0.08834460858162578\n",
      "Iteration 2304 => Loss: 0.0883423101040556\n",
      "Iteration 2305 => Loss: 0.08834001300134645\n",
      "Iteration 2306 => Loss: 0.08833771727210801\n",
      "Iteration 2307 => Loss: 0.08833542291495185\n",
      "Iteration 2308 => Loss: 0.08833312992849163\n",
      "Iteration 2309 => Loss: 0.08833083831134286\n",
      "Iteration 2310 => Loss: 0.08832854806212313\n",
      "Iteration 2311 => Loss: 0.08832625917945192\n",
      "Iteration 2312 => Loss: 0.08832397166195063\n",
      "Iteration 2313 => Loss: 0.08832168550824271\n",
      "Iteration 2314 => Loss: 0.08831940071695348\n",
      "Iteration 2315 => Loss: 0.08831711728671023\n",
      "Iteration 2316 => Loss: 0.08831483521614221\n",
      "Iteration 2317 => Loss: 0.08831255450388059\n",
      "Iteration 2318 => Loss: 0.08831027514855848\n",
      "Iteration 2319 => Loss: 0.08830799714881087\n",
      "Iteration 2320 => Loss: 0.08830572050327476\n",
      "Iteration 2321 => Loss: 0.08830344521058905\n",
      "Iteration 2322 => Loss: 0.08830117126939453\n",
      "Iteration 2323 => Loss: 0.08829889867833395\n",
      "Iteration 2324 => Loss: 0.08829662743605188\n",
      "Iteration 2325 => Loss: 0.08829435754119493\n",
      "Iteration 2326 => Loss: 0.08829208899241152\n",
      "Iteration 2327 => Loss: 0.08828982178835203\n",
      "Iteration 2328 => Loss: 0.08828755592766868\n",
      "Iteration 2329 => Loss: 0.08828529140901566\n",
      "Iteration 2330 => Loss: 0.08828302823104899\n",
      "Iteration 2331 => Loss: 0.08828076639242659\n",
      "Iteration 2332 => Loss: 0.08827850589180831\n",
      "Iteration 2333 => Loss: 0.08827624672785583\n",
      "Iteration 2334 => Loss: 0.08827398889923274\n",
      "Iteration 2335 => Loss: 0.08827173240460445\n",
      "Iteration 2336 => Loss: 0.08826947724263834\n",
      "Iteration 2337 => Loss: 0.08826722341200356\n",
      "Iteration 2338 => Loss: 0.08826497091137121\n",
      "Iteration 2339 => Loss: 0.08826271973941417\n",
      "Iteration 2340 => Loss: 0.08826046989480724\n",
      "Iteration 2341 => Loss: 0.088258221376227\n",
      "Iteration 2342 => Loss: 0.08825597418235202\n",
      "Iteration 2343 => Loss: 0.08825372831186257\n",
      "Iteration 2344 => Loss: 0.08825148376344082\n",
      "Iteration 2345 => Loss: 0.08824924053577082\n",
      "Iteration 2346 => Loss: 0.08824699862753843\n",
      "Iteration 2347 => Loss: 0.0882447580374313\n",
      "Iteration 2348 => Loss: 0.08824251876413895\n",
      "Iteration 2349 => Loss: 0.08824028080635281\n",
      "Iteration 2350 => Loss: 0.08823804416276594\n",
      "Iteration 2351 => Loss: 0.0882358088320734\n",
      "Iteration 2352 => Loss: 0.088233574812972\n",
      "Iteration 2353 => Loss: 0.08823134210416034\n",
      "Iteration 2354 => Loss: 0.08822911070433889\n",
      "Iteration 2355 => Loss: 0.08822688061220987\n",
      "Iteration 2356 => Loss: 0.08822465182647739\n",
      "Iteration 2357 => Loss: 0.08822242434584723\n",
      "Iteration 2358 => Loss: 0.08822019816902708\n",
      "Iteration 2359 => Loss: 0.0882179732947264\n",
      "Iteration 2360 => Loss: 0.08821574972165638\n",
      "Iteration 2361 => Loss: 0.08821352744853012\n",
      "Iteration 2362 => Loss: 0.08821130647406238\n",
      "Iteration 2363 => Loss: 0.08820908679696977\n",
      "Iteration 2364 => Loss: 0.08820686841597065\n",
      "Iteration 2365 => Loss: 0.08820465132978524\n",
      "Iteration 2366 => Loss: 0.08820243553713537\n",
      "Iteration 2367 => Loss: 0.08820022103674484\n",
      "Iteration 2368 => Loss: 0.08819800782733897\n",
      "Iteration 2369 => Loss: 0.08819579590764512\n",
      "Iteration 2370 => Loss: 0.08819358527639219\n",
      "Iteration 2371 => Loss: 0.088191375932311\n",
      "Iteration 2372 => Loss: 0.08818916787413397\n",
      "Iteration 2373 => Loss: 0.08818696110059537\n",
      "Iteration 2374 => Loss: 0.0881847556104312\n",
      "Iteration 2375 => Loss: 0.08818255140237918\n",
      "Iteration 2376 => Loss: 0.08818034847517883\n",
      "Iteration 2377 => Loss: 0.08817814682757132\n",
      "Iteration 2378 => Loss: 0.08817594645829961\n",
      "Iteration 2379 => Loss: 0.08817374736610845\n",
      "Iteration 2380 => Loss: 0.08817154954974414\n",
      "Iteration 2381 => Loss: 0.08816935300795488\n",
      "Iteration 2382 => Loss: 0.08816715773949053\n",
      "Iteration 2383 => Loss: 0.08816496374310269\n",
      "Iteration 2384 => Loss: 0.08816277101754463\n",
      "Iteration 2385 => Loss: 0.08816057956157136\n",
      "Iteration 2386 => Loss: 0.08815838937393962\n",
      "Iteration 2387 => Loss: 0.08815620045340784\n",
      "Iteration 2388 => Loss: 0.08815401279873611\n",
      "Iteration 2389 => Loss: 0.08815182640868632\n",
      "Iteration 2390 => Loss: 0.08814964128202198\n",
      "Iteration 2391 => Loss: 0.0881474574175083\n",
      "Iteration 2392 => Loss: 0.08814527481391222\n",
      "Iteration 2393 => Loss: 0.08814309347000235\n",
      "Iteration 2394 => Loss: 0.088140913384549\n",
      "Iteration 2395 => Loss: 0.08813873455632411\n",
      "Iteration 2396 => Loss: 0.08813655698410139\n",
      "Iteration 2397 => Loss: 0.08813438066665613\n",
      "Iteration 2398 => Loss: 0.08813220560276537\n",
      "Iteration 2399 => Loss: 0.08813003179120779\n",
      "Iteration 2400 => Loss: 0.0881278592307637\n",
      "Iteration 2401 => Loss: 0.08812568792021516\n",
      "Iteration 2402 => Loss: 0.08812351785834588\n",
      "Iteration 2403 => Loss: 0.08812134904394114\n",
      "Iteration 2404 => Loss: 0.08811918147578796\n",
      "Iteration 2405 => Loss: 0.08811701515267498\n",
      "Iteration 2406 => Loss: 0.08811485007339248\n",
      "Iteration 2407 => Loss: 0.08811268623673245\n",
      "Iteration 2408 => Loss: 0.08811052364148848\n",
      "Iteration 2409 => Loss: 0.08810836228645577\n",
      "Iteration 2410 => Loss: 0.08810620217043123\n",
      "Iteration 2411 => Loss: 0.08810404329221334\n",
      "Iteration 2412 => Loss: 0.08810188565060226\n",
      "Iteration 2413 => Loss: 0.08809972924439978\n",
      "Iteration 2414 => Loss: 0.08809757407240927\n",
      "Iteration 2415 => Loss: 0.08809542013343576\n",
      "Iteration 2416 => Loss: 0.08809326742628593\n",
      "Iteration 2417 => Loss: 0.08809111594976803\n",
      "Iteration 2418 => Loss: 0.08808896570269194\n",
      "Iteration 2419 => Loss: 0.08808681668386914\n",
      "Iteration 2420 => Loss: 0.08808466889211278\n",
      "Iteration 2421 => Loss: 0.08808252232623752\n",
      "Iteration 2422 => Loss: 0.08808037698505977\n",
      "Iteration 2423 => Loss: 0.08807823286739737\n",
      "Iteration 2424 => Loss: 0.08807608997206988\n",
      "Iteration 2425 => Loss: 0.08807394829789839\n",
      "Iteration 2426 => Loss: 0.08807180784370565\n",
      "Iteration 2427 => Loss: 0.08806966860831593\n",
      "Iteration 2428 => Loss: 0.08806753059055515\n",
      "Iteration 2429 => Loss: 0.08806539378925077\n",
      "Iteration 2430 => Loss: 0.08806325820323187\n",
      "Iteration 2431 => Loss: 0.08806112383132904\n",
      "Iteration 2432 => Loss: 0.08805899067237458\n",
      "Iteration 2433 => Loss: 0.08805685872520219\n",
      "Iteration 2434 => Loss: 0.08805472798864729\n",
      "Iteration 2435 => Loss: 0.08805259846154677\n",
      "Iteration 2436 => Loss: 0.08805047014273917\n",
      "Iteration 2437 => Loss: 0.08804834303106454\n",
      "Iteration 2438 => Loss: 0.0880462171253645\n",
      "Iteration 2439 => Loss: 0.08804409242448222\n",
      "Iteration 2440 => Loss: 0.08804196892726243\n",
      "Iteration 2441 => Loss: 0.08803984663255143\n",
      "Iteration 2442 => Loss: 0.08803772553919705\n",
      "Iteration 2443 => Loss: 0.08803560564604868\n",
      "Iteration 2444 => Loss: 0.08803348695195724\n",
      "Iteration 2445 => Loss: 0.0880313694557752\n",
      "Iteration 2446 => Loss: 0.0880292531563566\n",
      "Iteration 2447 => Loss: 0.08802713805255694\n",
      "Iteration 2448 => Loss: 0.08802502414323332\n",
      "Iteration 2449 => Loss: 0.08802291142724437\n",
      "Iteration 2450 => Loss: 0.0880207999034502\n",
      "Iteration 2451 => Loss: 0.08801868957071249\n",
      "Iteration 2452 => Loss: 0.08801658042789438\n",
      "Iteration 2453 => Loss: 0.08801447247386064\n",
      "Iteration 2454 => Loss: 0.0880123657074775\n",
      "Iteration 2455 => Loss: 0.08801026012761268\n",
      "Iteration 2456 => Loss: 0.08800815573313539\n",
      "Iteration 2457 => Loss: 0.08800605252291646\n",
      "Iteration 2458 => Loss: 0.08800395049582815\n",
      "Iteration 2459 => Loss: 0.0880018496507442\n",
      "Iteration 2460 => Loss: 0.08799974998653992\n",
      "Iteration 2461 => Loss: 0.08799765150209206\n",
      "Iteration 2462 => Loss: 0.08799555419627894\n",
      "Iteration 2463 => Loss: 0.0879934580679803\n",
      "Iteration 2464 => Loss: 0.08799136311607739\n",
      "Iteration 2465 => Loss: 0.08798926933945297\n",
      "Iteration 2466 => Loss: 0.0879871767369913\n",
      "Iteration 2467 => Loss: 0.08798508530757805\n",
      "Iteration 2468 => Loss: 0.0879829950501005\n",
      "Iteration 2469 => Loss: 0.08798090596344728\n",
      "Iteration 2470 => Loss: 0.08797881804650852\n",
      "Iteration 2471 => Loss: 0.08797673129817592\n",
      "Iteration 2472 => Loss: 0.08797464571734254\n",
      "Iteration 2473 => Loss: 0.08797256130290297\n",
      "Iteration 2474 => Loss: 0.08797047805375321\n",
      "Iteration 2475 => Loss: 0.08796839596879084\n",
      "Iteration 2476 => Loss: 0.08796631504691475\n",
      "Iteration 2477 => Loss: 0.08796423528702542\n",
      "Iteration 2478 => Loss: 0.08796215668802469\n",
      "Iteration 2479 => Loss: 0.08796007924881592\n",
      "Iteration 2480 => Loss: 0.08795800296830387\n",
      "Iteration 2481 => Loss: 0.08795592784539481\n",
      "Iteration 2482 => Loss: 0.08795385387899636\n",
      "Iteration 2483 => Loss: 0.0879517810680177\n",
      "Iteration 2484 => Loss: 0.08794970941136937\n",
      "Iteration 2485 => Loss: 0.08794763890796338\n",
      "Iteration 2486 => Loss: 0.08794556955671316\n",
      "Iteration 2487 => Loss: 0.08794350135653362\n",
      "Iteration 2488 => Loss: 0.08794143430634102\n",
      "Iteration 2489 => Loss: 0.08793936840505312\n",
      "Iteration 2490 => Loss: 0.08793730365158907\n",
      "Iteration 2491 => Loss: 0.08793524004486952\n",
      "Iteration 2492 => Loss: 0.08793317758381634\n",
      "Iteration 2493 => Loss: 0.08793111626735314\n",
      "Iteration 2494 => Loss: 0.08792905609440461\n",
      "Iteration 2495 => Loss: 0.08792699706389709\n",
      "Iteration 2496 => Loss: 0.08792493917475823\n",
      "Iteration 2497 => Loss: 0.08792288242591713\n",
      "Iteration 2498 => Loss: 0.08792082681630425\n",
      "Iteration 2499 => Loss: 0.0879187723448515\n",
      "Iteration 2500 => Loss: 0.08791671901049218\n",
      "Iteration 2501 => Loss: 0.08791466681216101\n",
      "Iteration 2502 => Loss: 0.08791261574879404\n",
      "Iteration 2503 => Loss: 0.08791056581932877\n",
      "Iteration 2504 => Loss: 0.08790851702270411\n",
      "Iteration 2505 => Loss: 0.08790646935786031\n",
      "Iteration 2506 => Loss: 0.08790442282373906\n",
      "Iteration 2507 => Loss: 0.08790237741928339\n",
      "Iteration 2508 => Loss: 0.08790033314343773\n",
      "Iteration 2509 => Loss: 0.0878982899951479\n",
      "Iteration 2510 => Loss: 0.0878962479733611\n",
      "Iteration 2511 => Loss: 0.08789420707702592\n",
      "Iteration 2512 => Loss: 0.08789216730509228\n",
      "Iteration 2513 => Loss: 0.08789012865651148\n",
      "Iteration 2514 => Loss: 0.08788809113023623\n",
      "Iteration 2515 => Loss: 0.0878860547252206\n",
      "Iteration 2516 => Loss: 0.08788401944041994\n",
      "Iteration 2517 => Loss: 0.08788198527479114\n",
      "Iteration 2518 => Loss: 0.08787995222729227\n",
      "Iteration 2519 => Loss: 0.08787792029688286\n",
      "Iteration 2520 => Loss: 0.08787588948252371\n",
      "Iteration 2521 => Loss: 0.08787385978317715\n",
      "Iteration 2522 => Loss: 0.0878718311978066\n",
      "Iteration 2523 => Loss: 0.08786980372537706\n",
      "Iteration 2524 => Loss: 0.08786777736485477\n",
      "Iteration 2525 => Loss: 0.08786575211520735\n",
      "Iteration 2526 => Loss: 0.08786372797540372\n",
      "Iteration 2527 => Loss: 0.08786170494441416\n",
      "Iteration 2528 => Loss: 0.0878596830212103\n",
      "Iteration 2529 => Loss: 0.08785766220476512\n",
      "Iteration 2530 => Loss: 0.08785564249405291\n",
      "Iteration 2531 => Loss: 0.08785362388804924\n",
      "Iteration 2532 => Loss: 0.08785160638573113\n",
      "Iteration 2533 => Loss: 0.08784958998607684\n",
      "Iteration 2534 => Loss: 0.08784757468806594\n",
      "Iteration 2535 => Loss: 0.08784556049067939\n",
      "Iteration 2536 => Loss: 0.08784354739289943\n",
      "Iteration 2537 => Loss: 0.0878415353937096\n",
      "Iteration 2538 => Loss: 0.08783952449209481\n",
      "Iteration 2539 => Loss: 0.08783751468704125\n",
      "Iteration 2540 => Loss: 0.08783550597753642\n",
      "Iteration 2541 => Loss: 0.0878334983625691\n",
      "Iteration 2542 => Loss: 0.08783149184112946\n",
      "Iteration 2543 => Loss: 0.08782948641220888\n",
      "Iteration 2544 => Loss: 0.0878274820748001\n",
      "Iteration 2545 => Loss: 0.08782547882789718\n",
      "Iteration 2546 => Loss: 0.08782347667049539\n",
      "Iteration 2547 => Loss: 0.08782147560159138\n",
      "Iteration 2548 => Loss: 0.0878194756201831\n",
      "Iteration 2549 => Loss: 0.08781747672526968\n",
      "Iteration 2550 => Loss: 0.08781547891585166\n",
      "Iteration 2551 => Loss: 0.08781348219093082\n",
      "Iteration 2552 => Loss: 0.08781148654951022\n",
      "Iteration 2553 => Loss: 0.08780949199059425\n",
      "Iteration 2554 => Loss: 0.08780749851318846\n",
      "Iteration 2555 => Loss: 0.08780550611629985\n",
      "Iteration 2556 => Loss: 0.08780351479893653\n",
      "Iteration 2557 => Loss: 0.08780152456010802\n",
      "Iteration 2558 => Loss: 0.08779953539882505\n",
      "Iteration 2559 => Loss: 0.08779754731409957\n",
      "Iteration 2560 => Loss: 0.08779556030494493\n",
      "Iteration 2561 => Loss: 0.08779357437037562\n",
      "Iteration 2562 => Loss: 0.08779158950940745\n",
      "Iteration 2563 => Loss: 0.0877896057210575\n",
      "Iteration 2564 => Loss: 0.08778762300434406\n",
      "Iteration 2565 => Loss: 0.08778564135828676\n",
      "Iteration 2566 => Loss: 0.08778366078190641\n",
      "Iteration 2567 => Loss: 0.08778168127422514\n",
      "Iteration 2568 => Loss: 0.08777970283426621\n",
      "Iteration 2569 => Loss: 0.08777772546105429\n",
      "Iteration 2570 => Loss: 0.0877757491536152\n",
      "Iteration 2571 => Loss: 0.08777377391097603\n",
      "Iteration 2572 => Loss: 0.08777179973216509\n",
      "Iteration 2573 => Loss: 0.08776982661621195\n",
      "Iteration 2574 => Loss: 0.08776785456214746\n",
      "Iteration 2575 => Loss: 0.08776588356900361\n",
      "Iteration 2576 => Loss: 0.08776391363581372\n",
      "Iteration 2577 => Loss: 0.08776194476161232\n",
      "Iteration 2578 => Loss: 0.0877599769454351\n",
      "Iteration 2579 => Loss: 0.08775801018631907\n",
      "Iteration 2580 => Loss: 0.08775604448330242\n",
      "Iteration 2581 => Loss: 0.08775407983542459\n",
      "Iteration 2582 => Loss: 0.08775211624172623\n",
      "Iteration 2583 => Loss: 0.0877501537012492\n",
      "Iteration 2584 => Loss: 0.0877481922130366\n",
      "Iteration 2585 => Loss: 0.08774623177613272\n",
      "Iteration 2586 => Loss: 0.0877442723895831\n",
      "Iteration 2587 => Loss: 0.08774231405243448\n",
      "Iteration 2588 => Loss: 0.08774035676373476\n",
      "Iteration 2589 => Loss: 0.08773840052253315\n",
      "Iteration 2590 => Loss: 0.08773644532787998\n",
      "Iteration 2591 => Loss: 0.08773449117882681\n",
      "Iteration 2592 => Loss: 0.08773253807442644\n",
      "Iteration 2593 => Loss: 0.08773058601373283\n",
      "Iteration 2594 => Loss: 0.08772863499580112\n",
      "Iteration 2595 => Loss: 0.08772668501968771\n",
      "Iteration 2596 => Loss: 0.08772473608445017\n",
      "Iteration 2597 => Loss: 0.08772278818914722\n",
      "Iteration 2598 => Loss: 0.08772084133283883\n",
      "Iteration 2599 => Loss: 0.08771889551458611\n",
      "Iteration 2600 => Loss: 0.08771695073345144\n",
      "Iteration 2601 => Loss: 0.08771500698849825\n",
      "Iteration 2602 => Loss: 0.0877130642787913\n",
      "Iteration 2603 => Loss: 0.08771112260339643\n",
      "Iteration 2604 => Loss: 0.0877091819613807\n",
      "Iteration 2605 => Loss: 0.08770724235181233\n",
      "Iteration 2606 => Loss: 0.08770530377376075\n",
      "Iteration 2607 => Loss: 0.08770336622629651\n",
      "Iteration 2608 => Loss: 0.08770142970849139\n",
      "Iteration 2609 => Loss: 0.08769949421941833\n",
      "Iteration 2610 => Loss: 0.08769755975815137\n",
      "Iteration 2611 => Loss: 0.08769562632376578\n",
      "Iteration 2612 => Loss: 0.08769369391533799\n",
      "Iteration 2613 => Loss: 0.08769176253194563\n",
      "Iteration 2614 => Loss: 0.08768983217266735\n",
      "Iteration 2615 => Loss: 0.08768790283658313\n",
      "Iteration 2616 => Loss: 0.08768597452277402\n",
      "Iteration 2617 => Loss: 0.08768404723032222\n",
      "Iteration 2618 => Loss: 0.0876821209583111\n",
      "Iteration 2619 => Loss: 0.08768019570582518\n",
      "Iteration 2620 => Loss: 0.08767827147195012\n",
      "Iteration 2621 => Loss: 0.08767634825577272\n",
      "Iteration 2622 => Loss: 0.087674426056381\n",
      "Iteration 2623 => Loss: 0.08767250487286403\n",
      "Iteration 2624 => Loss: 0.08767058470431206\n",
      "Iteration 2625 => Loss: 0.08766866554981646\n",
      "Iteration 2626 => Loss: 0.08766674740846979\n",
      "Iteration 2627 => Loss: 0.08766483027936571\n",
      "Iteration 2628 => Loss: 0.08766291416159899\n",
      "Iteration 2629 => Loss: 0.08766099905426557\n",
      "Iteration 2630 => Loss: 0.08765908495646253\n",
      "Iteration 2631 => Loss: 0.08765717186728803\n",
      "Iteration 2632 => Loss: 0.08765525978584143\n",
      "Iteration 2633 => Loss: 0.08765334871122311\n",
      "Iteration 2634 => Loss: 0.0876514386425347\n",
      "Iteration 2635 => Loss: 0.08764952957887884\n",
      "Iteration 2636 => Loss: 0.08764762151935937\n",
      "Iteration 2637 => Loss: 0.0876457144630812\n",
      "Iteration 2638 => Loss: 0.08764380840915037\n",
      "Iteration 2639 => Loss: 0.08764190335667407\n",
      "Iteration 2640 => Loss: 0.0876399993047605\n",
      "Iteration 2641 => Loss: 0.0876380962525191\n",
      "Iteration 2642 => Loss: 0.08763619419906035\n",
      "Iteration 2643 => Loss: 0.08763429314349586\n",
      "Iteration 2644 => Loss: 0.08763239308493832\n",
      "Iteration 2645 => Loss: 0.0876304940225015\n",
      "Iteration 2646 => Loss: 0.08762859595530037\n",
      "Iteration 2647 => Loss: 0.0876266988824509\n",
      "Iteration 2648 => Loss: 0.08762480280307022\n",
      "Iteration 2649 => Loss: 0.08762290771627652\n",
      "Iteration 2650 => Loss: 0.0876210136211891\n",
      "Iteration 2651 => Loss: 0.0876191205169284\n",
      "Iteration 2652 => Loss: 0.08761722840261583\n",
      "Iteration 2653 => Loss: 0.08761533727737403\n",
      "Iteration 2654 => Loss: 0.08761344714032664\n",
      "Iteration 2655 => Loss: 0.0876115579905984\n",
      "Iteration 2656 => Loss: 0.08760966982731512\n",
      "Iteration 2657 => Loss: 0.08760778264960378\n",
      "Iteration 2658 => Loss: 0.08760589645659235\n",
      "Iteration 2659 => Loss: 0.08760401124740991\n",
      "Iteration 2660 => Loss: 0.08760212702118661\n",
      "Iteration 2661 => Loss: 0.08760024377705364\n",
      "Iteration 2662 => Loss: 0.08759836151414338\n",
      "Iteration 2663 => Loss: 0.08759648023158915\n",
      "Iteration 2664 => Loss: 0.08759459992852547\n",
      "Iteration 2665 => Loss: 0.08759272060408779\n",
      "Iteration 2666 => Loss: 0.08759084225741273\n",
      "Iteration 2667 => Loss: 0.08758896488763794\n",
      "Iteration 2668 => Loss: 0.08758708849390212\n",
      "Iteration 2669 => Loss: 0.08758521307534504\n",
      "Iteration 2670 => Loss: 0.08758333863110758\n",
      "Iteration 2671 => Loss: 0.0875814651603316\n",
      "Iteration 2672 => Loss: 0.08757959266216007\n",
      "Iteration 2673 => Loss: 0.087577721135737\n",
      "Iteration 2674 => Loss: 0.08757585058020743\n",
      "Iteration 2675 => Loss: 0.08757398099471754\n",
      "Iteration 2676 => Loss: 0.08757211237841442\n",
      "Iteration 2677 => Loss: 0.08757024473044633\n",
      "Iteration 2678 => Loss: 0.08756837804996256\n",
      "Iteration 2679 => Loss: 0.08756651233611336\n",
      "Iteration 2680 => Loss: 0.08756464758805012\n",
      "Iteration 2681 => Loss: 0.08756278380492521\n",
      "Iteration 2682 => Loss: 0.08756092098589209\n",
      "Iteration 2683 => Loss: 0.08755905913010524\n",
      "Iteration 2684 => Loss: 0.08755719823672016\n",
      "Iteration 2685 => Loss: 0.0875553383048934\n",
      "Iteration 2686 => Loss: 0.08755347933378255\n",
      "Iteration 2687 => Loss: 0.0875516213225462\n",
      "Iteration 2688 => Loss: 0.087549764270344\n",
      "Iteration 2689 => Loss: 0.08754790817633663\n",
      "Iteration 2690 => Loss: 0.08754605303968584\n",
      "Iteration 2691 => Loss: 0.08754419885955428\n",
      "Iteration 2692 => Loss: 0.08754234563510575\n",
      "Iteration 2693 => Loss: 0.08754049336550503\n",
      "Iteration 2694 => Loss: 0.0875386420499179\n",
      "Iteration 2695 => Loss: 0.08753679168751116\n",
      "Iteration 2696 => Loss: 0.08753494227745268\n",
      "Iteration 2697 => Loss: 0.08753309381891128\n",
      "Iteration 2698 => Loss: 0.08753124631105687\n",
      "Iteration 2699 => Loss: 0.08752939975306027\n",
      "Iteration 2700 => Loss: 0.0875275541440934\n",
      "Iteration 2701 => Loss: 0.08752570948332915\n",
      "Iteration 2702 => Loss: 0.08752386576994145\n",
      "Iteration 2703 => Loss: 0.08752202300310519\n",
      "Iteration 2704 => Loss: 0.08752018118199631\n",
      "Iteration 2705 => Loss: 0.0875183403057917\n",
      "Iteration 2706 => Loss: 0.0875165003736693\n",
      "Iteration 2707 => Loss: 0.08751466138480805\n",
      "Iteration 2708 => Loss: 0.0875128233383879\n",
      "Iteration 2709 => Loss: 0.0875109862335897\n",
      "Iteration 2710 => Loss: 0.0875091500695954\n",
      "Iteration 2711 => Loss: 0.08750731484558798\n",
      "Iteration 2712 => Loss: 0.08750548056075122\n",
      "Iteration 2713 => Loss: 0.0875036472142701\n",
      "Iteration 2714 => Loss: 0.08750181480533048\n",
      "Iteration 2715 => Loss: 0.08749998333311924\n",
      "Iteration 2716 => Loss: 0.08749815279682424\n",
      "Iteration 2717 => Loss: 0.08749632319563433\n",
      "Iteration 2718 => Loss: 0.08749449452873931\n",
      "Iteration 2719 => Loss: 0.08749266679533\n",
      "Iteration 2720 => Loss: 0.08749083999459822\n",
      "Iteration 2721 => Loss: 0.0874890141257367\n",
      "Iteration 2722 => Loss: 0.08748718918793923\n",
      "Iteration 2723 => Loss: 0.0874853651804005\n",
      "Iteration 2724 => Loss: 0.08748354210231617\n",
      "Iteration 2725 => Loss: 0.08748171995288297\n",
      "Iteration 2726 => Loss: 0.08747989873129854\n",
      "Iteration 2727 => Loss: 0.08747807843676143\n",
      "Iteration 2728 => Loss: 0.08747625906847127\n",
      "Iteration 2729 => Loss: 0.08747444062562859\n",
      "Iteration 2730 => Loss: 0.08747262310743492\n",
      "Iteration 2731 => Loss: 0.08747080651309266\n",
      "Iteration 2732 => Loss: 0.08746899084180533\n",
      "Iteration 2733 => Loss: 0.08746717609277728\n",
      "Iteration 2734 => Loss: 0.08746536226521386\n",
      "Iteration 2735 => Loss: 0.08746354935832142\n",
      "Iteration 2736 => Loss: 0.08746173737130719\n",
      "Iteration 2737 => Loss: 0.08745992630337941\n",
      "Iteration 2738 => Loss: 0.08745811615374725\n",
      "Iteration 2739 => Loss: 0.08745630692162083\n",
      "Iteration 2740 => Loss: 0.08745449860621127\n",
      "Iteration 2741 => Loss: 0.08745269120673055\n",
      "Iteration 2742 => Loss: 0.08745088472239168\n",
      "Iteration 2743 => Loss: 0.08744907915240852\n",
      "Iteration 2744 => Loss: 0.087447274495996\n",
      "Iteration 2745 => Loss: 0.08744547075236989\n",
      "Iteration 2746 => Loss: 0.08744366792074698\n",
      "Iteration 2747 => Loss: 0.08744186600034488\n",
      "Iteration 2748 => Loss: 0.08744006499038232\n",
      "Iteration 2749 => Loss: 0.08743826489007878\n",
      "Iteration 2750 => Loss: 0.08743646569865478\n",
      "Iteration 2751 => Loss: 0.08743466741533179\n",
      "Iteration 2752 => Loss: 0.08743287003933214\n",
      "Iteration 2753 => Loss: 0.08743107356987914\n",
      "Iteration 2754 => Loss: 0.08742927800619703\n",
      "Iteration 2755 => Loss: 0.08742748334751092\n",
      "Iteration 2756 => Loss: 0.08742568959304699\n",
      "Iteration 2757 => Loss: 0.08742389674203216\n",
      "Iteration 2758 => Loss: 0.08742210479369437\n",
      "Iteration 2759 => Loss: 0.08742031374726253\n",
      "Iteration 2760 => Loss: 0.08741852360196636\n",
      "Iteration 2761 => Loss: 0.0874167343570366\n",
      "Iteration 2762 => Loss: 0.08741494601170482\n",
      "Iteration 2763 => Loss: 0.08741315856520361\n",
      "Iteration 2764 => Loss: 0.08741137201676637\n",
      "Iteration 2765 => Loss: 0.0874095863656275\n",
      "Iteration 2766 => Loss: 0.08740780161102225\n",
      "Iteration 2767 => Loss: 0.08740601775218683\n",
      "Iteration 2768 => Loss: 0.08740423478835829\n",
      "Iteration 2769 => Loss: 0.0874024527187747\n",
      "Iteration 2770 => Loss: 0.08740067154267493\n",
      "Iteration 2771 => Loss: 0.08739889125929881\n",
      "Iteration 2772 => Loss: 0.08739711186788704\n",
      "Iteration 2773 => Loss: 0.08739533336768131\n",
      "Iteration 2774 => Loss: 0.08739355575792411\n",
      "Iteration 2775 => Loss: 0.08739177903785884\n",
      "Iteration 2776 => Loss: 0.08739000320672988\n",
      "Iteration 2777 => Loss: 0.08738822826378241\n",
      "Iteration 2778 => Loss: 0.0873864542082626\n",
      "Iteration 2779 => Loss: 0.0873846810394174\n",
      "Iteration 2780 => Loss: 0.08738290875649479\n",
      "Iteration 2781 => Loss: 0.08738113735874353\n",
      "Iteration 2782 => Loss: 0.08737936684541332\n",
      "Iteration 2783 => Loss: 0.08737759721575476\n",
      "Iteration 2784 => Loss: 0.0873758284690193\n",
      "Iteration 2785 => Loss: 0.08737406060445929\n",
      "Iteration 2786 => Loss: 0.087372293621328\n",
      "Iteration 2787 => Loss: 0.08737052751887955\n",
      "Iteration 2788 => Loss: 0.08736876229636892\n",
      "Iteration 2789 => Loss: 0.08736699795305203\n",
      "Iteration 2790 => Loss: 0.08736523448818563\n",
      "Iteration 2791 => Loss: 0.0873634719010274\n",
      "Iteration 2792 => Loss: 0.08736171019083584\n",
      "Iteration 2793 => Loss: 0.08735994935687035\n",
      "Iteration 2794 => Loss: 0.08735818939839121\n",
      "Iteration 2795 => Loss: 0.08735643031465957\n",
      "Iteration 2796 => Loss: 0.08735467210493747\n",
      "Iteration 2797 => Loss: 0.08735291476848779\n",
      "Iteration 2798 => Loss: 0.08735115830457428\n",
      "Iteration 2799 => Loss: 0.08734940271246158\n",
      "Iteration 2800 => Loss: 0.08734764799141519\n",
      "Iteration 2801 => Loss: 0.08734589414070147\n",
      "Iteration 2802 => Loss: 0.08734414115958763\n",
      "Iteration 2803 => Loss: 0.08734238904734178\n",
      "Iteration 2804 => Loss: 0.08734063780323285\n",
      "Iteration 2805 => Loss: 0.08733888742653069\n",
      "Iteration 2806 => Loss: 0.08733713791650592\n",
      "Iteration 2807 => Loss: 0.08733538927243009\n",
      "Iteration 2808 => Loss: 0.08733364149357559\n",
      "Iteration 2809 => Loss: 0.08733189457921564\n",
      "Iteration 2810 => Loss: 0.08733014852862435\n",
      "Iteration 2811 => Loss: 0.08732840334107664\n",
      "Iteration 2812 => Loss: 0.08732665901584832\n",
      "Iteration 2813 => Loss: 0.08732491555221605\n",
      "Iteration 2814 => Loss: 0.08732317294945728\n",
      "Iteration 2815 => Loss: 0.0873214312068504\n",
      "Iteration 2816 => Loss: 0.08731969032367454\n",
      "Iteration 2817 => Loss: 0.08731795029920979\n",
      "Iteration 2818 => Loss: 0.08731621113273698\n",
      "Iteration 2819 => Loss: 0.08731447282353784\n",
      "Iteration 2820 => Loss: 0.08731273537089494\n",
      "Iteration 2821 => Loss: 0.0873109987740917\n",
      "Iteration 2822 => Loss: 0.08730926303241227\n",
      "Iteration 2823 => Loss: 0.0873075281451418\n",
      "Iteration 2824 => Loss: 0.08730579411156618\n",
      "Iteration 2825 => Loss: 0.08730406093097211\n",
      "Iteration 2826 => Loss: 0.08730232860264722\n",
      "Iteration 2827 => Loss: 0.08730059712587993\n",
      "Iteration 2828 => Loss: 0.08729886649995938\n",
      "Iteration 2829 => Loss: 0.08729713672417576\n",
      "Iteration 2830 => Loss: 0.0872954077978199\n",
      "Iteration 2831 => Loss: 0.08729367972018352\n",
      "Iteration 2832 => Loss: 0.08729195249055918\n",
      "Iteration 2833 => Loss: 0.08729022610824028\n",
      "Iteration 2834 => Loss: 0.087288500572521\n",
      "Iteration 2835 => Loss: 0.08728677588269634\n",
      "Iteration 2836 => Loss: 0.08728505203806214\n",
      "Iteration 2837 => Loss: 0.08728332903791511\n",
      "Iteration 2838 => Loss: 0.08728160688155266\n",
      "Iteration 2839 => Loss: 0.08727988556827314\n",
      "Iteration 2840 => Loss: 0.08727816509737564\n",
      "Iteration 2841 => Loss: 0.08727644546816007\n",
      "Iteration 2842 => Loss: 0.08727472667992721\n",
      "Iteration 2843 => Loss: 0.08727300873197857\n",
      "Iteration 2844 => Loss: 0.08727129162361653\n",
      "Iteration 2845 => Loss: 0.08726957535414427\n",
      "Iteration 2846 => Loss: 0.08726785992286579\n",
      "Iteration 2847 => Loss: 0.08726614532908582\n",
      "Iteration 2848 => Loss: 0.08726443157211\n",
      "Iteration 2849 => Loss: 0.08726271865124473\n",
      "Iteration 2850 => Loss: 0.08726100656579716\n",
      "Iteration 2851 => Loss: 0.08725929531507541\n",
      "Iteration 2852 => Loss: 0.08725758489838818\n",
      "Iteration 2853 => Loss: 0.08725587531504514\n",
      "Iteration 2854 => Loss: 0.08725416656435667\n",
      "Iteration 2855 => Loss: 0.08725245864563397\n",
      "Iteration 2856 => Loss: 0.08725075155818907\n",
      "Iteration 2857 => Loss: 0.08724904530133475\n",
      "Iteration 2858 => Loss: 0.0872473398743846\n",
      "Iteration 2859 => Loss: 0.08724563527665302\n",
      "Iteration 2860 => Loss: 0.08724393150745516\n",
      "Iteration 2861 => Loss: 0.08724222856610703\n",
      "Iteration 2862 => Loss: 0.08724052645192534\n",
      "Iteration 2863 => Loss: 0.08723882516422767\n",
      "Iteration 2864 => Loss: 0.08723712470233236\n",
      "Iteration 2865 => Loss: 0.08723542506555851\n",
      "Iteration 2866 => Loss: 0.08723372625322601\n",
      "Iteration 2867 => Loss: 0.08723202826465558\n",
      "Iteration 2868 => Loss: 0.0872303310991687\n",
      "Iteration 2869 => Loss: 0.08722863475608755\n",
      "Iteration 2870 => Loss: 0.08722693923473528\n",
      "Iteration 2871 => Loss: 0.08722524453443557\n",
      "Iteration 2872 => Loss: 0.08722355065451311\n",
      "Iteration 2873 => Loss: 0.08722185759429324\n",
      "Iteration 2874 => Loss: 0.08722016535310208\n",
      "Iteration 2875 => Loss: 0.08721847393026658\n",
      "Iteration 2876 => Loss: 0.0872167833251144\n",
      "Iteration 2877 => Loss: 0.087215093536974\n",
      "Iteration 2878 => Loss: 0.08721340456517467\n",
      "Iteration 2879 => Loss: 0.08721171640904637\n",
      "Iteration 2880 => Loss: 0.08721002906791986\n",
      "Iteration 2881 => Loss: 0.0872083425411267\n",
      "Iteration 2882 => Loss: 0.08720665682799919\n",
      "Iteration 2883 => Loss: 0.08720497192787043\n",
      "Iteration 2884 => Loss: 0.08720328784007422\n",
      "Iteration 2885 => Loss: 0.08720160456394521\n",
      "Iteration 2886 => Loss: 0.08719992209881867\n",
      "Iteration 2887 => Loss: 0.08719824044403082\n",
      "Iteration 2888 => Loss: 0.08719655959891849\n",
      "Iteration 2889 => Loss: 0.08719487956281934\n",
      "Iteration 2890 => Loss: 0.08719320033507176\n",
      "Iteration 2891 => Loss: 0.08719152191501492\n",
      "Iteration 2892 => Loss: 0.08718984430198874\n",
      "Iteration 2893 => Loss: 0.08718816749533384\n",
      "Iteration 2894 => Loss: 0.08718649149439167\n",
      "Iteration 2895 => Loss: 0.08718481629850441\n",
      "Iteration 2896 => Loss: 0.08718314190701497\n",
      "Iteration 2897 => Loss: 0.08718146831926703\n",
      "Iteration 2898 => Loss: 0.087179795534605\n",
      "Iteration 2899 => Loss: 0.08717812355237402\n",
      "Iteration 2900 => Loss: 0.08717645237192007\n",
      "Iteration 2901 => Loss: 0.08717478199258977\n",
      "Iteration 2902 => Loss: 0.08717311241373049\n",
      "Iteration 2903 => Loss: 0.08717144363469047\n",
      "Iteration 2904 => Loss: 0.08716977565481852\n",
      "Iteration 2905 => Loss: 0.08716810847346429\n",
      "Iteration 2906 => Loss: 0.0871664420899782\n",
      "Iteration 2907 => Loss: 0.08716477650371128\n",
      "Iteration 2908 => Loss: 0.08716311171401546\n",
      "Iteration 2909 => Loss: 0.08716144772024324\n",
      "Iteration 2910 => Loss: 0.08715978452174801\n",
      "Iteration 2911 => Loss: 0.08715812211788376\n",
      "Iteration 2912 => Loss: 0.08715646050800535\n",
      "Iteration 2913 => Loss: 0.08715479969146828\n",
      "Iteration 2914 => Loss: 0.08715313966762876\n",
      "Iteration 2915 => Loss: 0.0871514804358438\n",
      "Iteration 2916 => Loss: 0.08714982199547115\n",
      "Iteration 2917 => Loss: 0.08714816434586918\n",
      "Iteration 2918 => Loss: 0.0871465074863971\n",
      "Iteration 2919 => Loss: 0.0871448514164148\n",
      "Iteration 2920 => Loss: 0.08714319613528292\n",
      "Iteration 2921 => Loss: 0.08714154164236275\n",
      "Iteration 2922 => Loss: 0.0871398879370164\n",
      "Iteration 2923 => Loss: 0.08713823501860661\n",
      "Iteration 2924 => Loss: 0.08713658288649695\n",
      "Iteration 2925 => Loss: 0.0871349315400516\n",
      "Iteration 2926 => Loss: 0.08713328097863555\n",
      "Iteration 2927 => Loss: 0.08713163120161442\n",
      "Iteration 2928 => Loss: 0.08712998220835463\n",
      "Iteration 2929 => Loss: 0.08712833399822324\n",
      "Iteration 2930 => Loss: 0.08712668657058809\n",
      "Iteration 2931 => Loss: 0.08712503992481765\n",
      "Iteration 2932 => Loss: 0.08712339406028127\n",
      "Iteration 2933 => Loss: 0.08712174897634879\n",
      "Iteration 2934 => Loss: 0.08712010467239094\n",
      "Iteration 2935 => Loss: 0.08711846114777903\n",
      "Iteration 2936 => Loss: 0.08711681840188519\n",
      "Iteration 2937 => Loss: 0.08711517643408218\n",
      "Iteration 2938 => Loss: 0.08711353524374348\n",
      "Iteration 2939 => Loss: 0.08711189483024331\n",
      "Iteration 2940 => Loss: 0.08711025519295654\n",
      "Iteration 2941 => Loss: 0.08710861633125883\n",
      "Iteration 2942 => Loss: 0.08710697824452643\n",
      "Iteration 2943 => Loss: 0.08710534093213637\n",
      "Iteration 2944 => Loss: 0.08710370439346636\n",
      "Iteration 2945 => Loss: 0.0871020686278948\n",
      "Iteration 2946 => Loss: 0.08710043363480079\n",
      "Iteration 2947 => Loss: 0.08709879941356415\n",
      "Iteration 2948 => Loss: 0.08709716596356536\n",
      "Iteration 2949 => Loss: 0.08709553328418564\n",
      "Iteration 2950 => Loss: 0.08709390137480684\n",
      "Iteration 2951 => Loss: 0.08709227023481154\n",
      "Iteration 2952 => Loss: 0.08709063986358308\n",
      "Iteration 2953 => Loss: 0.08708901026050538\n",
      "Iteration 2954 => Loss: 0.08708738142496306\n",
      "Iteration 2955 => Loss: 0.08708575335634151\n",
      "Iteration 2956 => Loss: 0.08708412605402678\n",
      "Iteration 2957 => Loss: 0.08708249951740553\n",
      "Iteration 2958 => Loss: 0.08708087374586518\n",
      "Iteration 2959 => Loss: 0.08707924873879387\n",
      "Iteration 2960 => Loss: 0.08707762449558033\n",
      "Iteration 2961 => Loss: 0.08707600101561404\n",
      "Iteration 2962 => Loss: 0.08707437829828513\n",
      "Iteration 2963 => Loss: 0.08707275634298439\n",
      "Iteration 2964 => Loss: 0.08707113514910339\n",
      "Iteration 2965 => Loss: 0.08706951471603427\n",
      "Iteration 2966 => Loss: 0.08706789504316992\n",
      "Iteration 2967 => Loss: 0.08706627612990384\n",
      "Iteration 2968 => Loss: 0.08706465797563025\n",
      "Iteration 2969 => Loss: 0.08706304057974402\n",
      "Iteration 2970 => Loss: 0.08706142394164082\n",
      "Iteration 2971 => Loss: 0.08705980806071677\n",
      "Iteration 2972 => Loss: 0.08705819293636882\n",
      "Iteration 2973 => Loss: 0.08705657856799456\n",
      "Iteration 2974 => Loss: 0.08705496495499224\n",
      "Iteration 2975 => Loss: 0.08705335209676077\n",
      "Iteration 2976 => Loss: 0.08705173999269976\n",
      "Iteration 2977 => Loss: 0.08705012864220944\n",
      "Iteration 2978 => Loss: 0.08704851804469077\n",
      "Iteration 2979 => Loss: 0.08704690819954537\n",
      "Iteration 2980 => Loss: 0.08704529910617538\n",
      "Iteration 2981 => Loss: 0.08704369076398383\n",
      "Iteration 2982 => Loss: 0.08704208317237426\n",
      "Iteration 2983 => Loss: 0.0870404763307509\n",
      "Iteration 2984 => Loss: 0.08703887023851871\n",
      "Iteration 2985 => Loss: 0.08703726489508323\n",
      "Iteration 2986 => Loss: 0.08703566029985067\n",
      "Iteration 2987 => Loss: 0.08703405645222795\n",
      "Iteration 2988 => Loss: 0.08703245335162257\n",
      "Iteration 2989 => Loss: 0.08703085099744277\n",
      "Iteration 2990 => Loss: 0.08702924938909735\n",
      "Iteration 2991 => Loss: 0.08702764852599587\n",
      "Iteration 2992 => Loss: 0.08702604840754848\n",
      "Iteration 2993 => Loss: 0.08702444903316599\n",
      "Iteration 2994 => Loss: 0.08702285040225985\n",
      "Iteration 2995 => Loss: 0.08702125251424223\n",
      "Iteration 2996 => Loss: 0.08701965536852582\n",
      "Iteration 2997 => Loss: 0.08701805896452411\n",
      "Iteration 2998 => Loss: 0.08701646330165111\n",
      "Iteration 2999 => Loss: 0.08701486837932157\n",
      "Iteration 3000 => Loss: 0.0870132741969508\n",
      "Iteration 3001 => Loss: 0.08701168075395489\n",
      "Iteration 3002 => Loss: 0.08701008804975044\n",
      "Iteration 3003 => Loss: 0.0870084960837547\n",
      "Iteration 3004 => Loss: 0.08700690485538565\n",
      "Iteration 3005 => Loss: 0.08700531436406184\n",
      "Iteration 3006 => Loss: 0.08700372460920253\n",
      "Iteration 3007 => Loss: 0.08700213559022756\n",
      "Iteration 3008 => Loss: 0.08700054730655739\n",
      "Iteration 3009 => Loss: 0.08699895975761321\n",
      "Iteration 3010 => Loss: 0.08699737294281673\n",
      "Iteration 3011 => Loss: 0.08699578686159043\n",
      "Iteration 3012 => Loss: 0.08699420151335728\n",
      "Iteration 3013 => Loss: 0.08699261689754101\n",
      "Iteration 3014 => Loss: 0.0869910330135659\n",
      "Iteration 3015 => Loss: 0.0869894498608569\n",
      "Iteration 3016 => Loss: 0.08698786743883959\n",
      "Iteration 3017 => Loss: 0.08698628574694017\n",
      "Iteration 3018 => Loss: 0.08698470478458546\n",
      "Iteration 3019 => Loss: 0.08698312455120301\n",
      "Iteration 3020 => Loss: 0.0869815450462208\n",
      "Iteration 3021 => Loss: 0.08697996626906762\n",
      "Iteration 3022 => Loss: 0.08697838821917277\n",
      "Iteration 3023 => Loss: 0.08697681089596628\n",
      "Iteration 3024 => Loss: 0.0869752342988787\n",
      "Iteration 3025 => Loss: 0.0869736584273413\n",
      "Iteration 3026 => Loss: 0.08697208328078589\n",
      "Iteration 3027 => Loss: 0.08697050885864492\n",
      "Iteration 3028 => Loss: 0.08696893516035151\n",
      "Iteration 3029 => Loss: 0.08696736218533935\n",
      "Iteration 3030 => Loss: 0.08696578993304276\n",
      "Iteration 3031 => Loss: 0.08696421840289673\n",
      "Iteration 3032 => Loss: 0.08696264759433679\n",
      "Iteration 3033 => Loss: 0.08696107750679911\n",
      "Iteration 3034 => Loss: 0.08695950813972052\n",
      "Iteration 3035 => Loss: 0.08695793949253842\n",
      "Iteration 3036 => Loss: 0.0869563715646908\n",
      "Iteration 3037 => Loss: 0.08695480435561637\n",
      "Iteration 3038 => Loss: 0.08695323786475433\n",
      "Iteration 3039 => Loss: 0.08695167209154454\n",
      "Iteration 3040 => Loss: 0.0869501070354275\n",
      "Iteration 3041 => Loss: 0.0869485426958443\n",
      "Iteration 3042 => Loss: 0.0869469790722366\n",
      "Iteration 3043 => Loss: 0.08694541616404673\n",
      "Iteration 3044 => Loss: 0.08694385397071759\n",
      "Iteration 3045 => Loss: 0.08694229249169269\n",
      "Iteration 3046 => Loss: 0.08694073172641617\n",
      "Iteration 3047 => Loss: 0.08693917167433275\n",
      "Iteration 3048 => Loss: 0.08693761233488775\n",
      "Iteration 3049 => Loss: 0.08693605370752712\n",
      "Iteration 3050 => Loss: 0.08693449579169737\n",
      "Iteration 3051 => Loss: 0.08693293858684566\n",
      "Iteration 3052 => Loss: 0.08693138209241973\n",
      "Iteration 3053 => Loss: 0.0869298263078679\n",
      "Iteration 3054 => Loss: 0.08692827123263913\n",
      "Iteration 3055 => Loss: 0.08692671686618293\n",
      "Iteration 3056 => Loss: 0.08692516320794945\n",
      "Iteration 3057 => Loss: 0.08692361025738941\n",
      "Iteration 3058 => Loss: 0.08692205801395418\n",
      "Iteration 3059 => Loss: 0.08692050647709558\n",
      "Iteration 3060 => Loss: 0.08691895564626624\n",
      "Iteration 3061 => Loss: 0.08691740552091919\n",
      "Iteration 3062 => Loss: 0.08691585610050814\n",
      "Iteration 3063 => Loss: 0.0869143073844874\n",
      "Iteration 3064 => Loss: 0.08691275937231185\n",
      "Iteration 3065 => Loss: 0.08691121206343692\n",
      "Iteration 3066 => Loss: 0.08690966545731875\n",
      "Iteration 3067 => Loss: 0.08690811955341392\n",
      "Iteration 3068 => Loss: 0.0869065743511797\n",
      "Iteration 3069 => Loss: 0.0869050298500739\n",
      "Iteration 3070 => Loss: 0.08690348604955493\n",
      "Iteration 3071 => Loss: 0.08690194294908177\n",
      "Iteration 3072 => Loss: 0.08690040054811402\n",
      "Iteration 3073 => Loss: 0.08689885884611186\n",
      "Iteration 3074 => Loss: 0.08689731784253597\n",
      "Iteration 3075 => Loss: 0.08689577753684774\n",
      "Iteration 3076 => Loss: 0.08689423792850903\n",
      "Iteration 3077 => Loss: 0.08689269901698236\n",
      "Iteration 3078 => Loss: 0.08689116080173079\n",
      "Iteration 3079 => Loss: 0.08688962328221793\n",
      "Iteration 3080 => Loss: 0.08688808645790803\n",
      "Iteration 3081 => Loss: 0.08688655032826591\n",
      "Iteration 3082 => Loss: 0.08688501489275693\n",
      "Iteration 3083 => Loss: 0.08688348015084703\n",
      "Iteration 3084 => Loss: 0.08688194610200274\n",
      "Iteration 3085 => Loss: 0.08688041274569112\n",
      "Iteration 3086 => Loss: 0.08687888008137994\n",
      "Iteration 3087 => Loss: 0.08687734810853735\n",
      "Iteration 3088 => Loss: 0.08687581682663219\n",
      "Iteration 3089 => Loss: 0.08687428623513387\n",
      "Iteration 3090 => Loss: 0.08687275633351232\n",
      "Iteration 3091 => Loss: 0.08687122712123811\n",
      "Iteration 3092 => Loss: 0.08686969859778228\n",
      "Iteration 3093 => Loss: 0.08686817076261653\n",
      "Iteration 3094 => Loss: 0.08686664361521305\n",
      "Iteration 3095 => Loss: 0.08686511715504466\n",
      "Iteration 3096 => Loss: 0.08686359138158473\n",
      "Iteration 3097 => Loss: 0.08686206629430715\n",
      "Iteration 3098 => Loss: 0.0868605418926864\n",
      "Iteration 3099 => Loss: 0.0868590181761976\n",
      "Iteration 3100 => Loss: 0.08685749514431626\n",
      "Iteration 3101 => Loss: 0.08685597279651862\n",
      "Iteration 3102 => Loss: 0.0868544511322814\n",
      "Iteration 3103 => Loss: 0.0868529301510819\n",
      "Iteration 3104 => Loss: 0.08685140985239798\n",
      "Iteration 3105 => Loss: 0.08684989023570798\n",
      "Iteration 3106 => Loss: 0.08684837130049096\n",
      "Iteration 3107 => Loss: 0.08684685304622639\n",
      "Iteration 3108 => Loss: 0.08684533547239434\n",
      "Iteration 3109 => Loss: 0.0868438185784755\n",
      "Iteration 3110 => Loss: 0.08684230236395103\n",
      "Iteration 3111 => Loss: 0.08684078682830265\n",
      "Iteration 3112 => Loss: 0.08683927197101268\n",
      "Iteration 3113 => Loss: 0.08683775779156395\n",
      "Iteration 3114 => Loss: 0.08683624428943991\n",
      "Iteration 3115 => Loss: 0.08683473146412446\n",
      "Iteration 3116 => Loss: 0.0868332193151021\n",
      "Iteration 3117 => Loss: 0.0868317078418579\n",
      "Iteration 3118 => Loss: 0.08683019704387747\n",
      "Iteration 3119 => Loss: 0.0868286869206469\n",
      "Iteration 3120 => Loss: 0.08682717747165292\n",
      "Iteration 3121 => Loss: 0.08682566869638277\n",
      "Iteration 3122 => Loss: 0.08682416059432421\n",
      "Iteration 3123 => Loss: 0.0868226531649656\n",
      "Iteration 3124 => Loss: 0.0868211464077958\n",
      "Iteration 3125 => Loss: 0.08681964032230419\n",
      "Iteration 3126 => Loss: 0.08681813490798077\n",
      "Iteration 3127 => Loss: 0.08681663016431602\n",
      "Iteration 3128 => Loss: 0.086815126090801\n",
      "Iteration 3129 => Loss: 0.08681362268692729\n",
      "Iteration 3130 => Loss: 0.08681211995218695\n",
      "Iteration 3131 => Loss: 0.08681061788607268\n",
      "Iteration 3132 => Loss: 0.08680911648807771\n",
      "Iteration 3133 => Loss: 0.08680761575769569\n",
      "Iteration 3134 => Loss: 0.08680611569442097\n",
      "Iteration 3135 => Loss: 0.08680461629774831\n",
      "Iteration 3136 => Loss: 0.08680311756717306\n",
      "Iteration 3137 => Loss: 0.0868016195021911\n",
      "Iteration 3138 => Loss: 0.08680012210229886\n",
      "Iteration 3139 => Loss: 0.08679862536699322\n",
      "Iteration 3140 => Loss: 0.08679712929577169\n",
      "Iteration 3141 => Loss: 0.0867956338881323\n",
      "Iteration 3142 => Loss: 0.08679413914357352\n",
      "Iteration 3143 => Loss: 0.08679264506159448\n",
      "Iteration 3144 => Loss: 0.08679115164169476\n",
      "Iteration 3145 => Loss: 0.08678965888337443\n",
      "Iteration 3146 => Loss: 0.08678816678613421\n",
      "Iteration 3147 => Loss: 0.08678667534947525\n",
      "Iteration 3148 => Loss: 0.08678518457289924\n",
      "Iteration 3149 => Loss: 0.08678369445590842\n",
      "Iteration 3150 => Loss: 0.08678220499800554\n",
      "Iteration 3151 => Loss: 0.08678071619869393\n",
      "Iteration 3152 => Loss: 0.08677922805747729\n",
      "Iteration 3153 => Loss: 0.08677774057386003\n",
      "Iteration 3154 => Loss: 0.08677625374734699\n",
      "Iteration 3155 => Loss: 0.08677476757744348\n",
      "Iteration 3156 => Loss: 0.08677328206365546\n",
      "Iteration 3157 => Loss: 0.08677179720548929\n",
      "Iteration 3158 => Loss: 0.08677031300245194\n",
      "Iteration 3159 => Loss: 0.08676882945405083\n",
      "Iteration 3160 => Loss: 0.08676734655979396\n",
      "Iteration 3161 => Loss: 0.08676586431918974\n",
      "Iteration 3162 => Loss: 0.08676438273174723\n",
      "Iteration 3163 => Loss: 0.08676290179697596\n",
      "Iteration 3164 => Loss: 0.08676142151438589\n",
      "Iteration 3165 => Loss: 0.08675994188348761\n",
      "Iteration 3166 => Loss: 0.08675846290379217\n",
      "Iteration 3167 => Loss: 0.08675698457481117\n",
      "Iteration 3168 => Loss: 0.08675550689605666\n",
      "Iteration 3169 => Loss: 0.08675402986704124\n",
      "Iteration 3170 => Loss: 0.08675255348727799\n",
      "Iteration 3171 => Loss: 0.08675107775628062\n",
      "Iteration 3172 => Loss: 0.08674960267356316\n",
      "Iteration 3173 => Loss: 0.08674812823864028\n",
      "Iteration 3174 => Loss: 0.08674665445102712\n",
      "Iteration 3175 => Loss: 0.08674518131023932\n",
      "Iteration 3176 => Loss: 0.08674370881579312\n",
      "Iteration 3177 => Loss: 0.08674223696720504\n",
      "Iteration 3178 => Loss: 0.08674076576399234\n",
      "Iteration 3179 => Loss: 0.08673929520567271\n",
      "Iteration 3180 => Loss: 0.0867378252917643\n",
      "Iteration 3181 => Loss: 0.08673635602178575\n",
      "Iteration 3182 => Loss: 0.08673488739525634\n",
      "Iteration 3183 => Loss: 0.08673341941169567\n",
      "Iteration 3184 => Loss: 0.08673195207062398\n",
      "Iteration 3185 => Loss: 0.08673048537156193\n",
      "Iteration 3186 => Loss: 0.08672901931403072\n",
      "Iteration 3187 => Loss: 0.08672755389755209\n",
      "Iteration 3188 => Loss: 0.08672608912164811\n",
      "Iteration 3189 => Loss: 0.08672462498584156\n",
      "Iteration 3190 => Loss: 0.0867231614896556\n",
      "Iteration 3191 => Loss: 0.08672169863261393\n",
      "Iteration 3192 => Loss: 0.08672023641424068\n",
      "Iteration 3193 => Loss: 0.08671877483406053\n",
      "Iteration 3194 => Loss: 0.0867173138915987\n",
      "Iteration 3195 => Loss: 0.08671585358638083\n",
      "Iteration 3196 => Loss: 0.08671439391793301\n",
      "Iteration 3197 => Loss: 0.08671293488578194\n",
      "Iteration 3198 => Loss: 0.08671147648945478\n",
      "Iteration 3199 => Loss: 0.08671001872847911\n",
      "Iteration 3200 => Loss: 0.0867085616023831\n",
      "Iteration 3201 => Loss: 0.08670710511069532\n",
      "Iteration 3202 => Loss: 0.0867056492529449\n",
      "Iteration 3203 => Loss: 0.08670419402866143\n",
      "Iteration 3204 => Loss: 0.08670273943737496\n",
      "Iteration 3205 => Loss: 0.0867012854786161\n",
      "Iteration 3206 => Loss: 0.08669983215191585\n",
      "Iteration 3207 => Loss: 0.08669837945680577\n",
      "Iteration 3208 => Loss: 0.08669692739281792\n",
      "Iteration 3209 => Loss: 0.08669547595948479\n",
      "Iteration 3210 => Loss: 0.08669402515633932\n",
      "Iteration 3211 => Loss: 0.08669257498291509\n",
      "Iteration 3212 => Loss: 0.08669112543874596\n",
      "Iteration 3213 => Loss: 0.08668967652336641\n",
      "Iteration 3214 => Loss: 0.08668822823631141\n",
      "Iteration 3215 => Loss: 0.08668678057711635\n",
      "Iteration 3216 => Loss: 0.08668533354531706\n",
      "Iteration 3217 => Loss: 0.08668388714044994\n",
      "Iteration 3218 => Loss: 0.08668244136205185\n",
      "Iteration 3219 => Loss: 0.0866809962096601\n",
      "Iteration 3220 => Loss: 0.08667955168281252\n",
      "Iteration 3221 => Loss: 0.08667810778104736\n",
      "Iteration 3222 => Loss: 0.08667666450390336\n",
      "Iteration 3223 => Loss: 0.08667522185091982\n",
      "Iteration 3224 => Loss: 0.08667377982163636\n",
      "Iteration 3225 => Loss: 0.08667233841559321\n",
      "Iteration 3226 => Loss: 0.08667089763233107\n",
      "Iteration 3227 => Loss: 0.08666945747139097\n",
      "Iteration 3228 => Loss: 0.08666801793231461\n",
      "Iteration 3229 => Loss: 0.08666657901464403\n",
      "Iteration 3230 => Loss: 0.08666514071792177\n",
      "Iteration 3231 => Loss: 0.08666370304169084\n",
      "Iteration 3232 => Loss: 0.08666226598549474\n",
      "Iteration 3233 => Loss: 0.08666082954887742\n",
      "Iteration 3234 => Loss: 0.08665939373138333\n",
      "Iteration 3235 => Loss: 0.08665795853255738\n",
      "Iteration 3236 => Loss: 0.08665652395194488\n",
      "Iteration 3237 => Loss: 0.08665508998909172\n",
      "Iteration 3238 => Loss: 0.08665365664354416\n",
      "Iteration 3239 => Loss: 0.08665222391484897\n",
      "Iteration 3240 => Loss: 0.0866507918025534\n",
      "Iteration 3241 => Loss: 0.08664936030620513\n",
      "Iteration 3242 => Loss: 0.08664792942535232\n",
      "Iteration 3243 => Loss: 0.08664649915954359\n",
      "Iteration 3244 => Loss: 0.08664506950832801\n",
      "Iteration 3245 => Loss: 0.0866436404712552\n",
      "Iteration 3246 => Loss: 0.08664221204787508\n",
      "Iteration 3247 => Loss: 0.08664078423773819\n",
      "Iteration 3248 => Loss: 0.08663935704039542\n",
      "Iteration 3249 => Loss: 0.08663793045539818\n",
      "Iteration 3250 => Loss: 0.08663650448229834\n",
      "Iteration 3251 => Loss: 0.08663507912064815\n",
      "Iteration 3252 => Loss: 0.08663365437000047\n",
      "Iteration 3253 => Loss: 0.08663223022990842\n",
      "Iteration 3254 => Loss: 0.08663080669992579\n",
      "Iteration 3255 => Loss: 0.08662938377960665\n",
      "Iteration 3256 => Loss: 0.08662796146850561\n",
      "Iteration 3257 => Loss: 0.08662653976617775\n",
      "Iteration 3258 => Loss: 0.08662511867217852\n",
      "Iteration 3259 => Loss: 0.08662369818606393\n",
      "Iteration 3260 => Loss: 0.08662227830739036\n",
      "Iteration 3261 => Loss: 0.0866208590357147\n",
      "Iteration 3262 => Loss: 0.08661944037059427\n",
      "Iteration 3263 => Loss: 0.08661802231158681\n",
      "Iteration 3264 => Loss: 0.08661660485825057\n",
      "Iteration 3265 => Loss: 0.08661518801014419\n",
      "Iteration 3266 => Loss: 0.08661377176682684\n",
      "Iteration 3267 => Loss: 0.08661235612785806\n",
      "Iteration 3268 => Loss: 0.08661094109279785\n",
      "Iteration 3269 => Loss: 0.08660952666120672\n",
      "Iteration 3270 => Loss: 0.08660811283264555\n",
      "Iteration 3271 => Loss: 0.0866066996066757\n",
      "Iteration 3272 => Loss: 0.086605286982859\n",
      "Iteration 3273 => Loss: 0.08660387496075773\n",
      "Iteration 3274 => Loss: 0.08660246353993452\n",
      "Iteration 3275 => Loss: 0.08660105271995257\n",
      "Iteration 3276 => Loss: 0.08659964250037544\n",
      "Iteration 3277 => Loss: 0.08659823288076718\n",
      "Iteration 3278 => Loss: 0.08659682386069223\n",
      "Iteration 3279 => Loss: 0.08659541543971551\n",
      "Iteration 3280 => Loss: 0.08659400761740245\n",
      "Iteration 3281 => Loss: 0.08659260039331877\n",
      "Iteration 3282 => Loss: 0.08659119376703073\n",
      "Iteration 3283 => Loss: 0.08658978773810501\n",
      "Iteration 3284 => Loss: 0.08658838230610875\n",
      "Iteration 3285 => Loss: 0.08658697747060949\n",
      "Iteration 3286 => Loss: 0.08658557323117522\n",
      "Iteration 3287 => Loss: 0.0865841695873744\n",
      "Iteration 3288 => Loss: 0.08658276653877588\n",
      "Iteration 3289 => Loss: 0.08658136408494897\n",
      "Iteration 3290 => Loss: 0.08657996222546341\n",
      "Iteration 3291 => Loss: 0.0865785609598894\n",
      "Iteration 3292 => Loss: 0.08657716028779754\n",
      "Iteration 3293 => Loss: 0.08657576020875886\n",
      "Iteration 3294 => Loss: 0.0865743607223449\n",
      "Iteration 3295 => Loss: 0.08657296182812747\n",
      "Iteration 3296 => Loss: 0.08657156352567906\n",
      "Iteration 3297 => Loss: 0.0865701658145723\n",
      "Iteration 3298 => Loss: 0.08656876869438053\n",
      "Iteration 3299 => Loss: 0.08656737216467732\n",
      "Iteration 3300 => Loss: 0.08656597622503674\n",
      "Iteration 3301 => Loss: 0.08656458087503338\n",
      "Iteration 3302 => Loss: 0.08656318611424206\n",
      "Iteration 3303 => Loss: 0.08656179194223822\n",
      "Iteration 3304 => Loss: 0.08656039835859758\n",
      "Iteration 3305 => Loss: 0.08655900536289642\n",
      "Iteration 3306 => Loss: 0.08655761295471136\n",
      "Iteration 3307 => Loss: 0.08655622113361945\n",
      "Iteration 3308 => Loss: 0.08655482989919822\n",
      "Iteration 3309 => Loss: 0.08655343925102559\n",
      "Iteration 3310 => Loss: 0.08655204918867987\n",
      "Iteration 3311 => Loss: 0.08655065971173984\n",
      "Iteration 3312 => Loss: 0.08654927081978472\n",
      "Iteration 3313 => Loss: 0.08654788251239412\n",
      "Iteration 3314 => Loss: 0.08654649478914808\n",
      "Iteration 3315 => Loss: 0.08654510764962711\n",
      "Iteration 3316 => Loss: 0.086543721093412\n",
      "Iteration 3317 => Loss: 0.08654233512008411\n",
      "Iteration 3318 => Loss: 0.08654094972922513\n",
      "Iteration 3319 => Loss: 0.08653956492041727\n",
      "Iteration 3320 => Loss: 0.08653818069324304\n",
      "Iteration 3321 => Loss: 0.08653679704728544\n",
      "Iteration 3322 => Loss: 0.08653541398212787\n",
      "Iteration 3323 => Loss: 0.08653403149735417\n",
      "Iteration 3324 => Loss: 0.08653264959254854\n",
      "Iteration 3325 => Loss: 0.0865312682672957\n",
      "Iteration 3326 => Loss: 0.08652988752118064\n",
      "Iteration 3327 => Loss: 0.08652850735378889\n",
      "Iteration 3328 => Loss: 0.08652712776470635\n",
      "Iteration 3329 => Loss: 0.08652574875351932\n",
      "Iteration 3330 => Loss: 0.08652437031981457\n",
      "Iteration 3331 => Loss: 0.08652299246317918\n",
      "Iteration 3332 => Loss: 0.08652161518320077\n",
      "Iteration 3333 => Loss: 0.08652023847946727\n",
      "Iteration 3334 => Loss: 0.08651886235156706\n",
      "Iteration 3335 => Loss: 0.08651748679908897\n",
      "Iteration 3336 => Loss: 0.08651611182162217\n",
      "Iteration 3337 => Loss: 0.08651473741875625\n",
      "Iteration 3338 => Loss: 0.08651336359008131\n",
      "Iteration 3339 => Loss: 0.08651199033518771\n",
      "Iteration 3340 => Loss: 0.0865106176536663\n",
      "Iteration 3341 => Loss: 0.08650924554510839\n",
      "Iteration 3342 => Loss: 0.08650787400910559\n",
      "Iteration 3343 => Loss: 0.08650650304524997\n",
      "Iteration 3344 => Loss: 0.08650513265313398\n",
      "Iteration 3345 => Loss: 0.08650376283235053\n",
      "Iteration 3346 => Loss: 0.0865023935824929\n",
      "Iteration 3347 => Loss: 0.08650102490315478\n",
      "Iteration 3348 => Loss: 0.08649965679393026\n",
      "Iteration 3349 => Loss: 0.08649828925441382\n",
      "Iteration 3350 => Loss: 0.08649692228420039\n",
      "Iteration 3351 => Loss: 0.08649555588288524\n",
      "Iteration 3352 => Loss: 0.08649419005006409\n",
      "Iteration 3353 => Loss: 0.08649282478533307\n",
      "Iteration 3354 => Loss: 0.08649146008828863\n",
      "Iteration 3355 => Loss: 0.08649009595852777\n",
      "Iteration 3356 => Loss: 0.08648873239564776\n",
      "Iteration 3357 => Loss: 0.0864873693992463\n",
      "Iteration 3358 => Loss: 0.08648600696892149\n",
      "Iteration 3359 => Loss: 0.08648464510427188\n",
      "Iteration 3360 => Loss: 0.08648328380489635\n",
      "Iteration 3361 => Loss: 0.08648192307039423\n",
      "Iteration 3362 => Loss: 0.08648056290036518\n",
      "Iteration 3363 => Loss: 0.08647920329440935\n",
      "Iteration 3364 => Loss: 0.08647784425212723\n",
      "Iteration 3365 => Loss: 0.08647648577311966\n",
      "Iteration 3366 => Loss: 0.08647512785698802\n",
      "Iteration 3367 => Loss: 0.08647377050333391\n",
      "Iteration 3368 => Loss: 0.08647241371175945\n",
      "Iteration 3369 => Loss: 0.08647105748186709\n",
      "Iteration 3370 => Loss: 0.08646970181325973\n",
      "Iteration 3371 => Loss: 0.08646834670554061\n",
      "Iteration 3372 => Loss: 0.0864669921583134\n",
      "Iteration 3373 => Loss: 0.08646563817118208\n",
      "Iteration 3374 => Loss: 0.08646428474375115\n",
      "Iteration 3375 => Loss: 0.0864629318756254\n",
      "Iteration 3376 => Loss: 0.08646157956641008\n",
      "Iteration 3377 => Loss: 0.08646022781571076\n",
      "Iteration 3378 => Loss: 0.08645887662313345\n",
      "Iteration 3379 => Loss: 0.08645752598828454\n",
      "Iteration 3380 => Loss: 0.0864561759107708\n",
      "Iteration 3381 => Loss: 0.08645482639019941\n",
      "Iteration 3382 => Loss: 0.08645347742617787\n",
      "Iteration 3383 => Loss: 0.08645212901831414\n",
      "Iteration 3384 => Loss: 0.08645078116621657\n",
      "Iteration 3385 => Loss: 0.08644943386949383\n",
      "Iteration 3386 => Loss: 0.08644808712775501\n",
      "Iteration 3387 => Loss: 0.08644674094060964\n",
      "Iteration 3388 => Loss: 0.08644539530766752\n",
      "Iteration 3389 => Loss: 0.08644405022853896\n",
      "Iteration 3390 => Loss: 0.08644270570283452\n",
      "Iteration 3391 => Loss: 0.08644136173016524\n",
      "Iteration 3392 => Loss: 0.08644001831014259\n",
      "Iteration 3393 => Loss: 0.08643867544237824\n",
      "Iteration 3394 => Loss: 0.0864373331264844\n",
      "Iteration 3395 => Loss: 0.08643599136207361\n",
      "Iteration 3396 => Loss: 0.0864346501487588\n",
      "Iteration 3397 => Loss: 0.08643330948615327\n",
      "Iteration 3398 => Loss: 0.08643196937387068\n",
      "Iteration 3399 => Loss: 0.08643062981152512\n",
      "Iteration 3400 => Loss: 0.08642929079873102\n",
      "Iteration 3401 => Loss: 0.0864279523351032\n",
      "Iteration 3402 => Loss: 0.08642661442025683\n",
      "Iteration 3403 => Loss: 0.08642527705380754\n",
      "Iteration 3404 => Loss: 0.08642394023537124\n",
      "Iteration 3405 => Loss: 0.08642260396456425\n",
      "Iteration 3406 => Loss: 0.08642126824100331\n",
      "Iteration 3407 => Loss: 0.08641993306430547\n",
      "Iteration 3408 => Loss: 0.0864185984340882\n",
      "Iteration 3409 => Loss: 0.0864172643499693\n",
      "Iteration 3410 => Loss: 0.08641593081156701\n",
      "Iteration 3411 => Loss: 0.0864145978184999\n",
      "Iteration 3412 => Loss: 0.08641326537038686\n",
      "Iteration 3413 => Loss: 0.08641193346684733\n",
      "Iteration 3414 => Loss: 0.0864106021075009\n",
      "Iteration 3415 => Loss: 0.08640927129196768\n",
      "Iteration 3416 => Loss: 0.08640794101986811\n",
      "Iteration 3417 => Loss: 0.08640661129082296\n",
      "Iteration 3418 => Loss: 0.08640528210445343\n",
      "Iteration 3419 => Loss: 0.0864039534603811\n",
      "Iteration 3420 => Loss: 0.08640262535822786\n",
      "Iteration 3421 => Loss: 0.08640129779761598\n",
      "Iteration 3422 => Loss: 0.08639997077816813\n",
      "Iteration 3423 => Loss: 0.08639864429950735\n",
      "Iteration 3424 => Loss: 0.08639731836125701\n",
      "Iteration 3425 => Loss: 0.08639599296304086\n",
      "Iteration 3426 => Loss: 0.08639466810448306\n",
      "Iteration 3427 => Loss: 0.08639334378520806\n",
      "Iteration 3428 => Loss: 0.08639202000484072\n",
      "Iteration 3429 => Loss: 0.08639069676300631\n",
      "Iteration 3430 => Loss: 0.08638937405933035\n",
      "Iteration 3431 => Loss: 0.08638805189343884\n",
      "Iteration 3432 => Loss: 0.08638673026495808\n",
      "Iteration 3433 => Loss: 0.08638540917351474\n",
      "Iteration 3434 => Loss: 0.08638408861873584\n",
      "Iteration 3435 => Loss: 0.08638276860024884\n",
      "Iteration 3436 => Loss: 0.08638144911768146\n",
      "Iteration 3437 => Loss: 0.08638013017066189\n",
      "Iteration 3438 => Loss: 0.08637881175881851\n",
      "Iteration 3439 => Loss: 0.08637749388178023\n",
      "Iteration 3440 => Loss: 0.08637617653917629\n",
      "Iteration 3441 => Loss: 0.08637485973063622\n",
      "Iteration 3442 => Loss: 0.08637354345578997\n",
      "Iteration 3443 => Loss: 0.08637222771426781\n",
      "Iteration 3444 => Loss: 0.08637091250570038\n",
      "Iteration 3445 => Loss: 0.08636959782971869\n",
      "Iteration 3446 => Loss: 0.08636828368595412\n",
      "Iteration 3447 => Loss: 0.08636697007403837\n",
      "Iteration 3448 => Loss: 0.08636565699360353\n",
      "Iteration 3449 => Loss: 0.08636434444428205\n",
      "Iteration 3450 => Loss: 0.08636303242570667\n",
      "Iteration 3451 => Loss: 0.08636172093751053\n",
      "Iteration 3452 => Loss: 0.0863604099793272\n",
      "Iteration 3453 => Loss: 0.08635909955079048\n",
      "Iteration 3454 => Loss: 0.08635778965153457\n",
      "Iteration 3455 => Loss: 0.08635648028119407\n",
      "Iteration 3456 => Loss: 0.08635517143940384\n",
      "Iteration 3457 => Loss: 0.08635386312579918\n",
      "Iteration 3458 => Loss: 0.08635255534001575\n",
      "Iteration 3459 => Loss: 0.08635124808168944\n",
      "Iteration 3460 => Loss: 0.0863499413504566\n",
      "Iteration 3461 => Loss: 0.08634863514595394\n",
      "Iteration 3462 => Loss: 0.08634732946781842\n",
      "Iteration 3463 => Loss: 0.08634602431568747\n",
      "Iteration 3464 => Loss: 0.0863447196891988\n",
      "Iteration 3465 => Loss: 0.08634341558799047\n",
      "Iteration 3466 => Loss: 0.0863421120117009\n",
      "Iteration 3467 => Loss: 0.0863408089599689\n",
      "Iteration 3468 => Loss: 0.08633950643243352\n",
      "Iteration 3469 => Loss: 0.08633820442873429\n",
      "Iteration 3470 => Loss: 0.08633690294851097\n",
      "Iteration 3471 => Loss: 0.08633560199140378\n",
      "Iteration 3472 => Loss: 0.08633430155705317\n",
      "Iteration 3473 => Loss: 0.08633300164510002\n",
      "Iteration 3474 => Loss: 0.08633170225518554\n",
      "Iteration 3475 => Loss: 0.08633040338695123\n",
      "Iteration 3476 => Loss: 0.086329105040039\n",
      "Iteration 3477 => Loss: 0.0863278072140911\n",
      "Iteration 3478 => Loss: 0.08632650990875008\n",
      "Iteration 3479 => Loss: 0.08632521312365887\n",
      "Iteration 3480 => Loss: 0.0863239168584607\n",
      "Iteration 3481 => Loss: 0.08632262111279924\n",
      "Iteration 3482 => Loss: 0.08632132588631836\n",
      "Iteration 3483 => Loss: 0.0863200311786624\n",
      "Iteration 3484 => Loss: 0.086318736989476\n",
      "Iteration 3485 => Loss: 0.08631744331840403\n",
      "Iteration 3486 => Loss: 0.08631615016509195\n",
      "Iteration 3487 => Loss: 0.08631485752918526\n",
      "Iteration 3488 => Loss: 0.08631356541033007\n",
      "Iteration 3489 => Loss: 0.08631227380817265\n",
      "Iteration 3490 => Loss: 0.08631098272235962\n",
      "Iteration 3491 => Loss: 0.08630969215253809\n",
      "Iteration 3492 => Loss: 0.08630840209835533\n",
      "Iteration 3493 => Loss: 0.08630711255945904\n",
      "Iteration 3494 => Loss: 0.08630582353549726\n",
      "Iteration 3495 => Loss: 0.08630453502611829\n",
      "Iteration 3496 => Loss: 0.08630324703097089\n",
      "Iteration 3497 => Loss: 0.08630195954970403\n",
      "Iteration 3498 => Loss: 0.0863006725819671\n",
      "Iteration 3499 => Loss: 0.08629938612740978\n",
      "Iteration 3500 => Loss: 0.0862981001856821\n",
      "Iteration 3501 => Loss: 0.08629681475643443\n",
      "Iteration 3502 => Loss: 0.08629552983931747\n",
      "Iteration 3503 => Loss: 0.08629424543398229\n",
      "Iteration 3504 => Loss: 0.08629296154008019\n",
      "Iteration 3505 => Loss: 0.08629167815726288\n",
      "Iteration 3506 => Loss: 0.08629039528518245\n",
      "Iteration 3507 => Loss: 0.0862891129234912\n",
      "Iteration 3508 => Loss: 0.0862878310718418\n",
      "Iteration 3509 => Loss: 0.08628654972988738\n",
      "Iteration 3510 => Loss: 0.0862852688972812\n",
      "Iteration 3511 => Loss: 0.08628398857367699\n",
      "Iteration 3512 => Loss: 0.08628270875872872\n",
      "Iteration 3513 => Loss: 0.08628142945209076\n",
      "Iteration 3514 => Loss: 0.0862801506534178\n",
      "Iteration 3515 => Loss: 0.08627887236236481\n",
      "Iteration 3516 => Loss: 0.08627759457858714\n",
      "Iteration 3517 => Loss: 0.08627631730174042\n",
      "Iteration 3518 => Loss: 0.08627504053148068\n",
      "Iteration 3519 => Loss: 0.08627376426746415\n",
      "Iteration 3520 => Loss: 0.08627248850934759\n",
      "Iteration 3521 => Loss: 0.08627121325678784\n",
      "Iteration 3522 => Loss: 0.08626993850944227\n",
      "Iteration 3523 => Loss: 0.08626866426696844\n",
      "Iteration 3524 => Loss: 0.08626739052902432\n",
      "Iteration 3525 => Loss: 0.08626611729526817\n",
      "Iteration 3526 => Loss: 0.08626484456535859\n",
      "Iteration 3527 => Loss: 0.08626357233895443\n",
      "Iteration 3528 => Loss: 0.08626230061571502\n",
      "Iteration 3529 => Loss: 0.08626102939529987\n",
      "Iteration 3530 => Loss: 0.08625975867736883\n",
      "Iteration 3531 => Loss: 0.08625848846158214\n",
      "Iteration 3532 => Loss: 0.08625721874760033\n",
      "Iteration 3533 => Loss: 0.08625594953508421\n",
      "Iteration 3534 => Loss: 0.08625468082369497\n",
      "Iteration 3535 => Loss: 0.08625341261309409\n",
      "Iteration 3536 => Loss: 0.0862521449029434\n",
      "Iteration 3537 => Loss: 0.086250877692905\n",
      "Iteration 3538 => Loss: 0.08624961098264133\n",
      "Iteration 3539 => Loss: 0.08624834477181519\n",
      "Iteration 3540 => Loss: 0.08624707906008962\n",
      "Iteration 3541 => Loss: 0.08624581384712805\n",
      "Iteration 3542 => Loss: 0.08624454913259422\n",
      "Iteration 3543 => Loss: 0.0862432849161521\n",
      "Iteration 3544 => Loss: 0.08624202119746614\n",
      "Iteration 3545 => Loss: 0.08624075797620093\n",
      "Iteration 3546 => Loss: 0.08623949525202149\n",
      "Iteration 3547 => Loss: 0.08623823302459312\n",
      "Iteration 3548 => Loss: 0.08623697129358143\n",
      "Iteration 3549 => Loss: 0.08623571005865241\n",
      "Iteration 3550 => Loss: 0.08623444931947222\n",
      "Iteration 3551 => Loss: 0.08623318907570748\n",
      "Iteration 3552 => Loss: 0.08623192932702507\n",
      "Iteration 3553 => Loss: 0.08623067007309215\n",
      "Iteration 3554 => Loss: 0.08622941131357625\n",
      "Iteration 3555 => Loss: 0.0862281530481452\n",
      "Iteration 3556 => Loss: 0.08622689527646707\n",
      "Iteration 3557 => Loss: 0.08622563799821036\n",
      "Iteration 3558 => Loss: 0.08622438121304382\n",
      "Iteration 3559 => Loss: 0.08622312492063648\n",
      "Iteration 3560 => Loss: 0.08622186912065778\n",
      "Iteration 3561 => Loss: 0.08622061381277735\n",
      "Iteration 3562 => Loss: 0.08621935899666519\n",
      "Iteration 3563 => Loss: 0.08621810467199165\n",
      "Iteration 3564 => Loss: 0.08621685083842728\n",
      "Iteration 3565 => Loss: 0.08621559749564307\n",
      "Iteration 3566 => Loss: 0.08621434464331022\n",
      "Iteration 3567 => Loss: 0.08621309228110029\n",
      "Iteration 3568 => Loss: 0.08621184040868513\n",
      "Iteration 3569 => Loss: 0.08621058902573689\n",
      "Iteration 3570 => Loss: 0.08620933813192805\n",
      "Iteration 3571 => Loss: 0.08620808772693138\n",
      "Iteration 3572 => Loss: 0.08620683781041992\n",
      "Iteration 3573 => Loss: 0.08620558838206714\n",
      "Iteration 3574 => Loss: 0.08620433944154664\n",
      "Iteration 3575 => Loss: 0.08620309098853252\n",
      "Iteration 3576 => Loss: 0.08620184302269897\n",
      "Iteration 3577 => Loss: 0.08620059554372066\n",
      "Iteration 3578 => Loss: 0.08619934855127248\n",
      "Iteration 3579 => Loss: 0.08619810204502969\n",
      "Iteration 3580 => Loss: 0.08619685602466777\n",
      "Iteration 3581 => Loss: 0.08619561048986255\n",
      "Iteration 3582 => Loss: 0.08619436544029016\n",
      "Iteration 3583 => Loss: 0.086193120875627\n",
      "Iteration 3584 => Loss: 0.08619187679554988\n",
      "Iteration 3585 => Loss: 0.08619063319973569\n",
      "Iteration 3586 => Loss: 0.08618939008786192\n",
      "Iteration 3587 => Loss: 0.08618814745960614\n",
      "Iteration 3588 => Loss: 0.08618690531464625\n",
      "Iteration 3589 => Loss: 0.08618566365266052\n",
      "Iteration 3590 => Loss: 0.08618442247332748\n",
      "Iteration 3591 => Loss: 0.086183181776326\n",
      "Iteration 3592 => Loss: 0.08618194156133517\n",
      "Iteration 3593 => Loss: 0.08618070182803442\n",
      "Iteration 3594 => Loss: 0.0861794625761035\n",
      "Iteration 3595 => Loss: 0.08617822380522246\n",
      "Iteration 3596 => Loss: 0.08617698551507161\n",
      "Iteration 3597 => Loss: 0.08617574770533155\n",
      "Iteration 3598 => Loss: 0.08617451037568326\n",
      "Iteration 3599 => Loss: 0.08617327352580792\n",
      "Iteration 3600 => Loss: 0.08617203715538704\n",
      "Iteration 3601 => Loss: 0.08617080126410245\n",
      "Iteration 3602 => Loss: 0.08616956585163628\n",
      "Iteration 3603 => Loss: 0.08616833091767091\n",
      "Iteration 3604 => Loss: 0.08616709646188903\n",
      "Iteration 3605 => Loss: 0.08616586248397365\n",
      "Iteration 3606 => Loss: 0.08616462898360805\n",
      "Iteration 3607 => Loss: 0.0861633959604758\n",
      "Iteration 3608 => Loss: 0.08616216341426078\n",
      "Iteration 3609 => Loss: 0.0861609313446472\n",
      "Iteration 3610 => Loss: 0.08615969975131947\n",
      "Iteration 3611 => Loss: 0.08615846863396237\n",
      "Iteration 3612 => Loss: 0.08615723799226095\n",
      "Iteration 3613 => Loss: 0.08615600782590052\n",
      "Iteration 3614 => Loss: 0.08615477813456675\n",
      "Iteration 3615 => Loss: 0.08615354891794554\n",
      "Iteration 3616 => Loss: 0.08615232017572311\n",
      "Iteration 3617 => Loss: 0.08615109190758596\n",
      "Iteration 3618 => Loss: 0.08614986411322086\n",
      "Iteration 3619 => Loss: 0.08614863679231491\n",
      "Iteration 3620 => Loss: 0.08614740994455551\n",
      "Iteration 3621 => Loss: 0.08614618356963027\n",
      "Iteration 3622 => Loss: 0.0861449576672272\n",
      "Iteration 3623 => Loss: 0.08614373223703449\n",
      "Iteration 3624 => Loss: 0.08614250727874069\n",
      "Iteration 3625 => Loss: 0.0861412827920346\n",
      "Iteration 3626 => Loss: 0.08614005877660533\n",
      "Iteration 3627 => Loss: 0.08613883523214226\n",
      "Iteration 3628 => Loss: 0.08613761215833511\n",
      "Iteration 3629 => Loss: 0.08613638955487377\n",
      "Iteration 3630 => Loss: 0.08613516742144857\n",
      "Iteration 3631 => Loss: 0.08613394575774996\n",
      "Iteration 3632 => Loss: 0.0861327245634688\n",
      "Iteration 3633 => Loss: 0.08613150383829624\n",
      "Iteration 3634 => Loss: 0.08613028358192358\n",
      "Iteration 3635 => Loss: 0.08612906379404256\n",
      "Iteration 3636 => Loss: 0.0861278444743451\n",
      "Iteration 3637 => Loss: 0.08612662562252348\n",
      "Iteration 3638 => Loss: 0.0861254072382702\n",
      "Iteration 3639 => Loss: 0.08612418932127808\n",
      "Iteration 3640 => Loss: 0.0861229718712402\n",
      "Iteration 3641 => Loss: 0.0861217548878499\n",
      "Iteration 3642 => Loss: 0.0861205383708009\n",
      "Iteration 3643 => Loss: 0.08611932231978711\n",
      "Iteration 3644 => Loss: 0.08611810673450275\n",
      "Iteration 3645 => Loss: 0.08611689161464231\n",
      "Iteration 3646 => Loss: 0.08611567695990056\n",
      "Iteration 3647 => Loss: 0.0861144627699726\n",
      "Iteration 3648 => Loss: 0.08611324904455375\n",
      "Iteration 3649 => Loss: 0.0861120357833396\n",
      "Iteration 3650 => Loss: 0.08611082298602613\n",
      "Iteration 3651 => Loss: 0.08610961065230943\n",
      "Iteration 3652 => Loss: 0.08610839878188598\n",
      "Iteration 3653 => Loss: 0.08610718737445257\n",
      "Iteration 3654 => Loss: 0.08610597642970615\n",
      "Iteration 3655 => Loss: 0.08610476594734406\n",
      "Iteration 3656 => Loss: 0.08610355592706383\n",
      "Iteration 3657 => Loss: 0.08610234636856333\n",
      "Iteration 3658 => Loss: 0.08610113727154065\n",
      "Iteration 3659 => Loss: 0.08609992863569425\n",
      "Iteration 3660 => Loss: 0.08609872046072276\n",
      "Iteration 3661 => Loss: 0.08609751274632513\n",
      "Iteration 3662 => Loss: 0.08609630549220061\n",
      "Iteration 3663 => Loss: 0.08609509869804867\n",
      "Iteration 3664 => Loss: 0.08609389236356914\n",
      "Iteration 3665 => Loss: 0.08609268648846205\n",
      "Iteration 3666 => Loss: 0.08609148107242771\n",
      "Iteration 3667 => Loss: 0.0860902761151667\n",
      "Iteration 3668 => Loss: 0.08608907161637994\n",
      "Iteration 3669 => Loss: 0.08608786757576856\n",
      "Iteration 3670 => Loss: 0.08608666399303397\n",
      "Iteration 3671 => Loss: 0.0860854608678779\n",
      "Iteration 3672 => Loss: 0.08608425820000223\n",
      "Iteration 3673 => Loss: 0.08608305598910929\n",
      "Iteration 3674 => Loss: 0.08608185423490154\n",
      "Iteration 3675 => Loss: 0.08608065293708178\n",
      "Iteration 3676 => Loss: 0.08607945209535302\n",
      "Iteration 3677 => Loss: 0.0860782517094186\n",
      "Iteration 3678 => Loss: 0.08607705177898216\n",
      "Iteration 3679 => Loss: 0.08607585230374748\n",
      "Iteration 3680 => Loss: 0.08607465328341878\n",
      "Iteration 3681 => Loss: 0.08607345471770038\n",
      "Iteration 3682 => Loss: 0.08607225660629698\n",
      "Iteration 3683 => Loss: 0.08607105894891355\n",
      "Iteration 3684 => Loss: 0.08606986174525524\n",
      "Iteration 3685 => Loss: 0.08606866499502758\n",
      "Iteration 3686 => Loss: 0.08606746869793629\n",
      "Iteration 3687 => Loss: 0.08606627285368734\n",
      "Iteration 3688 => Loss: 0.08606507746198709\n",
      "Iteration 3689 => Loss: 0.08606388252254202\n",
      "Iteration 3690 => Loss: 0.08606268803505894\n",
      "Iteration 3691 => Loss: 0.08606149399924498\n",
      "Iteration 3692 => Loss: 0.08606030041480746\n",
      "Iteration 3693 => Loss: 0.08605910728145397\n",
      "Iteration 3694 => Loss: 0.08605791459889238\n",
      "Iteration 3695 => Loss: 0.08605672236683087\n",
      "Iteration 3696 => Loss: 0.08605553058497781\n",
      "Iteration 3697 => Loss: 0.08605433925304191\n",
      "Iteration 3698 => Loss: 0.08605314837073204\n",
      "Iteration 3699 => Loss: 0.08605195793775744\n",
      "Iteration 3700 => Loss: 0.08605076795382756\n",
      "Iteration 3701 => Loss: 0.08604957841865213\n",
      "Iteration 3702 => Loss: 0.08604838933194114\n",
      "Iteration 3703 => Loss: 0.08604720069340482\n",
      "Iteration 3704 => Loss: 0.08604601250275373\n",
      "Iteration 3705 => Loss: 0.08604482475969856\n",
      "Iteration 3706 => Loss: 0.08604363746395043\n",
      "Iteration 3707 => Loss: 0.08604245061522058\n",
      "Iteration 3708 => Loss: 0.08604126421322059\n",
      "Iteration 3709 => Loss: 0.08604007825766226\n",
      "Iteration 3710 => Loss: 0.08603889274825773\n",
      "Iteration 3711 => Loss: 0.08603770768471924\n",
      "Iteration 3712 => Loss: 0.08603652306675948\n",
      "Iteration 3713 => Loss: 0.08603533889409128\n",
      "Iteration 3714 => Loss: 0.0860341551664277\n",
      "Iteration 3715 => Loss: 0.08603297188348222\n",
      "Iteration 3716 => Loss: 0.0860317890449684\n",
      "Iteration 3717 => Loss: 0.08603060665060014\n",
      "Iteration 3718 => Loss: 0.08602942470009166\n",
      "Iteration 3719 => Loss: 0.08602824319315727\n",
      "Iteration 3720 => Loss: 0.08602706212951172\n",
      "Iteration 3721 => Loss: 0.08602588150886989\n",
      "Iteration 3722 => Loss: 0.08602470133094697\n",
      "Iteration 3723 => Loss: 0.0860235215954584\n",
      "Iteration 3724 => Loss: 0.08602234230211986\n",
      "Iteration 3725 => Loss: 0.08602116345064736\n",
      "Iteration 3726 => Loss: 0.08601998504075703\n",
      "Iteration 3727 => Loss: 0.08601880707216537\n",
      "Iteration 3728 => Loss: 0.0860176295445891\n",
      "Iteration 3729 => Loss: 0.08601645245774515\n",
      "Iteration 3730 => Loss: 0.0860152758113508\n",
      "Iteration 3731 => Loss: 0.08601409960512352\n",
      "Iteration 3732 => Loss: 0.08601292383878102\n",
      "Iteration 3733 => Loss: 0.0860117485120413\n",
      "Iteration 3734 => Loss: 0.08601057362462258\n",
      "Iteration 3735 => Loss: 0.08600939917624341\n",
      "Iteration 3736 => Loss: 0.08600822516662249\n",
      "Iteration 3737 => Loss: 0.08600705159547883\n",
      "Iteration 3738 => Loss: 0.0860058784625317\n",
      "Iteration 3739 => Loss: 0.08600470576750054\n",
      "Iteration 3740 => Loss: 0.0860035335101052\n",
      "Iteration 3741 => Loss: 0.08600236169006563\n",
      "Iteration 3742 => Loss: 0.08600119030710209\n",
      "Iteration 3743 => Loss: 0.08600001936093508\n",
      "Iteration 3744 => Loss: 0.0859988488512854\n",
      "Iteration 3745 => Loss: 0.08599767877787405\n",
      "Iteration 3746 => Loss: 0.08599650914042224\n",
      "Iteration 3747 => Loss: 0.08599533993865151\n",
      "Iteration 3748 => Loss: 0.08599417117228363\n",
      "Iteration 3749 => Loss: 0.0859930028410406\n",
      "Iteration 3750 => Loss: 0.08599183494464466\n",
      "Iteration 3751 => Loss: 0.08599066748281832\n",
      "Iteration 3752 => Loss: 0.08598950045528433\n",
      "Iteration 3753 => Loss: 0.0859883338617657\n",
      "Iteration 3754 => Loss: 0.0859871677019857\n",
      "Iteration 3755 => Loss: 0.08598600197566778\n",
      "Iteration 3756 => Loss: 0.0859848366825357\n",
      "Iteration 3757 => Loss: 0.08598367182231345\n",
      "Iteration 3758 => Loss: 0.08598250739472528\n",
      "Iteration 3759 => Loss: 0.08598134339949563\n",
      "Iteration 3760 => Loss: 0.08598017983634929\n",
      "Iteration 3761 => Loss: 0.08597901670501117\n",
      "Iteration 3762 => Loss: 0.08597785400520652\n",
      "Iteration 3763 => Loss: 0.08597669173666078\n",
      "Iteration 3764 => Loss: 0.08597552989909968\n",
      "Iteration 3765 => Loss: 0.08597436849224922\n",
      "Iteration 3766 => Loss: 0.0859732075158355\n",
      "Iteration 3767 => Loss: 0.08597204696958502\n",
      "Iteration 3768 => Loss: 0.08597088685322443\n",
      "Iteration 3769 => Loss: 0.0859697271664807\n",
      "Iteration 3770 => Loss: 0.08596856790908097\n",
      "Iteration 3771 => Loss: 0.08596740908075268\n",
      "Iteration 3772 => Loss: 0.08596625068122343\n",
      "Iteration 3773 => Loss: 0.08596509271022121\n",
      "Iteration 3774 => Loss: 0.08596393516747405\n",
      "Iteration 3775 => Loss: 0.08596277805271044\n",
      "Iteration 3776 => Loss: 0.08596162136565895\n",
      "Iteration 3777 => Loss: 0.08596046510604843\n",
      "Iteration 3778 => Loss: 0.08595930927360802\n",
      "Iteration 3779 => Loss: 0.08595815386806703\n",
      "Iteration 3780 => Loss: 0.08595699888915508\n",
      "Iteration 3781 => Loss: 0.08595584433660199\n",
      "Iteration 3782 => Loss: 0.08595469021013782\n",
      "Iteration 3783 => Loss: 0.08595353650949289\n",
      "Iteration 3784 => Loss: 0.0859523832343977\n",
      "Iteration 3785 => Loss: 0.08595123038458308\n",
      "Iteration 3786 => Loss: 0.08595007795978003\n",
      "Iteration 3787 => Loss: 0.08594892595971984\n",
      "Iteration 3788 => Loss: 0.085947774384134\n",
      "Iteration 3789 => Loss: 0.08594662323275422\n",
      "Iteration 3790 => Loss: 0.0859454725053125\n",
      "Iteration 3791 => Loss: 0.08594432220154104\n",
      "Iteration 3792 => Loss: 0.0859431723211723\n",
      "Iteration 3793 => Loss: 0.08594202286393897\n",
      "Iteration 3794 => Loss: 0.08594087382957397\n",
      "Iteration 3795 => Loss: 0.08593972521781044\n",
      "Iteration 3796 => Loss: 0.0859385770283818\n",
      "Iteration 3797 => Loss: 0.08593742926102166\n",
      "Iteration 3798 => Loss: 0.08593628191546392\n",
      "Iteration 3799 => Loss: 0.08593513499144263\n",
      "Iteration 3800 => Loss: 0.08593398848869217\n",
      "Iteration 3801 => Loss: 0.08593284240694711\n",
      "Iteration 3802 => Loss: 0.08593169674594224\n",
      "Iteration 3803 => Loss: 0.08593055150541262\n",
      "Iteration 3804 => Loss: 0.08592940668509348\n",
      "Iteration 3805 => Loss: 0.08592826228472035\n",
      "Iteration 3806 => Loss: 0.085927118304029\n",
      "Iteration 3807 => Loss: 0.08592597474275535\n",
      "Iteration 3808 => Loss: 0.08592483160063566\n",
      "Iteration 3809 => Loss: 0.08592368887740635\n",
      "Iteration 3810 => Loss: 0.08592254657280407\n",
      "Iteration 3811 => Loss: 0.08592140468656576\n",
      "Iteration 3812 => Loss: 0.08592026321842851\n",
      "Iteration 3813 => Loss: 0.08591912216812973\n",
      "Iteration 3814 => Loss: 0.085917981535407\n",
      "Iteration 3815 => Loss: 0.08591684131999816\n",
      "Iteration 3816 => Loss: 0.08591570152164126\n",
      "Iteration 3817 => Loss: 0.08591456214007456\n",
      "Iteration 3818 => Loss: 0.08591342317503665\n",
      "Iteration 3819 => Loss: 0.0859122846262662\n",
      "Iteration 3820 => Loss: 0.08591114649350225\n",
      "Iteration 3821 => Loss: 0.085910008776484\n",
      "Iteration 3822 => Loss: 0.08590887147495087\n",
      "Iteration 3823 => Loss: 0.08590773458864255\n",
      "Iteration 3824 => Loss: 0.08590659811729891\n",
      "Iteration 3825 => Loss: 0.0859054620606601\n",
      "Iteration 3826 => Loss: 0.08590432641846646\n",
      "Iteration 3827 => Loss: 0.08590319119045856\n",
      "Iteration 3828 => Loss: 0.08590205637637723\n",
      "Iteration 3829 => Loss: 0.0859009219759635\n",
      "Iteration 3830 => Loss: 0.08589978798895861\n",
      "Iteration 3831 => Loss: 0.08589865441510412\n",
      "Iteration 3832 => Loss: 0.08589752125414163\n",
      "Iteration 3833 => Loss: 0.0858963885058132\n",
      "Iteration 3834 => Loss: 0.08589525616986092\n",
      "Iteration 3835 => Loss: 0.08589412424602721\n",
      "Iteration 3836 => Loss: 0.0858929927340547\n",
      "Iteration 3837 => Loss: 0.08589186163368624\n",
      "Iteration 3838 => Loss: 0.08589073094466487\n",
      "Iteration 3839 => Loss: 0.0858896006667339\n",
      "Iteration 3840 => Loss: 0.08588847079963687\n",
      "Iteration 3841 => Loss: 0.08588734134311748\n",
      "Iteration 3842 => Loss: 0.08588621229691977\n",
      "Iteration 3843 => Loss: 0.08588508366078786\n",
      "Iteration 3844 => Loss: 0.08588395543446618\n",
      "Iteration 3845 => Loss: 0.08588282761769943\n",
      "Iteration 3846 => Loss: 0.08588170021023239\n",
      "Iteration 3847 => Loss: 0.08588057321181021\n",
      "Iteration 3848 => Loss: 0.08587944662217811\n",
      "Iteration 3849 => Loss: 0.08587832044108173\n",
      "Iteration 3850 => Loss: 0.08587719466826678\n",
      "Iteration 3851 => Loss: 0.08587606930347919\n",
      "Iteration 3852 => Loss: 0.08587494434646521\n",
      "Iteration 3853 => Loss: 0.08587381979697123\n",
      "Iteration 3854 => Loss: 0.08587269565474391\n",
      "Iteration 3855 => Loss: 0.08587157191953006\n",
      "Iteration 3856 => Loss: 0.08587044859107681\n",
      "Iteration 3857 => Loss: 0.08586932566913145\n",
      "Iteration 3858 => Loss: 0.0858682031534415\n",
      "Iteration 3859 => Loss: 0.08586708104375468\n",
      "Iteration 3860 => Loss: 0.08586595933981894\n",
      "Iteration 3861 => Loss: 0.0858648380413825\n",
      "Iteration 3862 => Loss: 0.08586371714819371\n",
      "Iteration 3863 => Loss: 0.08586259666000123\n",
      "Iteration 3864 => Loss: 0.0858614765765539\n",
      "Iteration 3865 => Loss: 0.08586035689760073\n",
      "Iteration 3866 => Loss: 0.08585923762289101\n",
      "Iteration 3867 => Loss: 0.08585811875217424\n",
      "Iteration 3868 => Loss: 0.0858570002852001\n",
      "Iteration 3869 => Loss: 0.08585588222171858\n",
      "Iteration 3870 => Loss: 0.08585476456147972\n",
      "Iteration 3871 => Loss: 0.08585364730423398\n",
      "Iteration 3872 => Loss: 0.08585253044973187\n",
      "Iteration 3873 => Loss: 0.0858514139977242\n",
      "Iteration 3874 => Loss: 0.08585029794796198\n",
      "Iteration 3875 => Loss: 0.08584918230019645\n",
      "Iteration 3876 => Loss: 0.085848067054179\n",
      "Iteration 3877 => Loss: 0.08584695220966133\n",
      "Iteration 3878 => Loss: 0.08584583776639532\n",
      "Iteration 3879 => Loss: 0.08584472372413302\n",
      "Iteration 3880 => Loss: 0.08584361008262675\n",
      "Iteration 3881 => Loss: 0.08584249684162899\n",
      "Iteration 3882 => Loss: 0.08584138400089254\n",
      "Iteration 3883 => Loss: 0.08584027156017028\n",
      "Iteration 3884 => Loss: 0.08583915951921539\n",
      "Iteration 3885 => Loss: 0.08583804787778124\n",
      "Iteration 3886 => Loss: 0.08583693663562139\n",
      "Iteration 3887 => Loss: 0.0858358257924897\n",
      "Iteration 3888 => Loss: 0.0858347153481401\n",
      "Iteration 3889 => Loss: 0.08583360530232688\n",
      "Iteration 3890 => Loss: 0.08583249565480441\n",
      "Iteration 3891 => Loss: 0.0858313864053274\n",
      "Iteration 3892 => Loss: 0.08583027755365069\n",
      "Iteration 3893 => Loss: 0.08582916909952931\n",
      "Iteration 3894 => Loss: 0.0858280610427186\n",
      "Iteration 3895 => Loss: 0.08582695338297398\n",
      "Iteration 3896 => Loss: 0.08582584612005124\n",
      "Iteration 3897 => Loss: 0.08582473925370625\n",
      "Iteration 3898 => Loss: 0.08582363278369512\n",
      "Iteration 3899 => Loss: 0.08582252670977424\n",
      "Iteration 3900 => Loss: 0.0858214210317001\n",
      "Iteration 3901 => Loss: 0.08582031574922946\n",
      "Iteration 3902 => Loss: 0.08581921086211934\n",
      "Iteration 3903 => Loss: 0.08581810637012686\n",
      "Iteration 3904 => Loss: 0.08581700227300942\n",
      "Iteration 3905 => Loss: 0.0858158985705246\n",
      "Iteration 3906 => Loss: 0.08581479526243023\n",
      "Iteration 3907 => Loss: 0.08581369234848427\n",
      "Iteration 3908 => Loss: 0.08581258982844503\n",
      "Iteration 3909 => Loss: 0.08581148770207084\n",
      "Iteration 3910 => Loss: 0.08581038596912036\n",
      "Iteration 3911 => Loss: 0.08580928462935246\n",
      "Iteration 3912 => Loss: 0.08580818368252616\n",
      "Iteration 3913 => Loss: 0.08580708312840073\n",
      "Iteration 3914 => Loss: 0.08580598296673562\n",
      "Iteration 3915 => Loss: 0.0858048831972905\n",
      "Iteration 3916 => Loss: 0.08580378381982524\n",
      "Iteration 3917 => Loss: 0.08580268483409997\n",
      "Iteration 3918 => Loss: 0.08580158623987491\n",
      "Iteration 3919 => Loss: 0.08580048803691058\n",
      "Iteration 3920 => Loss: 0.08579939022496769\n",
      "Iteration 3921 => Loss: 0.08579829280380714\n",
      "Iteration 3922 => Loss: 0.08579719577319002\n",
      "Iteration 3923 => Loss: 0.08579609913287765\n",
      "Iteration 3924 => Loss: 0.08579500288263157\n",
      "Iteration 3925 => Loss: 0.08579390702221346\n",
      "Iteration 3926 => Loss: 0.08579281155138528\n",
      "Iteration 3927 => Loss: 0.08579171646990917\n",
      "Iteration 3928 => Loss: 0.08579062177754744\n",
      "Iteration 3929 => Loss: 0.08578952747406263\n",
      "Iteration 3930 => Loss: 0.08578843355921746\n",
      "Iteration 3931 => Loss: 0.08578734003277491\n",
      "Iteration 3932 => Loss: 0.08578624689449811\n",
      "Iteration 3933 => Loss: 0.08578515414415042\n",
      "Iteration 3934 => Loss: 0.08578406178149539\n",
      "Iteration 3935 => Loss: 0.08578296980629674\n",
      "Iteration 3936 => Loss: 0.08578187821831848\n",
      "Iteration 3937 => Loss: 0.08578078701732474\n",
      "Iteration 3938 => Loss: 0.08577969620307985\n",
      "Iteration 3939 => Loss: 0.0857786057753484\n",
      "Iteration 3940 => Loss: 0.08577751573389515\n",
      "Iteration 3941 => Loss: 0.08577642607848508\n",
      "Iteration 3942 => Loss: 0.0857753368088833\n",
      "Iteration 3943 => Loss: 0.08577424792485522\n",
      "Iteration 3944 => Loss: 0.08577315942616638\n",
      "Iteration 3945 => Loss: 0.08577207131258255\n",
      "Iteration 3946 => Loss: 0.08577098358386968\n",
      "Iteration 3947 => Loss: 0.08576989623979395\n",
      "Iteration 3948 => Loss: 0.08576880928012166\n",
      "Iteration 3949 => Loss: 0.08576772270461949\n",
      "Iteration 3950 => Loss: 0.0857666365130541\n",
      "Iteration 3951 => Loss: 0.08576555070519244\n",
      "Iteration 3952 => Loss: 0.08576446528080174\n",
      "Iteration 3953 => Loss: 0.08576338023964931\n",
      "Iteration 3954 => Loss: 0.0857622955815027\n",
      "Iteration 3955 => Loss: 0.08576121130612964\n",
      "Iteration 3956 => Loss: 0.08576012741329814\n",
      "Iteration 3957 => Loss: 0.08575904390277628\n",
      "Iteration 3958 => Loss: 0.08575796077433245\n",
      "Iteration 3959 => Loss: 0.08575687802773516\n",
      "Iteration 3960 => Loss: 0.0857557956627531\n",
      "Iteration 3961 => Loss: 0.08575471367915531\n",
      "Iteration 3962 => Loss: 0.08575363207671081\n",
      "Iteration 3963 => Loss: 0.08575255085518901\n",
      "Iteration 3964 => Loss: 0.08575147001435936\n",
      "Iteration 3965 => Loss: 0.0857503895539916\n",
      "Iteration 3966 => Loss: 0.08574930947385566\n",
      "Iteration 3967 => Loss: 0.08574822977372162\n",
      "Iteration 3968 => Loss: 0.08574715045335982\n",
      "Iteration 3969 => Loss: 0.08574607151254068\n",
      "Iteration 3970 => Loss: 0.08574499295103497\n",
      "Iteration 3971 => Loss: 0.08574391476861348\n",
      "Iteration 3972 => Loss: 0.08574283696504738\n",
      "Iteration 3973 => Loss: 0.08574175954010792\n",
      "Iteration 3974 => Loss: 0.08574068249356653\n",
      "Iteration 3975 => Loss: 0.0857396058251949\n",
      "Iteration 3976 => Loss: 0.08573852953476488\n",
      "Iteration 3977 => Loss: 0.08573745362204847\n",
      "Iteration 3978 => Loss: 0.08573637808681797\n",
      "Iteration 3979 => Loss: 0.08573530292884579\n",
      "Iteration 3980 => Loss: 0.08573422814790455\n",
      "Iteration 3981 => Loss: 0.08573315374376707\n",
      "Iteration 3982 => Loss: 0.08573207971620632\n",
      "Iteration 3983 => Loss: 0.08573100606499555\n",
      "Iteration 3984 => Loss: 0.08572993278990812\n",
      "Iteration 3985 => Loss: 0.08572885989071762\n",
      "Iteration 3986 => Loss: 0.08572778736719786\n",
      "Iteration 3987 => Loss: 0.08572671521912273\n",
      "Iteration 3988 => Loss: 0.08572564344626643\n",
      "Iteration 3989 => Loss: 0.0857245720484033\n",
      "Iteration 3990 => Loss: 0.08572350102530786\n",
      "Iteration 3991 => Loss: 0.08572243037675488\n",
      "Iteration 3992 => Loss: 0.08572136010251923\n",
      "Iteration 3993 => Loss: 0.08572029020237602\n",
      "Iteration 3994 => Loss: 0.08571922067610059\n",
      "Iteration 3995 => Loss: 0.08571815152346839\n",
      "Iteration 3996 => Loss: 0.08571708274425506\n",
      "Iteration 3997 => Loss: 0.08571601433823653\n",
      "Iteration 3998 => Loss: 0.0857149463051888\n",
      "Iteration 3999 => Loss: 0.08571387864488814\n"
     ]
    }
   ],
   "source": [
    "w = train(X, Y, iterations=4000, lr=1e-5)\n",
    "\n",
    "# loss con 1000 iteraciones: 0.09350010375441301924"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]], shape=(10000, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ahora cargamos los datos de test\n",
    "X_test = load_mnist_images('mnist/t10k-images-idx3-ubyte.gz')\n",
    "X_test = np.insert(X_test, 0, 1, axis=1)\n",
    "\n",
    "Y_test = load_mnist_labels('mnist/t10k-labels-idx1-ubyte.gz')\n",
    "Y_test = (Y_test == 5).astype(int)\n",
    "\n",
    "Y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples: 10000, Correct results: 9762, Success: 97.61999999999999%\n"
     ]
    }
   ],
   "source": [
    "# hacemos inferencia para testear el modelo\n",
    "total_examples = X_test.shape[0]\n",
    "correct_results = np.sum(classify(X_test, w) == Y_test)\n",
    "success_rate = correct_results / total_examples\n",
    "print(f'Total examples: {total_examples}, Correct results: {correct_results}, Success: {success_rate * 100} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ejemplos de errores, vamos a visualizar solo 12 (3x4)\n",
    "# si el modelo predice 1 y la etiqueta real es 0, se considera un falso positivo (FP)\n",
    "# si el modelo predice 0 y la etiqueta real es 1, se considera un falso negativo (FN)\n",
    "samples = 1\n",
    "fig2 = plt.figure()\n",
    "\n",
    "for i in range(Y_test.shape[0]):\n",
    "\tlabel = Y_test[i]\n",
    "\tprediction = classify(X_test[i], w)[0]\n",
    "\n",
    "\tif prediction != Y_test[i]:\n",
    "\t\tax = fig2.add_subplot(3,4,samples)\n",
    "\t\tax.axis('off')\n",
    "\t\tax.imshow(X_test[i][1:].reshape((28,28)), cmap='Greys')\n",
    "\t\t# imprime la etiqueta real, la prediccion y la \"confianza\" del modelo de que el digito sea un 5\n",
    "\t\tax.set_title(f'L: {label}, P: {int(prediction)}, C: {forward(X_test[i], w)[0]*100:.1f}%', fontsize=8)\n",
    "\t\tsamples += 1\n",
    "\n",
    "\t\tif samples > 12:\n",
    "\t\t\tbreak\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
